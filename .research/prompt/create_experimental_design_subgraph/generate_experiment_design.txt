
LLM Name: o3-2025-04-16
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length √ó layers √ó R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.' method='Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{‚Ñì,t}) = œÉ( W_tok h_{‚Ñì,t})¬∑œÉ( w_‚Ñì )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok‚àà‚Ñù^{d√ók} and W_‚Ñì‚àà‚Ñù^{L√ók} with k‚â™d remove the O(T¬∑L¬∑R) cost (now O((T+L)¬∑k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian Œª enforces  Œ£_{‚Ñì,t,i}E_dir(g_{‚Ñì,t,i}) ‚â§  E_max  via the differentiable loss  ‚Ñí = task_loss + Œª¬∑(ƒí‚àíE_max).  Œª is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).' experimental_setup='Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6√ó TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.' primary_metric='Mean Joules per sample at equal or better Rouge-L.' experimental_code='import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,Œª,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=Œª*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc' expected_result='HEST matches TABS on Rouge-L (‚âà44.3) while cutting mean energy by ‚â•40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8√ó on the longest sequences.' expected_conclusion='A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget‚Äîrather than FLOPs alone‚Äîyields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP.'

# Current Research Method
Hierarchical Energy-constrained Spectral Tuning (HEST)
1. Factorised controller  g(h_{‚Ñì,t}) = œÉ( W_tok h_{‚Ñì,t})¬∑œÉ( w_‚Ñì )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok‚àà‚Ñù^{d√ók} and W_‚Ñì‚àà‚Ñù^{L√ók} with k‚â™d remove the O(T¬∑L¬∑R) cost (now O((T+L)¬∑k)).
2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ whose expected energy cost is E_{state}[E_dir(i,state)].
3. One global Lagrangian Œª enforces  Œ£_{‚Ñì,t,i}E_dir(g_{‚Ñì,t,i}) ‚â§  E_max  via the differentiable loss  ‚Ñí = task_loss + Œª¬∑(ƒí‚àíE_max).  Œª is updated every batch with a single dual ascent step (no projection).
4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.
5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).

# MODEL LIST
{
    "Llama-4-Scout-17B-16E": {
        "model_parameters": {
            "total_parameters": "109b",
            "active_parameters": "17b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "Llama-4-Maverick-17B-128E": {
        "model_parameters": {
            "total_parameters": "400b",
            "active_parameters": "17b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "Qwen3-0.6B": {
        "model_parameters": "0.6b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-1.7B": {
        "model_parameters": "1.7b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-1.7B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-4B": {
        "model_parameters": "4b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-4B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-8B": {
        "model_parameters": "8b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-8B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-14B": {
        "model_parameters": "14b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-14B",
        "language_distribution": "",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-32B": {
        "model_parameters": "32.8b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-32B",
        "language_distribution": "",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "DeepSeek-v3": {
        "model_parameters": {
            "total_parameters": "671b",
            "active_parameters": "37b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n    title={DeepSeek-V3 Technical Report},\n    author={DeepSeek-AI},\n    year={2024},\n    eprint={2412.19437},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2412.19437},\n}"
    },
    "DeepSeek-V3.1": {
        "model_parameters": {
            "total_parameters": "671B",
            "active_parameters": "37B"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "Text"
        ],
        "output_modalities": [
            "Text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n    title={DeepSeek-V3 Technical Report},\n    author={DeepSeek-AI},\n    year={2024},\n    eprint={2412.19437},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2412.19437},\n}"
    },
    "DeepSeek-V3.2-Exp": {
        "model_parameters": {
            "total_parameters": "671B",
            "active_parameters": "37B"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "Text"
        ],
        "output_modalities": [
            "Text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv32,\n    title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\n    author={DeepSeek-AI},\n    year={2025},\n}"
    },
    "gpt-oss-20b": {
        "model_parameters": {
            "total_parameters": "21b",
            "active_parameters": "3.6b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/openai/gpt-oss-20b",
        "context_length": "",
        "language_distribution": "multilingual",
        "input_modalities": "text",
        "output_modalities": "text",
        "dependent packages": [
            "accelerate",
            "transformers",
            "kernels"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@misc{openai2025gptoss120bgptoss20bmodel,\n    title={gpt-oss-120b & gpt-oss-20b Model Card},\n    author={OpenAI},\n    year={2025},\n    eprint={2508.10925},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2508.10925},\n}"
    },
    "gemma-3-1b-it": {
        "model_parameters": "1b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-1b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"Ëá™Â∑±Á¥π‰ªã„Åó„Å¶„Åè„Å†„Åï„ÅÑ\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    },
    "gemma-3-4b-it": {
        "model_parameters": "4b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-4b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"Ëá™Â∑±Á¥π‰ªã„Åó„Å¶„Åè„Å†„Åï„ÅÑ\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    },
    "gemma-3-27b-it": {
        "model_parameters": "27b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-27b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"Ëá™Â∑±Á¥π‰ªã„Åó„Å¶„Åè„Å†„Åï„ÅÑ\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    }
}

# DATASET LIST
{
    "alpaca-cleaned": {
        "discription": "",
        "num_training_samples": "",
        "num_validation_samples": "",
        "huggingface_url": "https://huggingface.co/datasets/yahma/alpaca-cleaned",
        "language_distribution": "",
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "databricks-dolly-15k": "",
    "gsm8k": {
        "discription": "A dataset of elementary school math word problems requiring 2 to 8 steps to solve",
        "num_training_samples": 7473,
        "num_validation_samples": 1319,
        "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
        "language_distribution": "English",
        "dependent packages": [],
        "code": "",
        "citation": "@article{cobbe2021gsm8k,\ntitle={Training Verifiers to Solve Math Word Problems},\nauthor={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\njournal={arXiv preprint arXiv:2110.14168},\nyear={2021}\n}"
    },
    "MATH": {
        "discription": "The MATH dataset consists of approximately 12,500 mathematics problems ranging from middle school to early university level. Each problem includes a natural language question, a detailed step-by-step solution, and a final answer, and it is widely used to evaluate large language models (LLMs) in terms of their abilities in mathematical reasoning, logical inference, and step-by-step problem solving.",
        "num_training_samples": 12500,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/qwedsacf/competition_math",
        "language_distribution": "English",
        "dependent packages": [],
        "code": "",
        "example": "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n'level': 'Level 1',\n'type': 'Counting & Probability',\n'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\frac{5}{12}+\\frac{1}{3}+x$, from which we have $x=\\boxed{\\frac{1}{4}}$.'}",
        "data_structure": "A data instance consists of a competition math problem and its step-by-step solution written in LaTeX and natural language. The step-by-step solution contains the final answer enclosed in LaTeX's \boxed tag.\n- problem: The competition math problem.\n- solution: The step-by-step solution.\n- level: The problem's difficulty level from 'Level 1' to 'Level 5', where a subject's easiest problems for humans are assigned to 'Level 1' and a subject's hardest problems are assigned to 'Level 5'.\n- type: The subject of the problem: Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra and Precalculus.",
        "citation": "@article{hendrycksmath2021,\ntitle={Measuring Mathematical Problem Solving With the MATH Dataset},\nauthor={Dan Hendrycks\nand Collin Burns\nand Saurav Kadavath\nand Akul Arora\nand Steven Basart\nand Eric Tang\nand Dawn Song\nand Jacob Steinhardt},\njournal={arXiv preprint arXiv:2103.03874},\nyear={2021}\n}"
    }
}

# Output Format
- experiment_summaryÔºö
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metricsÔºö
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
  - The primary metric specified in the hypothesis (Mean Joules per sample at equal or better Rouge-L.) MUST be included in this list.
- models_to_useÔºö
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided ‚Äú# MODEL LIST‚Äù for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_useÔºö
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided ‚Äú# DATASET LIST‚Äù for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- proposed_methodÔºö
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methodsÔºö
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_searchÔºö
  - Output a list of objects, where each object contains "name" (hyperparameter name) and "range" (search range).
  - For example: [{"name": "learning_rate", "range": "0.001-0.01"}, {"name": "batch_size", "range": "16,32,64"}, {"name": "weight_decay", "range": "0.0001-0.001"}]
  - Search ranges can be expressed as ranges (e.g., "0.001-0.01") or discrete values (e.g., "16,32,64").
Output:
{
    "experiment_summary": "We will fine-tune a pretrained Llama-2-7B-chat model on the CNN/DailyMail summarisation task using the proposed Hierarchical Energy-constrained Spectral Tuning (HEST) adapter. During training we jointly optimise task loss and an energy-budget Lagrangian so that, for every input sequence, the total expected energy of all spectral directions stays below a user-specified envelope. The controller is factorised across tokens and layers (rank-1 outer product) and produces a 3-way Gumbel-Softmax gate per spectral direction that selects one of {off, INT8, FP16}. After convergence we harden the gates and fuse INT8 choices into integer GEMM kernels; de-activated directions are skipped. We compare HEST against the strongest prior energy-oblivious spectral adapter, Token-wise Adaptive Basis Selection (TABS), under the same backbone, data split, and optimisation schedule. We report summarisation quality, energy consumed per sample (read with NVML), tail latency, and parameter-efficiency. A small grid search over learning-rate, controller width k, and the energy budget E_max is performed on the validation set. Experiments run on a single NVIDIA A100 80 GB GPU with plenty of system RAM.",
    "evaluation_metrics": [
        "Mean Joules per sample at equal or better Rouge-L",
        "ROUGE-L",
        "Accuracy",
        "99th percentile latency",
        "Controller parameter count"
    ],
    "models_to_use": [
        "Llama-2-7B-chat (7B)"
    ],
    "datasets_to_use": [
        "CNN/DailyMail summarisation"
    ],
    "proposed_method": "Hierarchical Energy-constrained Spectral Tuning (HEST) adds fewer than 200 lines of PyTorch to a standard low-rank spectral adapter.\n1. Factorised controller: g(h_{‚Ñì,t}) = œÉ(W_tok h_{‚Ñì,t}) ¬∑ œÉ(w_‚Ñì) with W_tok‚àà‚Ñù^{d√ók} and w_‚Ñì‚àà‚Ñù^{k}. This reduces controller complexity from O(T¬∑L¬∑R) to O((T+L)¬∑k).\n2. Precision/state gating: For every retained spectral direction i we learn a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ that selects {off, INT8, FP16}. Its expected energy is E_dir(i,state) and is differentiable via the Gumbel trick (œÑ=2‚ÅÑ3).\n3. Single global energy budget: A dual variable Œª is updated each batch (Œª‚Üê[Œª+Œ±(ƒí‚àíE_max)]‚Çä) so the loss becomes ùìõ = task_loss + Œª¬∑(ƒí‚àíE_max). This enforces Œ£_{‚Ñì,t,i}E_dir ‚â§ E_max per sequence.\n4. Inference hardening: At the end of training we sample argmax over gates, eliminate ‚Äúoff‚Äù directions, fuse INT8 directions into torch.int8 GEMMs, and leave the remainder in FP16. The bound E_max now holds exactly in worst-case.\n5. Implementation: relies only on standard PyTorch, torch.autocast for INT8, and NVML for energy readings; compatible with any transformer block.",
    "comparative_methods": [
        "Token-wise Adaptive Basis Selection (TABS)"
    ],
    "hyperparameters_to_search": [
        {
            "name": "learning_rate",
            "range": "1e-5-5e-4"
        },
        {
            "name": "controller_width_k",
            "range": "16,32,64"
        },
        {
            "name": "energy_budget_E_max_ratio",
            "range": "0.4-0.8"
        },
        {
            "name": "gumbel_temperature_tau",
            "range": "0.4-1.0"
        },
        {
            "name": "dual_step_size_alpha",
            "range": "0.01-0.2"
        }
    ]
}
