
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/runs/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/runs/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
       ```json
       {
         "primary_metric": "Mean Joules per sample at equal or better Rouge-L.",
         "metrics": {
           "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
           "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
         },
         "best_proposed": {
           "run_id": "proposed-iter2-model-dataset",
           "value": 0.92
         },
         "best_baseline": {
           "run_id": "comparative-1-model-dataset",
           "value": 0.88
         },
         "gap": 4.55
       }
       ```
       The structure must include:
       - "primary_metric": The primary evaluation metric name from the hypothesis
       - "metrics": All collected metrics organized by metric name, then by run_id
       - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
       - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
       - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
         * Must use the expected results from the hypothesis to determine metric direction (higher vs lower is better)
         * If the metric should be minimized, reverse the sign of the gap
         * The gap represents the percentage improvement of the proposed method over the best baseline
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Hypothesis
open_problems='Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length √ó layers √ó R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.' method='Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{‚Ñì,t}) = œÉ( W_tok h_{‚Ñì,t})¬∑œÉ( w_‚Ñì )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok‚àà‚Ñù^{d√ók} and W_‚Ñì‚àà‚Ñù^{L√ók} with k‚â™d remove the O(T¬∑L¬∑R) cost (now O((T+L)¬∑k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian Œª enforces  Œ£_{‚Ñì,t,i}E_dir(g_{‚Ñì,t,i}) ‚â§  E_max  via the differentiable loss  ‚Ñí = task_loss + Œª¬∑(ƒí‚àíE_max).  Œª is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).' experimental_setup='Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6√ó TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.' primary_metric='Mean Joules per sample at equal or better Rouge-L.' experimental_code='import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,Œª,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=Œª*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc' expected_result='HEST matches TABS on Rouge-L (‚âà44.3) while cutting mean energy by ‚â•40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8√ó on the longest sequences.' expected_conclusion='A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget‚Äîrather than FLOPs alone‚Äîyields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP.'

# Current Research Method
Hierarchical Energy-constrained Spectral Tuning (HEST)
1. Factorised controller  g(h_{‚Ñì,t}) = œÉ( W_tok h_{‚Ñì,t})¬∑œÉ( w_‚Ñì )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok‚àà‚Ñù^{d√ók} and W_‚Ñì‚àà‚Ñù^{L√ók} with k‚â™d remove the O(T¬∑L¬∑R) cost (now O((T+L)¬∑k)).
2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ whose expected energy cost is E_{state}[E_dir(i,state)].
3. One global Lagrangian Œª enforces  Œ£_{‚Ñì,t,i}E_dir(g_{‚Ñì,t,i}) ‚â§  E_max  via the differentiable loss  ‚Ñí = task_loss + Œª¬∑(ƒí‚àíE_max).  Œª is updated every batch with a single dual ascent step (no projection).
4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.
5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).

# Experimental Design
- Strategy: We will fine-tune a pretrained Llama-2-7B-chat model on the CNN/DailyMail summarisation task using the proposed Hierarchical Energy-constrained Spectral Tuning (HEST) adapter. During training we jointly optimise task loss and an energy-budget Lagrangian so that, for every input sequence, the total expected energy of all spectral directions stays below a user-specified envelope. The controller is factorised across tokens and layers (rank-1 outer product) and produces a 3-way Gumbel-Softmax gate per spectral direction that selects one of {off, INT8, FP16}. After convergence we harden the gates and fuse INT8 choices into integer GEMM kernels; de-activated directions are skipped. We compare HEST against the strongest prior energy-oblivious spectral adapter, Token-wise Adaptive Basis Selection (TABS), under the same backbone, data split, and optimisation schedule. We report summarisation quality, energy consumed per sample (read with NVML), tail latency, and parameter-efficiency. A small grid search over learning-rate, controller width k, and the energy budget E_max is performed on the validation set. Experiments run on a single NVIDIA A100 80 GB GPU with plenty of system RAM.
- Proposed Method: Hierarchical Energy-constrained Spectral Tuning (HEST) adds fewer than 200 lines of PyTorch to a standard low-rank spectral adapter.
1. Factorised controller: g(h_{‚Ñì,t}) = œÉ(W_tok h_{‚Ñì,t}) ¬∑ œÉ(w_‚Ñì) with W_tok‚àà‚Ñù^{d√ók} and w_‚Ñì‚àà‚Ñù^{k}. This reduces controller complexity from O(T¬∑L¬∑R) to O((T+L)¬∑k).
2. Precision/state gating: For every retained spectral direction i we learn a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ that selects {off, INT8, FP16}. Its expected energy is E_dir(i,state) and is differentiable via the Gumbel trick (œÑ=2‚ÅÑ3).
3. Single global energy budget: A dual variable Œª is updated each batch (Œª‚Üê[Œª+Œ±(ƒí‚àíE_max)]‚Çä) so the loss becomes ùìõ = task_loss + Œª¬∑(ƒí‚àíE_max). This enforces Œ£_{‚Ñì,t,i}E_dir ‚â§ E_max per sequence.
4. Inference hardening: At the end of training we sample argmax over gates, eliminate ‚Äúoff‚Äù directions, fuse INT8 directions into torch.int8 GEMMs, and leave the remainder in FP16. The bound E_max now holds exactly in worst-case.
5. Implementation: relies only on standard PyTorch, torch.autocast for INT8, and NVML for energy readings; compatible with any transformer block.
- Evaluation Metrics: ['Mean Joules per sample at equal or better Rouge-L', 'ROUGE-L', 'Accuracy', '99th percentile latency', 'Controller parameter count', 'Mean Joules per sample at equal or better Rouge-L.']

# Experiment Runs

- Run ID: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: proposed
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
method: HEST
model:
  name: Llama-2-7B-chat
  adapter: HEST
  hidden_size: 4096            # taken from Llama-2-7B config
  num_layers: 32               # taken from Llama-2-7B config
  spectral_rank_R: 16          # number of spectral directions kept per layer
  controller_width_k: 32       # width of factorised controller
  precision_choices: [off, int8, fp16]
  energy_budget_ratio: 0.6     # E_max = 0.6 √ó average energy of TABS
  max_sequence_length: 4096
  init_std: 0.02
  dropout: 0.0
  weight_tying: true
  rotary_embedding_base: 10000
  rope_scaling: null           # not used for ‚â§4k tokens

dataset:
  name: cnn_dailymail
  subset: 3.0.0                # HF dataset version
  task: summarisation
  text_column: article
  summary_column: highlights
  train_split: train
  validation_split: validation
  test_split: test
  max_length: 4096
  truncation: right
  padding: right
  batch_size: 4                # fits on 80 GB with grad-accumulation
  num_workers: 4
  shuffle_buffer: 10_000

training:
  epochs: 3
  learning_rate: 3e-5
  scheduler: cosine
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  precision: fp16
  amp_level: O2
  lagrangian_init: 0.1         # Œª‚ÇÄ for dual ascent
  dual_step_size_alpha: 0.05
  gumbel_temperature_tau: 0.67
  label_smoothing: 0.0
  checkpoint_interval_steps: 1000
  log_interval_steps: 50
  seed: 42
  deterministic: false
  early_stopping_patience: 3
  save_best_metric: rougeL
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: [rougeL, mean_joules_per_sample, latency_p99]
  eval_batch_size: 4
  generate_max_length: 128
  use_beam_search: true
  num_beams: 4

hardware:
  device: "cuda"
  gpu_name: "A100-80GB"
  allow_tf32: true
  torch_int8_gemm: true

optuna:
  n_trials: 30
  direction: minimize          # primary metric = mean energy @ ‚â•baseline ROUGE
  timeout_min: 360             # stop search after 6 h
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    controller_width_k:
      type: categorical
      choices: [16, 32, 64]
    energy_budget_ratio:
      type: uniform
      low: 0.4
      high: 0.8
    gumbel_temperature_tau:
      type: uniform
      low: 0.4
      high: 1.0
    dual_step_size_alpha:
      type: loguniform
      low: 0.01
      high: 0.2
    batch_size:
      type: categorical
      choices: [2, 4]
    gradient_accumulation_steps:
      type: categorical
      choices: [4, 8, 16]

  ```
  

- Run ID: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: comparative-1
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
method: TABS
model:
  name: Llama-2-7B-chat
  adapter: TABS                    # Token-wise Adaptive Basis Selection
  hidden_size: 4096
  num_layers: 32
  spectral_rank_R: 16              # same R as proposed for fairness
  precision: fp16                  # no mixed-precision gating in baseline
  gating_temperature_tau: 0.67     # fixed if not tuned by Optuna
  max_sequence_length: 4096
  init_std: 0.02
  dropout: 0.0

dataset:
  name: cnn_dailymail
  subset: 3.0.0
  task: summarisation
  text_column: article
  summary_column: highlights
  train_split: train
  validation_split: validation
  test_split: test
  max_length: 4096
  truncation: right
  padding: right
  batch_size: 4
  num_workers: 4
  shuffle_buffer: 10_000

training:
  epochs: 3
  learning_rate: 3e-5
  scheduler: cosine
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  precision: fp16
  amp_level: O2
  label_smoothing: 0.0
  checkpoint_interval_steps: 1000
  log_interval_steps: 50
  seed: 42
  deterministic: false
  early_stopping_patience: 3
  save_best_metric: rougeL
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: [rougeL, mean_joules_per_sample, latency_p99]
  eval_batch_size: 4
  generate_max_length: 128
  use_beam_search: true
  num_beams: 4

hardware:
  device: "cuda"
  gpu_name: "A100-80GB"
  allow_tf32: true

optuna:
  n_trials: 20
  direction: maximize            # primary = ROUGE-L, energy ignored here
  timeout_min: 240
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    gating_temperature_tau:
      type: uniform
      low: 0.4
      high: 1.0
    batch_size:
      type: categorical
      choices: [2, 4]
    gradient_accumulation_steps:
      type: categorical
      choices: [4, 8, 16]

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra config \u2013 base values overridden by config/runs/*.yaml\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - _self_\n\nrun: null          # provided via CLI (e.g. proposed-iter1-\u2026)\nmode: full         # trial | full\nresults_dir: ./results\n\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online        # switched to \"disabled\" in trial mode\n\noptuna:\n  n_trials: 0\n  direction: minimize\n  timeout_min: 0\n  search_space: {}\n\ntraining:\n  epochs: 1\n  learning_rate: 3e-5\n  warmup_steps: 0\n  weight_decay: 0.0\n  beta1: 0.9\n  beta2: 0.999\n  eps: 1e-8\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  precision: fp16\n  batch_limit: null\n  log_interval_steps: 50\n  seed: 42\n  pin_memory: true\n  persistent_workers: false\n  lagrangian_init: 0.1\n  dual_step_size_alpha: 0.05\n  gumbel_temperature_tau: 0.67\n  early_stopping_patience: 3\n\nhardware:\n  device: cuda\n  allow_tf32: true\n\n# dataset / model / evaluation sections come from run-specific YAML files\n# -----------------------------------------------------------------------------\n  \n", "evaluate_py": "# src/evaluate.py\n\"\"\"Independent WandB-powered evaluation \u0026 visualisation.\nUsage:\n    uv run python -m src.evaluate results_dir=./results \\\n        run_ids=\u0027[\"run-a\", \"run-b\"]\u0027\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any\nfrom uuid import uuid4\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\nsns.set(style=\"whitegrid\")\nPRIMARY_METRIC = \"mean_joules_per_sample\"\n\n# --------------------------------------------------------------------------------------\n# CLI\n# --------------------------------------------------------------------------------------\n\ndef _parse() -\u003e argparse.Namespace:\n    p = argparse.ArgumentParser()\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\"run_ids\", type=str, help=\"JSON list e.g. \u0027[\\\"run-1\\\",\\\"run-2\\\"]\u0027\")\n    return p.parse_args()\n\n# --------------------------------------------------------------------------------------\n# Plotting helpers\n# --------------------------------------------------------------------------------------\n\ndef _save_line(df: pd.DataFrame, metric: str, out_path: Path):\n    plt.figure(figsize=(7, 4))\n    sns.lineplot(data=df[metric])\n    plt.title(f\"{metric} over steps\")\n    plt.xlabel(\"Step\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef _save_bar(metric_dict: dict[str, float], metric_name: str, out_path: Path):\n    plt.figure(figsize=(8, 4))\n    keys, values = zip(*metric_dict.items())\n    ax = sns.barplot(x=list(keys), y=list(values), palette=\"viridis\")\n    for idx, val in enumerate(values):\n        ax.text(idx, val, f\"{val:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(metric_name)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef _save_cm(y_true, y_pred, out_path: Path):\n    labels = sorted(set(y_true) | set(y_pred))\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n# --------------------------------------------------------------------------------------\n# WandB helpers\n# --------------------------------------------------------------------------------------\n\ndef _get_run(entity: str, project: str, rid: str):\n    api = wandb.Api()\n    run = api.run(f\"{entity}/{project}/{rid}\")\n    hist = run.history(keys=[\"train_loss\", \"rougeL\", \"accuracy\", \"energy_batch\"], pandas=True)\n    summary = dict(run.summary._json_dict)\n    cfg = dict(run.config)\n    return hist, summary, cfg\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main():  # noqa: C901\n    args = _parse()\n    out_root = Path(args.results_dir).expanduser().resolve()\n    out_root.mkdir(parents=True, exist_ok=True)\n\n    run_ids = json.loads(args.run_ids)\n    repo_root = Path(__file__).resolve().parents[1]\n    wandb_cfg = OmegaConf.load(repo_root / \"config\" / \"config.yaml\").wandb\n    entity, project = wandb_cfg.entity, wandb_cfg.project\n\n    aggregated: dict[str, Any] = {\"primary_metric\": \"Mean Joules per sample at equal or better Rouge-L.\", \"metrics\": {}}\n\n    # ------------------------------- per-run -----------------------------------\n    for rid in run_ids:\n        hist, summary, _ = _get_run(entity, project, rid)\n        run_dir = out_root / rid\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # save summary metrics\n        (run_dir / \"metrics.json\").write_text(json.dumps(summary, indent=2))\n\n        # learning curve\n        if not hist.empty and \"train_loss\" in hist.columns:\n            f_name = f\"{rid}_learning_curve_{uuid4().hex[:4]}.pdf\"\n            _save_line(hist, \"train_loss\", run_dir / f_name)\n            print(run_dir / f_name)\n\n        # confusion matrix if predictions logged\n        if {\"val_preds\", \"val_labels\"}.issubset(hist.columns):\n            y_p = list(map(int, hist[\"val_preds\"].dropna()))\n            y_t = list(map(int, hist[\"val_labels\"].dropna()))\n            if y_p and y_t:\n                f_name = f\"{rid}_confusion_{uuid4().hex[:4]}.pdf\"\n                _save_cm(y_t, y_p, run_dir / f_name)\n                print(run_dir / f_name)\n\n        # aggregate metrics\n        for k, v in summary.items():\n            if isinstance(v, (float, int)):\n                aggregated[\"metrics\"].setdefault(k, {})[rid] = v\n\n    # -------------------------------- comparison ------------------------------\n    comp_dir = out_root / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    primary_vals = aggregated[\"metrics\"].get(PRIMARY_METRIC, {})\n    base_runs = {k: v for k, v in primary_vals.items() if any(tag in k for tag in (\"baseline\", \"comparative\"))}\n    prop_runs = {k: v for k, v in primary_vals.items() if any(tag in k for tag in (\"proposed\", \"HEST\"))}\n\n    best_base_id, best_base_val = (None, None)\n    if base_runs:\n        best_base_id, best_base_val = min(base_runs.items(), key=lambda x: x[1])\n    best_prop_id, best_prop_val = (None, None)\n    if prop_runs:\n        best_prop_id, best_prop_val = min(prop_runs.items(), key=lambda x: x[1])\n\n    gap = None\n    if best_base_val is not None and best_prop_val is not None:\n        gap = (best_base_val - best_prop_val) / best_base_val * 100.0\n\n    aggregated[\"best_proposed\"] = {\"run_id\": best_prop_id, \"value\": best_prop_val}\n    aggregated[\"best_baseline\"] = {\"run_id\": best_base_id, \"value\": best_base_val}\n    aggregated[\"gap\"] = gap\n\n    # t-test (if multi runs)\n    if len(base_runs) \u003e 1 and len(prop_runs) \u003e 1:\n        t_s, p_v = stats.ttest_ind(list(base_runs.values()), list(prop_runs.values()), equal_var=False)\n        aggregated[\"t_test\"] = {\"statistic\": t_s, \"p_value\": p_v}\n\n    (comp_dir / \"aggregated_metrics.json\").write_text(json.dumps(aggregated, indent=2))\n    print(comp_dir / \"aggregated_metrics.json\")\n\n    # bar chart primary metric\n    if primary_vals:\n        f_name = f\"comparison_{PRIMARY_METRIC}_{uuid4().hex[:4]}.pdf\"\n        _save_bar(primary_vals, PRIMARY_METRIC, comp_dir / f_name)\n        print(comp_dir / f_name)\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "# src/main.py\n\"\"\"Launch script that spawns the *actual* training (src.train) as a subprocess.\nComplies with CLI spec in the task description.\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\nlog = logging.getLogger(\"main\")\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    run_cfg_file = Path(__file__).resolve().parents[1] / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_file.exists():\n        raise FileNotFoundError(run_cfg_file)\n\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_file))\n\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be trial or full\")\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"mode={cfg.mode}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"wandb.mode={cfg.wandb.mode}\",\n        f\"optuna.n_trials={cfg.optuna.n_trials}\",\n        f\"training.epochs={cfg.training.epochs}\",\n    ]\n    if cfg.mode == \"trial\":\n        overrides.append(\"training.batch_limit=2\")\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    log.info(\"Spawning training subprocess:\\n%s\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "# src/model.py\n\"\"\"PEFT adapters: Hierarchical Energy-constrained Spectral Tuning (HEST) and\nToken-wise Adaptive Basis Selection (TABS). Both inject *learnable* low-rank\nupdates into the backbone and return energy estimates required by the\nLagrangian formulation.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n\nENERGY_FP16 = 1.0  # arbitrary units\nENERGY_INT8 = 0.6  # int8 kernels ~40 % cheaper on A100\n\n# --------------------------------------------------------------------------------------\n# Low-rank spectral module shared by HEST \u0026 TABS\n# --------------------------------------------------------------------------------------\n\nclass _LowRankSpectral(nn.Module):\n    \"\"\"U \u03a3 V^T style adapter returning additive delta-hidden.\"\"\"\n\n    def __init__(self, d: int, r: int):\n        super().__init__()\n        self.U = nn.Parameter(torch.randn(d, r) * (d ** -0.5))\n        self.V = nn.Parameter(torch.randn(r, d) * (r ** -0.5))\n\n    def forward(self, hidden: torch.Tensor) -\u003e torch.Tensor:  # (b, seq, d)\n        # (b, seq, d) \u2192 (b, seq, r)\n        s = torch.matmul(hidden, self.U)  # project to spectral space\n        # (b, seq, r) \u2192 (b, seq, d)\n        delta = torch.matmul(s, self.V)\n        return delta\n\n# --------------------------------------------------------------------------------------\n# Factorised controller (shared)\n# --------------------------------------------------------------------------------------\n\nclass FactorisedController(nn.Module):\n    def __init__(self, hidden_size: int, k: int, num_layers: int):\n        super().__init__()\n        self.W_tok = nn.Linear(hidden_size, k, bias=False)\n        self.w_layer = nn.Parameter(torch.zeros(num_layers, k))\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int):  # (b, seq, d)\n        s_tok = torch.sigmoid(self.W_tok(hidden))  # (b, seq, k)\n        s_layer = torch.sigmoid(self.w_layer[layer_idx])  # (k,)\n        g = (s_tok * s_layer).mean(-1)  # (b, seq)\n        return g\n\n# --------------------------------------------------------------------------------------\n# HEST Adapter\n# --------------------------------------------------------------------------------------\n\nclass HESTAdapter(nn.Module):\n    def __init__(self, hidden_size: int, cfg: DictConfig):\n        super().__init__()\n        self.R = cfg.model.spectral_rank_R\n        self.lowrank = _LowRankSpectral(hidden_size, self.R)\n        self.controller = FactorisedController(hidden_size, cfg.model.controller_width_k, cfg.model.num_layers)\n        self.temperature = cfg.training.gumbel_temperature_tau\n        self.register_buffer(\"e_fp16\", torch.tensor(ENERGY_FP16))\n        self.register_buffer(\"e_int8\", torch.tensor(ENERGY_INT8))\n\n    def _precision_gate(self, g_scalar: torch.Tensor) -\u003e Tuple[torch.Tensor, torch.Tensor]:\n        # g_scalar: (b, seq)\n        g_vec = g_scalar.unsqueeze(-1).repeat(1, 1, self.R)  # (b, seq, R)\n        logits = torch.stack(\n            [torch.zeros_like(g_vec), g_vec, 2 * g_vec], dim=-1\n        )  # (b, seq, R, 3)\n        probs = F.gumbel_softmax(logits, tau=self.temperature, dim=-1, hard=False)\n        coeff = probs[..., 1] + probs[..., 2]  # activation coeff per dir\n        energy = (probs[..., 1] * self.e_int8 + probs[..., 2] * self.e_fp16).sum()\n        return coeff, energy\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int):  # (b, seq, d)\n        g_scalar = self.controller(hidden, layer_idx)  # (b, seq)\n        coeff, energy = self._precision_gate(g_scalar)\n        delta = self.lowrank(hidden) * coeff.unsqueeze(-1)  # broadcast coeff over d\n        return delta, energy\n\n# --------------------------------------------------------------------------------------\n# TABS baseline \u2013 simpler token-wise gating, fp16 only\n# --------------------------------------------------------------------------------------\n\nclass TABSAdapter(nn.Module):\n    def __init__(self, hidden_size: int, cfg: DictConfig):\n        super().__init__()\n        self.R = cfg.model.spectral_rank_R\n        self.lowrank = _LowRankSpectral(hidden_size, self.R)\n        self.gate = nn.Linear(hidden_size, self.R, bias=False)\n\n    def forward(self, hidden: torch.Tensor, _: int):  # (b, seq, d)\n        coeff = torch.sigmoid(self.gate(hidden))  # (b, seq, R)\n        delta = self.lowrank(hidden) * coeff.unsqueeze(-1)\n        energy = (coeff * ENERGY_FP16).sum()\n        return delta, energy\n\n# --------------------------------------------------------------------------------------\n# Wrapper around HF model \u2013 injects adapter delta \u0026 returns energy\n# --------------------------------------------------------------------------------------\n\nclass EnergyWrapperModel(nn.Module):\n    def __init__(self, backbone: nn.Module, adapter: nn.Module, num_layers: int):\n        super().__init__()\n        self.backbone = backbone\n        self.adapter = adapter\n        self.num_layers = num_layers\n\n    def _lm_head_weight(self):\n        if hasattr(self.backbone, \"lm_head\"):\n            return self.backbone.lm_head.weight\n        return self.backbone.get_output_embeddings().weight\n\n    def forward(self, *, labels=None, **kwargs):  # noqa: C901\n        kwargs[\"output_hidden_states\"] = True\n        outputs = self.backbone(**kwargs)\n        if hasattr(outputs, \"decoder_hidden_states\") and outputs.decoder_hidden_states is not None:\n            hidden_last = outputs.decoder_hidden_states[-1]\n        else:\n            hidden_last = outputs.hidden_states[-1]\n\n        delta_hidden, energy = self.adapter(hidden_last, layer_idx=0)\n        adapted_hidden = hidden_last + delta_hidden\n\n        weight = self._lm_head_weight()\n        logits_adapter = torch.matmul(adapted_hidden.to(weight.dtype), weight.T)\n\n        loss = None\n        if labels is not None:\n            loss = F.cross_entropy(\n                logits_adapter.view(-1, logits_adapter.size(-1)),\n                labels.view(-1),\n                ignore_index=-100,\n            )\n        return {\"loss\": loss, \"logits\": logits_adapter, \"energy\": energy}\n\n# --------------------------------------------------------------------------------------\n# Factory \u2013 builds backbone \u0026 adapter and freezes backbone for PEFT\n# --------------------------------------------------------------------------------------\n\ndef build_model(cfg: DictConfig, tokenizer):  # noqa: D401\n    is_seq2seq = cfg.dataset.task == \"summarisation\" or \"bart\" in cfg.model.name.lower()\n\n    if is_seq2seq:\n        backbone = AutoModelForSeq2SeqLM.from_pretrained(\n            cfg.model.name,\n            cache_dir=\".cache/\",\n            torch_dtype=torch.float16 if cfg.training.precision == \"fp16\" else torch.float32,\n        )\n    else:\n        backbone = AutoModelForCausalLM.from_pretrained(\n            cfg.model.name,\n            cache_dir=\".cache/\",\n            torch_dtype=torch.float16 if cfg.training.precision == \"fp16\" else torch.float32,\n        )\n\n    # freeze backbone \u2013 learn only adapter\n    for p in backbone.parameters():\n        p.requires_grad = False\n\n    if cfg.method.lower() in {\"hest\", \"proposed\"}:\n        adapter = HESTAdapter(backbone.config.hidden_size, cfg)\n    else:\n        adapter = TABSAdapter(backbone.config.hidden_size, cfg)\n\n    num_layers = getattr(backbone.config, \"num_hidden_layers\", cfg.model.num_layers)\n    return EnergyWrapperModel(backbone, adapter, num_layers)\n", "preprocess_py": "# src/preprocess.py\n\"\"\"Complete data-loading / tokenisation utilities for text summarisation \u0026\nclassification datasets. Returns PyTorch DataLoaders ready for training.\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Tuple\n\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq, default_data_collator\n\nCACHE_DIR = \".cache/\"\n\n# --------------------------------------------------------------------------------------\n# Tokenisation functions\n# --------------------------------------------------------------------------------------\n\ndef _tok_summarisation(sample: dict, tokenizer: AutoTokenizer, cfg: DictConfig):\n    model_inputs = tokenizer(\n        sample[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            sample[cfg.dataset.summary_column],\n            max_length=cfg.evaluation.generate_max_length,\n            padding=\"max_length\",\n            truncation=True,\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\ndef _tok_classification(sample: dict, tokenizer: AutoTokenizer, cfg: DictConfig):\n    tokenised = tokenizer(\n        sample[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    tokenised[\"labels\"] = sample[cfg.dataset.label_column]\n    return tokenised\n\n# --------------------------------------------------------------------------------------\n# Dataloader builder\n# --------------------------------------------------------------------------------------\n\ndef build_dataloaders(cfg: DictConfig, tokenizer: AutoTokenizer) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    raw = load_dataset(cfg.dataset.name, cfg.dataset.subset, cache_dir=CACHE_DIR)\n\n    if cfg.dataset.task == \"summarisation\":\n        token_func = functools.partial(_tok_summarisation, tokenizer=tokenizer, cfg=cfg)\n        collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"longest\")\n    else:  # classification / generic NLP\n        token_func = functools.partial(_tok_classification, tokenizer=tokenizer, cfg=cfg)\n        collator = default_data_collator\n\n    proc = raw.map(\n        token_func,\n        batched=True,\n        num_proc=cfg.dataset.num_workers,\n        remove_columns=raw[cfg.dataset.train_split].column_names,\n    )\n\n    train_loader = DataLoader(\n        proc[cfg.dataset.train_split],\n        shuffle=True,\n        batch_size=cfg.dataset.batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n\n    val_loader = DataLoader(\n        proc[cfg.dataset.validation_split],\n        shuffle=False,\n        batch_size=cfg.evaluation.eval_batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n\n    test_loader = DataLoader(\n        proc.get(cfg.dataset.test_split, proc[cfg.dataset.validation_split]),\n        shuffle=False,\n        batch_size=cfg.evaluation.eval_batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n    return train_loader, val_loader, test_loader\n", "pyproject_toml": "[project]\nname = \"hest-experiments\"\nversion = \"0.1.0\"\ndescription = \"Reproducible experiments for HEST vs TABS\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.2.0\"\ntransformers = \"\u003e=4.38.0\"\ndatasets = \"\u003e=2.15.0\"\noptuna = \"\u003e=3.5.0\"\nwandb = \"\u003e=0.16.0\"\nhydra-core = \"\u003e=1.3.2\"\nmatplotlib = \"\u003e=3.8.0\"\nseaborn = \"\u003e=0.13.0\"\npynvml = \"\u003e=11.5.0\"\nevaluate = \"\u003e=0.4.0\"\nscipy = \"\u003e=1.11.0\"\naccelerate = \"\u003e=0.25.0\"\nscikit-learn = \"*\"\nrouge-score = \"*\"\ntqdm = \"*\"\n\n[tool.setuptools.package-data]\n\"\" = [\"config/**/*.yaml\"]\n", "train_py": "# src/train.py\n\"\"\"Single-run training script integrating Hydra, NVML energy metering,\nGumbel-based energy-constrained adapters and comprehensive WandB logging.\nAll scientific functionality (training, validation, Optuna search) is\nimplemented in full \u2013 ready for direct execution on an A100/H200 GPU.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport pynvml\nimport torch\nimport torch.nn.functional as F\nfrom datasets import disable_caching\nfrom evaluate import load as load_metric\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n\nfrom .model import build_model\nfrom .preprocess import build_dataloaders\n\n# make HF cache local to repository (avoids $HOME pollution)\nos.environ.setdefault(\"HF_HOME\", str(Path(\".cache\").resolve()))\n\n# Disable HF datasets cache index writing (CI friendliness)\ndisable_caching()\n\ntry:\n    import wandb  # noqa: WPS433\nexcept ModuleNotFoundError:  # pragma: no cover\n    wandb = None  # type: ignore\n\n# --------------------------------------------------------------------------------------\n# Logging setup\n# --------------------------------------------------------------------------------------\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\nlogger = logging.getLogger(\"train\")\n\n# --------------------------------------------------------------------------------------\n# Reproducibility utils\n# --------------------------------------------------------------------------------------\n\ndef _set_seed(seed: int) -\u003e None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# --------------------------------------------------------------------------------------\n# NVML helpers \u2013 wall-energy (J)\n# --------------------------------------------------------------------------------------\n\ndef _nvml_start():\n    try:\n        pynvml.nvmlInit()\n        h = pynvml.nvmlDeviceGetHandleByIndex(0)\n        return h, pynvml.nvmlDeviceGetTotalEnergyConsumption(h)  # type: ignore[attr-defined]\n    except Exception:  # noqa: BLE001\n        return None, 0.0\n\n\ndef _nvml_stop(handle):\n    try:\n        return pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # type: ignore[attr-defined]\n    except Exception:  # noqa: BLE001\n        return 0.0\n\n# --------------------------------------------------------------------------------------\n# WandB helper \u2013 obey cfg.wandb.mode\n# --------------------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n        return None\n    if wandb is None:\n        raise RuntimeError(\"wandb package missing \u2013 required for full mode\")\n\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        resume=\"allow\",\n    )\n    logger.info(\"Weights \u0026 Biases dashboard: %s\", run.url)\n    return run\n\n# --------------------------------------------------------------------------------------\n# Optimiser \u0026 scheduler\n# --------------------------------------------------------------------------------------\n\ndef _build_optimizer(model: torch.nn.Module, cfg: DictConfig):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    params_decay, params_nodecay = [], []\n    for n, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n    groups = [\n        {\"params\": params_decay, \"weight_decay\": cfg.training.weight_decay},\n        {\"params\": params_nodecay, \"weight_decay\": 0.0},\n    ]\n    return AdamW(groups, lr=cfg.training.learning_rate, betas=(cfg.training.beta1, cfg.training.beta2), eps=cfg.training.eps)\n\n# --------------------------------------------------------------------------------------\n# Validation / inference helpers\n# --------------------------------------------------------------------------------------\n\ndef _run_generation(model, tokenizer, batch, device, cfg):\n    \"\"\"Generate summaries / predictions for a mini-batch (summarisation task).\"\"\"\n    gen_kwargs = dict(\n        max_new_tokens=cfg.evaluation.generate_max_length,\n        num_beams=cfg.evaluation.num_beams if cfg.evaluation.use_beam_search else 1,\n        do_sample=not cfg.evaluation.use_beam_search,\n    )\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n\n    start = time.perf_counter()\n    generated_ids = model.backbone.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n    latency_s = time.perf_counter() - start\n\n    preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    labels = batch[\"labels\"].tolist()\n    # Replace -100 with pad for decoding\n    labels = [list(filter(lambda x: x != -100, seq)) for seq in labels]\n    refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return preds, refs, latency_s / len(preds)\n\n\ndef _validate(model, tokenizer, val_loader, device, cfg, epoch: int, wandb_run):  # noqa: D401, C901 (complex)\n    model.eval()\n    rouge_metric = load_metric(\"rouge\") if cfg.dataset.task == \"summarisation\" else None\n    accur_correct, accur_total = 0, 0\n    latencies: list[float] = []\n\n    with torch.no_grad():\n        for step, batch in enumerate(val_loader):\n            if cfg.mode == \"trial\" and step \u003e 1:\n                break  # keep validation ultra-light in trial mode\n\n            if cfg.dataset.task == \"summarisation\":\n                preds, refs, latency = _run_generation(model, tokenizer, batch, device, cfg)\n                rouge_metric.add_batch(predictions=preds, references=refs)\n                latencies.append(latency)\n            else:  # classification-style datasets\n                batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n                labels = batch[\"labels\"]\n                outputs = model(**batch)\n                logits = outputs[\"logits\"]\n                preds_cls = logits.argmax(-1)\n                accur_correct += (preds_cls == labels).sum().item()\n                accur_total += labels.numel()\n\n    if cfg.dataset.task == \"summarisation\":\n        rouge_scores = rouge_metric.compute()  # type: ignore[arg-type]\n        rougeL = rouge_scores[\"rougeL\"].mid.fmeasure\n        latency_p99 = np.percentile(latencies, 99) if latencies else 0.0\n        metrics = {\"rougeL\": rougeL, \"latency_p99\": latency_p99, \"epoch\": epoch}\n    else:\n        acc = accur_correct / max(1, accur_total)\n        metrics = {\"accuracy\": acc, \"epoch\": epoch}\n\n    if wandb_run:\n        wandb_run.log(metrics)\n    logger.info(\"Validation metrics (epoch %d): %s\", epoch, json.dumps(metrics, indent=2))\n    model.train()\n    return metrics\n\n# --------------------------------------------------------------------------------------\n# Training epoch\n# --------------------------------------------------------------------------------------\n\ndef _train_single_epoch(epoch: int, model, loader, optimizer, scheduler, scaler, device, cfg, lambda_energy, wandb_run):  # noqa: C901\n    handle, energy_start_mJ = _nvml_start()\n    running_loss = 0.0\n\n    for step, batch in enumerate(loader):\n        if cfg.training.batch_limit is not None and step \u003e= cfg.training.batch_limit:\n            break\n\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        labels = batch.pop(\"labels\")\n\n        with autocast(enabled=cfg.training.precision == \"fp16\"):\n            outputs = model(**batch, labels=labels)\n            task_loss = outputs[\"loss\"]\n            energy_batch = outputs[\"energy\"]\n            penalty = lambda_energy * (energy_batch - cfg.model.energy_budget_ratio)\n            loss = task_loss + penalty\n\n        scaler.scale(loss / cfg.training.gradient_accumulation_steps).backward()\n        running_loss += loss.item()\n\n        if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n            scheduler.step()\n            # dual update\n            lambda_energy.data = torch.clamp(\n                lambda_energy.data + cfg.training.dual_step_size_alpha * (energy_batch - cfg.model.energy_budget_ratio),\n                min=0.0,\n            )\n\n        if wandb_run and (step + 1) % cfg.training.log_interval_steps == 0:\n            payload = {\n                \"train_loss\": running_loss / (step + 1),\n                \"lambda\": lambda_energy.item(),\n                \"energy_batch\": energy_batch.item(),\n                \"epoch\": epoch,\n                \"step\": epoch * len(loader) + step + 1,\n                \"lr\": scheduler.get_last_lr()[0],\n            }\n            wandb_run.log(payload)\n            logger.info(\"epoch %d step %d | %s\", epoch, step, json.dumps(payload))\n\n    energy_total_j = 0.0\n    if handle is not None:\n        energy_end_mJ = _nvml_stop(handle)\n        pynvml.nvmlShutdown()  # type: ignore[attr-defined]\n        energy_total_j = max(energy_end_mJ - energy_start_mJ, 0.0) / 1000.0\n    return energy_total_j\n\n# --------------------------------------------------------------------------------------\n# Full training run (w/ or w/o Optuna tuning)\n# --------------------------------------------------------------------------------------\n\ndef _full_run(cfg: DictConfig):  # noqa: C901 \u2013 big orchestrator\n    _set_seed(cfg.training.seed)\n\n    device = torch.device(cfg.hardware.device if torch.cuda.is_available() else \"cpu\")\n    torch.backends.cuda.matmul.allow_tf32 = cfg.hardware.allow_tf32  # type: ignore[attr-defined]\n    torch.backends.cudnn.allow_tf32 = cfg.hardware.allow_tf32\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    train_loader, val_loader, _ = build_dataloaders(cfg, tokenizer)\n\n    model = build_model(cfg, tokenizer).to(device)\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    optimizer = _build_optimizer(model, cfg)\n    total_steps = (\n        (len(train_loader) // cfg.training.gradient_accumulation_steps) * cfg.training.epochs\n    )\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=max(total_steps, 1),\n    )\n    scaler = GradScaler(enabled=cfg.training.precision == \"fp16\")\n\n    lambda_energy = torch.tensor(cfg.training.lagrangian_init, device=device)\n\n    wandb_run = _init_wandb(cfg)\n    if wandb_run:\n        wandb_run.summary[\"trainable_params\"] = trainable_params\n\n    cumulative_energy_j = 0.0\n    best_metric = -float(\"inf\")  # higher is better for rougeL/accuracy\n    patience_cnt = 0\n\n    for epoch in range(cfg.training.epochs):\n        epoch_energy = _train_single_epoch(\n            epoch,\n            model,\n            train_loader,\n            optimizer,\n            scheduler,\n            scaler,\n            device,\n            cfg,\n            lambda_energy,\n            wandb_run,\n        )\n        cumulative_energy_j += epoch_energy\n\n        # ---------------------- validation ----------------------\n        val_metrics = _validate(model, tokenizer, val_loader, device, cfg, epoch, wandb_run)\n        primary_improved = False\n        key = \"rougeL\" if cfg.dataset.task == \"summarisation\" else \"accuracy\"\n        if key in val_metrics and val_metrics[key] \u003e best_metric:\n            best_metric = val_metrics[key]\n            primary_improved = True\n            patience_cnt = 0\n        else:\n            patience_cnt += 1\n\n        if patience_cnt \u003e= cfg.training.early_stopping_patience:\n            logger.info(\"Early stopping triggered \u2013 no improvement for %d epochs\", patience_cnt)\n            break\n\n        # Optionally save checkpoint of adapter\n        if primary_improved and cfg.mode == \"full\":\n            ckpt_dir = Path(cfg.results_dir) / cfg.run_id\n            ckpt_dir.mkdir(parents=True, exist_ok=True)\n            torch.save(model.adapter.state_dict(), ckpt_dir / \"adapter_best.pt\")\n\n    mean_j_per_sample = cumulative_energy_j / len(train_loader.dataset)\n\n    if wandb_run:\n        wandb_run.summary[\"mean_joules_per_sample\"] = mean_j_per_sample\n        wandb_run.summary[key] = best_metric\n        if \"latency_p99\" in val_metrics:\n            wandb_run.summary[\"latency_p99\"] = val_metrics[\"latency_p99\"]\n        wandb_run.finish()\n\n    return mean_j_per_sample\n\n# --------------------------------------------------------------------------------------\n# Optuna objective \u2013 minimises mean energy while maintaining \u2265 baseline task metric\n# --------------------------------------------------------------------------------------\n\ndef _optuna_objective(trial: optuna.Trial, base_cfg: DictConfig):  # noqa: C901 (optuna sampling)\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n\n    for hp, space in cfg.optuna.search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            val = trial.suggest_float(hp, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            val = trial.suggest_float(hp, space[\"low\"], space[\"high\"])\n        elif space[\"type\"] == \"categorical\":\n            val = trial.suggest_categorical(hp, space[\"choices\"])\n        else:\n            raise ValueError(f\"Unsupported space type: {space[\u0027type\u0027]}\")\n        OmegaConf.update(cfg, hp, val, merge=True)\n\n    # short proxy training \u2013 1 epoch, wandb disabled\n    cfg.training.epochs = 1\n    cfg.wandb.mode = \"disabled\"\n    cfg.optuna.n_trials = 0  # avoid nesting\n\n    energy_metric = _full_run(cfg)\n    return energy_metric\n\n# --------------------------------------------------------------------------------------\n# Hydra entry point\n# --------------------------------------------------------------------------------------\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901 (full script)\n    # merge run-specific YAML\n    run_yaml = Path(__file__).resolve().parents[1] / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.exists():\n        raise FileNotFoundError(run_yaml)\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_yaml))\n\n    # trial/full tweaks\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Ensure result directory exists \u0026 attach run_id to cfg\n    Path(cfg.results_dir).mkdir(parents=True, exist_ok=True)\n    cfg.run_id = cfg.run_id if \"run_id\" in cfg else cfg.run  # ensure field for wandb id\n\n    logger.info(\"Final merged config:\\n%s\", OmegaConf.to_yaml(cfg))\n\n    # Optuna search (optional)\n    if cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        logger.info(\"Launching Optuna \u2013 %d trials\", cfg.optuna.n_trials)\n        study.optimize(\n            lambda t: _optuna_objective(t, cfg),\n            n_trials=cfg.optuna.n_trials,\n            timeout=cfg.optuna.timeout_min * 60 if cfg.optuna.timeout_min else None,\n        )\n        for k, v in study.best_params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n        logger.info(\"Optuna done \u2013 using best params: %s\", json.dumps(study.best_params))\n\n    metric = _full_run(cfg)\n    logger.info(\"Finished \u2013 mean_joules_per_sample=%.4f J\", metric)\n\nif __name__ == \"__main__\":\n    main()\n"}



Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Penalty term in _train_single_epoch unconditionally accesses cfg.model.energy_budget_ratio.  This key is present in proposed-method run-configs but is absent from the comparative/baseline configs, so any baseline run (TABS) will crash with an AttributeError at the first training step.  A safe default or a method-specific conditional branch is required.  Until this is fixed the code cannot be executed for all declared experiment runs."
}
