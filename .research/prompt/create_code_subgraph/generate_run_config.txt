
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Hypothesis
open_problems='Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length √ó layers √ó R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.' method='Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{‚Ñì,t}) = œÉ( W_tok h_{‚Ñì,t})¬∑œÉ( w_‚Ñì )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok‚àà‚Ñù^{d√ók} and W_‚Ñì‚àà‚Ñù^{L√ók} with k‚â™d remove the O(T¬∑L¬∑R) cost (now O((T+L)¬∑k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian Œª enforces  Œ£_{‚Ñì,t,i}E_dir(g_{‚Ñì,t,i}) ‚â§  E_max  via the differentiable loss  ‚Ñí = task_loss + Œª¬∑(ƒí‚àíE_max).  Œª is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).' experimental_setup='Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6√ó TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.' primary_metric='Mean Joules per sample at equal or better Rouge-L.' experimental_code='import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,Œª,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=Œª*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc' expected_result='HEST matches TABS on Rouge-L (‚âà44.3) while cutting mean energy by ‚â•40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8√ó on the longest sequences.' expected_conclusion='A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget‚Äîrather than FLOPs alone‚Äîyields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP.'

## Research Method
Hierarchical Energy-constrained Spectral Tuning (HEST)
1. Factorised controller  g(h_{‚Ñì,t}) = œÉ( W_tok h_{‚Ñì,t})¬∑œÉ( w_‚Ñì )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok‚àà‚Ñù^{d√ók} and W_‚Ñì‚àà‚Ñù^{L√ók} with k‚â™d remove the O(T¬∑L¬∑R) cost (now O((T+L)¬∑k)).
2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ whose expected energy cost is E_{state}[E_dir(i,state)].
3. One global Lagrangian Œª enforces  Œ£_{‚Ñì,t,i}E_dir(g_{‚Ñì,t,i}) ‚â§  E_max  via the differentiable loss  ‚Ñí = task_loss + Œª¬∑(ƒí‚àíE_max).  Œª is updated every batch with a single dual ascent step (no projection).
4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.
5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).

## Experimental Design
experiment_summary='We will fine-tune a pretrained Llama-2-7B-chat model on the CNN/DailyMail summarisation task using the proposed Hierarchical Energy-constrained Spectral Tuning (HEST) adapter. During training we jointly optimise task loss and an energy-budget Lagrangian so that, for every input sequence, the total expected energy of all spectral directions stays below a user-specified envelope. The controller is factorised across tokens and layers (rank-1 outer product) and produces a 3-way Gumbel-Softmax gate per spectral direction that selects one of {off, INT8, FP16}. After convergence we harden the gates and fuse INT8 choices into integer GEMM kernels; de-activated directions are skipped. We compare HEST against the strongest prior energy-oblivious spectral adapter, Token-wise Adaptive Basis Selection (TABS), under the same backbone, data split, and optimisation schedule. We report summarisation quality, energy consumed per sample (read with NVML), tail latency, and parameter-efficiency. A small grid search over learning-rate, controller width k, and the energy budget E_max is performed on the validation set. Experiments run on a single NVIDIA A100 80 GB GPU with plenty of system RAM.' evaluation_metrics=['Mean Joules per sample at equal or better Rouge-L', 'ROUGE-L', 'Accuracy', '99th percentile latency', 'Controller parameter count', 'Mean Joules per sample at equal or better Rouge-L.'] proposed_method='Hierarchical Energy-constrained Spectral Tuning (HEST) adds fewer than 200 lines of PyTorch to a standard low-rank spectral adapter.\n1. Factorised controller: g(h_{‚Ñì,t}) = œÉ(W_tok h_{‚Ñì,t}) ¬∑ œÉ(w_‚Ñì) with W_tok‚àà‚Ñù^{d√ók} and w_‚Ñì‚àà‚Ñù^{k}. This reduces controller complexity from O(T¬∑L¬∑R) to O((T+L)¬∑k).\n2. Precision/state gating: For every retained spectral direction i we learn a 3-way Gumbel-Softmax gate g_{‚Ñì,t,i}‚ààŒî¬≤ that selects {off, INT8, FP16}. Its expected energy is E_dir(i,state) and is differentiable via the Gumbel trick (œÑ=2‚ÅÑ3).\n3. Single global energy budget: A dual variable Œª is updated each batch (Œª‚Üê[Œª+Œ±(ƒí‚àíE_max)]‚Çä) so the loss becomes ùìõ = task_loss + Œª¬∑(ƒí‚àíE_max). This enforces Œ£_{‚Ñì,t,i}E_dir ‚â§ E_max per sequence.\n4. Inference hardening: At the end of training we sample argmax over gates, eliminate ‚Äúoff‚Äù directions, fuse INT8 directions into torch.int8 GEMMs, and leave the remainder in FP16. The bound E_max now holds exactly in worst-case.\n5. Implementation: relies only on standard PyTorch, torch.autocast for INT8, and NVML for energy readings; compatible with any transformer block.' comparative_methods=['Token-wise Adaptive Basis Selection (TABS)'] models_to_use=['Llama-2-7B-chat (7B)'] datasets_to_use=['CNN/DailyMail summarisation'] hyperparameters_to_search={'learning_rate': '1e-5-5e-4', 'controller_width_k': '16,32,64', 'energy_budget_E_max_ratio': '0.4-0.8', 'gumbel_temperature_tau': '0.4-1.0', 'dual_step_size_alpha': '0.01-0.2'} external_resources=ExternalResources(hugging_face=HuggingFace(models=[HuggingFaceResource(id='Qwen/Qwen2.5-7B-Instruct', author='Qwen', sha='a09a35458c702b33eeacc393d103063234e8bc28', created_at=datetime.datetime(2024, 9, 16, 11, 55, 40, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2025, 1, 12, 2, 10, 10, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=8625380, likes=858, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model-00001-of-00004.safetensors', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model-00002-of-00004.safetensors', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model-00003-of-00004.safetensors', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model-00004-of-00004.safetensors', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license='apache-2.0', language=['en'], library_name='transformers', pipeline_tag='text-generation', tags=['chat'], datasets=[], model_type=None, base_model='Qwen/Qwen2.5-7B', task_categories=[], size_categories=[], metrics=[], widget=[]), tags=['transformers', 'safetensors', 'qwen2', 'text-generation', 'chat', 'conversational', 'en', 'arxiv:2309.00071', 'arxiv:2407.10671', 'base_model:Qwen/Qwen2.5-7B', 'base_model:finetune:Qwen/Qwen2.5-7B', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us'], pipeline_tag='text-generation', library_name='transformers', readme='---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-7B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-7B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: \'qwen2\'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-7B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code='from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-7B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]')], datasets=[])) experiment_code=None

## Experiment Runs

- Run ID: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: proposed
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation

- Run ID: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: comparative-1
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation",
      "run_config_yaml": "run_id: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation\nmethod: HEST\nmodel:\n  name: Llama-2-7B-chat\n  adapter: HEST\n  hidden_size: 4096            # taken from Llama-2-7B config\n  num_layers: 32               # taken from Llama-2-7B config\n  spectral_rank_R: 16          # number of spectral directions kept per layer\n  controller_width_k: 32       # width of factorised controller\n  precision_choices: [off, int8, fp16]\n  energy_budget_ratio: 0.6     # E_max = 0.6 √ó average energy of TABS\n  max_sequence_length: 4096\n  init_std: 0.02\n  dropout: 0.0\n  weight_tying: true\n  rotary_embedding_base: 10000\n  rope_scaling: null           # not used for ‚â§4k tokens\n\ndataset:\n  name: cnn_dailymail\n  subset: 3.0.0                # HF dataset version\n  task: summarisation\n  text_column: article\n  summary_column: highlights\n  train_split: train\n  validation_split: validation\n  test_split: test\n  max_length: 4096\n  truncation: right\n  padding: right\n  batch_size: 4                # fits on 80 GB with grad-accumulation\n  num_workers: 4\n  shuffle_buffer: 10_000\n\ntraining:\n  epochs: 3\n  learning_rate: 3e-5\n  scheduler: cosine\n  warmup_steps: 500\n  optimizer: adamw\n  weight_decay: 0.01\n  beta1: 0.9\n  beta2: 0.95\n  eps: 1e-8\n  gradient_accumulation_steps: 8\n  max_grad_norm: 1.0\n  precision: fp16\n  amp_level: O2\n  lagrangian_init: 0.1         # Œª‚ÇÄ for dual ascent\n  dual_step_size_alpha: 0.05\n  gumbel_temperature_tau: 0.67\n  label_smoothing: 0.0\n  checkpoint_interval_steps: 1000\n  log_interval_steps: 50\n  seed: 42\n  deterministic: false\n  early_stopping_patience: 3\n  save_best_metric: rougeL\n  pin_memory: true\n  persistent_workers: true\n\nevaluation:\n  metrics: [rougeL, mean_joules_per_sample, latency_p99]\n  eval_batch_size: 4\n  generate_max_length: 128\n  use_beam_search: true\n  num_beams: 4\n\nhardware:\n  device: \"cuda\"\n  gpu_name: \"A100-80GB\"\n  allow_tf32: true\n  torch_int8_gemm: true\n\noptuna:\n  n_trials: 30\n  direction: minimize          # primary metric = mean energy @ ‚â•baseline ROUGE\n  timeout_min: 360             # stop search after 6 h\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 5.0e-4\n    controller_width_k:\n      type: categorical\n      choices: [16, 32, 64]\n    energy_budget_ratio:\n      type: uniform\n      low: 0.4\n      high: 0.8\n    gumbel_temperature_tau:\n      type: uniform\n      low: 0.4\n      high: 1.0\n    dual_step_size_alpha:\n      type: loguniform\n      low: 0.01\n      high: 0.2\n    batch_size:\n      type: categorical\n      choices: [2, 4]\n    gradient_accumulation_steps:\n      type: categorical\n      choices: [4, 8, 16]\n"
    },
    {
      "run_id": "comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation",
      "run_config_yaml": "run_id: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation\nmethod: TABS\nmodel:\n  name: Llama-2-7B-chat\n  adapter: TABS                    # Token-wise Adaptive Basis Selection\n  hidden_size: 4096\n  num_layers: 32\n  spectral_rank_R: 16              # same R as proposed for fairness\n  precision: fp16                  # no mixed-precision gating in baseline\n  gating_temperature_tau: 0.67     # fixed if not tuned by Optuna\n  max_sequence_length: 4096\n  init_std: 0.02\n  dropout: 0.0\n\ndataset:\n  name: cnn_dailymail\n  subset: 3.0.0\n  task: summarisation\n  text_column: article\n  summary_column: highlights\n  train_split: train\n  validation_split: validation\n  test_split: test\n  max_length: 4096\n  truncation: right\n  padding: right\n  batch_size: 4\n  num_workers: 4\n  shuffle_buffer: 10_000\n\ntraining:\n  epochs: 3\n  learning_rate: 3e-5\n  scheduler: cosine\n  warmup_steps: 500\n  optimizer: adamw\n  weight_decay: 0.01\n  beta1: 0.9\n  beta2: 0.95\n  eps: 1e-8\n  gradient_accumulation_steps: 8\n  max_grad_norm: 1.0\n  precision: fp16\n  amp_level: O2\n  label_smoothing: 0.0\n  checkpoint_interval_steps: 1000\n  log_interval_steps: 50\n  seed: 42\n  deterministic: false\n  early_stopping_patience: 3\n  save_best_metric: rougeL\n  pin_memory: true\n  persistent_workers: true\n\nevaluation:\n  metrics: [rougeL, mean_joules_per_sample, latency_p99]\n  eval_batch_size: 4\n  generate_max_length: 128\n  use_beam_search: true\n  num_beams: 4\n\nhardware:\n  device: \"cuda\"\n  gpu_name: \"A100-80GB\"\n  allow_tf32: true\n\noptuna:\n  n_trials: 20\n  direction: maximize            # primary = ROUGE-L, energy ignored here\n  timeout_min: 240\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 5.0e-4\n    gating_temperature_tau:\n      type: uniform\n      low: 0.4\n      high: 1.0\n    batch_size:\n      type: categorical\n      choices: [2, 4]\n    gradient_accumulation_steps:\n      type: categorical\n      choices: [4, 8, 16]\n"
    }
  ]
}
