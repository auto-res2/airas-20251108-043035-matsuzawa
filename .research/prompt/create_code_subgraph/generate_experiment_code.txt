
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/runs/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
      ```json
      {
        "primary_metric": "Mean Joules per sample at equal or better Rouge-L.",
        "metrics": {
          "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
          "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
        },
        "best_proposed": {
          "run_id": "proposed-iter2-model-dataset",
          "value": 0.92
        },
        "best_baseline": {
          "run_id": "comparative-1-model-dataset",
          "value": 0.88
        },
        "gap": 4.55
      }
      ```
      The structure includes:
      - "primary_metric": The primary evaluation metric name from the hypothesis (Mean Joules per sample at equal or better Rouge-L.)
      - "metrics": All collected metrics organized by metric name, then by run_id
      - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
      - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
      - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
        * Use the expected results from the hypothesis (HEST matches TABS on Rouge-L (≈44.3) while cutting mean energy by ≥40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8× on the longest sequences.) to determine metric direction
        * If the metric should be minimized (like "loss", "perplexity", "error"), reverse the sign of the gap
        * The gap represents the percentage improvement of the proposed method over the best baseline
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 2025-1108-1
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Experiment code is NOT ready for execution. Critical blocking problems:

1. Syntax errors: All type-hints use the literal string "-\u003e" instead of "->" (e.g. in evaluate.py, model.py, preprocess.py, train.py).  Python will raise a SyntaxError on import.
2. Wrong WandB run id reference: _init_wandb() calls cfg.run.run_id but cfg.run is a string; correct key is cfg.run_id.  Causes AttributeError before training starts.
3. evaluate.py misses `from typing import Any` (used in annotations) and therefore crashes.
4. Mandatory evaluation artefacts are incomplete – no confusion matrices, no statistical tests, no cross-run figures except a single bar chart, violating Core Validation Criterion 8.
5. Model implementation is functionally incomplete: HESTAdapter only returns an energy scalar; it never applies gating nor modifies the backbone activations, so the proposed method is not implemented as described.  Criterion 4 requires a fully functional model.
6. energy_penalty_from_hidden uses a rough token count heuristic unrelated to the controller and ignores INT8/fp16 choices; budget is not enforced.  Violates Criteria 3 & 4.
7. FactorisedController is instantiated with num_layers but layer_idx passed to forward is always -1 (dummy), so per-layer gating is unused.

Because of these issues the code will not run and does not satisfy the publication-quality, production-ready requirements.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# *Base* Hydra configuration.  Each experiment run merges this file **first**,\n# then applies ``config/runs/\u003crun_id\u003e.yaml`` via CLI override.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - _self_\n\n# CLI arguments (populated by GitHub Actions)\nrun: null          # e.g. proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation\nmode: full         # \"trial\" or \"full\"\nresults_dir: ./results\n\n# -------------------------------------------------------------------------\n# WandB \u2013 credentials are set via *environment variable* WANDB_API_KEY\n# -------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online  # gets overwritten to \"disabled\" automatically in trial mode\n\n# -------------------------------------------------------------------------\n# Optuna \u2013 sensible defaults (overridden by run configs / CLI)\n# -------------------------------------------------------------------------\noptuna:\n  n_trials: 0\n  direction: minimize\n  timeout_min: 0\n  search_space: {}\n\n# -------------------------------------------------------------------------\n# Generic training defaults (will be superseded by run-specific YAML)\n# -------------------------------------------------------------------------\ntraining:\n  epochs: 1\n  learning_rate: 3e-5\n  warmup_steps: 0\n  weight_decay: 0.0\n  beta1: 0.9\n  beta2: 0.999\n  eps: 1e-8\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  precision: fp16\n  clamp_grad: false\n  batch_limit: null   # only used in trial mode\n  log_interval_steps: 50\n  seed: 42\n  pin_memory: true\n  persistent_workers: false\n  lagrangian_init: 0.1\n  dual_step_size_alpha: 0.05\n  gumbel_temperature_tau: 0.67\n\n# -------------------------------------------------------------------------\n# Hardware (defaults) \u2013 many options will be redefined in run YAMLs\n# -------------------------------------------------------------------------\nhardware:\n  device: cuda\n  allow_tf32: true\n\n# Please refer to individual ``config/runs/*.yaml`` files for full per-run\n# configurations covering `model`, `dataset`, `evaluation`, etc.\n# -----------------------------------------------------------------------------\n", "evaluate_py": "# src/evaluate.py\n\"\"\"Independent, post-training evaluation \u0026 visualisation script.\n\nExample:\n    uv run python -m src.evaluate \\\n        results_dir=./results \\\n        run_ids=\u0027[\"run-1\", \"run-2\"]\u0027\n\nThis script **never** gets invoked from ``src.main``/``src.train`` \u2013 instead it\nis scheduled by a separate GitHub Actions workflow *after* all training runs\nhave completed.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom uuid import uuid4\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\n\nsns.set(style=\"whitegrid\")\n\nPRIMARY_METRIC = \"mean_joules_per_sample\"  # customise as per hypothesis\n\n\n# --------------------------------------------------------------------------------------\n# CLI helpers\n# --------------------------------------------------------------------------------------\n\ndef _parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory to save metrics \u0026 figures\")\n    parser.add_argument(\"run_ids\", type=str, help=\u0027JSON list e.g. \"[\\\"run-1\\\", \\\"run-2\\\"]\"\u0027)\n    return parser.parse_args()\n\n\n# --------------------------------------------------------------------------------------\n# WandB retrieval utilities\n# --------------------------------------------------------------------------------------\n\ndef _retrieve_run(entity: str, project: str, run_id: str):\n    api = wandb.Api()\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history: pd.DataFrame = run.history()  # full metric history\n    summary = dict(run.summary._json_dict)\n    config = dict(run.config)\n    return history, summary, config\n\n\n# --------------------------------------------------------------------------------------\n# Plotting helpers\n# --------------------------------------------------------------------------------------\n\ndef _save_learning_curve(history: pd.DataFrame, out_path: Path, metric: str = \"train_loss\") -\u003e None:\n    plt.figure(figsize=(7, 4))\n    sns.lineplot(data=history, x=history.index, y=metric)\n    plt.title(f\"Learning curve \u2013 {metric}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(metric)\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef _comparison_bar(metric_dict: dict[str, float], out_path: Path, metric_name: str):\n    plt.figure(figsize=(8, 5))\n    runs = list(metric_dict.keys())\n    values = [metric_dict[r] for r in runs]\n    ax = sns.barplot(x=runs, y=values, palette=\"viridis\")\n    for i, v in enumerate(values):\n        ax.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(metric_name)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\n# --------------------------------------------------------------------------------------\n# Main evaluation logic\n# --------------------------------------------------------------------------------------\n\ndef main():\n    args = _parse_args()\n    run_ids = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).expanduser().resolve()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    # Load *global* WandB credentials \u0026 project config\n    repo_root = Path(__file__).resolve().parents[1]\n    wandb_cfg = OmegaConf.load(repo_root / \"config\" / \"config.yaml\").wandb\n    entity, project = wandb_cfg.entity, wandb_cfg.project\n\n    aggregated: dict[str, Any] = {\n        \"primary_metric\": \"Mean Joules per sample at equal or better Rouge-L.\",\n        \"metrics\": {},\n    }\n\n    # ------------------------------------------------------------------------------\n    # Per-run processing (metrics.json + learning curve figures)\n    # ------------------------------------------------------------------------------\n    for run_id in run_ids:\n        history, summary, _ = _retrieve_run(entity, project, run_id)\n\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # save *full* summary / scalar metrics\n        (run_dir / \"metrics.json\").write_text(json.dumps(summary, indent=2))\n\n        # learning curve \u2013 we pick `train_loss` if available\n        metric_to_plot = \"train_loss\" if \"train_loss\" in history.columns else history.columns[0]\n        fig_name = f\"{run_id}_learning_curve_{uuid4().hex[:4]}.pdf\"\n        _save_learning_curve(history, run_dir / fig_name, metric=metric_to_plot)\n        print(run_dir / fig_name)\n\n        # add to aggregated structure (only float/int scalars)\n        for k, v in summary.items():\n            if isinstance(v, (float, int)):\n                aggregated.setdefault(\"metrics\", {}).setdefault(k, {})[run_id] = v\n\n    # ------------------------------------------------------------------------------\n    # Aggregated analysis / comparison between runs\n    # ------------------------------------------------------------------------------\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(exist_ok=True, parents=True)\n\n    primary_values = aggregated[\"metrics\"].get(PRIMARY_METRIC, {})\n    baseline_candidates = {k: v for k, v in primary_values.items() if any(t in k for t in (\"baseline\", \"comparative\"))}\n    proposed_candidates = {k: v for k, v in primary_values.items() if \"proposed\" in k}\n\n    # Lower energy is *better* \u2013 we therefore take **min**\n    best_baseline_id, best_baseline_val = (None, None)\n    if baseline_candidates:\n        best_baseline_id, best_baseline_val = min(baseline_candidates.items(), key=lambda x: x[1])\n\n    best_proposed_id, best_proposed_val = (None, None)\n    if proposed_candidates:\n        best_proposed_id, best_proposed_val = min(proposed_candidates.items(), key=lambda x: x[1])\n\n    gap = None\n    if best_baseline_val is not None and best_proposed_val is not None:\n        gap = (best_baseline_val - best_proposed_val) / best_baseline_val * 100.0\n\n    aggregated[\"best_proposed\"] = {\"run_id\": best_proposed_id, \"value\": best_proposed_val}\n    aggregated[\"best_baseline\"] = {\"run_id\": best_baseline_id, \"value\": best_baseline_val}\n    aggregated[\"gap\"] = gap\n\n    # Save aggregated JSON\n    (comparison_dir / \"aggregated_metrics.json\").write_text(json.dumps(aggregated, indent=2))\n    print(comparison_dir / \"aggregated_metrics.json\")\n\n    # Comparison bar chart for primary metric\n    if primary_values:\n        fig_name = f\"comparison_{PRIMARY_METRIC}_{uuid4().hex[:4]}.pdf\"\n        _comparison_bar(primary_values, comparison_dir / fig_name, PRIMARY_METRIC)\n        print(comparison_dir / fig_name)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "# src/main.py\n\"\"\"Lightweight launcher \u2013 merges configs and spawns the *actual* training script\n(``src.train``) as a **subprocess**.  This keeps the public CLI clean:\n\n    uv run python -u -m src.main run=\u003crun_id\u003e results_dir=\u003cpath\u003e mode=\u003ctrial|full\u003e\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901\n    # ------------------------ Merge *run-specific* YAML ------------------------\n    run_cfg_path = Path(__file__).resolve().parents[1] / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config not found: {run_cfg_path}\")\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n\n    # ---------------------------- Mode adjustments ----------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Echo merged config to log\n    logger.info(\"Final config (from main):\\n\" + OmegaConf.to_yaml(cfg))\n\n    # ----------------------------- Spawn subprocess ---------------------------\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"mode={cfg.mode}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"wandb.mode={cfg.wandb.mode}\",\n        f\"optuna.n_trials={cfg.optuna.n_trials}\",\n        f\"training.epochs={cfg.training.epochs}\",\n    ]\n    if cfg.mode == \"trial\":\n        overrides.append(\"training.batch_limit=2\")\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    logger.info(\"Launching training subprocess:\\n\" + \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "# src/model.py\n\"\"\"Model architectures for both *proposed* (HEST) and *baseline* (TABS) methods.\n\nWe wrap HuggingFace Transformers models and attach an **adapter** that is\nresponsible for computing the *energy* penalty.  The entire backbone stays\nfrozen (typical PEFT setup).\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n\n# -----------------------------------------------------------------------------\n# Constants \u2013 rough relative energy per multiply-accumulate (arbitrary units)\n# -----------------------------------------------------------------------------\nENERGY_FP16 = 1.0\nENERGY_INT8 = 0.6\n\n\n# -----------------------------------------------------------------------------\n# Controller \u0026 adapter implementations\n# -----------------------------------------------------------------------------\nclass FactorisedController(nn.Module):\n    \"\"\"\u03c3(W_tok h) \u00b7 \u03c3(w_layer) \u2192 scalar per token (broadcast to R directions).\"\"\"\n\n    def __init__(self, hidden_size: int, k: int, num_layers: int):\n        super().__init__()\n        self.W_tok = nn.Linear(hidden_size, k, bias=False)\n        self.w_layer = nn.Parameter(torch.zeros(num_layers, k))  # one row per layer\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int) -\u003e torch.Tensor:  # type: ignore[override]\n        # hidden: (batch, seq, hidden_size)\n        s_tok = torch.sigmoid(self.W_tok(hidden))  # (b, seq, k)\n        s_layer = torch.sigmoid(self.w_layer[layer_idx])  # (k,)\n        g = (s_tok * s_layer).mean(-1)  # (b, seq)\n        return g\n\n\nclass HESTAdapter(nn.Module):\n    \"\"\"Hierarchical Energy-constrained Spectral Tuning adapter \u2013 *proposed* method.\"\"\"\n\n    def __init__(self, hidden_size: int, cfg: DictConfig):\n        super().__init__()\n        self.R = cfg.model.spectral_rank_R\n        self.controller = FactorisedController(hidden_size, cfg.model.controller_width_k, cfg.model.num_layers)\n        self.temperature = cfg.training.gumbel_temperature_tau\n        # Energy constants registered as buffers (move with .to(device))\n        self.register_buffer(\"e_fp16\", torch.tensor(ENERGY_FP16))\n        self.register_buffer(\"e_int8\", torch.tensor(ENERGY_INT8))\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int) -\u003e torch.Tensor:  # type: ignore[override]\n        # hidden: (b, seq, hidden_size)\n        p = self.controller(hidden, layer_idx).unsqueeze(-1).repeat(1, 1, self.R)  # (b, seq, R)\n        logits = torch.log(torch.clamp(p, min=1e-6)).unsqueeze(-1).repeat(1, 1, 1, 3)  # (b, seq, R, 3)\n        g = F.gumbel_softmax(logits, tau=self.temperature, hard=False)  # (*,3)\n        energy = (g[..., 1] * self.e_int8 + g[..., 2] * self.e_fp16).sum()\n        return energy\n\n\nclass TABSAdapter(nn.Module):\n    \"\"\"Baseline \u2013 Token-wise Adaptive Basis Selection (always FP16).\"\"\"\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int) -\u003e torch.Tensor:  # noqa: D401\n        # Baseline ignores energy penalties (always FP16)\n        del hidden, layer_idx\n        return torch.tensor(0.0, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# -----------------------------------------------------------------------------\n# Full wrapper that exposes *standard* HF forward + .energy attribute\n# -----------------------------------------------------------------------------\nclass EnergyWrapperModel(nn.Module):\n    def __init__(self, backbone: nn.Module, adapter: nn.Module):\n        super().__init__()\n        self.backbone = backbone\n        self.adapter = adapter\n\n    def forward(self, **kwargs):  # type: ignore[override]\n        kwargs.setdefault(\"output_hidden_states\", True)\n        outputs = self.backbone(**kwargs)\n        # Collect the *last* hidden state of the encoder for energy calculation\n        hidden_states = getattr(outputs, \"encoder_last_hidden_state\", None)\n        if hidden_states is not None:\n            outputs.energy = self.adapter(hidden_states, layer_idx=-1)  # dummy layer idx\n        else:\n            outputs.energy = torch.tensor(0.0, device=next(self.parameters()).device)\n        return outputs\n\n\n# -----------------------------------------------------------------------------\n# Factory \u2013 build backbone + adapter given Hydra ``cfg``\n# -----------------------------------------------------------------------------\n\ndef build_model(cfg: DictConfig, tokenizer) -\u003e nn.Module:  # noqa: D401\n    if \"Seq2Seq\" in cfg.model.name or \"bart\" in cfg.model.name.lower() or cfg.dataset.task == \"summarisation\":\n        backbone = AutoModelForSeq2SeqLM.from_pretrained(\n            cfg.model.name,\n            cache_dir=\".cache/\",\n            torch_dtype=torch.float16 if cfg.training.precision == \"fp16\" else torch.float32,\n        )\n    else:\n        backbone = AutoModelForCausalLM.from_pretrained(\n            cfg.model.name,\n            cache_dir=\".cache/\",\n            torch_dtype=torch.float16 if cfg.training.precision == \"fp16\" else torch.float32,\n        )\n\n    # Freeze backbone \u2013 PEFT updates only adapter/controller params\n    for p in backbone.parameters():\n        p.requires_grad = False\n\n    adapter: nn.Module\n    if cfg.method.lower() in {\"hest\", \"proposed\"}:\n        adapter = HESTAdapter(backbone.config.hidden_size, cfg)\n    else:\n        adapter = TABSAdapter()\n\n    return EnergyWrapperModel(backbone, adapter)\n\n\n# -----------------------------------------------------------------------------\n# Helper \u2013 compute energy penalty \u0026 optional dual ascent term\n# -----------------------------------------------------------------------------\n\ndef energy_penalty_from_hidden(hidden: torch.Tensor, lambda_energy, cfg: DictConfig, return_energy_only: bool = False):\n    \"\"\"Proxy energy cost \u221d (#tokens \u00d7 R) \u00d7 fp16_energy.  If ``return_energy_only`` is\n    *True* we skip \u03bb scaling (used for updating \u03bb itself).\n    \"\"\"\n    batch_energy = hidden.numel() / hidden.size(-1) * cfg.model.spectral_rank_R * ENERGY_FP16 * 1e-5  # rough\n    if return_energy_only:\n        return batch_energy\n    return lambda_energy * (batch_energy - cfg.model.energy_budget_ratio)\n", "preprocess_py": "# src/preprocess.py\n\"\"\"Data-loading / tokenisation utilities.\n\nThe implementation is deliberately *generic* so that it works for\nsummarisation datasets (CNN/DM) as well as other seq-2-seq tasks.\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Tuple\n\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq\n\nCACHE_DIR = \".cache/\"\n\n\ndef _tokenise_function(examples: dict, tokenizer: AutoTokenizer, cfg: DictConfig):\n    model_inputs = tokenizer(\n        examples[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[cfg.dataset.summary_column],\n            max_length=cfg.evaluation.generate_max_length,\n            truncation=True,\n            padding=\"max_length\",\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\ndef build_dataloaders(cfg: DictConfig, tokenizer: AutoTokenizer) -\u003e Tuple[DataLoader, DataLoader]:\n    raw_dataset = load_dataset(cfg.dataset.name, cfg.dataset.subset, cache_dir=CACHE_DIR)\n\n    tokenised = raw_dataset.map(\n        functools.partial(_tokenise_function, tokenizer=tokenizer, cfg=cfg),\n        batched=True,\n        num_proc=cfg.dataset.num_workers,\n        remove_columns=raw_dataset[cfg.dataset.train_split].column_names,\n    )\n\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"longest\")\n\n    train_dl = DataLoader(\n        tokenised[cfg.dataset.train_split],\n        shuffle=True,\n        batch_size=cfg.dataset.batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=data_collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n    val_dl = DataLoader(\n        tokenised[cfg.dataset.validation_split],\n        shuffle=False,\n        batch_size=cfg.evaluation.eval_batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=data_collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n    return train_dl, val_dl\n", "pyproject_toml": "[project]\nname = \"hest-experiments\"\nversion = \"0.1.0\"\ndescription = \"Reproducible experiments for HEST vs. TABS\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.2.0\"\ntransformers = \"\u003e=4.38.0\"\ndatasets = \"\u003e=2.15.0\"\noptuna = \"\u003e=3.5.0\"\nwandb = \"\u003e=0.16.0\"\nhydra-core = \"\u003e=1.3.2\"\nmatplotlib = \"\u003e=3.8.0\"\nseaborn = \"\u003e=0.13.0\"\npynvml = \"\u003e=11.5.0\"\nevaluate = \"\u003e=0.4.0\"\nscipy = \"\u003e=1.11.0\"\naccelerate = \"\u003e=0.25.0\"\n\n[tool.setuptools.package-data]\n\"\" = [\"config/**/*.yaml\"]\n", "train_py": "# src/train.py\n\"\"\"Single-run training script with Hydra configuration, Optuna (optional) and\ncomprehensive Weights \u0026 Biases logging.\n\nCLI is *indirect* \u2013 this module is launched as a subprocess by ``src.main`` via\n\n    uv run python -u -m src.train run=\u003crun_id\u003e results_dir=\u003cpath\u003e mode=\u003ctrial|full\u003e\n\nThe Hydra config (``config/config.yaml`` + ``config/runs/\u003crun_id\u003e.yaml``) is\nassembled exactly the same way as in ``src.main`` \u2013 we just receive the same\ncommand-line overrides.  All heavy-lifting (data loading, model building,\ntraining loop, energy tracking, Lagrangian dual ascent, etc.) happens here.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport pynvml\nimport torch\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim import AdamW\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    get_cosine_schedule_with_warmup,\n)\n\n# local imports (relative to package `src`)\nfrom .model import build_model, energy_penalty_from_hidden\nfrom .preprocess import build_dataloaders\n\ntry:\n    import wandb  # heavy import \u2013 do it lazily to avoid cost in trial mode\nexcept ModuleNotFoundError:  # pragma: no cover \u2013 CI always has wandb\n    wandb = None  # type: ignore\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n\n# --------------------------------------------------------------------------------------\n# Helper utilities (inlined here \u2013 NO extra files allowed by the task description)\n# --------------------------------------------------------------------------------------\n\ndef set_seed(seed: int) -\u003e None:\n    \"\"\"Make *everything* deterministic (as far as reasonably possible).\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef format_step_logs(log_dict: Dict[str, Any]) -\u003e str:\n    \"\"\"Pretty-print a dict of scalars for console logging.\"\"\"\n    return \" \".join(f\"{k}={v:.4f}\" if isinstance(v, float) else f\"{k}={v}\" for k, v in log_dict.items())\n\n\n# --------------------------------------------------------------------------------------\n# Energy measurement helpers \u2013 NVIDIA NVML (Joules).\n# --------------------------------------------------------------------------------------\n\ndef _energy_meter_start():\n    try:\n        pynvml.nvmlInit()\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n        return handle, pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # in millijoules\n    except Exception:\n        return None, 0.0\n\n\ndef _energy_meter_end(handle):\n    try:\n        return pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # in millijoules\n    except Exception:\n        return 0.0\n\n\n# --------------------------------------------------------------------------------------\n# WandB initialisation (may be *disabled* e.g. in `trial` mode).\n# --------------------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n        return None\n    if wandb is None:\n        raise RuntimeError(\"wandb is required for full mode but could not be imported.\")\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run.run_id,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        resume=\"allow\",\n    )\n    logger.info(f\"Weights \u0026 Biases URL: {run.url}\")\n    return run\n\n\n# --------------------------------------------------------------------------------------\n# Optimiser / scheduler helpers\n# --------------------------------------------------------------------------------------\n\ndef _build_optimizer(model: torch.nn.Module, cfg: DictConfig) -\u003e torch.optim.Optimizer:\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    parameters_with_decay = [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)]\n    parameters_without_decay = [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)]\n\n    param_groups = [\n        {\"params\": parameters_with_decay, \"weight_decay\": cfg.training.weight_decay},\n        {\"params\": parameters_without_decay, \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(\n        param_groups,\n        lr=cfg.training.learning_rate,\n        betas=(cfg.training.beta1, cfg.training.beta2),\n        eps=cfg.training.eps,\n    )\n    return optimizer\n\n\n# --------------------------------------------------------------------------------------\n# Training loop (single epoch \u0026 full run)\n# --------------------------------------------------------------------------------------\n\ndef _train_single_epoch(\n    epoch: int,\n    model: torch.nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    tokenizer: Any,\n    optimizer: torch.optim.Optimizer,\n    scheduler: torch.optim.lr_scheduler.LambdaLR,\n    scaler: GradScaler,\n    device: torch.device,\n    lambda_energy: torch.Tensor,\n    energy_budget: float,\n    cfg: DictConfig,\n    wandb_run,\n) -\u003e float:\n    \"\"\"Run **one** epoch \u2013 returns *energy spent* (Joules).\"\"\"\n\n    model.train()\n\n    handle, energy_start_mJ = _energy_meter_start()\n\n    for step, batch in enumerate(dataloader):\n        if cfg.training.batch_limit is not None and step \u003e= cfg.training.batch_limit:\n            break\n        # move to GPU (only tensor values)\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        labels = batch[\"labels\"].clone()\n\n        with autocast(enabled=cfg.training.precision == \"fp16\"):\n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=labels,\n            )\n            task_loss = outputs.loss\n            hidden_states = getattr(outputs, \"encoder_last_hidden_state\", None)\n            penalty = 0.0\n            if hidden_states is not None:\n                penalty = energy_penalty_from_hidden(hidden_states, lambda_energy, cfg)\n            loss = task_loss + penalty\n\n        scaler.scale(loss / cfg.training.gradient_accumulation_steps).backward()\n\n        if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n            scheduler.step()\n\n        # Dual ascent for \u03bb (after grads propagated)\n        if hidden_states is not None and (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n            energy_batch = energy_penalty_from_hidden(hidden_states, None, cfg, return_energy_only=True)\n            lambda_energy.data = torch.clamp(\n                lambda_energy.data + cfg.training.dual_step_size_alpha * (energy_batch.item() - energy_budget),\n                min=0.0,\n            )\n\n        # --------------------------------- WandB logging ----------------------------------\n        if wandb_run and (step + 1) % cfg.training.log_interval_steps == 0:\n            log_payload = {\n                \"train_loss\": loss.item(),\n                \"task_loss\": task_loss.item(),\n                \"penalty\": penalty.item() if isinstance(penalty, torch.Tensor) else penalty,\n                \"lambda\": lambda_energy.item(),\n                \"lr\": scheduler.get_last_lr()[0],\n                \"epoch\": epoch,\n                \"step\": epoch * len(dataloader) + step + 1,\n            }\n            wandb_run.log(log_payload)\n            logger.info(format_step_logs(log_payload))\n\n    # --------------------------------------------------------------------------------------\n    energy_total_J = 0.0\n    if handle is not None:\n        energy_end_mJ = _energy_meter_end(handle)\n        pynvml.nvmlShutdown()\n        energy_total_J = max(energy_end_mJ - energy_start_mJ, 0.0) / 1000.0  # convert mJ\u2192J\n    return energy_total_J\n\n\ndef _train_full_run(cfg: DictConfig) -\u003e float:\n    \"\"\"End-to-end training *once* (no hyper-param search).  Returns *mean Joules per sample*.\"\"\"\n\n    # set seeds *before* anything else\n    set_seed(cfg.training.seed)\n\n    # ----------------------------------------------------------------------------------\n    # Hardware / precision settings\n    # ----------------------------------------------------------------------------------\n    if cfg.hardware.allow_tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n\n    device = torch.device(cfg.hardware.device if torch.cuda.is_available() else \"cpu\")\n\n    # ----------------------------------------------------------------------------------\n    # Data pipeline\n    # ----------------------------------------------------------------------------------\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    train_loader, _ = build_dataloaders(cfg, tokenizer)\n\n    # ----------------------------------------------------------------------------------\n    # Model + optimisation stack\n    # ----------------------------------------------------------------------------------\n    model = build_model(cfg, tokenizer)\n    model.to(device)\n\n    optimizer = _build_optimizer(model, cfg)\n    num_total_steps = (\n        (len(train_loader) // cfg.training.gradient_accumulation_steps) * cfg.training.epochs\n    )\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=num_total_steps,\n    )\n    scaler = GradScaler(enabled=cfg.training.precision == \"fp16\")\n\n    # ----------------------------------------------------------------------------------\n    # Lagrangian (\u03bb) initialisation\n    # ----------------------------------------------------------------------------------\n    lambda_energy = torch.tensor(cfg.training.lagrangian_init, dtype=torch.float32, device=device)\n    energy_budget = cfg.model.energy_budget_ratio\n\n    # ----------------------------------------------------------------------------------\n    # WandB run (may be None in `trial` mode)\n    # ----------------------------------------------------------------------------------\n    wandb_run = _init_wandb(cfg)\n\n    # ----------------------------------------------------------------------------------\n    # Actual *epoch* training loop\n    # ----------------------------------------------------------------------------------\n    cumulative_energy_J = 0.0\n    for epoch in range(cfg.training.epochs):\n        epoch_energy = _train_single_epoch(\n            epoch,\n            model,\n            train_loader,\n            tokenizer,\n            optimizer,\n            scheduler,\n            scaler,\n            device,\n            lambda_energy,\n            energy_budget,\n            cfg,\n            wandb_run,\n        )\n        cumulative_energy_J += epoch_energy\n\n    mean_joules_per_sample = cumulative_energy_J / len(train_loader.dataset)\n\n    if wandb_run:\n        wandb_run.summary[\"mean_joules_per_sample\"] = mean_joules_per_sample\n        wandb_run.finish()\n\n    return mean_joules_per_sample\n\n\n# --------------------------------------------------------------------------------------\n# Optuna \u2013 *optional* hyper-parameter optimisation.\n# --------------------------------------------------------------------------------------\n\ndef _optuna_objective(trial: optuna.Trial, base_cfg: DictConfig):\n    cfg: DictConfig = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n\n    # --------------- Sample hyper-parameters dynamically (per Optuna space) -------------\n    for hp, space in cfg.optuna.search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            val = trial.suggest_float(hp, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            val = trial.suggest_float(hp, space[\"low\"], space[\"high\"])\n        elif space[\"type\"] == \"categorical\":\n            val = trial.suggest_categorical(hp, space[\"choices\"])\n        else:\n            raise ValueError(f\"Unknown Optuna space type: {space[\u0027type\u0027]}\")\n        OmegaConf.update(cfg, hp, val, merge=True)\n\n    # Proxy-speed: single epoch + WandB disabled\n    cfg.training.epochs = 1\n    cfg.wandb.mode = \"disabled\"\n\n    energy_metric = _train_full_run(cfg)\n    return energy_metric  # we *minimise* mean energy whilst keeping quality\n\n\n# --------------------------------------------------------------------------------------\n# Top-level Hydra entry-point (the *only* public function in this module).\n# --------------------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901 (complexity)\n    \"\"\"Entrypoint that honours `mode` and optionally runs Optuna *before* the final\n    training according to best hyper-parameters.\n    \"\"\"\n\n    # Merge run-specific YAML (mandatory)\n    run_cfg_path = Path(__file__).resolve().parents[1] / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config does not exist: {run_cfg_path}\")\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n\n    # ------------------------- Mode adjustments (trial / full) -------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2  # run only 2 batches for speed\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = cfg.wandb.mode  # keep whatever default is in config (usually \"online\")\n    else:\n        raise ValueError(\"`mode` must be either \u0027trial\u0027 or \u0027full\u0027.\")\n\n    # Echo final config to log for reproducibility\n    logger.info(\"Final merged config:\\n\" + OmegaConf.to_yaml(cfg))\n\n    # -------------------------------- Optuna section ----------------------------------\n    if cfg.optuna.n_trials \u003e 0:\n        direction = cfg.optuna.direction  # \"minimize\" or \"maximize\"\n        study = optuna.create_study(direction=direction)\n        logger.info(f\"Starting Optuna study \u2013 {cfg.optuna.n_trials} trials \u2026\")\n        study.optimize(\n            lambda t: _optuna_objective(t, cfg),\n            n_trials=cfg.optuna.n_trials,\n            timeout=cfg.optuna.timeout_min * 60 if cfg.optuna.timeout_min else None,\n        )\n        logger.info(f\"Optuna finished \u2013 best value {study.best_value:.4f} with params {study.best_params}\")\n        # Update cfg with *best* params so the *real* training uses them\n        for k, v in study.best_params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n\n    # --------------------------- Main (once-off) training ------------------------------\n    metric = _train_full_run(cfg)\n    logger.info(f\"Training finished \u2013 mean_joules_per_sample={metric:.4f} J\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length × layers × R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.' method='Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).' experimental_setup='Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6× TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.' primary_metric='Mean Joules per sample at equal or better Rouge-L.' experimental_code='import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,λ,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=λ*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc' expected_result='HEST matches TABS on Rouge-L (≈44.3) while cutting mean energy by ≥40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8× on the longest sequences.' expected_conclusion='A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget—rather than FLOPs alone—yields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP.'

# Current Research Method
Hierarchical Energy-constrained Spectral Tuning (HEST)
1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).
2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].
3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).
4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.
5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).

# Experimental Design
- Summary: We will fine-tune a pretrained Llama-2-7B-chat model on the CNN/DailyMail summarisation task using the proposed Hierarchical Energy-constrained Spectral Tuning (HEST) adapter. During training we jointly optimise task loss and an energy-budget Lagrangian so that, for every input sequence, the total expected energy of all spectral directions stays below a user-specified envelope. The controller is factorised across tokens and layers (rank-1 outer product) and produces a 3-way Gumbel-Softmax gate per spectral direction that selects one of {off, INT8, FP16}. After convergence we harden the gates and fuse INT8 choices into integer GEMM kernels; de-activated directions are skipped. We compare HEST against the strongest prior energy-oblivious spectral adapter, Token-wise Adaptive Basis Selection (TABS), under the same backbone, data split, and optimisation schedule. We report summarisation quality, energy consumed per sample (read with NVML), tail latency, and parameter-efficiency. A small grid search over learning-rate, controller width k, and the energy budget E_max is performed on the validation set. Experiments run on a single NVIDIA A100 80 GB GPU with plenty of system RAM.
- Evaluation metrics: ['Mean Joules per sample at equal or better Rouge-L', 'ROUGE-L', 'Accuracy', '99th percentile latency', 'Controller parameter count', 'Mean Joules per sample at equal or better Rouge-L.']

# Experiment Runs

- Run ID: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: proposed
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation
  Config File: config/runs/proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
method: HEST
model:
  name: Llama-2-7B-chat
  adapter: HEST
  hidden_size: 4096            # taken from Llama-2-7B config
  num_layers: 32               # taken from Llama-2-7B config
  spectral_rank_R: 16          # number of spectral directions kept per layer
  controller_width_k: 32       # width of factorised controller
  precision_choices: [off, int8, fp16]
  energy_budget_ratio: 0.6     # E_max = 0.6 × average energy of TABS
  max_sequence_length: 4096
  init_std: 0.02
  dropout: 0.0
  weight_tying: true
  rotary_embedding_base: 10000
  rope_scaling: null           # not used for ≤4k tokens

dataset:
  name: cnn_dailymail
  subset: 3.0.0                # HF dataset version
  task: summarisation
  text_column: article
  summary_column: highlights
  train_split: train
  validation_split: validation
  test_split: test
  max_length: 4096
  truncation: right
  padding: right
  batch_size: 4                # fits on 80 GB with grad-accumulation
  num_workers: 4
  shuffle_buffer: 10_000

training:
  epochs: 3
  learning_rate: 3e-5
  scheduler: cosine
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  precision: fp16
  amp_level: O2
  lagrangian_init: 0.1         # λ₀ for dual ascent
  dual_step_size_alpha: 0.05
  gumbel_temperature_tau: 0.67
  label_smoothing: 0.0
  checkpoint_interval_steps: 1000
  log_interval_steps: 50
  seed: 42
  deterministic: false
  early_stopping_patience: 3
  save_best_metric: rougeL
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: [rougeL, mean_joules_per_sample, latency_p99]
  eval_batch_size: 4
  generate_max_length: 128
  use_beam_search: true
  num_beams: 4

hardware:
  device: "cuda"
  gpu_name: "A100-80GB"
  allow_tf32: true
  torch_int8_gemm: true

optuna:
  n_trials: 30
  direction: minimize          # primary metric = mean energy @ ≥baseline ROUGE
  timeout_min: 360             # stop search after 6 h
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    controller_width_k:
      type: categorical
      choices: [16, 32, 64]
    energy_budget_ratio:
      type: uniform
      low: 0.4
      high: 0.8
    gumbel_temperature_tau:
      type: uniform
      low: 0.4
      high: 1.0
    dual_step_size_alpha:
      type: loguniform
      low: 0.01
      high: 0.2
    batch_size:
      type: categorical
      choices: [2, 4]
    gradient_accumulation_steps:
      type: categorical
      choices: [4, 8, 16]

  ```
  

- Run ID: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: comparative-1
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation
  Config File: config/runs/comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
method: TABS
model:
  name: Llama-2-7B-chat
  adapter: TABS                    # Token-wise Adaptive Basis Selection
  hidden_size: 4096
  num_layers: 32
  spectral_rank_R: 16              # same R as proposed for fairness
  precision: fp16                  # no mixed-precision gating in baseline
  gating_temperature_tau: 0.67     # fixed if not tuned by Optuna
  max_sequence_length: 4096
  init_std: 0.02
  dropout: 0.0

dataset:
  name: cnn_dailymail
  subset: 3.0.0
  task: summarisation
  text_column: article
  summary_column: highlights
  train_split: train
  validation_split: validation
  test_split: test
  max_length: 4096
  truncation: right
  padding: right
  batch_size: 4
  num_workers: 4
  shuffle_buffer: 10_000

training:
  epochs: 3
  learning_rate: 3e-5
  scheduler: cosine
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  precision: fp16
  amp_level: O2
  label_smoothing: 0.0
  checkpoint_interval_steps: 1000
  log_interval_steps: 50
  seed: 42
  deterministic: false
  early_stopping_patience: 3
  save_best_metric: rougeL
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: [rougeL, mean_joules_per_sample, latency_p99]
  eval_batch_size: 4
  generate_max_length: 128
  use_beam_search: true
  num_beams: 4

hardware:
  device: "cuda"
  gpu_name: "A100-80GB"
  allow_tf32: true

optuna:
  n_trials: 20
  direction: maximize            # primary = ROUGE-L, energy ignored here
  timeout_min: 240
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    gating_temperature_tau:
      type: uniform
      low: 0.4
      high: 1.0
    batch_size:
      type: categorical
      choices: [2, 4]
    gradient_accumulation_steps:
      type: categorical
      choices: [4, 8, 16]

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: Qwen/Qwen2.5-7B-Instruct

- Code: from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]



**HuggingFace Datasets:**





Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "# src/train.py\n\"\"\"Single-run training script with Hydra configuration, Optuna (optional) and\ncomprehensive Weights & Biases logging.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport hydra\nimport numpy as np\nimport optuna\nimport pynvml\nimport torch\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n\nfrom .model import build_model  # local import\nfrom .preprocess import build_dataloaders\n\ntry:\n    import wandb  # noqa: WPS433\nexcept ModuleNotFoundError:  # pragma: no cover – CI always has wandb\n    wandb = None  # type: ignore\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n\n# --------------------------------------------------------------------------------------\n# Helper utilities\n# --------------------------------------------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Make *everything* deterministic (as far as reasonably possible).\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef format_step_logs(log_dict: Dict[str, Any]) -> str:\n    \"\"\"Pretty-print a dict of scalars for console logging.\"\"\"\n    return \" \".join(\n        f\"{k}={v:.4f}\" if isinstance(v, (float, int)) else f\"{k}={v}\" for k, v in log_dict.items()\n    )\n\n# --------------------------------------------------------------------------------------\n# NVML helpers – energy measurement (Joules)\n# --------------------------------------------------------------------------------------\n\ndef _energy_meter_start():\n    try:\n        pynvml.nvmlInit()\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n        return handle, pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # type: ignore[attr-defined]\n    except Exception:  # noqa: WPS429\n        return None, 0.0\n\n\ndef _energy_meter_end(handle):\n    try:\n        return pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # type: ignore[attr-defined]\n    except Exception:  # noqa: WPS429\n        return 0.0\n\n# --------------------------------------------------------------------------------------\n# WandB initialisation (may be disabled)\n# --------------------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):  # noqa: WPS231 (complex fn)\n    if cfg.wandb.mode == \"disabled\":\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n        return None\n    if wandb is None:\n        raise RuntimeError(\"wandb is required for full-mode runs but is not available.\")\n\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        resume=\"allow\",\n    )\n    logger.info(\"Weights & Biases URL: %s\", run.url)\n    return run\n\n# --------------------------------------------------------------------------------------\n# Optimiser & scheduler helpers\n# --------------------------------------------------------------------------------------\n\ndef _build_optimizer(model: torch.nn.Module, cfg: DictConfig) -> torch.optim.Optimizer:  # noqa: D401\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    params_decay, params_no_decay = [], []\n    for n, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        (params_no_decay if any(nd in n for nd in no_decay) else params_decay).append(p)\n    param_groups = [\n        {\"params\": params_decay, \"weight_decay\": cfg.training.weight_decay},\n        {\"params\": params_no_decay, \"weight_decay\": 0.0},\n    ]\n    return AdamW(\n        param_groups,\n        lr=cfg.training.learning_rate,\n        betas=(cfg.training.beta1, cfg.training.beta2),\n        eps=cfg.training.eps,\n    )\n\n# --------------------------------------------------------------------------------------\n# Training loop helpers\n# --------------------------------------------------------------------------------------\n\ndef _train_single_epoch(\n    epoch: int,\n    model: torch.nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    scheduler: torch.optim.lr_scheduler.LambdaLR,\n    scaler: GradScaler,\n    device: torch.device,\n    lambda_energy: torch.Tensor,\n    energy_budget: float,\n    cfg: DictConfig,\n    wandb_run,\n) -> float:  # noqa: WPS231 (complex function)\n    \"\"\"Run **one** epoch and return energy in Joules.\"\"\"\n\n    model.train()\n    handle, energy_start_mJ = _energy_meter_start()\n\n    for step, batch in enumerate(dataloader):\n        if cfg.training.batch_limit is not None and step >= cfg.training.batch_limit:\n            break\n\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        labels = batch[\"labels\"].clone()\n\n        with autocast(enabled=cfg.training.precision == \"fp16\"):\n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=labels,\n            )\n            task_loss = outputs.loss\n            energy_batch = outputs.energy  # scalar tensor\n            penalty = lambda_energy * (energy_batch - energy_budget)\n            loss = task_loss + penalty\n\n        scaler.scale(loss / cfg.training.gradient_accumulation_steps).backward()\n\n        if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n            scheduler.step()\n\n            # dual ascent update of λ\n            lambda_energy.data = torch.clamp(\n                lambda_energy.data + cfg.training.dual_step_size_alpha * (energy_batch - energy_budget),\n                min=0.0,\n            )\n\n        # ------------------------------ logging ----------------------------------\n        if wandb_run and (step + 1) % cfg.training.log_interval_steps == 0:\n            log_payload = {\n                \"train_loss\": loss.item(),\n                \"task_loss\": task_loss.item(),\n                \"penalty\": penalty.item(),\n                \"lambda\": lambda_energy.item(),\n                \"energy_batch\": energy_batch.item(),\n                \"lr\": scheduler.get_last_lr()[0],\n                \"epoch\": epoch,\n                \"step\": epoch * len(dataloader) + step + 1,\n            }\n            wandb_run.log(log_payload)\n            logger.info(format_step_logs(log_payload))\n\n    energy_total_j = 0.0\n    if handle is not None:\n        energy_end_mJ = _energy_meter_end(handle)\n        pynvml.nvmlShutdown()  # type: ignore[attr-defined]\n        energy_total_j = max(energy_end_mJ - energy_start_mJ, 0.0) / 1000.0\n    return energy_total_j\n\n\ndef _train_full_run(cfg: DictConfig) -> float:\n    \"\"\"Full training run (no hyper-parameter search). Returns mean Joules / sample.\"\"\"\n\n    set_seed(cfg.training.seed)\n\n    # hardware prefs\n    if cfg.hardware.allow_tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n    device = torch.device(cfg.hardware.device if torch.cuda.is_available() else \"cpu\")\n\n    # data\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    train_loader, _ = build_dataloaders(cfg, tokenizer)\n\n    # model & optimiser\n    model = build_model(cfg, tokenizer).to(device)\n    optimizer = _build_optimizer(model, cfg)\n    total_steps = (\n        (len(train_loader) // cfg.training.gradient_accumulation_steps) * cfg.training.epochs\n    )\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=total_steps,\n    )\n    scaler = GradScaler(enabled=cfg.training.precision == \"fp16\")\n\n    lambda_energy = torch.tensor(cfg.training.lagrangian_init, device=device)\n    energy_budget = cfg.model.energy_budget_ratio\n\n    wandb_run = _init_wandb(cfg)\n\n    cumulative_energy_j = 0.0\n    for epoch in range(cfg.training.epochs):\n        epoch_energy = _train_single_epoch(\n            epoch,\n            model,\n            train_loader,\n            optimizer,\n            scheduler,\n            scaler,\n            device,\n            lambda_energy,\n            energy_budget,\n            cfg,\n            wandb_run,\n        )\n        cumulative_energy_j += epoch_energy\n\n    mean_joules_per_sample = cumulative_energy_j / len(train_loader.dataset)\n\n    if wandb_run:\n        wandb_run.summary[\"mean_joules_per_sample\"] = mean_joules_per_sample\n        wandb_run.finish()\n\n    return mean_joules_per_sample\n\n# --------------------------------------------------------------------------------------\n# Optuna objective\n# --------------------------------------------------------------------------------------\n\ndef _optuna_objective(trial: optuna.Trial, base_cfg: DictConfig):\n    cfg: DictConfig = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n\n    # sample hyper-params per Optuna search_space\n    for hp, space in cfg.optuna.search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            val = trial.suggest_float(hp, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            val = trial.suggest_float(hp, space[\"low\"], space[\"high\"])\n        elif space[\"type\"] == \"categorical\":\n            val = trial.suggest_categorical(hp, space[\"choices\"])\n        else:\n            raise ValueError(f\"Unknown Optuna space type {space['type']}\")\n        OmegaConf.update(cfg, hp, val, merge=True)\n\n    # quick proxy training\n    cfg.training.epochs = 1\n    cfg.wandb.mode = \"disabled\"\n\n    energy_metric = _train_full_run(cfg)\n    return energy_metric  # we minimise mean energy\n\n# --------------------------------------------------------------------------------------\n# Hydra entry-point\n# --------------------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # noqa: C901 (complex)\n    run_cfg_path = Path(__file__).resolve().parents[1] / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config not found: {run_cfg_path}\")\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n\n    # mode-specific tweaks\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = cfg.wandb.mode  # keep as in YAML (usually \"online\")\n    else:\n        raise ValueError(\"mode must be either 'trial' or 'full'\")\n\n    logger.info(\"Final merged config:\\n%s\", OmegaConf.to_yaml(cfg))\n\n    if cfg.optuna.n_trials > 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        logger.info(\"Starting Optuna: %d trials\", cfg.optuna.n_trials)\n        study.optimize(\n            lambda t: _optuna_objective(t, cfg),\n            n_trials=cfg.optuna.n_trials,\n            timeout=cfg.optuna.timeout_min * 60 if cfg.optuna.timeout_min else None,\n        )\n        logger.info(\"Optuna best value %.4f with params %s\", study.best_value, json.dumps(study.best_params))\n        for k, v in study.best_params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n\n    metric = _train_full_run(cfg)\n    logger.info(\"Run completed – mean_joules_per_sample=%.4f J\", metric)\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "# src/evaluate.py\n\"\"\"Independent evaluation & visualisation script.\n\nExample:\n    uv run python -m src.evaluate results_dir=./results run_ids='[\"run-1\", \"run-2\"]'\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\nfrom uuid import uuid4\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\nsns.set(style=\"whitegrid\")\nPRIMARY_METRIC = \"mean_joules_per_sample\"  # global hypothesis metric\n\n# --------------------------------------------------------------------------------------\n# CLI parsing\n# --------------------------------------------------------------------------------------\n\ndef _parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help='JSON list e.g. \"[\\\"run-1\\\", \\\"run-2\\\"]\"')\n    return parser.parse_args()\n\n# --------------------------------------------------------------------------------------\n# Plotting helpers\n# --------------------------------------------------------------------------------------\n\ndef _save_learning_curve(history: pd.DataFrame, out_path: Path, metric: str) -> None:\n    plt.figure(figsize=(7, 4))\n    sns.lineplot(data=history, y=metric, x=np.arange(len(history)))\n    plt.title(f\"Learning curve – {metric}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(metric)\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef _comparison_bar(metric_dict: dict[str, float], out_path: Path, metric_name: str):\n    plt.figure(figsize=(8, 5))\n    runs = list(metric_dict.keys())\n    values = [metric_dict[r] for r in runs]\n    ax = sns.barplot(x=runs, y=values, palette=\"viridis\")\n    for i, v in enumerate(values):\n        ax.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(metric_name)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef _comparison_box(metric_dict: dict[str, float], out_path: Path, metric_name: str):\n    df = pd.DataFrame({\"run\": list(metric_dict.keys()), metric_name: list(metric_dict.values())})\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(data=df, y=metric_name)\n    sns.stripplot(data=df, y=metric_name, color=\"black\", size=6)\n    plt.title(f\"Distribution of {metric_name}\")\n    plt.ylabel(metric_name)\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef _save_confusion_matrix(y_true: List[int], y_pred: List[int], out_path: Path):\n    labels = sorted(set(y_true) | set(y_pred))\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp.plot(ax=ax, cmap=\"Blues\")\n    plt.title(\"Confusion matrix\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n# --------------------------------------------------------------------------------------\n# WandB helpers\n# --------------------------------------------------------------------------------------\n\ndef _retrieve_run(entity: str, project: str, run_id: str):\n    api = wandb.Api()\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history: pd.DataFrame = run.history()  # type: ignore\n    summary = dict(run.summary._json_dict)\n    config = dict(run.config)\n    return history, summary, config\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main():  # noqa: WPS231 (complex)\n    args = _parse_args()\n    run_ids = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).expanduser().resolve()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    repo_root = Path(__file__).resolve().parents[1]\n    wandb_cfg = OmegaConf.load(repo_root / \"config\" / \"config.yaml\").wandb\n    entity, project = wandb_cfg.entity, wandb_cfg.project\n\n    aggregated: dict[str, Any] = {\n        \"primary_metric\": \"Mean Joules per sample at equal or better Rouge-L.\",\n        \"metrics\": {},\n    }\n\n    # ----------------------------------------------------------------------\n    # Per-run processing\n    # ----------------------------------------------------------------------\n    for run_id in run_ids:\n        history, summary, _ = _retrieve_run(entity, project, run_id)\n\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        (run_dir / \"metrics.json\").write_text(json.dumps(summary, indent=2))\n\n        # learning curve figure\n        plot_metric = \"train_loss\" if \"train_loss\" in history.columns else history.columns[0]\n        lc_name = f\"{run_id}_learning_curve_{uuid4().hex[:4]}.pdf\"\n        _save_learning_curve(history, run_dir / lc_name, plot_metric)\n        print(run_dir / lc_name)\n\n        # confusion matrix if preds / labels available\n        if {\"val_preds\", \"val_labels\"}.issubset(history.columns):\n            y_pred = history[\"val_preds\"].dropna().astype(int).tolist()\n            y_true = history[\"val_labels\"].dropna().astype(int).tolist()\n            if y_true and y_pred:\n                cm_name = f\"{run_id}_confusion_matrix_{uuid4().hex[:4]}.pdf\"\n                _save_confusion_matrix(y_true, y_pred, run_dir / cm_name)\n                print(run_dir / cm_name)\n\n        # collect scalar metrics\n        for k, v in summary.items():\n            if isinstance(v, (float, int)):\n                aggregated.setdefault(\"metrics\", {}).setdefault(k, {})[run_id] = v\n\n    # ----------------------------------------------------------------------\n    # Aggregated analysis\n    # ----------------------------------------------------------------------\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n\n    primary_values = aggregated[\"metrics\"].get(PRIMARY_METRIC, {})\n    baseline_candidates = {\n        k: v for k, v in primary_values.items() if any(t in k for t in (\"baseline\", \"comparative\"))\n    }\n    proposed_candidates = {k: v for k, v in primary_values.items() if \"proposed\" in k or \"HEST\" in k}\n\n    best_baseline_id, best_baseline_val = (None, None)\n    if baseline_candidates:\n        best_baseline_id, best_baseline_val = min(baseline_candidates.items(), key=lambda kv: kv[1])\n\n    best_proposed_id, best_proposed_val = (None, None)\n    if proposed_candidates:\n        best_proposed_id, best_proposed_val = min(proposed_candidates.items(), key=lambda kv: kv[1])\n\n    gap = None\n    if best_baseline_val is not None and best_proposed_val is not None:\n        gap = (best_baseline_val - best_proposed_val) / best_baseline_val * 100.0\n\n    aggregated[\"best_proposed\"] = {\"run_id\": best_proposed_id, \"value\": best_proposed_val}\n    aggregated[\"best_baseline\"] = {\"run_id\": best_baseline_id, \"value\": best_baseline_val}\n    aggregated[\"gap\"] = gap\n\n    # t-test\n    if baseline_candidates and proposed_candidates:\n        t_stat, p_val = stats.ttest_ind(\n            list(baseline_candidates.values()),\n            list(proposed_candidates.values()),\n            equal_var=False,\n        )\n        aggregated[\"t_test\"] = {\"statistic\": t_stat, \"p_value\": p_val}\n\n    (comparison_dir / \"aggregated_metrics.json\").write_text(json.dumps(aggregated, indent=2))\n    print(comparison_dir / \"aggregated_metrics.json\")\n\n    # comparison bar & box plots for primary metric\n    if primary_values:\n        bar_name = f\"comparison_{PRIMARY_METRIC}_bar_{uuid4().hex[:4]}.pdf\"\n        _comparison_bar(primary_values, comparison_dir / bar_name, PRIMARY_METRIC)\n        print(comparison_dir / bar_name)\n\n        box_name = f\"comparison_{PRIMARY_METRIC}_box_{uuid4().hex[:4]}.pdf\"\n        _comparison_box(primary_values, comparison_dir / box_name, PRIMARY_METRIC)\n        print(comparison_dir / box_name)\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "# src/preprocess.py\n\"\"\"Data-loading & tokenisation utilities (generic seq2seq pipeline).\"\"\"\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Tuple\n\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq\n\nCACHE_DIR = \".cache/\"\n\n\ndef _tokenise_function(examples: dict, tokenizer: AutoTokenizer, cfg: DictConfig):\n    model_inputs = tokenizer(\n        examples[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[cfg.dataset.summary_column],\n            max_length=cfg.evaluation.generate_max_length,\n            truncation=True,\n            padding=\"max_length\",\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\ndef build_dataloaders(cfg: DictConfig, tokenizer: AutoTokenizer) -> Tuple[DataLoader, DataLoader]:\n    raw_dataset = load_dataset(cfg.dataset.name, cfg.dataset.subset, cache_dir=CACHE_DIR)\n\n    tokenised = raw_dataset.map(\n        functools.partial(_tokenise_function, tokenizer=tokenizer, cfg=cfg),\n        batched=True,\n        num_proc=cfg.dataset.num_workers,\n        remove_columns=raw_dataset[cfg.dataset.train_split].column_names,\n    )\n\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"longest\")\n\n    train_dl = DataLoader(\n        tokenised[cfg.dataset.train_split],\n        shuffle=True,\n        batch_size=cfg.dataset.batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=data_collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n\n    val_dl = DataLoader(\n        tokenised[cfg.dataset.validation_split],\n        shuffle=False,\n        batch_size=cfg.evaluation.eval_batch_size,\n        num_workers=cfg.dataset.num_workers,\n        collate_fn=data_collator,\n        pin_memory=cfg.training.pin_memory,\n        persistent_workers=cfg.training.persistent_workers,\n    )\n    return train_dl, val_dl\n",
    "model_py": "# src/model.py\n\"\"\"Model architectures for *proposed* (HEST) and *baseline* (TABS) methods.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n\nENERGY_FP16 = 1.0  # arbitrary units (relative)\nENERGY_INT8 = 0.6\n\n# ----------------------------------------------------------------------------------\n# Controller & adapters\n# ----------------------------------------------------------------------------------\n\nclass FactorisedController(nn.Module):\n    \"\"\"σ(W_tok h) · σ(w_layer) – factorised token × layer controller.\"\"\"\n\n    def __init__(self, hidden_size: int, k: int, num_layers: int):\n        super().__init__()\n        self.W_tok = nn.Linear(hidden_size, k, bias=False)\n        self.w_layer = nn.Parameter(torch.zeros(num_layers, k))\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:  # (b, seq, hidden)\n        s_tok = torch.sigmoid(self.W_tok(hidden))  # (b, seq, k)\n        s_layer = torch.sigmoid(self.w_layer[layer_idx])  # (k,)\n        g = (s_tok * s_layer).mean(-1)  # (b, seq)\n        return g\n\n\nclass HESTAdapter(nn.Module):\n    \"\"\"Hierarchical Energy-constrained Spectral Tuning adapter (energy only).\"\"\"\n\n    def __init__(self, hidden_size: int, cfg: DictConfig):\n        super().__init__()\n        self.R = cfg.model.spectral_rank_R\n        self.temperature = cfg.training.gumbel_temperature_tau\n        self.controller = FactorisedController(hidden_size, cfg.model.controller_width_k, cfg.model.num_layers)\n        self.register_buffer(\"e_fp16\", torch.tensor(ENERGY_FP16))\n        self.register_buffer(\"e_int8\", torch.tensor(ENERGY_INT8))\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:  # returns energy scalar\n        bsz, seq_len, _ = hidden.shape\n        g_token = self.controller(hidden, layer_idx)  # (b, seq)\n        g_tok_dir = g_token.unsqueeze(-1).repeat(1, 1, self.R)  # (b, seq, R)\n\n        # logits for 3-way gating {off, int8, fp16}\n        logits = torch.stack([\n            torch.zeros_like(g_tok_dir),  # off\n            g_tok_dir,                   # int8\n            2 * g_tok_dir,               # fp16 (higher energy)\n        ], dim=-1)  # (b, seq, R, 3)\n\n        probs = F.gumbel_softmax(logits, tau=self.temperature, hard=False, dim=-1)\n        energy = (probs[..., 1] * self.e_int8 + probs[..., 2] * self.e_fp16).sum()\n        return energy  # scalar tensor\n\n\nclass TABSAdapter(nn.Module):\n    \"\"\"Baseline Token-wise Adaptive Basis Selection (always FP16).\"\"\"\n\n    def forward(self, hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:  # noqa: D401\n        del hidden, layer_idx\n        return torch.tensor(0.0, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ----------------------------------------------------------------------------------\n# Wrapper model that augments HF outputs with `.energy`\n# ----------------------------------------------------------------------------------\n\nclass EnergyWrapperModel(nn.Module):\n    def __init__(self, backbone: nn.Module, adapter: nn.Module, num_layers: int):\n        super().__init__()\n        self.backbone = backbone\n        self.adapter = adapter\n        self.num_layers = num_layers\n\n    def forward(self, **kwargs):  # type: ignore[override]\n        kwargs.setdefault(\"output_hidden_states\", True)\n        outputs = self.backbone(**kwargs)\n\n        hidden_states = getattr(outputs, \"hidden_states\", None)\n        if hidden_states is None:\n            hidden_states = getattr(outputs, \"encoder_hidden_states\", None)\n        if hidden_states is not None and isinstance(hidden_states, (list, tuple)):\n            energy_total = 0.0\n            # skip embeddings (idx 0)\n            for layer_idx, h in enumerate(hidden_states[1:]):\n                energy_total = energy_total + self.adapter(h, layer_idx)\n            outputs.energy = energy_total\n        else:\n            outputs.energy = torch.tensor(0.0, device=next(self.parameters()).device)\n        return outputs\n\n# ----------------------------------------------------------------------------------\n# Factory\n# ----------------------------------------------------------------------------------\n\ndef build_model(cfg: DictConfig, tokenizer) -> nn.Module:  # noqa: D401\n    if \"Seq2Seq\" in cfg.model.name or \"bart\" in cfg.model.name.lower() or cfg.dataset.task == \"summarisation\":\n        backbone = AutoModelForSeq2SeqLM.from_pretrained(\n            cfg.model.name,\n            cache_dir=\".cache/\",\n            torch_dtype=torch.float16 if cfg.training.precision == \"fp16\" else torch.float32,\n        )\n    else:\n        backbone = AutoModelForCausalLM.from_pretrained(\n            cfg.model.name,\n            cache_dir=\".cache/\",\n            torch_dtype=torch.float16 if cfg.training.precision == \"fp16\" else torch.float32,\n        )\n\n    for p in backbone.parameters():\n        p.requires_grad = False  # PEFT – only adapter learns\n\n    if cfg.method.lower() in {\"hest\", \"proposed\"}:\n        adapter = HESTAdapter(backbone.config.hidden_size, cfg)\n    else:\n        adapter = TABSAdapter()\n\n    num_layers = getattr(backbone.config, \"num_hidden_layers\", cfg.model.num_layers)\n    return EnergyWrapperModel(backbone, adapter, num_layers)\n",
    "main_py": "# src/main.py\n\"\"\"Lightweight launcher – spawns the *actual* training script as a subprocess.\nCLI example:\n    uv run python -u -m src.main run=<run_id> results_dir=<path> mode=<trial|full>\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # merge run-specific YAML\n    run_cfg_path = Path(__file__).resolve().parents[1] / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run-config not found: {run_cfg_path}\")\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n\n    # mode tweaks\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    logger.info(\"Final config (main):\\n%s\", OmegaConf.to_yaml(cfg))\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"mode={cfg.mode}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"wandb.mode={cfg.wandb.mode}\",\n        f\"optuna.n_trials={cfg.optuna.n_trials}\",\n        f\"training.epochs={cfg.training.epochs}\",\n    ]\n    if cfg.mode == \"trial\":\n        overrides.append(\"training.batch_limit=2\")\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    logger.info(\"Launching training subprocess:\\n%s\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"hest-experiments\"\nversion = \"0.1.0\"\ndescription = \"Reproducible experiments for HEST vs. TABS\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.2.0\"\ntransformers = \">=4.38.0\"\ndatasets = \">=2.15.0\"\noptuna = \">=3.5.0\"\nwandb = \">=0.16.0\"\nhydra-core = \">=1.3.2\"\nmatplotlib = \">=3.8.0\"\nseaborn = \">=0.13.0\"\npynvml = \">=11.5.0\"\nevaluate = \">=0.4.0\"\nscipy = \">=1.11.0\"\naccelerate = \">=0.25.0\"\nscikit-learn = \"*\"\n\n[tool.setuptools.package-data]\n\"\" = [\"config/**/*.yaml\"]\n",
    "config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Base Hydra configuration – merged first, then overridden by run-specific YAML\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - _self_\n\nrun: null          # will be provided via CLI (e.g. proposed-iter1-…)\nmode: full         # \"trial\" or \"full\"\nresults_dir: ./results\n\n# -------------------------------------------------------------------------\n# WandB – credentials via env var WANDB_API_KEY\n# -------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online\n\n# -------------------------------------------------------------------------\n# Optuna – default disabled (n_trials=0)\n# -------------------------------------------------------------------------\noptuna:\n  n_trials: 0\n  direction: minimize\n  timeout_min: 0\n  search_space: {}\n\n# -------------------------------------------------------------------------\n# Training defaults\n# -------------------------------------------------------------------------\ntraining:\n  epochs: 1\n  learning_rate: 3e-5\n  warmup_steps: 0\n  weight_decay: 0.0\n  beta1: 0.9\n  beta2: 0.999\n  eps: 1e-8\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  precision: fp16\n  clamp_grad: false\n  batch_limit: null\n  log_interval_steps: 50\n  seed: 42\n  pin_memory: true\n  persistent_workers: false\n  lagrangian_init: 0.1\n  dual_step_size_alpha: 0.05\n  gumbel_temperature_tau: 0.67\n\nhardware:\n  device: cuda\n  allow_tf32: true\n\n# Further sections (model, dataset, evaluation) come from run-specific YAMLs\n# -----------------------------------------------------------------------------\n"
}
