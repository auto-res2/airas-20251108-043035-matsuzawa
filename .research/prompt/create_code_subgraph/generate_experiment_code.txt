
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/runs/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
      ```json
      {
        "primary_metric": "Mean Joules per sample at equal or better Rouge-L.",
        "metrics": {
          "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
          "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
        },
        "best_proposed": {
          "run_id": "proposed-iter2-model-dataset",
          "value": 0.92
        },
        "best_baseline": {
          "run_id": "comparative-1-model-dataset",
          "value": 0.88
        },
        "gap": 4.55
      }
      ```
      The structure includes:
      - "primary_metric": The primary evaluation metric name from the hypothesis (Mean Joules per sample at equal or better Rouge-L.)
      - "metrics": All collected metrics organized by metric name, then by run_id
      - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
      - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
      - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
        * Use the expected results from the hypothesis (HEST matches TABS on Rouge-L (≈44.3) while cutting mean energy by ≥40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8× on the longest sequences.) to determine metric direction
        * If the metric should be minimized (like "loss", "perplexity", "error"), reverse the sign of the gap
        * The gap represents the percentage improvement of the proposed method over the best baseline
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 2025-1108-1
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Code does not satisfy several mandatory requirements:
1. Missing Hydra run configs: config/config.yaml refers to the group `runs:${run}` but the file-structure rule allows only config/config.yaml. Because the mandatory files `config/runs/*.yaml` are absent, Hydra will crash at launch.
2. Train script accesses `accelerator.state._setup.tokenizer`, an internal attribute that does not exist; this raises AttributeError before data loading starts.
3. Energy accounting is broken: `_patch_layer` stores per-batch energy in an external list (`energy_buffer`) but `EnergyAwareModel.energy` reads an internal list that is never populated, so `outputs.energy` is always 0.0, invalidating the HEST energy constraint and primary metric.
4. Evaluation script is incomplete: it generates only learning-curve and simple bar-chart figures, omitting the required confusion matrices, box plots and statistical significance tests demanded in STEP 2 of the specification.
5. `TABSAdapter.set_lambda` is a stub (`pass`), violating the "no placeholders" rule even if unused.

Because of these issues the code is not immediately executable or publication-ready.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra config \u2013 base values overridden by config/runs/*.yaml\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - _self_\n  - runs: ${run}\n\nrun: ??        # will be provided via CLI\nmode: full      # trial | full\nresults_dir: ./results\n\n# -----------------------------------------------------------------------------\n# WandB -----------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online      # switched to \"disabled\" in trial mode\n\n# -----------------------------------------------------------------------------\n# Optuna (global defaults \u2013 may be overridden by run-specific YAML) -----------\noptuna:\n  n_trials: 0\n  direction: minimize\n  timeout_min: 0\n  search_space: {}\n\n# -----------------------------------------------------------------------------\n# Training (defaults) ---------------------------------------------------------\ntraining:\n  epochs: 1\n  learning_rate: 3e-5\n  warmup_steps: 0\n  weight_decay: 0.0\n  beta1: 0.9\n  beta2: 0.999\n  eps: 1e-8\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  precision: fp16            # fp16 | fp32\n  batch_limit: null          # used in trial mode to cut after N batches\n  log_interval_steps: 50\n  seed: 42\n  pin_memory: true\n  persistent_workers: false\n  lagrangian_init: 0.1\n  dual_step_size_alpha: 0.05\n  gumbel_temperature_tau: 0.67\n  early_stopping_patience: 3\n\n# -----------------------------------------------------------------------------\n# Evaluation defaults ---------------------------------------------------------\nevaluation:\n  eval_batch_size: 4\n  generate_max_length: 128\n  num_beams: 4\n\nhardware:\n  device: cuda\n  allow_tf32: true\n\n# dataset / model sections are supplied by each run-specific YAML\n# -----------------------------------------------------------------------------\n", "evaluate_py": "# src/evaluate.py\n\"\"\"Cross-run evaluation \u0026 aggregated visualisation.\nFetches runs from the WandB back-end, stores metrics/figures locally and\ncreates aggregated comparison charts.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\n\ndef _save_json(data: Dict, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(data, fp, indent=2)\n\n\ndef _plot_learning_curve(df, run_id: str, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    if \"train/loss\" in df:\n        sns.lineplot(data=df, x=\"step\", y=\"train/loss\", label=\"train_loss\", ax=ax)\n    if \"val/loss\" in df:\n        sns.lineplot(data=df, x=\"step\", y=\"val/loss\", label=\"val_loss\", ax=ax)\n    ax.set_title(f\"Learning curve \u2013 {run_id}\")\n    ax.set_xlabel(\"step\")\n    ax.set_ylabel(\"loss\")\n    ax.legend()\n    fig.tight_layout()\n    fig_path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(fig_path)\n    plt.close(fig)\n    print(fig_path.resolve())\n\n\ndef _plot_metric_bar(metric_map: Dict[str, Dict[str, float]], metric_name: str, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    run_ids = list(metric_map[metric_name].keys())\n    values = list(metric_map[metric_name].values())\n    sns.barplot(x=run_ids, y=values, ax=ax)\n    ax.set_xticklabels(run_ids, rotation=45, ha=\"right\")\n    for i, v in enumerate(values):\n        ax.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n    ax.set_title(metric_name)\n    fig.tight_layout()\n    fig_path = out_dir / f\"comparison_{metric_name.replace(\u0027/\u0027, \u0027_\u0027)}_bar_chart.pdf\"\n    fig.savefig(fig_path)\n    plt.close(fig)\n    print(fig_path.resolve())\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"output directory\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list of run IDs\")\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir)\n\n    # -------------------------------------------------------------------------\n    cfg_global = OmegaConf.load(\"config/config.yaml\")\n    entity = cfg_global.wandb.entity\n    project = cfg_global.wandb.project\n\n    api = wandb.Api()\n\n    metric_map: Dict[str, Dict[str, float]] = {}\n\n    for run_id in run_ids:\n        print(f\"Processing {run_id} \u2026\")\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history = run.history(keys=[\"step\"], pandas=True)  # all keys loaded lazily\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        # Persist raw metrics ---------------------------------------------------\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        _save_json(summary, run_dir / \"metrics.json\")\n        _save_json(config, run_dir / \"config.json\")\n\n        # Learning curve --------------------------------------------------------\n        _plot_learning_curve(history, run_id, run_dir)\n\n        # Collect for aggregated table -----------------------------------------\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                metric_map.setdefault(k, {})[run_id] = v\n\n    # -------------------------------------------------------------------------\n    # Aggregated comparison\n    # -------------------------------------------------------------------------\n    comp_dir = results_root / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    # Primary metric -----------------------------------------------------------\n    primary_metric = \"mean_joules_per_sample\"\n    proposed_best_run, baseline_best_run = None, None\n    proposed_best_val, baseline_best_val = float(\"inf\"), float(\"inf\")\n    for run_id, val in metric_map.get(primary_metric, {}).items():\n        if \"proposed\" in run_id and val \u003c proposed_best_val:\n            proposed_best_val, proposed_best_run = val, run_id\n        if (\"baseline\" in run_id or \"comparative\" in run_id) and val \u003c baseline_best_val:\n            baseline_best_val, baseline_best_run = val, run_id\n\n    gap_pct = (\n        (baseline_best_val - proposed_best_val) / baseline_best_val * 100.0 if baseline_best_val else 0.0\n    )\n\n    aggregated = {\n        \"primary_metric\": \"Mean Joules per sample at equal or better Rouge-L.\",\n        \"metrics\": metric_map,\n        \"best_proposed\": {\"run_id\": proposed_best_run, \"value\": proposed_best_val},\n        \"best_baseline\": {\"run_id\": baseline_best_run, \"value\": baseline_best_val},\n        \"gap\": gap_pct,\n    }\n    _save_json(aggregated, comp_dir / \"aggregated_metrics.json\")\n\n    # Bar charts for key metrics ----------------------------------------------\n    for metric_name in [primary_metric, \"val_rougeL\", \"latency_p99\"]:\n        if metric_name in metric_map:\n            _plot_metric_bar(metric_map, metric_name, comp_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "# src/main.py\n\"\"\"Hydra orchestrator that spawns *src.train* as a subprocess.\n\nUsage example:\nuv run python -u -m src.main run=proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation \\\n              results_dir=./results mode=full\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\n\ndef _build_overrides(cfg: DictConfig) -\u003e List[str]:\n    return [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg: DictConfig):  # noqa: D401\n    # ---------------- Mode-specific patching ----------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be trial|full\")\n\n    # Store patched config for transparency ------------------------------------\n    run_dir = Path(cfg.results_dir) / cfg.run\n    run_dir.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, run_dir / \"composed_config.yaml\")\n\n    # Spawn training subprocess -------------------------------------------------\n    python = sys.executable\n    cmd = [python, \"-u\", \"-m\", \"src.train\"] + _build_overrides(cfg)\n    print(\"Launching:\", \" \".join(cmd))\n    env = os.environ.copy()\n    subprocess.check_call(cmd, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "# src/model.py\n\"\"\"Model + adapter construction utilities.\n\nThe module creates *energy-aware* variants of HuggingFace causal/seq2seq\nmodels and injects either HEST or TABS adapters into each transformer\nlayer.  The adapters are truly learnable and contribute to the forward\npass, yet are kept lightweight (O(hidden_dim \u00d7 R)).\n\"\"\"\nfrom __future__ import annotations\n\nimport types\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM, PreTrainedModel\n\n# -----------------------------------------------------------------------------\n# Low-rank spectral adapter building blocks\n# -----------------------------------------------------------------------------\n\nclass FactorisedController(nn.Module):\n    \"\"\"g(h_{\u2113,t}) = \u03c3(W_tok h_{\u2113,t}) \u00b7 \u03c3(w_\u2113)  \u2013 rank-1 factorisation.\"\"\"\n\n    def __init__(self, hidden_dim: int, num_layers: int, k: int):\n        super().__init__()\n        self.W_tok = nn.Linear(hidden_dim, k, bias=False)\n        self.W_layer = nn.Parameter(torch.zeros(num_layers, k))\n\n    def forward(self, h: torch.Tensor, layer_id: int):\n        # h: (B, T, d)\n        s_tok = torch.sigmoid(self.W_tok(h))  # (B, T, k)\n        s_layer = torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        gate = (s_tok * s_layer).mean(-1)  # (B, T)\n        return gate  # in [0,1]\n\n\nclass BaseSpectralAdapter(nn.Module):\n    def __init__(self, hidden_dim: int, rank: int, init_std: float):\n        super().__init__()\n        self.U = nn.Parameter(torch.randn(hidden_dim, rank) * init_std)\n        self.V = nn.Parameter(torch.randn(rank, hidden_dim) * init_std)\n        self.S = nn.Parameter(torch.ones(rank))  # singular values\n\n    def spectral_forward(self, x: torch.Tensor):\n        # x:(B,T,d) \u00b7 U:(d,R) -\u003e (B,T,R)  * S  -\u003e (B,T,R) -\u003e (B,T,d)\n        proj = torch.matmul(x, self.U) * self.S  # (B,T,R)\n        out = torch.matmul(proj, self.V)  # (B,T,d)\n        return out\n\n\nclass HESTAdapter(BaseSpectralAdapter):\n    \"\"\"Implements 3-way Gumbel-Softmax precision/off gating + energy counting.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        rank: int,\n        layer_id: int,\n        num_layers: int,\n        k: int,\n        energy_fp16: float = 1.0,\n        energy_int8: float = 0.5,\n        tau: float = 2 / 3,\n        init_std: float = 0.02,\n    ):\n        super().__init__(hidden_dim, rank, init_std)\n        self.ctrl = FactorisedController(hidden_dim, num_layers, k)\n        self.energy_fp16 = energy_fp16\n        self.energy_int8 = energy_int8\n        self.tau = tau\n        self.layer_id = layer_id\n        self.current_lambda = None  # set by trainer each batch\n        self.last_energy = torch.tensor(0.0)\n\n    def set_lambda(self, lam):\n        self.current_lambda = lam\n\n    def forward(self, x: torch.Tensor):  # x:(B,T,d)\n        p_gate = self.ctrl(x, self.layer_id)  # (B,T)\n        # expand to R directions\n        p_gate = p_gate.unsqueeze(-1).expand(-1, -1, self.S.numel())  # (B,T,R)\n        logits = torch.log(p_gate + 1e-6).unsqueeze(-1).expand(-1, -1, -1, 3)\n        g = F.gumbel_softmax(logits, tau=self.tau, hard=False, dim=-1)  # (...,3)\n        mask_off, mask_int8, mask_fp16 = g.unbind(-1)  # each (B,T,R)\n        E_batch = (mask_int8 * self.energy_int8 + mask_fp16 * self.energy_fp16).mean()\n        self.last_energy = E_batch.detach()\n        out = self.spectral_forward(x)  # (B,T,d)\n        # Combine precisions (int8 path approximated as fp16-cast for simplicity)\n        out = out * (mask_int8 + mask_fp16)  # broadcasting \u2013 off=0\n        return out\n\n\nclass TABSAdapter(BaseSpectralAdapter):\n    \"\"\"Token-wise gating \u2013 no precision choice (always fp16).\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        rank: int,\n        layer_id: int,\n        num_layers: int,\n        tau: float = 2 / 3,\n        init_std: float = 0.02,\n    ):\n        super().__init__(hidden_dim, rank, init_std)\n        self.ctrl = FactorisedController(hidden_dim, num_layers, 1)\n        self.tau = tau\n        self.layer_id = layer_id\n        self.last_energy = torch.tensor(0.0)\n\n    def set_lambda(self, lam):\n        pass  # unused \u2013 no energy constraint\n\n    def forward(self, x: torch.Tensor):\n        gate = self.ctrl(x, self.layer_id)  # (B,T)\n        mask = (gate \u003e 0.5).float().unsqueeze(-1)  # simple hard gate\n        self.last_energy = mask.mean()  # proxy energy = #tokens processed\n        out = self.spectral_forward(x) * mask\n        return out\n\n\n# -----------------------------------------------------------------------------\n# Injection helpers\n# -----------------------------------------------------------------------------\n\ndef _patch_layer(layer: nn.Module, adapter: nn.Module, energy_buffer: List[torch.Tensor]):\n    \"\"\"Monkey-patch ``layer.forward`` to inject *adapter* output.\"\"\"\n\n    orig_fwd = layer.forward\n\n    def new_forward(self, *args, **kwargs):  # noqa: D401 \u2013 signature kept\n        outputs = orig_fwd(*args, **kwargs)\n        hidden = outputs[0] if isinstance(outputs, tuple) else outputs  # (B,T,d)\n        hidden = hidden + adapter(hidden)\n        energy_buffer.append(adapter.last_energy)\n        if isinstance(outputs, tuple):\n            outputs = (hidden,) + outputs[1:]\n        else:\n            outputs = hidden\n        return outputs\n\n    layer.forward = types.MethodType(new_forward, layer)  # type: ignore[arg-type]\n\n\n# -----------------------------------------------------------------------------\n# Public factory\n# -----------------------------------------------------------------------------\n\nclass EnergyAwareModel(PreTrainedModel):\n    \"\"\"Wraps a HF causal LM and aggregates per-batch energy statistics.\"\"\"\n\n    def __init__(self, base: PreTrainedModel, adapters: List[nn.Module]):\n        config = base.config\n        super().__init__(config)\n        self.base = base\n        self.adapters = nn.ModuleList(adapters)\n        self._energy_records: List[torch.Tensor] = []\n\n    def set_lambda(self, lam):\n        for adp in self.adapters:\n            if hasattr(adp, \"set_lambda\"):\n                adp.set_lambda(lam)\n\n    @property\n    def energy(self):\n        return torch.stack(self._energy_records).mean() if self._energy_records else torch.tensor(0.0)\n\n    def forward(self, *args, **kwargs):  # noqa: D401\n        self._energy_records.clear()\n        out = self.base(*args, **kwargs)\n        energy_batch = self.energy\n        out.energy = energy_batch\n        return out\n\n    # Proxy generate -----------------------------------------------------------\n    def generate(self, *args, **kwargs):  # noqa: D401\n        return self.base.generate(*args, **kwargs)\n\n\n# -----------------------------------------------------------------------------\n\n\ndef build_energy_aware_model(cfg, tokenizer):\n    \"\"\"Load backbone + inject adapters according to *cfg.model.adapter*.\"\"\"\n\n    base = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=(torch.float16 if cfg.training.precision == \"fp16\" else torch.float32),\n        device_map=\"auto\",\n    )\n    if tokenizer.pad_token_id is None:\n        tokenizer.add_special_tokens({\"pad_token\": \"\u003cpad\u003e\"})\n        base.resize_token_embeddings(len(tokenizer))\n\n    adapters: List[nn.Module] = []\n    energy_buffer: List[torch.Tensor] = []\n\n    num_layers = cfg.model.num_layers\n    d_model = cfg.model.hidden_size\n    rank = cfg.model.spectral_rank_R\n\n    for i, layer in enumerate(base.model.layers):  # type: ignore[attr-defined]\n        if cfg.model.adapter == \"HEST\":\n            adapter = HESTAdapter(\n                hidden_dim=d_model,\n                rank=rank,\n                layer_id=i,\n                num_layers=num_layers,\n                k=cfg.model.controller_width_k,\n                energy_fp16=1.0,\n                energy_int8=0.5,\n                tau=cfg.training.gumbel_temperature_tau,\n                init_std=cfg.model.init_std,\n            )\n        else:  # TABS baseline\n            adapter = TABSAdapter(\n                hidden_dim=d_model,\n                rank=rank,\n                layer_id=i,\n                num_layers=num_layers,\n                tau=cfg.model.get(\"gating_temperature_tau\", 0.67),\n                init_std=cfg.model.init_std,\n            )\n        adapters.append(adapter)\n        _patch_layer(layer, adapter, energy_buffer)\n\n    energy_model = EnergyAwareModel(base, adapters)\n    return energy_model\n", "preprocess_py": "# src/preprocess.py\n\"\"\"Dataset loading \u0026 tokenisation utilities.\n\nThis module supports:\n  * Summarisation   \u2013 CNN/DailyMail (\"cnn_dailymail\")\n  * Text-classification tasks (GLUE style)\n\nReturned *DataLoader*s yield dicts compatible with HuggingFace causal/seq2seq\nmodels (i.e. contain the keys ``input_ids``, ``attention_mask`` \u0026 ``labels``).\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Tuple\n\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Tokenisation helpers\n# -----------------------------------------------------------------------------\n\ndef _mask_pad(token_ids: list[int], pad_id: int):\n    return [-100 if tok == pad_id else tok for tok in token_ids]\n\n\ndef _tok_summarisation(examples, *, tokenizer: AutoTokenizer, cfg: DictConfig):\n    model_inputs = tokenizer(\n        examples[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[cfg.dataset.summary_column],\n            max_length=cfg.evaluation.generate_max_length,\n            truncation=True,\n            padding=\"max_length\",\n        )\n    pad_id = tokenizer.pad_token_id\n    labels_proc = [_mask_pad(seq, pad_id) for seq in labels[\"input_ids\"]]\n    model_inputs[\"labels\"] = labels_proc\n    return model_inputs\n\n\ndef _tok_classification(examples, *, tokenizer: AutoTokenizer, cfg: DictConfig):\n    out = tokenizer(\n        examples[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    out[\"labels\"] = examples[cfg.dataset.label_column]\n    return out\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(\n    cfg: DictConfig, tokenizer: AutoTokenizer\n) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    raw = load_dataset(cfg.dataset.name, cfg.dataset.subset, cache_dir=CACHE_DIR)\n\n    if cfg.dataset.task == \"summarisation\":\n        map_fn = functools.partial(_tok_summarisation, tokenizer=tokenizer, cfg=cfg)\n    else:\n        map_fn = functools.partial(_tok_classification, tokenizer=tokenizer, cfg=cfg)\n\n    proc = raw.map(\n        map_fn,\n        batched=True,\n        num_proc=cfg.dataset.num_workers,\n        remove_columns=raw[cfg.dataset.train_split].column_names,\n    )\n\n    collator = DataCollatorWithPadding(\n        tokenizer,\n        pad_to_multiple_of=8 if cfg.training.precision == \"fp16\" else None,\n    )\n\n    def mk_loader(split: str, shuffle: bool, batch_size: int):\n        return DataLoader(\n            proc[split],\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_workers=cfg.dataset.num_workers,\n            collate_fn=collator,\n            pin_memory=cfg.training.pin_memory,\n            persistent_workers=cfg.training.persistent_workers,\n        )\n\n    train_dl = mk_loader(cfg.dataset.train_split, True, cfg.dataset.batch_size)\n    val_dl = mk_loader(cfg.dataset.validation_split, False, cfg.evaluation.eval_batch_size)\n    test_split = cfg.dataset.get(\"test_split\", cfg.dataset.validation_split)\n    test_dl = mk_loader(test_split, False, cfg.evaluation.eval_batch_size)\n    return train_dl, val_dl, test_dl\n", "pyproject_toml": "[project]\nname = \"hest-experiments\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndescription = \"Reproducible HEST vs TABS experiments\"\n\n[project.dependencies]\ntorch = \"\u003e=2.2.0\"\ntransformers = \"\u003e=4.38.0\"\ndatasets = \"\u003e=2.15.0\"\noptuna = \"\u003e=3.5.0\"\nwandb = \"\u003e=0.16.0\"\nhydra-core = \"\u003e=1.3.2\"\nmatplotlib = \"\u003e=3.8.0\"\nseaborn = \"\u003e=0.13.0\"\npynvml = \"*\"\nevaluate = \"*\"\nscipy = \"*\"\nscikit-learn = \"*\"\nrouge-score = \"*\"\npandas = \"\u003e=2.0.0\"\nnumpy = \"\u003e=1.24.0\"\ntqdm = \"*\"\naccelerate = \"*\"\n", "train_py": "# src/train.py\n\"\"\"Single-run training script.\nTrains one experiment according to a Hydra config.  The script handles\n  * mode-aware behaviour (trial/full)\n  * Optuna hyper-parameter search (search first, final run once with best params)\n  * complete WandB life-cycle\n  * energy-constrained training for HEST (dual-ascent on the Lagrange multiplier)\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport sys\nimport time\nimport types\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom accelerate import Accelerator\nfrom datasets import load_metric\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch import nn\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\n\nsys.path.append(str(Path(__file__).resolve().parent))  # to import sibling modules\nfrom model import build_energy_aware_model  # noqa: E402  pylint: disable=wrong-import-position\nfrom preprocess import build_dataloaders  # noqa: E402  pylint: disable=wrong-import-position\n\nCACHE_DIR = \".cache/\"\n\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef _seed_everything(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _wandb_init(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n    )\n    print(f\"WandB URL: {run.url}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Training + validation one single set of hyper-parameters\n# -----------------------------------------------------------------------------\n\ndef _train_one(\n    cfg: DictConfig,\n    accelerator: Accelerator,\n    trial_mode: bool = False,\n) -\u003e Dict[str, float]:\n    \"\"\"Train *exactly once* with the hyper-parameters coded in *cfg*.\n\n    Returns\n    -------\n    metrics : Dict[str, float]\n        Final validation metrics.\n    \"\"\"\n    device = accelerator.device\n    tokenizer_name = cfg.model.name\n    # --------------- Datasets -------------------------------------------------\n    tokenizer = accelerator.state._setup.tokenizer or None  # workaround accelerate internals\n    if tokenizer is None:  # we call the HF tokenizer directly here\n        from transformers import AutoTokenizer\n\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=CACHE_DIR, use_fast=True)\n        if tokenizer.pad_token_id is None:\n            tokenizer.add_special_tokens({\"pad_token\": \"\u003cpad\u003e\"})\n\n    train_dl, val_dl, test_dl = build_dataloaders(cfg, tokenizer)\n\n    # ---------------- Model ---------------------------------------------------\n    model = build_energy_aware_model(cfg, tokenizer)\n    model.to(device)\n\n    # ---------------- Optimiser + scheduler ----------------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    param_groups = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimiser = torch.optim.AdamW(\n        param_groups,\n        lr=cfg.training.learning_rate,\n        betas=(cfg.training.beta1, cfg.training.beta2),\n        eps=cfg.training.eps,\n    )\n    # Cosine schedule with warm-up ------------------------------------------------\n    total_train_steps = (\n        len(train_dl) // cfg.training.gradient_accumulation_steps * cfg.training.epochs\n    )\n    warmup_steps = cfg.training.warmup_steps\n\n    def lr_lambda(current_step: int):\n        if current_step \u003c warmup_steps:\n            return float(current_step) / float(max(1, warmup_steps))\n        progress = float(current_step - warmup_steps) / float(max(1, total_train_steps - warmup_steps))\n        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimiser, lr_lambda)\n\n    # ---------------- AMP ------------------------------------------------------\n    scaler = GradScaler(enabled=cfg.training.precision == \"fp16\")\n\n    # ---------------- Metrics --------------------------------------------------\n    rouge = load_metric(\"rouge\")\n\n    # ---------------- Energy/Lagrange vars ------------------------------------\n    lambda_param = torch.tensor(cfg.training.lagrangian_init, device=device)\n    E_target = cfg.model.energy_budget_ratio  # we view ratio w.r.t some reference energy=1.0\n\n    # ---------------- WandB ----------------------------------------------------\n    run = _wandb_init(cfg)\n\n    # ---------------- Prepare Accelerator -------------------------------------\n    model, optimiser, train_dl, val_dl = accelerator.prepare(model, optimiser, train_dl, val_dl)\n\n    # ---------------- Training loop -------------------------------------------\n    global_step = 0\n    best_val_rougeL: float | None = None\n    epochs_range = 1 if trial_mode else cfg.training.epochs\n\n    for epoch in range(epochs_range):\n        model.train()\n        epoch_pbar = tqdm(train_dl, dynamic_ncols=True, desc=f\"Epoch {epoch + 1}\")\n        optimiser.zero_grad(set_to_none=True)\n        for step, batch in enumerate(epoch_pbar):\n            with autocast(enabled=cfg.training.precision == \"fp16\"):\n                # Set \u03bb into all adapters -------------------------------------------------\n                model.module.set_lambda(lambda_param) if isinstance(model, torch.nn.DataParallel) else model.set_lambda(lambda_param)  # type: ignore[attr-defined]\n                outputs = model(**batch)\n                loss_primary = outputs.loss\n                batch_energy = outputs.energy.detach()\n                # (\u0112 \u2212 E_max)\n                energy_violation = batch_energy - E_target\n                total_loss = loss_primary + lambda_param * energy_violation\n                total_loss = total_loss / cfg.training.gradient_accumulation_steps\n            scaler.scale(total_loss).backward()\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                scaler.unscale_(optimiser)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                scaler.step(optimiser)\n                scaler.update()\n                scheduler.step()\n                optimiser.zero_grad(set_to_none=True)\n                global_step += 1\n\n                # Dual-ascent update ----------------------------------------------------\n                with torch.no_grad():\n                    lambda_param = torch.clamp(\n                        lambda_param + cfg.training.dual_step_size_alpha * energy_violation.mean(), min=0.0\n                    )\n\n                # Logging --------------------------------------------------------------\n                if run and global_step % cfg.training.log_interval_steps == 0:\n                    to_log = {\n                        \"train/loss\": loss_primary.item(),\n                        \"train/total_loss\": total_loss.item(),\n                        \"train/energy_mean\": batch_energy.item(),\n                        \"train/lambda\": lambda_param.item(),\n                        \"train/lr\": scheduler.get_last_lr()[0],\n                        \"step\": global_step,\n                        \"epoch\": epoch + (step / len(train_dl)),\n                    }\n                    run.log(to_log, step=global_step)\n\n                # Trial-mode quick exit -----------------------------------------------\n                if trial_mode and global_step \u003e= 2:\n                    break\n            epoch_pbar.set_postfix({\"loss\": f\"{loss_primary.item():.4f}\", \"E\": f\"{batch_energy.item():.3f}\"})\n        # ---------------- Validation (end of epoch) -----------------------------\n        model.eval()\n        val_losses, refs, hyps, energies, latencies = [], [], [], [], []\n        with torch.no_grad():\n            for batch in tqdm(val_dl, desc=\"Valid\", leave=False):\n                t0 = time.time()\n                outputs = model(**batch)\n                latency = time.time() - t0\n                val_losses.append(outputs.loss.item())\n                energies.append(outputs.energy.item())\n                latencies.append(latency)\n                # Generate summaries --------------------------------------------------\n                gen_ids = model.generate(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    max_new_tokens=cfg.evaluation.generate_max_length,\n                    num_beams=cfg.evaluation.num_beams,\n                )\n                preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n                targets = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n                hyps.extend(preds)\n                refs.extend(targets)\n                if trial_mode and len(hyps) \u003e= 4:\n                    break\n        # Compute ROUGE ------------------------------------------------------------\n        rouge_scores = rouge.compute(predictions=hyps, references=refs, use_agregator=True)\n        rougeL = rouge_scores[\"rougeLsum\"].mid.fmeasure * 100\n        mean_loss = float(np.mean(val_losses))\n        mean_energy = float(np.mean(energies))\n        p99_latency = float(np.percentile(latencies, 99))\n\n        if run:\n            run.log(\n                {\n                    \"val/rougeL\": rougeL,\n                    \"val/loss\": mean_loss,\n                    \"val/energy_mean\": mean_energy,\n                    \"val/latency_p99\": p99_latency,\n                    \"epoch\": epoch + 1,\n                },\n                step=global_step,\n            )\n\n        # Early stopping -----------------------------------------------------------\n        improved = best_val_rougeL is None or rougeL \u003e best_val_rougeL\n        if improved:\n            best_val_rougeL = rougeL\n            # Save checkpoint -------------------------------------------------------\n            chkpt_dir = Path(cfg.results_dir) / cfg.run.run_id / \"checkpoints\"\n            chkpt_dir.mkdir(parents=True, exist_ok=True)\n            accelerator.wait_for_everyone()\n            unwrapped = accelerator.unwrap_model(model)\n            accelerator.save(unwrapped.state_dict(), chkpt_dir / \"best.pt\")\n        else:\n            patience_left = cfg.training.early_stopping_patience - (epoch - (best_val_rougeL or 0))\n            if patience_left \u003c= 0:\n                break\n    # ---------------- Finalise ----------------------------------------------------\n    final_metrics = {\n        \"val_loss\": mean_loss,\n        \"val_rougeL\": rougeL,\n        \"mean_joules_per_sample\": mean_energy,\n        \"latency_p99\": p99_latency,\n    }\n    if run:\n        for k, v in final_metrics.items():\n            run.summary[k] = v\n        run.finish()\n    return final_metrics\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective wrapper\n# -----------------------------------------------------------------------------\n\ndef _objective(trial: optuna.Trial, cfg_base: DictConfig) -\u003e float:\n    cfg = OmegaConf.create(OmegaConf.to_container(cfg_base, resolve=True, throw_on_missing=True))\n    # Override parameters sampled by Optuna ---------------------------------------\n    for hp_name, hp_spec in cfg_base.optuna.search_space.items():\n        if hp_spec[\"type\"] == \"loguniform\":\n            val = trial.suggest_float(hp_name, hp_spec[\"low\"], hp_spec[\"high\"], log=True)\n        elif hp_spec[\"type\"] == \"uniform\":\n            val = trial.suggest_float(hp_name, hp_spec[\"low\"], hp_spec[\"high\"])\n        elif hp_spec[\"type\"] == \"categorical\":\n            val = trial.suggest_categorical(hp_name, hp_spec[\"choices\"])\n        else:\n            raise ValueError(f\"Unknown optuna type {hp_spec[\u0027type\u0027]}\")\n        # Apply value into cfg (supports nested keys via dotted path) --------------\n        OmegaConf.update(cfg, hp_name, val, merge=False)\n    # Disable wandb for trials ----------------------------------------------------\n    cfg.wandb.mode = \"disabled\"\n    accelerator = Accelerator(log_with=None)\n    result = _train_one(cfg, accelerator, trial_mode=True)\n    # We *minimise* mean energy for \u2265 baseline ROUGE, so return energy ------------\n    return result[\"mean_joules_per_sample\"]\n\n\n# -----------------------------------------------------------------------------\n# Hydra main entrypoint\n# -----------------------------------------------------------------------------\nimport hydra\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg: DictConfig):  # noqa: D401  pylint: disable=too-many-locals\n    \"\"\"Entry-point executed by subprocess call from *src.main*.\n\n    The current working directory has been changed by Hydra to\n    `hydra.run.dir`; therefore *cfg.results_dir* must be made absolute\n    before any file-system interaction.\n    \"\"\"\n    # ---------------- Mode injection --------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    # --------------------------------------------------------------------------\n    _seed_everything(cfg.training.seed)\n    accelerator = Accelerator()\n\n    # ---------------- Optuna ---------------------------------------------------\n    if cfg.optuna.n_trials \u003e 0:\n        sampler = optuna.samplers.TPESampler()\n        study = optuna.create_study(direction=cfg.optuna.direction, sampler=sampler)\n        study.optimize(\n            lambda trial: _objective(trial, cfg),\n            n_trials=cfg.optuna.n_trials,\n            timeout=cfg.optuna.timeout_min * 60 if cfg.optuna.timeout_min else None,\n            show_progress_bar=True,\n        )\n        best_params = study.best_trial.params\n        # Inject best params into config -----------------------------------------\n        for k, v in best_params.items():\n            OmegaConf.update(cfg, k, v, merge=False)\n        # Logging best params -----------------------------------------------------\n        print(\"Best hyper-parameters:\", json.dumps(best_params, indent=2))\n\n    # ---------------- Final run with best params -------------------------------\n    _train_one(cfg, accelerator, trial_mode=cfg.mode == \"trial\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length × layers × R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.' method='Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).' experimental_setup='Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6× TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.' primary_metric='Mean Joules per sample at equal or better Rouge-L.' experimental_code='import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,λ,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=λ*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc' expected_result='HEST matches TABS on Rouge-L (≈44.3) while cutting mean energy by ≥40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8× on the longest sequences.' expected_conclusion='A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget—rather than FLOPs alone—yields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP.'

# Current Research Method
Hierarchical Energy-constrained Spectral Tuning (HEST)
1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).
2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].
3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).
4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.
5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).

# Experimental Design
- Summary: We will fine-tune a pretrained Llama-2-7B-chat model on the CNN/DailyMail summarisation task using the proposed Hierarchical Energy-constrained Spectral Tuning (HEST) adapter. During training we jointly optimise task loss and an energy-budget Lagrangian so that, for every input sequence, the total expected energy of all spectral directions stays below a user-specified envelope. The controller is factorised across tokens and layers (rank-1 outer product) and produces a 3-way Gumbel-Softmax gate per spectral direction that selects one of {off, INT8, FP16}. After convergence we harden the gates and fuse INT8 choices into integer GEMM kernels; de-activated directions are skipped. We compare HEST against the strongest prior energy-oblivious spectral adapter, Token-wise Adaptive Basis Selection (TABS), under the same backbone, data split, and optimisation schedule. We report summarisation quality, energy consumed per sample (read with NVML), tail latency, and parameter-efficiency. A small grid search over learning-rate, controller width k, and the energy budget E_max is performed on the validation set. Experiments run on a single NVIDIA A100 80 GB GPU with plenty of system RAM.
- Evaluation metrics: ['Mean Joules per sample at equal or better Rouge-L', 'ROUGE-L', 'Accuracy', '99th percentile latency', 'Controller parameter count', 'Mean Joules per sample at equal or better Rouge-L.']

# Experiment Runs

- Run ID: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: proposed
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation
  Config File: config/runs/proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
method: HEST
model:
  name: Llama-2-7B-chat
  adapter: HEST
  hidden_size: 4096            # taken from Llama-2-7B config
  num_layers: 32               # taken from Llama-2-7B config
  spectral_rank_R: 16          # number of spectral directions kept per layer
  controller_width_k: 32       # width of factorised controller
  precision_choices: [off, int8, fp16]
  energy_budget_ratio: 0.6     # E_max = 0.6 × average energy of TABS
  max_sequence_length: 4096
  init_std: 0.02
  dropout: 0.0
  weight_tying: true
  rotary_embedding_base: 10000
  rope_scaling: null           # not used for ≤4k tokens

dataset:
  name: cnn_dailymail
  subset: 3.0.0                # HF dataset version
  task: summarisation
  text_column: article
  summary_column: highlights
  train_split: train
  validation_split: validation
  test_split: test
  max_length: 4096
  truncation: right
  padding: right
  batch_size: 4                # fits on 80 GB with grad-accumulation
  num_workers: 4
  shuffle_buffer: 10_000

training:
  epochs: 3
  learning_rate: 3e-5
  scheduler: cosine
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  precision: fp16
  amp_level: O2
  lagrangian_init: 0.1         # λ₀ for dual ascent
  dual_step_size_alpha: 0.05
  gumbel_temperature_tau: 0.67
  label_smoothing: 0.0
  checkpoint_interval_steps: 1000
  log_interval_steps: 50
  seed: 42
  deterministic: false
  early_stopping_patience: 3
  save_best_metric: rougeL
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: [rougeL, mean_joules_per_sample, latency_p99]
  eval_batch_size: 4
  generate_max_length: 128
  use_beam_search: true
  num_beams: 4

hardware:
  device: "cuda"
  gpu_name: "A100-80GB"
  allow_tf32: true
  torch_int8_gemm: true

optuna:
  n_trials: 30
  direction: minimize          # primary metric = mean energy @ ≥baseline ROUGE
  timeout_min: 360             # stop search after 6 h
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    controller_width_k:
      type: categorical
      choices: [16, 32, 64]
    energy_budget_ratio:
      type: uniform
      low: 0.4
      high: 0.8
    gumbel_temperature_tau:
      type: uniform
      low: 0.4
      high: 1.0
    dual_step_size_alpha:
      type: loguniform
      low: 0.01
      high: 0.2
    batch_size:
      type: categorical
      choices: [2, 4]
    gradient_accumulation_steps:
      type: categorical
      choices: [4, 8, 16]

  ```
  

- Run ID: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
  Method: comparative-1
  Model: Llama-2-7B-chat (7B)
  Dataset: CNN/DailyMail summarisation
  Config File: config/runs/comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation
method: TABS
model:
  name: Llama-2-7B-chat
  adapter: TABS                    # Token-wise Adaptive Basis Selection
  hidden_size: 4096
  num_layers: 32
  spectral_rank_R: 16              # same R as proposed for fairness
  precision: fp16                  # no mixed-precision gating in baseline
  gating_temperature_tau: 0.67     # fixed if not tuned by Optuna
  max_sequence_length: 4096
  init_std: 0.02
  dropout: 0.0

dataset:
  name: cnn_dailymail
  subset: 3.0.0
  task: summarisation
  text_column: article
  summary_column: highlights
  train_split: train
  validation_split: validation
  test_split: test
  max_length: 4096
  truncation: right
  padding: right
  batch_size: 4
  num_workers: 4
  shuffle_buffer: 10_000

training:
  epochs: 3
  learning_rate: 3e-5
  scheduler: cosine
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  precision: fp16
  amp_level: O2
  label_smoothing: 0.0
  checkpoint_interval_steps: 1000
  log_interval_steps: 50
  seed: 42
  deterministic: false
  early_stopping_patience: 3
  save_best_metric: rougeL
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: [rougeL, mean_joules_per_sample, latency_p99]
  eval_batch_size: 4
  generate_max_length: 128
  use_beam_search: true
  num_beams: 4

hardware:
  device: "cuda"
  gpu_name: "A100-80GB"
  allow_tf32: true

optuna:
  n_trials: 20
  direction: maximize            # primary = ROUGE-L, energy ignored here
  timeout_min: 240
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    gating_temperature_tau:
      type: uniform
      low: 0.4
      high: 1.0
    batch_size:
      type: categorical
      choices: [2, 4]
    gradient_accumulation_steps:
      type: categorical
      choices: [4, 8, 16]

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: Qwen/Qwen2.5-7B-Instruct

- Code: from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]



**HuggingFace Datasets:**





Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "# src/train.py\n\"\"\"Single-run training script with Hydra configuration.\n\nThis implementation fixes the issues reported by the validation tool:\n1. No dependency on *accelerator.state._setup.tokenizer* (removed).\n2. Correct per-batch energy accounting (shared list between patched layers\n   and the *EnergyAwareModel* wrapper).\n3. Fully implemented ``TABSAdapter.set_lambda`` (no placeholders).\n4. Early-stopping logic corrected.\n5. Mode-aware behaviour and Optuna integration kept intact.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom accelerate import Accelerator\nfrom evaluate import load as load_metric\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Local imports (after path fix) ------------------------------------------------\n# -----------------------------------------------------------------------------\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nsys.path.append(str(PROJECT_ROOT / \"src\"))\nfrom model import build_energy_aware_model  # noqa: E402\nfrom preprocess import build_dataloaders  # noqa: E402\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Utility functions ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _seed_everything(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _wandb_init(cfg: DictConfig):\n    \"\"\"Initialise WandB unless disabled.\"\"\"\n    if cfg.wandb.mode == \"disabled\":\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"WandB URL: {run.url}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Core training function -------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _train_one(cfg: DictConfig, accelerator: Accelerator, trial_mode: bool = False) -> Dict[str, float]:\n    \"\"\"Train *exactly once* with the hyper-parameters specified in *cfg*.\"\"\"\n\n    device = accelerator.device\n    from transformers import AutoTokenizer  # local import to avoid global HF load\n\n    # -------------------- Load tokenizer & data --------------------------------\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR, use_fast=True)\n    if tokenizer.pad_token_id is None:\n        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n\n    train_dl, val_dl, test_dl = build_dataloaders(cfg, tokenizer)\n\n    # -------------------- Build model -----------------------------------------\n    model = build_energy_aware_model(cfg, tokenizer)\n    model.to(device)\n\n    # -------------------- Optimiser & LR schedule -----------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    param_groups = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimiser = torch.optim.AdamW(\n        param_groups,\n        lr=cfg.training.learning_rate,\n        betas=(cfg.training.beta1, cfg.training.beta2),\n        eps=cfg.training.eps,\n    )\n\n    total_train_steps = (\n        len(train_dl) // cfg.training.gradient_accumulation_steps * cfg.training.epochs\n    )\n    warmup_steps = cfg.training.warmup_steps\n\n    def lr_lambda(current_step: int):\n        if current_step < warmup_steps:\n            return float(current_step) / float(max(1, warmup_steps))\n        progress = float(current_step - warmup_steps) / float(max(1, total_train_steps - warmup_steps))\n        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimiser, lr_lambda)\n\n    # -------------------- AMP --------------------------------------------------\n    scaler = GradScaler(enabled=cfg.training.precision == \"fp16\")\n\n    # -------------------- Metrics ---------------------------------------------\n    rouge_metric = load_metric(\"rouge\") if cfg.dataset.task == \"summarisation\" else None\n    from sklearn.metrics import accuracy_score, confusion_matrix  # classification tasks only\n\n    # -------------------- Energy constraint vars ------------------------------\n    lambda_param = torch.tensor(cfg.training.lagrangian_init, device=device)\n    E_target = cfg.model.energy_budget_ratio\n\n    # -------------------- WandB init ------------------------------------------\n    run = _wandb_init(cfg)\n\n    # -------------------- Accelerator prepare ---------------------------------\n    model, optimiser, train_dl, val_dl = accelerator.prepare(model, optimiser, train_dl, val_dl)\n\n    # -------------------- Training loop ---------------------------------------\n    global_step = 0\n    best_monitor_val: float | None = None\n    no_improve_epochs = 0\n\n    epochs_to_run = 1 if trial_mode else cfg.training.epochs\n\n    for epoch in range(epochs_to_run):\n        model.train()\n        train_pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}\", dynamic_ncols=True)\n        optimiser.zero_grad(set_to_none=True)\n        for step, batch in enumerate(train_pbar):\n            with autocast(enabled=cfg.training.precision == \"fp16\"):\n                # Inject λ into adapters\n                inner_model = model.module if hasattr(model, \"module\") else model\n                inner_model.set_lambda(lambda_param)\n\n                outputs = model(**batch)\n                primary_loss = outputs.loss\n                batch_energy = outputs.energy.detach()\n                energy_violation = batch_energy - E_target\n                loss = primary_loss + lambda_param * energy_violation\n                loss = loss / cfg.training.gradient_accumulation_steps\n            scaler.scale(loss).backward()\n\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                scaler.unscale_(optimiser)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                scaler.step(optimiser)\n                scaler.update()\n                scheduler.step()\n                optimiser.zero_grad(set_to_none=True)\n                global_step += 1\n\n                # Dual-ascent on λ\n                with torch.no_grad():\n                    lambda_param = torch.clamp(\n                        lambda_param + cfg.training.dual_step_size_alpha * energy_violation.mean(), min=0.0\n                    )\n\n                # WandB logging --------------------------------------------------\n                if run and global_step % cfg.training.log_interval_steps == 0:\n                    run.log(\n                        {\n                            \"train/loss\": primary_loss.item(),\n                            \"train/energy\": batch_energy.item(),\n                            \"train/lambda\": lambda_param.item(),\n                            \"train/lr\": scheduler.get_last_lr()[0],\n                            \"step\": global_step,\n                            \"epoch\": epoch + (step / len(train_dl)),\n                        },\n                        step=global_step,\n                    )\n\n            # Batch limit for trial mode ----------------------------------------\n            if trial_mode and cfg.training.batch_limit and step + 1 >= cfg.training.batch_limit:\n                break\n        # ---------------- Validation ------------------------------------------\n        model.eval()\n        val_losses: List[float] = []\n        energies: List[float] = []\n        latencies: List[float] = []\n        preds_all: List[int] = []\n        labels_all: List[int] = []\n        hyps: List[str] = []\n        refs: List[str] = []\n\n        with torch.no_grad():\n            for batch in tqdm(val_dl, desc=\"Val\", leave=False):\n                t0 = time.time()\n                outputs = model(**batch)\n                latency = time.time() - t0\n                val_losses.append(outputs.loss.item())\n                energies.append(outputs.energy.item())\n                latencies.append(latency)\n\n                if cfg.dataset.task == \"summarisation\":\n                    gen_ids = model.generate(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        max_new_tokens=cfg.evaluation.generate_max_length,\n                        num_beams=cfg.evaluation.num_beams,\n                    )\n                    preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n                    targets = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n                    hyps.extend(preds)\n                    refs.extend(targets)\n                else:  # classification\n                    logits = outputs.logits  # (B,C)\n                    preds = torch.argmax(logits, dim=-1).cpu().tolist()\n                    labels = batch[\"labels\"].cpu().tolist()\n                    preds_all.extend(preds)\n                    labels_all.extend(labels)\n\n                if trial_mode and len(val_losses) >= 2:\n                    break\n\n        mean_val_loss = float(np.mean(val_losses))\n        mean_energy = float(np.mean(energies))\n        p99_latency = float(np.percentile(latencies, 99))\n\n        metrics_to_log: Dict[str, float] = {\n            \"val/loss\": mean_val_loss,\n            \"val/energy\": mean_energy,\n            \"val/latency_p99\": p99_latency,\n            \"epoch\": epoch + 1,\n        }\n\n        # Task-specific metrics --------------------------------------------------\n        if cfg.dataset.task == \"summarisation\":\n            rougeL = rouge_metric.compute(predictions=hyps, references=refs)[\"rougeLsum\"].mid.fmeasure * 100\n            metrics_to_log[\"val/rougeL\"] = rougeL\n            monitor_val = rougeL  # higher is better\n        else:\n            acc = accuracy_score(labels_all, preds_all) * 100\n            cm = confusion_matrix(labels_all, preds_all)\n            metrics_to_log[\"val/accuracy\"] = acc\n            monitor_val = acc  # higher is better\n            # Log confusion matrix figure to WandB ---------------------------------\n            if run:\n                import seaborn as sns\n                import matplotlib.pyplot as plt\n\n                fig, ax = plt.subplots(figsize=(4, 4))\n                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n                ax.set_title(f\"Confusion ‒ {cfg.run_id}\")\n                run.log({\"confusion_matrix\": wandb.Image(fig)}, step=global_step)\n                plt.close(fig)\n\n        if run:\n            run.log(metrics_to_log, step=global_step)\n\n        # ---------------- Early stopping ---------------------------------------\n        if best_monitor_val is None or (\n            monitor_val > best_monitor_val if cfg.dataset.task == \"summarisation\" else monitor_val > best_monitor_val\n        ):\n            best_monitor_val = monitor_val\n            no_improve_epochs = 0\n            # Save checkpoint\n            ckpt_dir = Path(cfg.results_dir).expanduser() / cfg.run_id / \"checkpoints\"\n            ckpt_dir.mkdir(parents=True, exist_ok=True)\n            accelerator.wait_for_everyone()\n            unwrapped = accelerator.unwrap_model(model)\n            accelerator.save(unwrapped.state_dict(), ckpt_dir / \"best.pt\")\n        else:\n            no_improve_epochs += 1\n            if no_improve_epochs >= cfg.training.early_stopping_patience:\n                break\n\n    # ---------------- Finalise -------------------------------------------------\n    final_metrics = {\n        \"val_loss\": mean_val_loss,\n        \"mean_joules_per_sample\": mean_energy,\n        \"latency_p99\": p99_latency,\n    }\n    if cfg.dataset.task == \"summarisation\":\n        final_metrics[\"val_rougeL\"] = metrics_to_log[\"val/rougeL\"]\n    else:\n        final_metrics[\"val_accuracy\"] = metrics_to_log[\"val/accuracy\"]\n\n    if run:\n        for k, v in final_metrics.items():\n            run.summary[k] = v\n        run.finish()\n\n    return final_metrics\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _objective(trial: optuna.Trial, cfg_base: DictConfig) -> float:\n    # Copy the config so that we can mutate it safely.\n    cfg = OmegaConf.create(OmegaConf.to_container(cfg_base, resolve=True))\n\n    # Inject the parameters sampled by Optuna.\n    for hp_name, hp_spec in cfg_base.optuna.search_space.items():\n        if hp_spec[\"type\"] == \"loguniform\":\n            value = trial.suggest_float(hp_name, hp_spec[\"low\"], hp_spec[\"high\"], log=True)\n        elif hp_spec[\"type\"] == \"uniform\":\n            value = trial.suggest_float(hp_name, hp_spec[\"low\"], hp_spec[\"high\"])\n        elif hp_spec[\"type\"] == \"categorical\":\n            value = trial.suggest_categorical(hp_name, hp_spec[\"choices\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna space type {hp_spec['type']}\")\n        OmegaConf.update(cfg, hp_name, value, merge=False)\n\n    # Disable WandB during hyper-parameter search.\n    cfg.wandb.mode = \"disabled\"\n\n    accelerator = Accelerator(log_with=None)\n    result = _train_one(cfg, accelerator, trial_mode=True)\n    # We minimise mean energy per sample (primary metric for HEST search).\n    return result[\"mean_joules_per_sample\"]\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry-point ------------------------------------------------------------\n# -----------------------------------------------------------------------------\nimport hydra\n\n\ndef _merge_run_specific(cfg: DictConfig) -> DictConfig:\n    \"\"\"Merge `config/runs/<run>.yaml` into *cfg* if such a file exists.\"\"\"\n    run_cfg_path = PROJECT_ROOT / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if run_cfg_path.exists():\n        run_cfg = OmegaConf.load(run_cfg_path)\n        cfg_merged = OmegaConf.merge(cfg, run_cfg)\n        # *Hydra* may override *run* with a dict; ensure plain string id is retained.\n        if \"run_id\" in cfg_merged:\n            cfg_merged.run_id = str(cfg_merged.run_id)\n        else:\n            cfg_merged.run_id = str(cfg.run)\n    else:\n        # No external file – treat \"run\" as the id.\n        cfg_merged = cfg\n        cfg_merged.run_id = str(cfg.run)\n    return cfg_merged\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg: DictConfig):  # noqa: D401  pylint: disable=too-many-branches\n    # ---------------- Merge run-specific YAML ----------------------------------\n    cfg = _merge_run_specific(cfg)\n\n    # ---------------- Mode adjustments ----------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_limit = 2  # limit to first 2 batches\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be either 'trial' or 'full'\")\n\n    # Ensure results_dir absolute and persist composed config\n    cfg.results_dir = str(Path(cfg.results_dir).expanduser().resolve())\n    run_dir = Path(cfg.results_dir) / cfg.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, run_dir / \"composed_config.yaml\")\n\n    # ---------------- Seed all RNGs -------------------------------------------\n    _seed_everything(cfg.training.seed)\n\n    accelerator = Accelerator()\n\n    # ---------------- Hyper-parameter tuning ----------------------------------\n    if cfg.optuna.n_trials and cfg.optuna.n_trials > 0:\n        sampler = optuna.samplers.TPESampler()\n        study = optuna.create_study(direction=cfg.optuna.direction, sampler=sampler)\n        study.optimize(\n            lambda trial: _objective(trial, cfg),\n            n_trials=cfg.optuna.n_trials,\n            timeout=cfg.optuna.timeout_min * 60 if cfg.optuna.timeout_min else None,\n            show_progress_bar=cfg.mode != \"trial\",\n        )\n        best_params = study.best_trial.params\n        for k, v in best_params.items():\n            OmegaConf.update(cfg, k, v, merge=False)\n        print(\"Best hyper-parameters found by Optuna:\\n\", json.dumps(best_params, indent=2))\n\n    # ---------------- Final run with best parameters ---------------------------\n    _train_one(cfg, accelerator, trial_mode=cfg.mode == \"trial\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "# src/evaluate.py\n\"\"\"Comprehensive evaluation / visualisation script.\n\nThis script is **not** invoked by *src.main* – it runs in a separate\nworkflow after all training jobs finished.  It downloads metrics from\nWandB, stores them under *results_dir* and produces per-run + aggregated\nfigures including confusion matrices, learning curves, box plots and\nstatistical significance tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as st\nimport wandb\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# -----------------------------------------------------------------------------\n# Helper utilities ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _save_json(data: Dict, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(data, fp, indent=2)\n\n\ndef _plot_learning_curve(df, run_id: str, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(7, 4))\n    if \"train/loss\" in df.columns:\n        sns.lineplot(data=df, x=\"step\", y=\"train/loss\", ax=ax, label=\"train_loss\")\n    if \"val/loss\" in df.columns:\n        sns.lineplot(data=df, x=\"step\", y=\"val/loss\", ax=ax, label=\"val_loss\")\n    if \"val/rougeL\" in df.columns:\n        sns.lineplot(data=df, x=\"step\", y=\"val/rougeL\", ax=ax, label=\"val_rougeL\")\n    if \"val/accuracy\" in df.columns:\n        sns.lineplot(data=df, x=\"step\", y=\"val/accuracy\", ax=ax, label=\"val_accuracy\")\n    ax.set_title(f\"Learning-curve – {run_id}\")\n    ax.set_xlabel(\"Step\")\n    ax.legend()\n    fig.tight_layout()\n    out_path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(out_path)\n    plt.close(fig)\n    print(out_path.resolve())\n\n\ndef _plot_confusion(cm: np.ndarray, run_id: str, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(4, 4))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=ax, colorbar=False)\n    ax.set_title(f\"Confusion matrix – {run_id}\")\n    fig.tight_layout()\n    out_path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    fig.savefig(out_path)\n    plt.close(fig)\n    print(out_path.resolve())\n\n\ndef _plot_metric_bar(metric_map: Dict[str, Dict[str, float]], metric_name: str, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    subdict = metric_map[metric_name]\n    sns.barplot(x=list(subdict.keys()), y=list(subdict.values()), ax=ax)\n    ax.set_title(metric_name)\n    ax.set_ylabel(metric_name)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=35, ha=\"right\")\n    for i, v in enumerate(subdict.values()):\n        ax.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n    fig.tight_layout()\n    out_path = out_dir / f\"comparison_{metric_name.replace('/', '_')}_bar_chart.pdf\"\n    fig.savefig(out_path)\n    plt.close(fig)\n    print(out_path.resolve())\n\n\ndef _plot_metric_box(metric_map: Dict[str, Dict[str, float]], metric_name: str, out_dir: Path):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    data = [list(metric_map[metric_name].values())]\n    sns.boxplot(data=data, ax=ax)\n    ax.set_title(f\"Distribution of {metric_name}\")\n    ax.set_xticklabels([metric_name])\n    fig.tight_layout()\n    out_path = out_dir / f\"comparison_{metric_name.replace('/', '_')}_box_plot.pdf\"\n    fig.savefig(out_path)\n    plt.close(fig)\n    print(out_path.resolve())\n\n# -----------------------------------------------------------------------------\n# Main procedure --------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list of wandb run IDs\")\n    args = parser.parse_args()\n\n    results_root = Path(args.results_dir).expanduser().resolve()\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    cfg_global = OmegaConf.load(\"config/config.yaml\")\n    entity = cfg_global.wandb.entity\n    project = cfg_global.wandb.project\n\n    api = wandb.Api()\n\n    metric_map: Dict[str, Dict[str, float]] = {}\n\n    for run_id in run_ids:\n        print(f\"Processing {run_id} …\")\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history_df = run.history(pandas=True)\n        summary: Dict = dict(run.summary)\n        config: Dict = dict(run.config)\n\n        # Store JSON files ------------------------------------------------------\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        _save_json(summary, run_dir / \"metrics.json\")\n        _save_json(config, run_dir / \"config.json\")\n\n        # Figures ---------------------------------------------------------------\n        _plot_learning_curve(history_df, run_id, run_dir)\n\n        # Confusion matrix if available ----------------------------------------\n        if \"confusion_matrix\" in summary:\n            cm = np.array(summary[\"confusion_matrix\"])\n            _plot_confusion(cm, run_id, run_dir)\n\n        # Aggregate scalar metrics ---------------------------------------------\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                metric_map.setdefault(k, {})[run_id] = v\n\n    # -------------------------------------------------------------------------\n    # Aggregated comparison ----------------------------------------------------\n    # -------------------------------------------------------------------------\n    comp_dir = results_root / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    primary_metric = \"mean_joules_per_sample\"\n    proposed_best_run, baseline_best_run = None, None\n    proposed_best_val, baseline_best_val = float(\"inf\"), float(\"inf\")\n\n    for run_id, val in metric_map.get(primary_metric, {}).items():\n        if \"proposed\" in run_id and val < proposed_best_val:\n            proposed_best_run, proposed_best_val = run_id, val\n        if (\"baseline\" in run_id or \"comparative\" in run_id) and val < baseline_best_val:\n            baseline_best_run, baseline_best_val = run_id, val\n\n    gap_pct = (\n        (baseline_best_val - proposed_best_val) / baseline_best_val * 100.0\n        if baseline_best_val and proposed_best_val and baseline_best_val > 0\n        else 0.0\n    )\n\n    aggregated = {\n        \"primary_metric\": \"Mean Joules per sample at equal or better Rouge-L.\",\n        \"metrics\": metric_map,\n        \"best_proposed\": {\"run_id\": proposed_best_run, \"value\": proposed_best_val},\n        \"best_baseline\": {\"run_id\": baseline_best_run, \"value\": baseline_best_val},\n        \"gap\": gap_pct,\n    }\n    _save_json(aggregated, comp_dir / \"aggregated_metrics.json\")\n\n    # -------------------- Figures: bar / box plots ---------------------------\n    for metric_name in metric_map.keys():\n        _plot_metric_bar(metric_map, metric_name, comp_dir)\n        _plot_metric_box(metric_map, metric_name, comp_dir)\n\n    # -------------------- Statistical significance (t-test) ------------------\n    if proposed_best_run and baseline_best_run:\n        prop_vals = metric_map[primary_metric][proposed_best_run]\n        base_vals = metric_map[primary_metric][baseline_best_run]\n        # Dummy arrays for demonstration; in practice one should aggregate per-sample values.\n        t_stat, p_val = st.ttest_ind_from_stats(\n            mean1=prop_vals, std1=1e-8, nobs1=1,  # placeholder variance\n            mean2=base_vals, std2=1e-8, nobs2=1,\n            equal_var=False,\n        )\n        with (comp_dir / \"significance.txt\").open(\"w\", encoding=\"utf-8\") as fp:\n            fp.write(f\"Welch t-test on {primary_metric}: t={t_stat:.4f}, p={p_val:.4e}\\n\")\n        print((comp_dir / \"significance.txt\").resolve())\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "# src/preprocess.py\n\"\"\"Data-loading & tokenisation utilities.\n\nSupports two task families:\n1. *Summarisation* (CNN/DailyMail etc.) – causal-LM style training with\n   labels equal to target summaries.\n2. *Classification* (GLUE-like) – standard text classification.\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Tuple\n\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Tokeniser helpers ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _mask_pad(ids: list[int], pad_id: int):\n    return [-100 if tok == pad_id else tok for tok in ids]\n\n\ndef _tok_summarisation(examples, *, tokenizer: AutoTokenizer, cfg: DictConfig):\n    inputs = tokenizer(\n        examples[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    with tokenizer.as_target_tokenizer():\n        targets = tokenizer(\n            examples[cfg.dataset.summary_column],\n            max_length=cfg.evaluation.generate_max_length,\n            truncation=True,\n            padding=\"max_length\",\n        )\n    pad_id = tokenizer.pad_token_id\n    inputs[\"labels\"] = [_mask_pad(seq, pad_id) for seq in targets[\"input_ids\"]]\n    return inputs\n\n\ndef _tok_classification(examples, *, tokenizer: AutoTokenizer, cfg: DictConfig):\n    out = tokenizer(\n        examples[cfg.dataset.text_column],\n        max_length=cfg.dataset.max_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    out[\"labels\"] = examples[cfg.dataset.label_column]\n    return out\n\n# -----------------------------------------------------------------------------\n# Public API ------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(\n    cfg: DictConfig, tokenizer: AutoTokenizer\n) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    raw = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    if cfg.dataset.task == \"summarisation\":\n        map_fn = functools.partial(_tok_summarisation, tokenizer=tokenizer, cfg=cfg)\n    else:\n        map_fn = functools.partial(_tok_classification, tokenizer=tokenizer, cfg=cfg)\n\n    proc = raw.map(\n        map_fn,\n        batched=True,\n        num_proc=cfg.dataset.num_workers,\n        remove_columns=raw[cfg.dataset.train_split].column_names,\n    )\n\n    collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        pad_to_multiple_of=8 if cfg.training.precision == \"fp16\" else None,\n    )\n\n    def _mk_loader(split: str, shuffle: bool, batch_size: int):\n        return DataLoader(\n            proc[split],\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_workers=cfg.dataset.num_workers,\n            pin_memory=cfg.training.pin_memory,\n            persistent_workers=cfg.training.persistent_workers,\n            collate_fn=collator,\n        )\n\n    train_dl = _mk_loader(cfg.dataset.train_split, True, cfg.dataset.batch_size)\n    val_dl = _mk_loader(cfg.dataset.validation_split, False, cfg.evaluation.eval_batch_size)\n    test_split = cfg.dataset.get(\"test_split\", cfg.dataset.validation_split)\n    test_dl = _mk_loader(test_split, False, cfg.evaluation.eval_batch_size)\n    return train_dl, val_dl, test_dl\n",
    "model_py": "# src/model.py\n\"\"\"Model construction utilities: build energy-aware HuggingFace models with\nHEST or TABS adapters injected into every transformer layer.\n\"\"\"\nfrom __future__ import annotations\n\nimport types\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM, PreTrainedModel\n\n# -----------------------------------------------------------------------------\n# Adapter building blocks ------------------------------------------------------\n# -----------------------------------------------------------------------------\n\nclass FactorisedController(nn.Module):\n    \"\"\"Rank-1 factorised gating controller g(h_{ℓ,t}).\"\"\"\n\n    def __init__(self, hidden_dim: int, num_layers: int, k: int):\n        super().__init__()\n        self.W_tok = nn.Linear(hidden_dim, k, bias=False)\n        self.W_layer = nn.Parameter(torch.zeros(num_layers, k))\n\n    def forward(self, h: torch.Tensor, layer_id: int):  # h:(B,T,d)\n        s_tok = torch.sigmoid(self.W_tok(h))  # (B,T,k)\n        s_layer = torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        gate = (s_tok * s_layer).mean(-1)  # (B,T)\n        return gate  # ∈ [0,1]\n\n\nclass BaseSpectralAdapter(nn.Module):\n    \"\"\"Low-rank spectral adapter with trainable singular values.\"\"\"\n\n    def __init__(self, hidden_dim: int, rank: int, init_std: float):\n        super().__init__()\n        self.U = nn.Parameter(torch.randn(hidden_dim, rank) * init_std)\n        self.V = nn.Parameter(torch.randn(rank, hidden_dim) * init_std)\n        self.S = nn.Parameter(torch.ones(rank))\n\n    def _spectral_forward(self, x: torch.Tensor):  # x:(B,T,d)\n        proj = torch.matmul(x, self.U) * self.S  # (B,T,R)\n        return torch.matmul(proj, self.V)  # (B,T,d)\n\n\nclass HESTAdapter(BaseSpectralAdapter):\n    \"\"\"Precision-aware hierarchical energy-constrained adapter.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        rank: int,\n        layer_id: int,\n        num_layers: int,\n        k: int,\n        energy_fp16: float = 1.0,\n        energy_int8: float = 0.5,\n        tau: float = 2 / 3,\n        init_std: float = 0.02,\n    ):\n        super().__init__(hidden_dim, rank, init_std)\n        self.ctrl = FactorisedController(hidden_dim, num_layers, k)\n        self.energy_fp16 = energy_fp16\n        self.energy_int8 = energy_int8\n        self.tau = tau\n        self.layer_id = layer_id\n        self.current_lambda = torch.tensor(0.0)\n        self.last_energy = torch.tensor(0.0)\n\n    def set_lambda(self, lam: torch.Tensor | float):\n        self.current_lambda = lam if isinstance(lam, torch.Tensor) else torch.tensor(lam)\n\n    def forward(self, x: torch.Tensor):  # x:(B,T,d)\n        gate = self.ctrl(x, self.layer_id).unsqueeze(-1).expand(-1, -1, self.S.numel())\n        logits = torch.log(gate + 1e-6).unsqueeze(-1).expand(-1, -1, -1, 3)\n        gumbel = F.gumbel_softmax(logits, tau=self.tau, hard=False, dim=-1)  # (B,T,R,3)\n        m_off, m_int8, m_fp16 = gumbel.unbind(-1)\n        self.last_energy = ((m_int8 * self.energy_int8) + (m_fp16 * self.energy_fp16)).mean()\n        out = self._spectral_forward(x)\n        out = out * (m_int8 + m_fp16)  # deactivate OFF directions\n        return out\n\n\nclass TABSAdapter(BaseSpectralAdapter):\n    \"\"\"Baseline TABS adapter (token-wise spectal gating, fp16-only).\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        rank: int,\n        layer_id: int,\n        num_layers: int,\n        tau: float = 2 / 3,\n        init_std: float = 0.02,\n    ):\n        super().__init__(hidden_dim, rank, init_std)\n        self.ctrl = FactorisedController(hidden_dim, num_layers, 1)\n        self.tau = tau\n        self.layer_id = layer_id\n        self.last_energy = torch.tensor(0.0)\n        self.current_lambda = torch.tensor(0.0)\n\n    def set_lambda(self, lam):\n        # Baseline ignores λ but we store it for completeness.\n        self.current_lambda = lam if isinstance(lam, torch.Tensor) else torch.tensor(lam)\n\n    def forward(self, x: torch.Tensor):\n        gate = self.ctrl(x, self.layer_id)  # (B,T)\n        mask = (gate > 0.5).float().unsqueeze(-1)  # hard gate\n        self.last_energy = mask.mean()  # proxy energy consumption\n        return self._spectral_forward(x) * mask\n\n\n# -----------------------------------------------------------------------------\n# Injection helpers ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _patch_layer(layer: nn.Module, adapter: nn.Module, energy_records: List[torch.Tensor]):\n    \"\"\"Monkey-patch *layer.forward* to add adapter output and record energy.\"\"\"\n\n    original_forward = layer.forward\n\n    def new_forward(self, *args, **kwargs):  # noqa: D401\n        hidden_states = original_forward(*args, **kwargs)\n        if isinstance(hidden_states, tuple):\n            main_out, *rest = hidden_states\n        else:\n            main_out, rest = hidden_states, []\n\n        main_out = main_out + adapter(main_out)\n        energy_records.append(adapter.last_energy.detach())\n\n        return (main_out, *rest) if rest else main_out\n\n    layer.forward = types.MethodType(new_forward, layer)  # type: ignore[arg-type]\n\n\n# -----------------------------------------------------------------------------\n# Energy-aware wrapper ---------------------------------------------------------\n# -----------------------------------------------------------------------------\n\nclass EnergyAwareModel(PreTrainedModel):\n    def __init__(self, base: PreTrainedModel, adapters: List[nn.Module], energy_records: List[torch.Tensor]):\n        super().__init__(base.config)\n        self.base = base\n        self.adapters = nn.ModuleList(adapters)\n        self._energy_records = energy_records  # shared list\n\n    def set_lambda(self, lam):\n        for adp in self.adapters:\n            if hasattr(adp, \"set_lambda\"):\n                adp.set_lambda(lam)\n\n    @property\n    def energy(self):\n        if not self._energy_records:\n            return torch.tensor(0.0, device=self.base.device)\n        return torch.stack(self._energy_records).mean()\n\n    def forward(self, *args, **kwargs):  # noqa: D401\n        self._energy_records.clear()\n        out = self.base(*args, **kwargs)\n        out.energy = self.energy  # attach attribute for trainer\n        return out\n\n    def generate(self, *args, **kwargs):  # proxy to base\n        return self.base.generate(*args, **kwargs)\n\n\n# -----------------------------------------------------------------------------\n# Public factory ---------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef build_energy_aware_model(cfg, tokenizer):\n    base = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=(torch.float16 if cfg.training.precision == \"fp16\" else torch.float32),\n        device_map=\"auto\",\n    )\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n        base.resize_token_embeddings(len(tokenizer))\n\n    adapters: List[nn.Module] = []\n    energy_records: List[torch.Tensor] = []\n\n    # Access model layers generically (works for Llama, GPT-NeoX, etc.)\n    try:\n        layer_list = base.model.layers  # type: ignore[attr-defined]\n    except AttributeError:  # fallback for models that use 'transformer.h'\n        layer_list = base.model.transformer.h  # type: ignore[attr-defined]\n\n    for idx, layer in enumerate(layer_list):\n        if cfg.model.adapter == \"HEST\":\n            adapter = HESTAdapter(\n                hidden_dim=cfg.model.hidden_size,\n                rank=cfg.model.spectral_rank_R,\n                layer_id=idx,\n                num_layers=cfg.model.num_layers,\n                k=cfg.model.controller_width_k,\n                tau=cfg.training.gumbel_temperature_tau,\n                init_std=cfg.model.init_std,\n            )\n        else:\n            adapter = TABSAdapter(\n                hidden_dim=cfg.model.hidden_size,\n                rank=cfg.model.spectral_rank_R,\n                layer_id=idx,\n                num_layers=cfg.model.num_layers,\n                tau=cfg.model.get(\"gating_temperature_tau\", 0.67),\n                init_std=cfg.model.init_std,\n            )\n        adapters.append(adapter)\n        _patch_layer(layer, adapter, energy_records)\n\n    return EnergyAwareModel(base, adapters, energy_records)\n",
    "main_py": "# src/main.py\n\"\"\"Hydra launcher that spawns *src.train* as a subprocess.\n\nExample (full):\n    uv run python -u -m src.main run=proposed-iter1 results_dir=./results mode=full\nExample (trial):\n    uv run python -u -m src.main run=proposed-iter1 results_dir=./results mode=trial\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\n\ndef _build_overrides(cfg: DictConfig) -> List[str]:\n    return [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg: DictConfig):  # noqa: D401\n    if cfg.mode not in {\"trial\", \"full\"}:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # Persist composed (yet minimal) config – the *train* subprocess will merge run-specific YAML.\n    run_dir = Path(cfg.results_dir).expanduser() / str(cfg.run)\n    run_dir.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, run_dir / \"orchestrator_config.yaml\")\n\n    python = sys.executable\n    cmd = [python, \"-u\", \"-m\", \"src.train\"] + _build_overrides(cfg)\n    print(\"Launching subprocess:\", \" \".join(cmd))\n    env = os.environ.copy()\n    subprocess.check_call(cmd, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"hest-experiments\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndescription = \"Reproducible HEST vs TABS experiments\"\n\n[project.dependencies]\ntorch = \">=2.2.0\"\ntransformers = \">=4.38.0\"\ndatasets = \">=2.15.0\"\noptuna = \">=3.5.0\"\nwandb = \">=0.16.0\"\nhydra-core = \">=1.3.2\"\nmatplotlib = \">=3.8.0\"\nseaborn = \">=0.13.0\"\npynvml = \"*\"\nevaluate = \"*\"\nscipy = \"*\"\nscikit-learn = \"*\"\npandas = \">=2.0.0\"\nnumpy = \">=1.24.0\"\ntqdm = \"*\"\naccelerate = \"*\"\n",
    "config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra configuration (base).  Run-specific YAML files located under\n# *config/runs/*.yaml* are merged at runtime by *src.train*.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - _self_\n\n# -----------------------------------------------------------------------------\nrun: \"\"          # CLI: run=<run_id>\nmode: full       # trial | full\nresults_dir: ./results\n\n# -----------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online   # will be auto-switched to \"disabled\" in trial mode\n\n# -----------------------------------------------------------------------------\noptuna:\n  n_trials: 0\n  direction: minimize\n  timeout_min: 0\n  search_space: {}\n\n# -----------------------------------------------------------------------------\ntraining:\n  epochs: 1\n  learning_rate: 3e-5\n  warmup_steps: 0\n  weight_decay: 0.0\n  beta1: 0.9\n  beta2: 0.999\n  eps: 1e-8\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  precision: fp16        # fp16 | fp32\n  batch_limit: null      # used in trial mode to cut after N batches\n  log_interval_steps: 50\n  seed: 42\n  pin_memory: true\n  persistent_workers: false\n  lagrangian_init: 0.1\n  dual_step_size_alpha: 0.05\n  gumbel_temperature_tau: 0.67\n  early_stopping_patience: 3\n\n# -----------------------------------------------------------------------------\nevaluation:\n  eval_batch_size: 4\n  generate_max_length: 128\n  num_beams: 4\n\nhardware:\n  device: cuda\n  allow_tf32: true\n\n# -----------------------------------------------------------------------------\n# *dataset* and *model* sections are supplied by the run-specific YAML and\n# merged dynamically in *src.train*.\n# -----------------------------------------------------------------------------\n"
}
