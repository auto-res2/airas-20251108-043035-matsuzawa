
LLM Name: o3-2025-04-16
Input:
You are an accomplished researcher in machine learning. You are considering a new hypothesis described in "New Hypothesis" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new hypothesis.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed hypothesis, and quantitatively evaluate them.

# Research Topic
Improving fine-tuning performance of language models.

# New Hypothesis
{
    "Open Problems": "Even the input-adaptive BHSP / CBSP keeps *one set of spectral gates per layer* that is shared by every token of the sequence.  This ignores the large intra-sequence variability that characterises language: (a) only a handful of “hard” tokens (rare words, entity spans, long-range dependencies) need the full adapter capacity while the majority can be served with rank 0–2; (b) sentences differ in length, so a fixed *average* budget B either over-allocates on short inputs or under-allocates on long ones, breaking the promised latency envelope.  We lack a PEFT mechanism that •decides the rank at *token granularity*, •keeps the *sequence-level* compute strictly below a user-specified FLOP cap regardless of length, and •does so with negligible controller overhead.",
    "Methods": "Token–Aware Budgeted Spectral Pruning (TABS)\n1.  For each transformer layer ℓ we expose its top-R singular directions as in Spectral-Adapter.  A *token controller* g_θ maps every hidden state h_{ℓ,t}∈ℝ^d to R Bernoulli gates:      s_{ℓ,t}=W²_ℓ ReLU(W¹_ℓ h_{ℓ,t})  →  ĝ_{ℓ,t}=HardConcrete(s_{ℓ,t}).  The two 1×R MLPs add 2·R·d params per layer (≪1 %).\n2.  Token-wise adapter output is   ŷ_{ℓ,t}=U_ℓ diag(S_ℓ ⊙ ĝ_{ℓ,t}·α) V_ℓᵀ h_{ℓ,t}.  Multiplications with deactivated directions are skipped via sparse matmul.\n3.  Enforce a *hard sequence budget* B_max by projecting the expected token ranks onto the simplex:  after each forward pass we compute r_seq=Σ_{t,i} ĝ_{ℓ,t,i}.  If r_seq>B_max we down-scale the top-k logits s_{ℓ,t,i} with largest gradients until the constraint is met (differentiable “soft top-k” projection).\n4.  Training objective = task_loss + λ·Σ_ℓ⟨max(0,r_seq−B_max)⟩².  No extra hyper-parameters besides λ and B_max.\n5.  At inference HardConcrete collapses to {0,1}; projection guarantees FLOPs ≤ B_max·d per layer independent of sentence length.",
    "Experimental Setup": "Model/Data: DeBERTa-v3-base on (i) GLUE, (ii) Long Range Arena (for length stress-test).\nBaselines: LoRA-r8, BHSP (rank budget 8·L), CBSP (avg budget 8·L).\nProposed: TABS with R=16, B_max=8·T where T is input length (tokens), λ=0.05.\nMetrics: • glue_avg, • LRA accuracy, • mean & 99-th percentile latency, • realised FLOPs vs. length, • per-token rank histogram.\nAblations: remove projection (violates budget), share gates across tokens (reverts to CBSP).",
    "Primary Metric": "99-th-percentile latency at equal or better GLUE accuracy.",
    "Experimental Code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass HC(nn.Module):\n    def __init__(self,beta=2/3,gamma=-.1,zeta=1.1):\n        super().__init__(); self.beta=beta; self.gamma=gamma; self.zeta=zeta\n    def forward(self,logits,train=True):\n        if train:\n            u=torch.rand_like(logits); s=torch.sigmoid((torch.log(u)-torch.log1p(-u)+logits)/self.beta)\n            y=s*(self.zeta-self.gamma)+self.gamma; return torch.clamp(y,0,1)\n        else:\n            return (logits>0).float()\nclass TokenController(nn.Module):\n    def __init__(self,d,R):\n        super().__init__(); self.mlp=nn.Sequential(nn.Linear(d,d//4),nn.ReLU(),nn.Linear(d//4,R))\n        self.gate=HC()\n    def forward(self,h,train=True):\n        return self.gate(self.mlp(h),train)\nclass TABSLayer(nn.Module):\n    def __init__(self,U,S,V,R,B_max,d):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.ctrl=TokenController(d,R); self.B=B_max\n    def forward(self,h,train=True):\n        g=self.ctrl(h,train)                          # (T,R)\n        #   simplex projection per sequence to meet B_max\n        if train and g.sum()>self.B:\n            excess=g.sum()-self.B; flat=g.view(-1); topk=torch.topk(flat,flat.numel())[1]\n            flat[topk[:int(excess)]]*=0  # soft drop\n            g=flat.view_as(g)\n        S_adj=self.S[:self.R]; out=[]\n        for tok_g,ht in zip(g,h):\n            W=(self.U[:,:self.R]* (S_adj*tok_g)).matmul(self.V[:self.R])\n            out.append(ht @ W.T)\n        return torch.stack(out)",
    "Expected Result": "TABS matches CBSP on GLUE (≈89.2 %) but cuts 99-th latency by ≥40 % and average FLOPs by 55 %. On 4k-token LRA tasks it stays within budget whereas CBSP exceeds it by 2-3×.",
    "Expected Conclusion": "Fine-grained token-aware spectral gating under a strict sequence budget reconciles worst-case latency guarantees with parameter-efficient accuracy, pushing PEFT towards *real-time adaptive* language models suitable for latency-critical applications like dialogue or on-device NLP."
}

# Related Works
{
    "Title": "Spectral Adapter: Fine-Tuning in Spectral Space",
    "Main Contributions": "This research introduces the \"Spectral Adapter\" framework, a novel Parameter-Efficient Fine-Tuning (PEFT) method that leverages the spectral information of pretrained weight matrices. The main contributions include: 1) Proposing two spectral adaptation mechanisms, additive (Spectral AdapterA) and rotational (Spectral AdapterR), which fine-tune the top singular vectors obtained via Singular Value Decomposition (SVD) of pretrained weights. 2) Providing theoretical analysis showing that Spectral AdapterA offers twice the rank capacity of LoRA for a fixed trainable parameter budget. 3) Demonstrating through extensive experiments that Spectral Adapters achieve superior parameter efficiency and tuning performance compared to state-of-the-art PEFT methods across large language models (LLMs) and diffusion models. 4) Showing that Spectral AdapterA naturally addresses multi-adapter fusion problems by distributing concept tunings along different spectral spaces. 5) Highlighting Spectral AdapterR's ability to offer finer-grained parameter choices and improved efficiency. The method maintains practicality with negligible runtime and storage overhead.",
    "Methodology": "The methodology centers on incorporating spectral information into fine-tuning by first performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) and then adaptively tuning the top-r columns of the singular vector matrices (U and V). Two primary mechanisms are explored: 1) Spectral AdapterA, an additive approach, modifies the top-r columns of U and V with trainable matrices (AU, AV) initialized to zero. This mechanism resembles LoRA. 2) Spectral AdapterR, a rotational approach, applies trainable orthogonal matrices (RU, RV) to rotate the top-r columns of U and V. Orthogonality is efficiently maintained using Cayley parameterization, ensuring an exact rotation operation and preserving the SVD structure for subsequent fine-tunings. Theoretical insights support this choice by showing increased rank capacity for additive tuning and improved neuron alignment with top spectral directions.",
    "Experimental Setup": "Experiments were conducted on Large Language Models (LLMs) and Diffusion Models. For LLMs, Llama3 8B was fine-tuned on the Orca Math dataset and evaluated on GSM8K, while DeBERTaV3-base and Mistral 7B were fine-tuned on GLUE benchmarks and GSM8K, respectively. For diffusion models, the Chilloutmix model was fine-tuned on various custom concepts (animals, toys, vase, chair) and multi-character generation tasks. Baselines included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg. Evaluation metrics included training loss, validation scores (accuracy for LLMs), and qualitative visual generation results, complemented by quantitative alignment scores (CLIP embedding cosine similarity with reference images and prompt texts) for diffusion models. All experiments were performed on NVIDIA RTX A6000 GPUs, with hyperparameters for baselines following their original reports and Spectral Adapter hyperparameters tuned or specified in the appendix.",
    "Limitations": "The current work has several limitations. Firstly, the choice of exclusively tuning the top spectral space, while theoretically and empirically supported under simple settings, requires further in-depth investigation to understand the broader role of spectral information when tuning different columns of singular vector matrices. Secondly, there is a need for further study on fine-tuning the spectral representation of specific model components, such as only the attention layer, in large models. Lastly, the computational cost of the Singular Value Decomposition (SVD) procedure increases with larger models, necessitating faster SVD methods to maintain efficiency as model sizes grow.",
    "Future Research Directions": "Future research could explore several avenues. One key direction is to investigate fine-tuning the spectral representation of different components within large models, for instance, focusing solely on the attention layer. Another promising area involves dynamically combining spectral adaptation with other PEFT methods, such as AdaLoRA, to potentially achieve more adaptive parameter allocation. A deeper investigation into the effects of tuning various columns of singular vector matrices, beyond just the top spectral space, is crucial for a comprehensive understanding of spectral information's role in fine-tuning. Furthermore, developing and integrating faster Singular Value Decomposition (SVD) methods will be beneficial to efficiently handle the increasing scale of future large models.",
    "Experiment Code": "class SpectralLinearLayer_OFT(nn.Module):    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):        rank = 8        super().__init__()        self.name = name        if original_module.__class__.__name__ == 'Conv2d':            self.conv = True            in_channels, out_channels = original_module.in_channels, original_module.out_channels        else:            self.conv = False            in_channels, out_channels = original_module.in_features, original_module.out_features        W = original_module.weight.data.view(out_channels, in_channels)        U, S, V = torch.svd(W)        self.U = torch.nn.Parameter(U, requires_grad=False)        self.S = torch.nn.Parameter(S, requires_grad=False)        self.V = torch.nn.Parameter(V, requires_grad=False)        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)        original_module.forward = self.forward        self.original_module = original_module        self.top = top        self.idx = idx        assert revised_r>0        self.rank = revised_r    def cayley(self, data: torch.Tensor) -> torch.Tensor:        r, _ = data.shape        skew = 0.5 * (data - data.T)        I = torch.eye(r, device=data.device)        Q = torch.mm(I - skew, torch.inverse(I + skew))        return Q    def forward(self, hidden_states):        if self.top:            pad_U = self.U.clone()            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)            pad_S = self.S.clone()            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C            pad_V = self.V.clone()            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)        else:            raise Exception('')        pad_W = pad_U@pad_S.diag()@pad_V.T        if self.conv :            raise Exception('')        else:            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)",
    "Experiment Result": "The method incorporates spectral information into fine-tuning by first performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) of target layers. It then adaptively tunes the top-r columns of the singular vector matrices (U and V) and scales the corresponding singular values (S).\n\nKey mechanisms and settings:\n- **SVD Implementation**: SVD is applied directly on the weight data of the original linear/convolutional modules (`torch.svd(W)`).\n- **Adapted Components**: The top-`r` columns of `U` and `V` (singular vector matrices) are adapted, and the corresponding `r` singular values in `S` are scaled.\n- **Adaptation Mechanism (Spectral AdapterR-like with S-scaling)**:\n  - For `U` and `V`, rotational transformations are applied using trainable matrices (`self.spectral_A` and `self.spectral_B`), which are parameterized by the Cayley transformation (`self.cayley`) to maintain orthogonality.\n  - For `S`, the singular values are scaled element-wise by a trainable vector (`self.spectral_C`).\n  - The adapted `U`, `S`, `V` are then recombined (`pad_U@pad_S.diag()@pad_V.T`) to form the new weight matrix.\n- **Initialization**: The trainable parameters `self.spectral_A` and `self.spectral_B` (for U and V) are initialized to `torch.zeros(r, r)`, while `self.spectral_C` (for S) is initialized to `torch.ones(r)`. This ensures that the initial modifications to the pretrained weights are minimal (identity transformation for rotations and no scaling for singular values).\n- **Target Layers**: The adaptation is applied to `Linear` layers within `CLIPEncoderLayer` or `CLIPAttention` in the text encoder, and `Linear` or `Conv2d` layers (with kernel_size=(1,1)) within `Transformer2DModel` or `Attention` blocks in the UNet.\n- **Rank (`r`)**: The number of top singular vector columns and singular values to adapt is determined by a configurable `rank` parameter (referred to as `revised_r` in the code, which takes the value of `rank` from `lora_cfg` in the configuration file, e.g., `unet_cfg['lora_cfg']['rank']`).\n- **Scaling Factor (`alpha`)**: An `alpha` parameter is used during the merging process (`merge_spectraloft_into_weight`) to scale the magnitude of the learned spectral modifications. Specifically, `alpha` scales the deviation from the identity matrix for the Cayley-transformed rotation matrices and the deviation from `torch.ones` for the singular value scaling vector.\n- **Loss Function**: Mean Squared Error (MSE) loss is used (`F.mse_loss`), optionally combined with an attention regularization term (`self.attn_reg_weight * (loss_subject + loss_adjective)`).\n- **Optimizer and Scheduler**: The `AdamW` optimizer is used with a `linear` learning rate scheduler.\n- **Training Data**: `LoraDataset` is used for fine-tuning.\n- **Base Model**: Stable Diffusion is used as the base model, loaded from a `pretrained_path`."
}{
    "Title": "Parameter-Efficient Fine-Tuning Design Spaces",
    "Main Contributions": "The main research problem addresses the lack of understanding of design patterns in parameter-efficient fine-tuning (PEFT) strategies, which are typically hand-crafted. The paper introduces PEFT design spaces, characterized by layer grouping, trainable parameter allocation, tunable groups, and strategy assignment, to systematically discover such patterns. Key findings include the discovery of effective design patterns: (i) grouping layers in a spindle pattern; (ii) uniformly allocating trainable parameters to layers; (iii) tuning all layer groups; and (iv) assigning proper tuning strategies to different groups. These patterns lead to new PEFT methods (S4-model and S4-3b-model) that consistently and significantly outperform existing PEFT strategies across various backbone models (T5, RoBERTa, BART, XLNet) and different NLP tasks (GLUE, XSum, WMT, SuperGLUE).",
    "Methodology": "The methodology involves introducing parameter-efficient fine-tuning design spaces that parameterize both tuning structures and strategies. These design spaces are defined by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. The process starts with a relatively unconstrained initial design space (S0). This space is then progressively refined by applying different constraints to each component. Design patterns are discovered through a greedy selection approach at each stage, comparing the overall quality of models sampled from design spaces with different constraints. Model quality is quantified by randomly sampling 100 models, fine-tuning them for 3 epochs (a low-compute, low-epoch regime), and computing the average performance on the GLUE benchmark.",
    "Experimental Setup": "For discovering design patterns, the T5-base and T5-3b pretrained backbone models were used with the GLUE benchmark, covering single-sentence, similarity/paraphrase, and inference tasks. Metrics included Matthews correlation (CoLA), Spearman correlation (STS-B), and accuracy for others. For evaluation, the discovered patterns were applied to T5-base/3b, RoBERTa-base/large, and BART-base/large on GLUE, XSum (Abstractive Summarization, ROUGE scores), and WMT 2016 en-ro (Machine Translation, BLEU scores). Additionally, XLNet-base/large models were evaluated on SuperGLUE datasets (BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, WSC). Implementations used Hugging Face, with total trainable parameters set to 0.5% for most methods and 0.1% for BitFit. Training involved a linear decay scheduler with a warmup ratio of 0.06, batch sizes of 128 (base) or 64 (large), a maximum learning rate of 5e-5, and up to 10 training epochs (or 3 for pattern discovery). All experiments were performed using 8 A100 GPUs.",
    "Limitations": "The study's primary limitation is that its goal was not to exhaustively enumerate all possible design spaces or constraints, but rather to demonstrate the utility of the design space perspective in informing PEFT research. Computational efficiency constraints limited the exhaustive exploration of all possible design choices and combinations within the defined design space components. While a low-epoch regime (3 epochs) was used for design pattern discovery, the authors assert its sufficiency for stable performance conclusions. The impact of varying the fixed discovery sequence (grouping, parameter allocation, tunable groups, strategy assignment) was not explored.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "if is_torch_available():_import_structure[\"adapters\"] = [\"ADAPTER_CACHE\",\"ADAPTER_CONFIG_MAP\",\"ADAPTERFUSION_CONFIG_MAP\",\"ADAPTER_MODEL_MAPPING\",\"DEFAULT_ADAPTER_CONFIG\",\"DEFAULT_ADAPTERFUSION_CONFIG\",\"MODEL_WITH_HEADS_MAPPING\",\"AdapterArguments\",\"AdapterConfig\",\"AdapterConfigBase\",\"AdapterFusionConfig\",\"AdapterInfo\",\"AdapterLayer\",\"AdapterLayerBase\",\"AdapterSetup\",\"AdapterTrainer\",\"AdapterType\",\"AutoAdapterModel\",\"AutoModelWithHeads\",\"BartAdapterModel\",\"BartModelWithHeads\",\"BeitAdapterModel\",\"BertAdapterModel\",\"BertModelWithHeads\",\"CompacterConfig\",\"CompacterPlusPlusConfig\",\"ConfigUnion\",\"DebertaAdapterModel\",\"DebertaV2AdapterModel\",\"DistilBertAdapterModel\",\"DistilBertModelWithHeads\",\"DynamicAdapterFusionConfig\",\"EmbeddingAdaptersMixin\",\"ForwardContext\",\"GPT2AdapterModel\",\"GPT2ModelWithHeads\",\"GPTJAdapterModel\",\"HoulsbyConfig\",\"HoulsbyInvConfig\",\"IA3Config\",\"InvertibleAdaptersMixin\",\"LoRAConfig\",\"MAMConfig\",\"MBartAdapterModel\",\"MBartModelWithHeads\",\"ModelAdaptersConfig\",\"ModelAdaptersMixin\",\"ModelWithFlexibleHeadsAdaptersMixin\",\"ModelWithHeadsAdaptersMixin\",\"MultiLingAdapterArguments\",\"ParallelConfig\",\"PfeifferConfig\",\"PfeifferInvConfig\",\"PrefixTuningConfig\",\"RobertaAdapterModel\",\"RobertaModelWithHeads\",\"Seq2SeqAdapterTrainer\",\"StaticAdapterFusionConfig\",\"T5AdapterModel\",\"T5ModelWithHeads\",\"PEFTConfig\",\"ViTAdapterModel\",\"XLMRobertaAdapterModel\",\"XLMRobertaModelWithHeads\",\"get_adapter_config_hash\",\"get_adapter_info\",\"list_adapters\",]",
    "Experiment Result": "The repository, named \"adapter-transformers\", explicitly implements parameter-efficient fine-tuning (PEFT) using the Adapter framework, as indicated by the import structure for 'adapters' in `models/transformers/__init__.py`. This includes various adapter configurations (e.g., HoulsbyConfig, LoRAConfig, PfeifferConfig) and components (AdapterLayer, AdapterTrainer) that facilitate defining tuning structures and strategies. The `setup.py` file lists `datasets` as a dependency, which is relevant for computing performance on the GLUE benchmark. Additionally, `optuna` and `ray[tune]` are listed as dependencies, suggesting capabilities for hyperparameter optimization and exploring design spaces. However, the provided repository content (specifically `setup.py` and `__init__.py` files) does not contain explicit code or configurations detailing the experimental setup of \"randomly sampling 100 models,\" \"fine-tuning them for 3 epochs,\" or the precise \"greedy selection approach\" for discovering design patterns, as described in the method."
}{
    "Title": "ReFT: Representation Finetuning for Language Models",
    "Main Contributions": "The paper introduces Representation Finetuning (ReFT), a family of parameter-efficient finetuning (PEFT) methods that adapt large language models (LMs) by learning task-specific interventions on hidden representations, rather than modifying model weights. The primary contribution is Low-rank Linear Subspace ReFT (LoReFT), and its ablation, DiReFT. These methods are demonstrated to be 15x–65x more parameter-efficient than state-of-the-art PEFTs like LoRA, while achieving competitive or superior performance across diverse NLP benchmarks, including commonsense reasoning, instruction-following, and natural language understanding. A generic ReFT training library is publicly released.",
    "Methodology": "ReFT methods operate on a frozen base model and manipulate a small fraction of hidden representations at inference time. LoReFT, a core instantiation, intervenes on hidden representations within a linear subspace defined by a low-rank projection matrix (R) with orthonormal rows, learning a linear projection (W) and bias (b) such that ΦLoReFT(h) = h + Rᵀ(Wh + b - Rh). DiReFT is an ablation that removes the orthogonality constraint and difference operation, resembling LoRA applied to representations. Training objectives include minimizing cross-entropy loss for generation tasks and using a classification head for single-label classification. The ReFT framework is generalized as a set of non-overlapping interventions, each specified by an intervention function, target input positions, and target layer. Hyperparameters such as prefix/suffix intervention positions, specific layers, and tied parameters are tuned.",
    "Experimental Setup": "Experiments were conducted on LLaMA-family models (7B, 13B) and RoBERTa-base/large (125M, 350M) across four NLP benchmarks. Commonsense reasoning involved eight datasets combined into COMMONSENSE 170K (e.g., BoolQ, HellaSwag). Arithmetic reasoning used seven datasets combined into MATH10K (e.g., AQuA, GSM8K). Instruction-following utilized Llama-2 7B with Ultrafeedback, evaluated by Alpaca-Eval v1.0 win-rate. Natural Language Understanding used the GLUE benchmark. Baselines included Prefix-tuning, Adapter-tuning (Series/Parallel), BitFit, RED, LoRA, and DoRA. Hyperparameter tuning was performed exclusively on development sets (e.g., GSM8K, Alpaca-52K, split GLUE validation sets) to avoid test-set overfitting. All experiments were run on single NVIDIA A100/RTX 6000 GPUs, loading models in torch.bfloat16. Accuracy and win-rate were primary metrics, alongside parameter efficiency.",
    "Limitations": "The research primarily focused on LLaMA-family models due to limited resources, suggesting a need for exploration on other model families and vision-language models like LLaVA. The full capabilities of ReFT are yet to be explored due to the large hyperparameter search space, indicating a need for automated search. Deeper understanding of ReFT's working mechanisms, particularly concerning causal pathways and upstream computations, is required. The paper notes that arithmetic reasoning performance was not as strong as other PEFTs, potentially due to longer generation lengths reducing intervention effects or increased task difficulty. It also highlights a general limitation in PEFT research practices, where test-set 'hill-climbing' leads to overfitting and unfair comparisons, advocating for improved evaluation benchmarks.",
    "Future Research Directions": "Future work includes expanding ReFT's evaluation to other model architectures and vision-language models. Automating the hyperparameter search process for ReFT is a key direction. Further research is planned to understand *why* ReFT is effective, investigating its causal effects and how it modifies or creates causal pathways within LMs. Exploring more structured ReFTs to manipulate complex causal pathways is also suggested. Initial explorations into the compositionality of learned orthogonal subspaces for multi-task learning and few-shot adaptation for LM personalization are presented as promising avenues for extension. The authors also call for the introduction of new benchmarks for PEFTs and ReFTs that enforce compute/time-matched hyperparameter tuning and disallow test-set-based model selection.",
    "Experiment Code": "import torch\nfrom collections import OrderedDict\n\nfrom pyvene import (\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention,\n)\nfrom transformers.activations import ACT2FN\n\n\nclass LowRankRotateLayer(torch.nn.Module):\n    \"\"\"A linear transformation with orthogonal initialization.\"\"\"\n\n    def __init__(self, n, m, init_orth=True):\n        super().__init__()\n        # n > m\n        self.weight = torch.nn.Parameter(torch.empty(n, m), requires_grad=True)\n        if init_orth:\n            torch.nn.init.orthogonal_(self.weight)\n\n    def forward(self, x):\n        return torch.matmul(x.to(self.weight.dtype), self.weight)\n\n\nclass LoreftIntervention(\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention\n):\n    \"\"\"\n    LoReFT(h) = h + R^T(Wh + b − Rh)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, keep_last_dim=True)\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, kwargs[\"low_rank_dimension\"], init_orth=True)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.learned_source = torch.nn.Linear(\n            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(\n            kwargs[\"dtype\"] if \"dtype\" in kwargs else torch.bfloat16)\n        self.dropout = torch.nn.Dropout(kwargs[\"dropout\"] if \"dropout\" in kwargs else 0.0)\n        self.act_fn = ACT2FN[\"linear\"] if \"act_fn\" not in kwargs or kwargs[\"act_fn\"] is None else ACT2FN[kwargs[\"act_fn\"]]\n        \n    def forward(\n        self, base, source=None, subspaces=None\n    ):\n        rotated_base = self.rotate_layer(base)\n        output = base + torch.matmul(\n            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n        )\n        return self.dropout(output.to(base.dtype))\n\n    def state_dict(self, *args, **kwargs):\n        state_dict = OrderedDict()\n        for k, v in self.learned_source.state_dict().items():\n            state_dict[k] = v\n        state_dict[\"rotate_layer\"] = self.rotate_layer.weight.data\n        return state_dict\n\n    def load_state_dict(self, state_dict, *args, **kwargs):\n        self.learned_source.load_state_dict(state_dict, strict=False)\n\n        overload_w = state_dict[\"rotate_layer\"].to(\n            self.learned_source.weight.device)\n        overload_w_width = overload_w.shape[-1]\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, overload_w_width, init_orth=True).to(\n            self.learned_source.weight.device)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.rotate_layer.parametrizations.weight[0].base[:,:overload_w_width] = overload_w\n        assert torch.allclose(self.rotate_layer.weight.data, overload_w.data) == True\n        \n        return\n\n\nclass DireftIntervention(\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention\n):\n    \"\"\"\n    DiReFT(h) = h + R^T(Wh + b)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, keep_last_dim=True)\n        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"], init_orth=True)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.learned_source = torch.nn.Linear(\n            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(\n            kwargs[\"dtype\"] if \"dtype\" in kwargs else torch.bfloat16)\n        self.dropout = torch.nn.Dropout(kwargs[\"dropout\"] if \"dropout\" in kwargs else 0.0)\n        self.act_fn = ACT2FN[\"linear\"] if \"act_fn\" not in kwargs or kwargs[\"act_fn\"] is None else ACT2FN[kwargs[\"act_fn\"]]\n        \n    def forward(\n        self, base, source=None, subspaces=None\n    ):\n        cast_base = base.to(self.learned_source.weight.dtype)\n        output = base + torch.matmul(\n            (self.act_fn(self.learned_source(cast_base))).to(self.rotate_layer.weight.dtype), self.rotate_layer.weight.T\n        )\n        return self.dropout(output.to(base.dtype))\n\n# --- Initialization and Application of ReFT (simplified from examples/loreft/train.py) --- \n\nfrom pyreft import get_reft_model, ReftConfig, LoreftIntervention, ReftTrainerForCausalLM, ReftDataCollator\nimport transformers\nimport torch\n\ndef setup_reft_model_and_trainer(model_name_or_path, training_args, config, tokenizer, train_dataset, data_collator_fn, task_type):\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_name_or_path,\n        torch_dtype=training_args.dtype if training_args.dtype != \"float8\" else None,\n        load_in_8bit=True if training_args.dtype == \"float8\" else False,\n        device_map=\"cuda\"\n    )\n\n    # Parsing layers arg\n    layers_to_intervene = []\n    if training_args.layers.strip() == \"\":\n        layers_to_intervene = []\n    elif training_args.layers != \"all\":\n        layers_to_intervene = [int(l) for l in training_args.layers.split(\";\")]\n    else:\n        temp_config = transformers.AutoConfig.from_pretrained(model_name_or_path)\n        layers_to_intervene = [l for l in range(temp_config.num_hidden_layers)]\n\n    if \"+\" in training_args.position and not training_args.share_weights:\n        layers_to_intervene += layers_to_intervene\n\n    representations = [{\n        \"layer\": l, \"component\": \"block_output\", # For Llama-style models\n        \"low_rank_dimension\": training_args.rank,\n        \"intervention\": LoreftIntervention(\n            embed_dim=config.hidden_size, \n            low_rank_dimension=training_args.rank,\n            dropout=training_args.dropout, \n            dtype=torch.bfloat16, # Assuming bfloat16 for interventions\n            act_fn=training_args.act_fn, \n            device=\"cuda\",\n            add_bias=training_args.add_bias\n        )\n    } for l in layers_to_intervene]\n    \n    reft_config = ReftConfig(representations=representations)\n    reft_model = get_reft_model(model, reft_config, set_device=True, disable_model_grads=True)\n    \n    reft_model.print_trainable_parameters()\n    reft_model.model.train()\n\n    data_collator = ReftDataCollator(data_collator=data_collator_fn)\n\n    trainer_class = ReftTrainerForCausalLM # Example for CausalLM task\n    trainer = trainer_class(\n        model=reft_model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=None, # Simplified for example\n        data_collator=data_collator\n    )\n    return trainer\n\n# --- Intervention Location Calculation (from pyreft/dataset.py) ---\n\ndef parse_positions(positions: str):\n    first_n, last_n = 0, 0\n    if \"+\" in positions:\n        first_n = int(positions.split(\"+\")[0].strip(\"f\"))\n        last_n = int(positions.split(\"+\")[1].strip(\"l\"))\n    else:\n        if \"f\" in positions:\n            first_n = int(positions.strip(\"f\"))\n        elif \"l\" in positions:\n            last_n = int(positions.strip(\"l\"))\n    return first_n, last_n\n\ndef get_intervention_locations(**kwargs):\n    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n    last_position = kwargs[\"last_position\"]\n    if \"positions\" in kwargs:\n        _first_n, _last_n = parse_positions(kwargs[\"positions\"])\n    else:\n        _first_n, _last_n = kwargs[\"first_n\"], kwargs[\"last_n\"]\n    num_interventions = kwargs[\"num_interventions\"]\n    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n\n    first_n = min(last_position // 2, _first_n)\n    last_n = min(last_position // 2, _last_n)\n\n    pad_amount = (_first_n - first_n) + (_last_n - last_n)\n    pad_position = -1 if pad_mode == \"first\" else last_position\n    if share_weights or (first_n == 0 and last_n == 0):\n        position_list = [i for i in range(first_n)] + \\\n            [i for i in range(last_position - last_n, last_position)] + \\\n            [pad_position for _ in range(pad_amount)]\n        intervention_locations = [position_list]*num_interventions\n    else:\n        left_pad_amount = (_first_n - first_n)\n        right_pad_amount = (_last_n - last_n)\n        left_intervention_locations = [i for i in range(first_n)] + [pad_position for _ in range(left_pad_amount)]\n        right_intervention_locations = [i for i in range(last_position - last_n, last_position)] + \\\n            [pad_position for _ in range(right_pad_amount)]\n        left_len = len(left_intervention_locations)\n        right_len = len(right_intervention_locations)\n        if left_len > right_len:\n            right_intervention_locations += [pad_position for _ in range(left_len-right_len)]\n        else:\n            left_intervention_locations += [pad_position for _ in range(right_len-left_len)]\n        intervention_locations = [left_intervention_locations]*(num_interventions//2) + \\\n            [right_intervention_locations]*(num_interventions//2)\n    \n    return intervention_locations\n\n# --- DPO Training Loop (from examples/dpo/dpo_trainer.py) --- \nfrom typing import Dict, List, Union, Tuple\nclass DPOReftTrainer(transformers.Trainer):\n    def concatenated_forward(\n        self, model: torch.nn.Module, batch: Dict[str, Union[List, torch.LongTensor]], reference: bool = False\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        concatenated_batch = self.concatenated_inputs(\n            batch,\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n            padding_value=self.padding_value,\n            device=self.accelerator.device,\n        )\n        len_chosen = batch[\"chosen_labels\"].shape[0]\n\n        # Create concatenated intervention locations by doubling the list\n        intervention_locations = torch.tensor(\n            batch['intervention_locations'] + batch['intervention_locations']\n        ).transpose(0, 1).tolist()\n\n        model_kwargs = (\n            {\n                \"labels\": concatenated_batch[\"concatenated_labels\"],\n                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n            }\n            if self.is_encoder_decoder\n            else {}\n        )\n        if reference:\n            all_outputs = model.model(\n                input_ids=concatenated_batch[\"concatenated_input_ids\"].to(model.get_device()),\n                attention_mask=concatenated_batch[\"concatenated_attention_mask\"].to(model.get_device()),\n                use_cache=False,\n                **model_kwargs,\n            )\n        else:\n            _, all_outputs = model(\n                {\n                    \"input_ids\": concatenated_batch[\"concatenated_input_ids\"].to(model.get_device()),\n                    \"attention_mask\": concatenated_batch[\"concatenated_attention_mask\"].to(model.get_device()),\n                },\n                unit_locations={\n                    \"sources->base\": (None, intervention_locations)\n                },\n                use_cache=False,\n                **model_kwargs,\n            )\n        all_logits = all_outputs.logits\n        all_logps = self.get_batch_logps(\n            all_logits,\n            concatenated_batch[\"concatenated_labels\"],\n            average_log_prob=self.loss_type == \"ipo\",\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n        )\n\n        chosen_logps = all_logps[:len_chosen]\n        rejected_logps = all_logps[len_chosen:]\n        chosen_logits = all_logits[:len_chosen]\n        rejected_logits = all_logits[len_chosen:]\n\n        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)\n",
    "Experiment Result": "ReFT methods operate on a frozen base model and manipulate a small fraction of hidden representations at inference time. LoReFT, a core instantiation, intervenes on hidden representations within a linear subspace defined by a low-rank projection matrix (R) with orthonormal rows, learning a linear projection (W) and bias (b) such that ΦLoReFT(h) = h + Rᵀ(Wh + b - Rh). DiReFT is an ablation that removes the orthogonality constraint and difference operation, resembling LoRA applied to representations (ΦDiReFT(h) = h + Rᵀ(Wh + b)).\n\n**Experimental Settings & Configurations:**\n\n*   **Intervention Types:**\n    *   `LoreftIntervention`: Implements the core LoReFT method.\n    *   `DireftIntervention`: Implements the DiReFT method.\n    *   Other variations include `NoreftIntervention` (removes orthogonality), `ConsreftIntervention` (constant bias only), `LobireftIntervention` (low-rank bitfit), `NodireftIntervention` (removes orthogonality + direct edit like LoRA on timestep).\n\n*   **Base Models:** Supported models include `transformers.AutoModelForCausalLM` (e.g., Llama, Gemma) for generation tasks and `transformers.AutoModelForSequenceClassification` (e.g., RoBERTa) for classification tasks. Models are typically loaded with `torch_dtype=torch.bfloat16` for memory efficiency.\n\n*   **Intervention Layers (`layers`):** Can be specified as `\"all\"` (intervene on all hidden layers) or a semicolon-separated list of layer indices (e.g., `\"2;10;18;26\"`). If `\"+\"` is in `position` and `share_weights` is `False`, the `layers` list is doubled to apply separate interventions.\n\n*   **Intervention Position (`position`):** Determines where in the sequence interventions are applied:\n    *   `\"fN\"`: First N tokens.\n    *   `\"lN\"`: Last N tokens.\n    *   `\"fN+lN\"`: First N and last N tokens (e.g., `\"f1+l1\"`).\n\n*   **Low-Rank Dimension (`rank`):** The dimensionality of the low-rank projection (e.g., 1, 4, 8).\n\n*   **Shared Weights (`share_weights`):** A boolean flag; if `True` for `fN+lN` positions, the same intervention weights are used for both parts of the sequence.\n\n*   **Training Objectives:**\n    *   **Generation Tasks (e.g., Alpaca, Commonsense, Math, GSM8k):** Minimizes standard cross-entropy loss, using `ReftTrainerForCausalLM`. Labels are masked to only compute loss on the generated portion.\n    *   **Direct Preference Optimization (DPO):** Uses `DPOReftTrainer` (a subclass of `trl.DPOTrainer`) with a DPO loss function to align model responses with human preferences. The loss maximizes the difference between log-probabilities of chosen and rejected responses, incorporating the `beta` hyperparameter (e.g., 0.1).\n    *   **Reward Modeling:** Uses `ReftTrainerForRewardModelling` with a custom loss that maximizes the difference between reward scores of chosen and rejected outputs using a log-sigmoid function.\n    *   **Classification Tasks (e.g., GLUE):** Minimizes cross-entropy loss (for multi-class) or MSE loss (for regression like STS-B), using `ReftTrainerForSequenceClassification`. Optionally, the base model's classifier head gradients can be enabled (`allow_cls_grad`).\n\n*   **Hyperparameters:**\n    *   `epochs`: Number of training epochs (e.g., 1, 3).\n    *   `lr`: Learning rate (e.g., 5e-3, 1e-3).\n    *   `batch_size` (`per_device_train_batch_size`): Per-device training batch size (e.g., 4, 10).\n    *   `gradient_accumulation_steps`: Steps for accumulating gradients (e.g., 4).\n    *   `warmup_ratio`: Learning rate scheduler warmup ratio (e.g., 0.00).\n    *   `weight_decay`: Weight decay for the optimizer (e.g., 0.00).\n    *   `dropout`: Dropout rate for the intervention layer (e.g., 0.00).\n    *   `act_fn`: Activation function for the `learned_source` layer within interventions (e.g., `linear`, `None`).\n    *   `add_bias`: Whether to add a bias term in certain intervention types (`True`/`False`).\n    *   `max_length` (`model_max_length`): Maximum sequence length for tokenization (e.g., 512, 2048).\n    *   `dtype`: Data type for model and interventions (e.g., `bfloat16`, `float32`).\n\n*   **Data Processing:**\n    *   Datasets are loaded using `ReftDataset` subclasses like `ReftSupervisedDataset`, `ReftRewardDataset`, `LoReftGLUEDataset` for task-specific formatting. Custom functions like `make_multiple_position_supervised_data_module` are used for specific data preparation (e.g., DPO).\n    *   `ReftDataCollator` handles padding of `input_ids`, `labels`, and `intervention_locations` to the longest sequence in a batch.\n    *   Tokenizers are configured with `padding_side=\"right\"` for training and `\"left\"` for generation tasks to enable interventions on prompts.\n\n*   **Inference & Evaluation:**\n    *   Text generation is performed using `reft_model.generate`, passing `unit_locations` for dynamic intervention application. Decoding strategies include `greedy_decoding`, `temperature`, `top_p`, `top_k`, and `num_beams` (e.g., 4 for complex reasoning tasks, 1 for greedy).\n    *   Evaluation metrics vary by task (e.g., Exact Match for reasoning, accuracy for GLUE/reward modeling, DPO specific metrics).\n\n*   **Model Saving:** The intervened model is saved using `reft_model.save()` which saves only the intervention parameters and references the original model, allowing for efficient deployment of small adapter weights."
}{
    "Title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",
    "Main Contributions": "The paper introduces SVFT (Singular Vectors guided Fine-Tuning), a novel Parameter-Efficient Fine-Tuning (PEFT) method that addresses the performance gap between existing PEFT techniques and full fine-tuning, while maintaining high parameter efficiency. SVFT updates pre-trained weight matrices as a sparse combination of outer products of their singular vectors, training only the coefficients of these combinations. This approach ensures the perturbation structure depends on the specific weight matrix, unlike prior methods. SVFT recovers up to 96% of full fine-tuning performance using 0.006% to 0.25% of parameters, outperforming existing PEFT methods that typically recover up to 85% performance with 0.03% to 0.8% of the parameter budget. The work also introduces and empirically validates four variants for parameterizing weight updates (Plain, Random, Banded, Top-k) and theoretically demonstrates that SVFT can induce higher-rank perturbations for a given parameter budget compared to other PEFT techniques.",
    "Methodology": "SVFT operates by first performing Singular Value Decomposition (SVD) on a pre-trained weight matrix W0 (W0 = UΣV^T). The weight update ∆W is then parameterized as ∆W = UMV^T, where U and V are fixed and frozen left and right singular vectors of W0, and M is a d1 × d2 sparse trainable matrix. The forward pass becomes h = W0x + ∆Wx = U(Σ + M)V^T x. The sparsity pattern of M is pre-determined and fixed, with four main variants explored: Plain (SVFTP) where M is diagonal, Banded (SVFTB d) where M is a banded matrix, Random (SVFTR d) where k elements are randomly selected to be learnable, and Top-k (SVFTT d) where k elements are selected based on the alignment u^T_i v_j. This method allows for a single trainable parameter (m_ij) per rank-one matrix in the update, in contrast to methods like LoRA which require d1+d2 parameters.",
    "Experimental Setup": "The research adapted widely-used language models (encoder-only DeBERTaV3base, decoder-only Gemma-2B/7B, LLaMA-3-8B) and vision transformer models (ViT-B/16, ViT-L/16 pre-trained on ImageNet-21k). Baselines included Full Fine-Tuning (FT), LoRA, DoRA, BOFT, and VeRA. Language tasks comprised Natural Language Generation (GSM-8K, MATH fine-tuned on MetaMathQA-40K), Commonsense Reasoning (8 benchmarks like BoolQ, PIQA, HellaSwag, trained on an amalgamated 15K examples), and Natural Language Understanding (GLUE benchmark for classification/regression). Vision tasks included 4 benchmarks (CIFAR-100, Food101, RESISC45, Flowers102), fine-tuned on 10 samples per class. Performance was evaluated using accuracy for most tasks, Matthew's correlation for CoLA, and Pearson correlation for STS-B. Hardware-level optimizations like mixed-precision weights (bfloat16) were employed. SVFT was applied to various transformer modules (e.g., Q, K, V, U, D, O, G for LLMs; Q, V, K, U, D for Vision).",
    "Limitations": "SVFT incurs additional GPU memory usage compared to LoRA, as it requires computing and storing both left and right singular vectors, roughly doubling LoRA's memory footprint (though still less than BOFT). While system-level optimizations like mixed-precision weights (bfloat16) were used to mitigate this, further memory reduction techniques are needed. The paper also acknowledges that enabling easier personalization of foundational models through computational efficiency (smaller parameter footprint) can have both positive and negative societal impacts.",
    "Future Research Directions": "Future work will focus on exploring further memory-reduction techniques for SVFT, such as quantization, to address the increased GPU memory usage. Additionally, the researchers plan to develop more principled methods for generating sparsity patterns within the trainable matrix M, aiming to potentially achieve further performance improvements beyond the empirically validated choices.",
    "Experiment Code": "import time\nimport math\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, transpose\n\n\ndef create_orthonormal_matrix(A):\n    # returns an orthonormal matrix (square) of size (min(A.shape), min(A.shape))\n    Q, R = torch.qr(A)\n    return Q\n\n\ndef get_target_modules_list(model, target_modules):\n    target_names = []\n    for n, _ in model.named_modules():\n        if any(t in n for t in target_modules):\n            target_names.append(n)\n    return target_names\n\n\ndef replace_svft_with_fused_linear(model, target_modules_list):\n    print(\"Replacing SVFT layers with new Linear layers\")\n\n    # filter out svft layer\n    target_modules_list = [l for l in target_modules_list if \"svft_layer\" not in l]\n\n    for target_path in tqdm(reversed(target_modules_list), total=len(target_modules_list)):\n        parent_path = target_path[: target_path.rfind(\".\")] if \".\" in target_path else \"\"\n        target_name = target_path.split(\".\")[-1]\n        parent = model.get_submodule(parent_path) if parent_path else model\n        target = model.get_submodule(target_path)\n        in_dim = target.svft_layer.v.shape[1]\n        out_dim = target.svft_layer.u.shape[0]\n        if target.bias is None:\n            lin = torch.nn.Linear(in_dim, out_dim, bias=False)\n        else:\n            lin = torch.nn.Linear(in_dim, out_dim, bias=True)\n            lin.bias.data = target.bias.data\n        lin.weight.data = target.merge_and_unload()\n        parent.__setattr__(target_name, lin)\n\n\ndef create_and_replace_modules(model, target_modules_list, create_fn):\n    print(\"Replacing Linear layers with SVFT layers\")\n\n    for target_path in tqdm(reversed(target_modules_list), total=len(target_modules_list)):\n        parent_path = target_path[: target_path.rfind(\".\")] if \".\" in target_path else \"\"\n        target_name = target_path.split(\".\")[-1]\n        parent = model.get_submodule(parent_path) if parent_path else model\n        target = model.get_submodule(target_path)\n        parent.__setattr__(target_name, create_fn(target))\n\n\nclass SVFTLayer(nn.Module):\n    def __init__(self, u, s, v, off_diag, pattern=\"banded\", rank=None, fill_orthonormal=False):\n\n        \"\"\"\n        @inputs:\n            u: torch.Tensor. Left singular vectors of pre-trained weight matrix\n            s: torch.Tensor. Singular values of pre-trained weight matrix\n            v: torch.Tensor. Right singular vectors of pre-trained weight matrix\n            off_diag: int. Total off-diagonals to be used to populate matrix M (as referred in main paper)\n            pattern: str. Choices: \"banded\", \"random\", \"top_k\". Using \"banded\" with off_diag=1 simulates SVFT-plain\n            rank: int. Constraints how many singular vectors and values to use.\n            fill_orthonormal: bool. To determine if random orthonormal basis should be used\n        \"\"\"\n\n        super().__init__()\n\n        self.off_diag = off_diag\n        rank = s.shape[0] if rank is None else min(s.shape[0], rank)\n        self.n = rank\n        diff_rank = s.shape[0] - rank\n\n        if fill_orthonormal:\n            Q_u = torch.randn_like(u).to(s.device)\n            torch.nn.init.orthogonal_(Q_u)\n            Q_v = torch.randn_like(v).to(s.device)\n            torch.nn.init.orthogonal_(Q_v)\n\n            u = torch.cat([u[:, :rank], Q_u[:, :diff_rank]], dim=1)\n            v = torch.cat([v[:rank, :], Q_v[:diff_rank, :]], dim=0)\n            s = torch.cat([s[:rank], torch.zeros(diff_rank).to(s.device)], dim=0)\n            self.n = s.shape[0]\n\n        else:\n            s = s[:rank]\n            u = u[:, :rank]\n            v = v[:rank, :]\n\n        self.u = nn.Parameter(u.clone().detach().contiguous(), requires_grad=False)\n\n        s_pre = s.cpu().detach().clone().contiguous()\n        self.s_pre_edge_index = torch.sparse.spdiags(s_pre, torch.LongTensor([0]), (self.n, self.n)).coalesce().indices()\n        self.s_pre = nn.Parameter(s_pre, requires_grad=False)\n        \n        if pattern==\"banded\":  \n            diags = 2*self.off_diag + 1\n            offsets_positive = torch.arange(0, self.off_diag+1)\n            offsets_negative = torch.arange(-1, -self.off_diag-1, -1)\n            self.offsets  = torch.cat([offsets_positive, offsets_negative])\n            self.s_edge_index = torch.sparse.spdiags(torch.randn([diags, self.n]), self.offsets, (self.n, self.n)).coalesce().indices()\n            self.s = torch.nn.Parameter(torch.zeros(self.s_edge_index.shape[1]), requires_grad=True)\n\n        elif pattern==\"random\":\n            print(\"Random pattern\")\n            k = self.n*(2*self.off_diag+1) - self.off_diag*(self.off_diag+1)\n            rows = torch.randint(0, self.n, (k,))\n            cols = torch.randint(0, self.n, (k,))\n            self.s_edge_index = torch.stack([rows, cols])\n            self.s = torch.nn.Parameter(torch.zeros(k), requires_grad=True)\n\n        elif pattern==\"top_k\":\n\n            if u.shape == v.shape:\n                coeffs = u@v.T\n            else:\n                coeffs = u if u.shape[0]==u.shape[1] else v\n\n            k = self.n*(2*self.off_diag+1) - self.off_diag*(self.off_diag+1)\n            # Flatten the tensor to 1D\n            flattened_tensor = coeffs.contiguous().view(-1)\n            _, top_indices_flat = torch.topk(flattened_tensor, k)\n            num_rows, num_cols = coeffs.size()\n            rows = top_indices_flat // num_cols\n            cols = top_indices_flat % num_cols\n            self.s_edge_index = torch.stack([rows, cols])\n            self.s = torch.nn.Parameter(torch.zeros(k), requires_grad=True)\n       \n        torch.nn.init.kaiming_normal_(self.s[None, :])\n        self.s.squeeze()\n\n        self.register_buffer('s_pre_row', self.s_pre_edge_index[0])\n        self.register_buffer('s_pre_col', self.s_pre_edge_index[1])\n        self.register_buffer('s_row', self.s_edge_index[0])\n        self.register_buffer('s_col', self.s_edge_index[1])\n\n        self.gate = nn.Parameter(torch.tensor([0.], dtype=torch.float32), requires_grad=True)\n\n        self.v = nn.Parameter(v.clone().detach().contiguous(), requires_grad=False) \n\n\n    def forward(self, x):\n        x  = x @ self.get_weights() \n        return x\n\n\n    def get_weights(self):\n        s = SparseTensor(row=self.s_row, col=self.s_col, value=self.s*F.sigmoid(self.gate))\n        s_pre = SparseTensor(row=self.s_pre_row, col=self.s_pre_col, value=self.s_pre)\n        del_s = s_pre + s\n        weight = (del_s @ self.v).T\n        weight = weight @ self.u.T\n        return weight\n    \n\n    def merge_and_unload(self):\n        return self.get_weights().T.contiguous()\n\n   \nclass LinearWithSVFT(nn.Module):\n\n    def __init__(self, linear, off_diag, pattern=\"banded\", rank=None, fill_orthonormal=False):\n        \"\"\"\n        @inputs:\n                linear: torch.Tensor. Linear Layer that has to adapted\n                off_diag: int. total number off diagonals to be used if pattern is 'banded' \n                          for remaining patterns, equivalent number of learnable parameters are learnt\n                rank: SVD rank \n                fill_orthonormal: bool. To determine if random orthonormal basis should be used\n        \"\"\"\n        \n        super().__init__()\n\n        self.bias = linear.bias\n\n        # since linear.weight is on GPU, computing SVD will be significantly faster\n        svd = torch.linalg.svd(linear.weight, full_matrices=False)\n\n        self.svft_layer = SVFTLayer(svd[0],\n                                    svd[1],\n                                    svd[2],\n                                    off_diag=off_diag,\n                                    pattern=pattern,\n                                    rank=rank,\n                                    fill_orthonormal=fill_orthonormal)\n\n    def forward(self, x):\n        if self.bias is not None:\n            return self.svft_layer(x) + self.bias\n\n        else:\n            return self.svft_layer(x)\n\n    def merge_and_unload(self):\n        return self.svft_layer.merge_and_unload()\n\n\ndef freeze_model(model, exclude_list = None):\n    ''' Freeze all parameters of the model '''\n    if exclude_list is None:\n        exclude_list = []\n\n    for n, p in model.named_parameters():\n        if not any(e in n for e in exclude_list):\n            p.requires_grad = False\n\n@dataclass\nclass SVFTArguments:\n    adapter_name: str = field(default=\"svft\", metadata={\"help\": \"Adapter name.\"})\n    pattern: str = field(default=\"banded\", metadata={\"help\": \"Choose from 'banded', 'random', 'top_k'.\"})\n    off_diag: int = field(default=0, metadata={\"help\": \"Number of off-diagonal blocks.\"})\n    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n    rank: int = field(default=None, metadata={\"help\": \"Rank of the low-rank decomposition. Only used in truncated-SVFT\"})\n    fill_orthonormal: bool = field(default=False, metadata={\"help\": \"Initialize singular vectors from a random orthonomal bases. Only applicable if less than full-rank.\"})\n\n# Code from train() function relevant to SVFT adapter setup\nif svft_args.adapter_name == 'svft':\n    # for SVFT turn off gradient requirement for all layers\n    # PEFT library handles this internally\n    for param in model.parameters():\n        param.requires_grad = False\n\n    print(f\"Target Modules: {svft_args.target_modules}\")\n    assign_svft_layer = partial(LinearWithSVFT, \n                                off_diag=svft_args.off_diag, \n                                pattern=svft_args.pattern, \n                                rank=svft_args.rank, \n                                fill_orthonormal=svft_args.fill_orthonormal)\n    \n    create_and_replace_modules(model, get_target_modules_list(model, svft_args.target_modules), assign_svft_layer)\n\n# Code from train() function for merging SVFT layers after training\nif svft_args.adapter_name == 'svft':\n    replace_svft_with_fused_linear(model, get_target_modules_list(model, svft_args.target_modules))\n",
    "Experiment Result": "SVFT operates by first performing Singular Value Decomposition (SVD) on a pre-trained weight matrix W0 (W0 = UΣV^T). The weight update ∆W is then parameterized as ∆W = UMV^T, where U and V are fixed and frozen left and right singular vectors of W0, and M is a d1 × d2 sparse trainable matrix. The forward pass becomes h = W0x + ∆Wx = U(Σ + M)V^T x. The sparsity pattern of M is pre-determined and fixed, with four main variants explored: Plain (SVFTP) where M is diagonal, Banded (SVFTB d) where M is a banded matrix, Random (SVFTR d) where k elements are randomly selected to be learnable, and Top-k (SVFTT d) where k elements are selected based on the alignment u^T_i v_j. A learnable gate parameter (self.gate) is used in the implementation to scale the trainable sparse matrix M with a sigmoid activation.\n\nKey configurable parameters for SVFT are:\n- `pattern`: Specifies the sparsity pattern of the trainable matrix M. Choices are \"banded\", \"random\", or \"top_k\".\n- `off_diag`: For the \"banded\" pattern, this defines the number of off-diagonals. For \"random\" and \"top_k\" patterns, this parameter is used to determine the total number of learnable elements.\n- `rank`: An integer specifying the SVD truncation rank, limiting the number of singular vectors and values used in the decomposition.\n- `fill_orthonormal`: A boolean flag. If `True`, and if the specified `rank` is less than the full rank, the singular vectors corresponding to the truncated ranks are initialized using random orthonormal bases.\n- `target_modules`: A list of strings identifying specific linear layer module names (e.g., [\"q_proj\", \"v_proj\"]) within the base model where the SVFT adaptation is applied. By default, it targets `[\"q_proj\", \"v_proj\"]`.\n\nDuring training, all parameters of the base model are frozen. After training, the SVFT layers are replaced with fused linear layers that incorporate the learned adaptations."
}{
    "Title": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning",
    "Main Contributions": "The paper addresses the memory inefficiency of existing Parameter-Efficient Fine-Tuning (PEFT) methods, which still require caching most intermediate activations, similar to full fine-tuning. The main contributions include identifying that preserving the Pre-trained Language Model's (PLM) starting point is crucial for PEFT success. Based on this, the authors propose Memory-Efficient Fine-Tuning (MEFT), a novel approach that inserts adapters into a PLM to make it reversible without additional pre-training. MEFT significantly reduces activation memory by up to 84% compared to full fine-tuning while requiring a negligible amount of trainable parameters. It achieves comparable performance to full fine-tuning on GLUE and question-answering tasks, with similar findings for image classification.",
    "Methodology": "The core methodology revolves around making PLMs reversible by integrating adapters while strictly adhering to the 'Starting Point Hypothesis'. This hypothesis states that new parameters must be initialized to preserve the PLM's output at the beginning of training. Reversible models, by design, recompute intermediate activations during back-propagation from the final output, reducing activation memory from O(N) to O(1) for model depth. Three MEFT architectures are proposed: MEFT 1 designs the PLM layer with an adapter as F and an adapter as G (setting λ→0 and switching output order); MEFT 2 uses an adapter as F and the PLM layer with an adapter as G (setting λ→1, β→0, and switching order); MEFT 3 uses the attention block with an adapter as F and the MLP block with an adapter as G (setting both λ→0 and β→0). Adapters are initialized with weights from N(0, σ^2) to ensure their initial output is near zero. For deeper models, a hybrid approach is suggested, combining vanilla gradient layers (caching) for top layers with reversible shallow layers.",
    "Experimental Setup": "The effectiveness of MEFT was evaluated on eight sequence representation tasks from the GLUE benchmark (MNLI, QQP, QNLI, SST-2, MRPC, RTE, STS-B, CoLA) and five sequence-to-sequence question-answering tasks (OpenBookQA, PIQA, ARC Easy, ARC Challenge, SciQ). An image classification task on SVHN was also used. Various PLM backbones were utilized: BERTbase, RoBERTalarge, and BARTlarge encoder for GLUE tasks; OPT 1.3B and OPT6.7B for question-answering; and ViT for image classification. Baselines included full fine-tuning, several PEFT methods (Houlsby Adapter, Pfeiffer Adapter, Prefix-Tuning, LoRA, MAM, AutoPEFT), and memory-efficient baselines (Y-Tuning, LST). Performance was measured using accuracy, Pearson correlation, and Matthews correlation for GLUE, accuracy for Q&A, and Acc@1 for image classification. Hyperparameters for scaling factors were tuned (optimal 0.1 for those tending to 0), and learning rates, batch sizes, and epochs were swept across specific ranges. Experiments were run on a single NVIDIA RTX A6000 GPU with 48GB memory, with results reported as mean and standard deviation over multiple runs.",
    "Limitations": "The study's limitations include evaluation on a restricted set of tasks, specifically lacking experiments on encoder-decoder models, although decoder-only models were used for sequence-to-sequence tasks. MEFT exhibits lower performance when trained in FP16 compared to FP32, particularly in deeper models, due to accumulated reconstruction error from numerical instability. The recomputation of activations also leads to increased training time, approximately twice that of other PEFT methods or vanilla gradient MEFTs. For very large models (e.g., OPT1.3B, OPT6.7B), the peak memory footprint is dominated by model parameters rather than activations, meaning MEFT alone is insufficient for substantial peak memory reduction, necessitating combination with other techniques like quantization (FP16/int8 loading) or ZeRO.",
    "Future Research Directions": "Future work will explore applying MEFT to other domains beyond NLP, such as computer vision and automatic speech recognition. Further research will also investigate its applicability to different model architectures and larger backbones for more sequence-to-sequence tasks, explicitly mentioning LLAMA for instruction fine-tuning.",
    "Experiment Code": "import torch\nimport torch.nn as nn\n\nfrom transformers.activations import ACT2FN\n\n# Adapter architecture\nclass Adapter(nn.Module):\n    \"\"\"\n    The adapter architecture is borrowed from https://arxiv.org/abs/1902.00751\n    \"\"\"\n    def __init__(self, config, layernorm=False):\n        super().__init__()\n        self.dense1 = nn.Linear(config.hidden_size, config.adapter_bottleneck_dim)\n        self.dense2 = nn.Linear(config.adapter_bottleneck_dim, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        if isinstance(config.hidden_act, str):\n            self.act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.act_fn = config.hidden_act\n        if layernorm:\n            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        else:\n            self.LayerNorm = None\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        if self.LayerNorm is not None:\n            hidden_states = self.LayerNorm(hidden_states)\n        hidden_states = self.dense1(hidden_states)\n        hidden_states = self.act_fn(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.dense2(hidden_states)\n        return hidden_states\nfrom typing import List, Optional, Tuple, Union\nimport sys\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom torch.autograd import Function\n\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.bert.configuration_bert import BertConfig\nfrom transformers.models.bert.modeling_bert import (\n    BertSelfOutput,\n    BertAttention,\n    BertOutput,\n    BertLayer,\n    load_tf_weights_in_bert,\n    BertEmbeddings,\n    BertPooler,\n    BERT_INPUTS_DOCSTRING,\n    _CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n    _SEQ_CLASS_EXPECTED_OUTPUT,\n    _SEQ_CLASS_EXPECTED_LOSS,\n)\nfrom .modules import Adapter\n\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n_CONFIG_FOR_DOC = \"BertConfig\"\n\n\nclass RevBertSelfOutput(BertSelfOutput):\n    def __init__(self, config):\n        super().__init__(config)\n        if config.f_arch == \"attn\":\n            self.attn_adapter = Adapter(config)\n        else:\n            self.attn_adapter = None\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        if self.attn_adapter is not None: # For MEFT3\n            hidden_states = hidden_states + self.attn_adapter(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass RevBertAttention(BertAttention):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__(config, position_embedding_type=position_embedding_type)\n        self.output = RevBertSelfOutput(config)\n\n\nclass RevBertOutput(BertOutput):\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, adapter_change=None) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        # For MEFT1 and MEFT2: inserting an adapter in parallel to two consecutive feed-forward layers\n        if adapter_change is not None:\n            hidden_states = hidden_states + adapter_change\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass RevBertLayer(BertLayer):\n    def __init__(self, config):\n        super().__init__(config)\n        self.output = RevBertOutput(config)\n        self.ffn_adapter = Adapter(config)\n        if config.f_arch == \"attn\":  # For MEFT3: insert an adapter to the attention block\n            self.rev_adapter = None\n            self.attention = RevBertAttention(config)\n        else:  # For MEFT1 and MEFT2\n            self.rev_adapter = Adapter(config, layernorm=config.layernorm_in_adapter)\n\n        self.x1_factor = config.x1_factor  # by default, lambda = 0.1\n        self.x2_factor = config.x2_factor  # by default, beta = 0.1\n        self.f_arch = config.f_arch  # What is the choice for the F architecture\n        if config.f_arch == \"layer\":  # MEFT1\n            self.F = self.forward_layer\n            self.G = self.forward_adapter\n        elif config.f_arch == \"adapter\":  # MEFT2\n            self.F = self.forward_adapter\n            self.G = self.forward_layer\n        elif config.f_arch == \"attn\":  # MEFT3\n            self.F = self.forward_attention\n            self.G = self.forward_mlp\n        self.seeds = {}\n\n    def set_seed(self, key):\n        \"\"\"\n        Record the seed during the forward pass, so the activation can be recomputed.\n        \"\"\"\n        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:\n            # GPU\n            device_idx = torch.cuda.current_device()\n            seed = torch.cuda.default_generators[device_idx].seed()\n        else:\n            # CPU\n            seed = int(torch.seed() % sys.maxsize)\n\n        self.seeds[key] = seed\n        torch.manual_seed(seed)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        x1, x2 = torch.chunk(hidden_states, 2, dim=-1)\n        if self.training:\n            self.set_seed(\"F\")\n        f_outputs = self.F(\n            x2,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions\n        )\n        if isinstance(f_outputs, tuple):\n            f_x2 = f_outputs[0]\n            outputs = f_outputs[1:]\n        else:\n            f_x2 = f_outputs\n\n        y1 = self.x1_factor * x1 + f_x2\n\n        if self.training:\n            self.set_seed(\"G\")\n        g_outputs = self.G(\n            y1,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions\n        )\n        if isinstance(g_outputs, tuple):\n            g_y1 = g_outputs[0]\n            outputs = g_outputs[1:]\n        else:\n            g_y1 = g_outputs\n\n        y2 = self.x2_factor * x2 + g_y1\n\n        if self.f_arch == \"attn\":\n            y = torch.cat([y1, y2], dim=-1)  # Don't switch for MEFT3\n        else:\n            y = torch.cat([y2, y1], dim=-1)  # Switch for MEFT1 and MEFT2\n        outputs = (y,) + outputs\n\n        return outputs\n\n    def backward_pass(\n        self,\n        y,\n        dy,\n        attention_mask,\n        head_mask,\n        encoder_hidden_states,\n        encoder_attention_mask,\n        past_key_value\n    ):\n        assert self.training, (\n            \"If you want to train `ReversibleModel` and its variations, make sure to use `model.train()` to put the\"\n            \" model into training mode.\"\n        )\n        if self.f_arch == \"attn\":\n            y1, y2 = torch.chunk(y, 2, dim=-1)  # Don't Switch for MEFT3\n            dy1, dy2 = torch.chunk(dy, 2, dim=-1)\n        else:\n            y2, y1 = torch.chunk(y, 2, dim=-1)  # Swich for MEFT1 and MEFT2\n            dy2, dy1 = torch.chunk(dy, 2, dim=-1)\n        with torch.enable_grad():\n            y1.requires_grad = True\n            torch.manual_seed(self.seeds[\"G\"])\n            g_outputs = self.G(\n                y1,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                past_key_value=past_key_value,\n                output_attentions=False\n            )\n            if isinstance(g_outputs, tuple):\n                g_y1 = g_outputs[0]\n            else:\n                g_y1 = g_outputs\n\n            g_y1.backward(dy2, retain_graph=True)\n\n        with torch.no_grad():\n            x2 = (y2 - g_y1) / self.x2_factor\n            # save memory\n            del g_y1, y2\n            dy1 += y1.grad\n            y1.grad = None\n\n        with torch.enable_grad():\n            x2.requires_grad = True\n            torch.manual_seed(self.seeds[\"F\"])\n            f_outputs = self.F(\n                x2,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                past_key_value=past_key_value,\n                output_attentions=False\n            )\n            if isinstance(f_outputs, tuple):\n                f_x2 = f_outputs[0]\n            else:\n                f_x2 = f_outputs\n            f_x2.backward(dy1, retain_graph=False)\n\n        with torch.no_grad():\n            x1 = (y1 - f_x2) / self.x1_factor\n            del f_x2, y1\n            dy2 *= self.x2_factor\n            dy2 += x2.grad\n            x2.grad = None\n            dy1 *= self.x1_factor\n            x2 = x2.detach()\n\n        return torch.cat([x1, x2], dim=-1), torch.cat([dy1, dy2], dim=-1)\n\n    def forward_adapter(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        return self.rev_adapter(hidden_states)\n\n    def forward_layer(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n\n        # if decoder, the last output is tuple of self-attn cache\n        if self.is_decoder:\n            outputs = self_attention_outputs[1:-1]\n            present_key_value = self_attention_outputs[-1]\n        else:\n            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        cross_attn_present_key_value = None\n        if self.is_decoder and encoder_hidden_states is not None:\n            if not hasattr(self, \"crossattention\"):\n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                    \" by setting `config.add_cross_attention=True`\"\n                )\n\n            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                cross_attn_past_key_value,\n                output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n\n            # add cross-attn cache to positions 3,4 of present_key_value tuple\n            cross_attn_present_key_value = cross_attention_outputs[-1]\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        # if decoder, return the attn key/values as the last output\n        if self.is_decoder:\n            outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def forward_attention(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=None,\n        )\n        return self_attention_outputs\n\n    def forward_mlp(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        hidden_states = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, hidden_states\n        )\n        return hidden_states\n\n    def feed_forward_chunk(self, attention_output):\n        if self.ffn_adapter is not None:\n            adapter_change = self.ffn_adapter(attention_output)\n        else:\n            adapter_change = None\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, adapter_change=adapter_change)\n        return layer_output\n\n\nclass RevBackProp(Function):\n    @staticmethod\n    def forward(\n        ctx,\n        hidden_states,\n        layers,\n        attention_mask,\n        layer_head_mask,\n        encoder_hidden_states,\n        encoder_attention_mask,\n        past_key_value,\n        output_attentions,\n    ):\n        for i, layer_module in enumerate(layers):\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                layer_head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                past_key_value,\n                output_attentions,\n            )\n            hidden_states = layer_outputs[0]\n\n        ctx.save_for_backward(hidden_states.detach())\n        ctx.layers = layers\n        ctx.attention_mask = attention_mask\n        return hidden_states\n\n    def backward(ctx, dy):\n        y, = ctx.saved_tensors\n        layers = ctx.layers\n        attention_mask = ctx.attention_mask\n        for i, layer_module in enumerate(layers[::-1]):\n            y, dy = layer_module.backward_pass(\n                y,\n                dy,\n                attention_mask=attention_mask,\n                head_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                past_key_value=None\n            )\n        return dy, None, None, None, None, None, None, None\n\n\nclass RevBertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([RevBertLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n        self.num_rev_layers = config.num_rev_layers\n        assert self.num_rev_layers <= config.num_hidden_layers\n\n    def vanilla_backward(\n        hidden_states,\n        layers,\n        attention_mask\n    ):\n        for i, layer_module in enumerate(layers):\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask=attention_mask,\n                head_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                past_key_value=None,\n                output_attentions=False,\n            )\n            hidden_states = layer_outputs[0]\n        return hidden_states\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        # Set h_0^1 = h_1^1 = h_0\n        hidden_states = torch.cat([hidden_states, hidden_states], dim=-1)\n\n        # Layers with reversible gradient\n        layers_for_reverse_backward = self.layer[:self.num_rev_layers]\n        # Layers with vanilla gradient\n        layers_for_vanilla_backward = self.layer[self.num_rev_layers:]\n        if len(layers_for_reverse_backward) == 0:  # In this case, MEFTs are simply PEFT methods\n            executing_fn = RevBertEncoder.vanilla_backward\n            hidden_states = executing_fn(\n                hidden_states,\n                layers_for_vanilla_backward,\n                attention_mask\n            )\n        elif len(layers_for_reverse_backward) == self.config.num_hidden_layers:  # All layers are reversible\n            executing_fn = RevBackProp.apply\n            hidden_states = executing_fn(\n                hidden_states,\n                layers_for_reverse_backward,\n                attention_mask,\n                None,\n                None,\n                None,\n                None,\n                False\n            )\n        else:  # The first setting in Figure 7\n            lower_executing_fn = RevBackProp.apply\n            hidden_states = lower_executing_fn(\n                hidden_states,\n                layers_for_reverse_backward,\n                attention_mask,\n                None,\n                None,\n                None,\n                None,\n                False\n            )\n            higher_executing_fn = RevBertEncoder.vanilla_backward\n            hidden_states = higher_executing_fn(\n                hidden_states,\n                layers_for_vanilla_backward,\n                attention_mask\n            )\n\n        x1, x2 = torch.chunk(hidden_states, 2, dim=-1)\n        hidden_states = (x1 + x2) / 2.\n\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=None,\n            hidden_states=None,\n            attentions=None,\n            cross_attentions=None,\n        )\nfrom typing import Dict, Union, Any\nimport torch\nimport torch.nn as nn\nfrom packaging import version\nfrom transformers.trainer import Trainer\nfrom transformers.utils import is_sagemaker_mp_enabled, is_apex_available\n\nif is_sagemaker_mp_enabled():\n    import smdistributed.modelparallel.torch as smp\n    from smdistributed.modelparallel import __version__ as SMP_VERSION\n\n    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse(\"1.10\")\n\n    from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\nelse:\n    IS_SAGEMAKER_MP_POST_1_10 = False\n\nif is_apex_available():\n    from apex import amp\n\nclass CustomTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to train.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.do_grad_scaling:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            loss = self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        # Don't update the gradient of the layernorm parameters\n        if self.args.start_layer == -1:\n            model.bert.embeddings.LayerNorm.bias.grad = None\n            model.bert.embeddings.LayerNorm.weight.grad = None\n        else:\n            model.bert.encoder.layer[self.args.start_layer].output.LayerNorm.bias.grad = None\n            model.bert.encoder.layer[self.args.start_layer].output.LayerNorm.weight.grad = None\n\n        return loss.detach()",
    "Experiment Result": "The methodology utilizes adapters to make PLMs reversible, adhering to the 'Starting Point Hypothesis', aiming to reduce activation memory. Three Memory Efficient Fine-Tuning (MEFT) architectures are proposed and controlled by the `f_arch` configuration parameter:\n1.  **MEFT 1 (`f_arch=\"layer\"`)**: The PLM layer acts as function `F` (`self.forward_layer`), and an adapter acts as function `G` (`self.forward_adapter`). The output order is switched in the forward pass (`y = torch.cat([y2, y1], dim=-1)`).\n2.  **MEFT 2 (`f_arch=\"adapter\"`)**: An adapter acts as function `F` (`self.forward_adapter`), and the PLM layer (which may also contain an adapter in its output) acts as function `G` (`self.forward_layer`). The output order is switched in the forward pass.\n3.  **MEFT 3 (`f_arch=\"attn\"`)**: The attention block with an integrated adapter acts as function `F` (`self.forward_attention`), and the MLP block with an integrated adapter acts as function `G` (`self.forward_mlp`). The output order is *not* switched in the forward pass (`y = torch.cat([y1, y2], dim=-1)`).\n\n**Reversibility Parameters**:\n*   `x1_factor` (λ) and `x2_factor` (β): Scaling factors for the reversible blocks, configurable (defaults to 1 in `run_glue.py`'s `ModelArguments`).\n*   `adapter_bottleneck_dim`: The bottleneck dimension for the adapter layers; a value of 0 implies no adapter is used.\n*   `layernorm_in_adapter`: A boolean flag indicating whether a LayerNorm is included within the adapter for G.\n\n**Hybrid Reversibility Approach**:\n*   `num_rev_layers`: Specifies the number of bottom layers that will be configured as reversible. If `num_rev_layers` is 0, vanilla back-propagation is used for all layers. If `num_rev_layers` equals the total number of hidden layers, all layers are reversible. Otherwise, a hybrid approach is employed where the specified number of bottom layers are reversible, and the remaining top layers use vanilla back-propagation (caching).\n\n**Starting Point Hypothesis Implementation**:\n*   **Adapter Initialization**: Adapters (specifically their `nn.Linear` layers) are initialized with weights drawn from a normal distribution with `mean=0.0` and `std=config.initializer_range`, ensuring their initial output is near zero as required by the hypothesis.\n*   **Parameter Freezing**: If `adapter_bottleneck_dim > 0`, all base model parameters are initially frozen. Only adapter parameters, the classifier, and specific LayerNorms are unfrozen to be trainable.\n*   **Gradient Handling**: To strictly adhere to the hypothesis and preserve the PLM's output at the beginning of training, the `CustomTrainer` explicitly sets the gradients of certain LayerNorm parameters to `None` after the backward pass. This prevents their update during training:\n    *   If `freeze_irreversible_layers` is false (`start_layer == -1`), `model.bert.embeddings.LayerNorm`'s bias and weight gradients are set to `None`.\n    *   If `freeze_irreversible_layers` is true, the `LayerNorm` of the layer *before* the first reversible layer (`model.bert.encoder.layer[self.args.start_layer].output.LayerNorm`) has its gradients set to `None`."
}{
    "Title": "Spectral Adapter: Fine-Tuning in Spectral Space",
    "Main Contributions": "The paper introduces \"Spectral Adapter,\" a novel Parameter-Efficient Fine-Tuning (PEFT) method that enhances existing PEFT techniques by incorporating spectral information from pretrained weight matrices. It proposes two mechanisms: additive tuning (Spectral AdapterA) and orthogonal rotation (Spectral AdapterR) of the top singular vectors, obtained via Singular Value Decomposition (SVD). The work provides theoretical analysis showing improved rank capacity for Spectral AdapterA over LoRA and demonstrates superior parameter efficiency, tuning performance, and benefits for multi-adapter fusion.",
    "Methodology": "The core methodology involves performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) and then fine-tuning only the top-r columns of the singular vector matrices (U and V). Spectral AdapterA additively tunes these top columns using trainable matrices (AU, AV) initialized to zero. Spectral AdapterR orthogonally rotates these top columns using trainable orthogonal matrices (RU, RV) initialized as identity, with orthogonality maintained via Cayley parameterization. The approach focuses on the top spectral space, justified by theoretical insights into adapter rank capacity and weight subspace alignment.",
    "Experimental Setup": "The method was evaluated through extensive experiments on both Large Language Models (LLMs) and Diffusion Models. LLM experiments included fine-tuning Llama3 8B on Orca Math/GSM8K, DeBERTaV3-base on GLUE benchmarks, and Mistral 7B on GSM8K. Diffusion model experiments used Chilloutmix for multi-object fine-tuning (custom animals, toys, multi-character generation) and concept tuning (vase, chair, table). Baselines for comparison included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg. Evaluation metrics included training loss, validation scores (accuracy for LLMs), and CLIP-based alignment scores for diffusion models. All experiments were conducted on NVIDIA RTX A6000 GPUs, with hyperparameters largely following official baseline implementations.",
    "Limitations": "A primary limitation lies in the fixed choice of tuning only the top spectral space, despite theoretical verification under simple settings. Further investigation into tuning different columns of singular vector matrices is needed to fully understand the role of spectral information. Additionally, the time consumption of the Singular Value Decomposition (SVD) procedure increases with larger models, posing a challenge.",
    "Future Research Directions": "Future work could explore fine-tuning the spectral representation of specific components within large models, such as only the attention layer. Dynamically combining spectral adaptation with other PEFT methods, like AdaLoRA, is also a promising avenue. Further research is encouraged to investigate tuning different columns of singular vector matrices beyond just the top ones. Developing faster Singular Value Decomposition methods would also greatly benefit the scalability of this approach.",
    "Experiment Code": "import math\n\nimport torch\nimport torch.nn as nn\nfrom diffusers.models.attention_processor import AttnProcessor\nfrom diffusers.utils.import_utils import is_xformers_available\nimport torch.nn.functional as F\nimport numpy as np\n\nif is_xformers_available():\n    import xformers\n    \n\ndef remove_edlora_unet_attention_forward(unet):\n    def change_forward(unet):  # omit proceesor in new diffusers\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and name == 'attn2':\n                layer.set_processor(AttnProcessor())\n            else:\n                change_forward(layer)\n    change_forward(unet)\n\n\nclass EDLoRA_Control_AttnProcessor:\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n    def __init__(self, cross_attention_idx, place_in_unet, controller, attention_op=None):\n        self.cross_attention_idx = cross_attention_idx\n        self.place_in_unet = place_in_unet\n        self.controller = controller\n        self.attention_op = attention_op\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            is_cross = False\n            encoder_hidden_states = hidden_states\n        else:\n            is_cross = True\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available() and not is_cross:\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            attention_probs = self.controller(attention_probs, is_cross, self.place_in_unet)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass EDLoRA_AttnProcessor:\n    def __init__(self, cross_attention_idx, attention_op=None):\n        self.attention_op = attention_op\n        self.cross_attention_idx = cross_attention_idx\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available():\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\ndef revise_edlora_unet_attention_forward(unet):\n    def change_forward(unet, count):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:\n                layer.set_processor(EDLoRA_AttnProcessor(count))\n                count += 1\n            else:\n                count = change_forward(layer, count)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0)\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx)\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx)\n    print(f'Number of attention layer registered {cross_attention_idx}')\n\n\ndef revise_edlora_unet_attention_controller_forward(unet, controller):\n    class DummyController:\n        def __call__(self, *args):\n            return args[0]\n\n        def __init__(self):\n            self.num_att_layers = 0\n\n    if controller is None:\n        controller = DummyController()\n\n    def change_forward(unet, count, place_in_unet):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:  # only register controller for cross-attention\n                layer.set_processor(EDLoRA_Control_AttnProcessor(count, place_in_unet, controller))\n                count += 1\n            else:\n                count = change_forward(layer, count, place_in_unet)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0, 'down')\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx, 'mid')\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx, 'up')\n    print(f'Number of attention layer registered {cross_attention_idx}')\n    controller.num_att_layers = cross_attention_idx\n\nclass SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        rank = 8\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r\n\n    def cayley(self, data: torch.Tensor) -> torch.Tensor:\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)\n        else:\n            raise Exception('')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv :\n            raise Exception('')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)\n",
    "Experiment Result": "The method performs Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) and fine-tunes only the top-`r` columns of the singular vector matrices (U and V) and the corresponding singular values (S). The specific implementation uses `SpectralLinearLayer_OFT`.\n\nKey experimental settings:\n- **Rank (`r`)**: The number of top singular values and vectors that are fine-tuned. This is specified by `revised_r` in the `SpectralLinearLayer_OFT` constructor and stored as `self.rank`. In the given code, `rank` is explicitly set to `8` within the `SpectralLinearLayer_OFT` constructor, overriding any passed `rank` parameter.\n- **Initialization**: \n    - `self.spectral_A` and `self.spectral_B` (trainable matrices for `U` and `V` modifications) are initialized to `torch.zeros(r, r)`. When passed through the `cayley` function, these initially result in identity orthogonal matrices, aligning with the "
}{
    "Title": "Adapters Strike Back",
    "Main Contributions": "Addressed the underperformance of adapters in Vision Transformers (ViTs) by conducting the first systematic, in-depth study of adapter position, inner structure, and parameter initialization. Proposed a novel learnable, channel-wise scaling mechanism. Introduced Adapter+, an improved adapter architecture (Post-Adapter position, Houlsby initialization, channel-wise scaling) that achieves state-of-the-art average accuracy on VTAB (77.6% without per-task hyperparameter optimization, 77.9% with optimization) and FGVC (90.7% with the lowest number of parameters) benchmarks. Demonstrated Adapter+'s superior parameter-accuracy trade-off and high robustness, requiring minimal manual intervention.",
    "Methodology": "The methodology focuses on parameter-efficient adaptation of Vision Transformers (ViT-B/16 with frozen parameters). Adapters are small bottleneck modules added to transformer layers, comprising a down-projection to rank 'r', GELU activation, and an up-projection. Key enhancements in Adapter+ include: selecting the Post-Adapter position (at the end of the transformer layer, after the FFN and its skip connection) after systematic evaluation of four positions; employing Houlsby initialization (zero-centered Gaussian, σ=0.01, truncated at 2σ, zero biases); and integrating a learnable, channel-wise scaling (s ∈ Rd) mechanism. Only the adapter parameters and the linear classifier are trained.",
    "Experimental Setup": "Experiments utilized a ViT-B/16 network pre-trained on ImageNet-21k (with ablations on ImageNet-1k fine-tuned and AugReg pre-trained versions, and a DINO backbone for generality). Datasets include the Visual Task Adaptation Benchmark (VTAB), consisting of 19 tasks grouped into Natural, Specialized, and Structured categories (800 training, 200 validation images per task), and five Fine-Grained Visual Classification (FGVC) datasets (CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, Stanford Cars) for data-abundant settings. Models were trained with an AdamW optimizer, learning rates of 10^-3 (adapters) or 10^-4 (full fine-tuning), weight decay of 10^-4, and a batch size of 64. A cosine learning rate schedule with a linear warm-up (first 10 epochs) was used for a total of 100 epochs. Stochastic depth regularization (0-0.1 for frozen layers, 0.1 for adapters) was applied. Input images were resized to 224x224px (VTAB) or randomly cropped/flipped to 224x224px (FGVC), emphasizing data normalization consistent with pre-training. Validation sets were used for ablations and hyperparameter tuning, with validation data included in training for final results. Competing methods were re-evaluated under consistent conditions for fair comparison.",
    "Limitations": "The paper highlights that while Adapter+ overcomes previous limitations of adapter implementations, its performance is still impacted by unsuitable data normalization, although it demonstrates higher robustness than other methods. Achieving optimal performance often requires determining the adapter rank 'r' on a per-task basis, suggesting that a single fixed rank may not be universally optimal. The overall performance is also dependent on the generalization capabilities and specific pre-training strategy of the underlying frozen backbone model.",
    "Future Research Directions": "Future research could focus on developing adaptive mechanisms for automatically determining the optimal adapter rank 'r' per task, reducing the need for manual hyperparameter tuning. Further exploration of Adapter+'s applicability across a wider array of pre-trained vision transformer backbones and diverse pre-training strategies, beyond supervised and self-supervised models, is also a promising direction. Additionally, investigating the integration of Adapter+ into various computer vision tasks beyond classification, such as object detection or semantic segmentation, could be explored.",
    "Experiment Code": "import timm\nfrom .vit_adapter import *\nfrom .vit_adapter import _create_vision_transformer_adapter\n\n__all__ = [\n    \"Adapter\",\n    \"LoRAAttention\",\n    \"AdapterBlock\",\n    \"AdapterResPostBlock\",\n    \"VisionTransformerAdapter\",\n]\n\ndef patch_timm():\n    timm.models.vision_transformer._create_vision_transformer = (\n        _create_vision_transformer_adapter\n    )\n\npatch_timm()\n\n# for Adapter+ set norm_layer to None and scaling to \"channel\"\nclass Adapter(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        bottleneck_dim=8,\n        drop_path=0.0,\n        dropout=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        scaling=1.0,\n        init=\"houlsby\",\n        bias=True,\n        pre_dropout=False,\n    ):\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Dropout(dropout) if dropout > 0 and pre_dropout else nn.Identity(),\n            nn.Linear(embed_dim, bottleneck_dim, bias=bias),\n            act_layer() if act_layer else nn.Identity(),\n            nn.Dropout(dropout) if dropout > 0 and not pre_dropout else nn.Identity(),\n            nn.Linear(bottleneck_dim, embed_dim, bias=bias),\n        )\n        self.norm_a = norm_layer(embed_dim) if norm_layer else nn.Identity()\n        self.drop_path_a = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.bottleneck_dim = bottleneck_dim\n        if scaling == \"learned\":\n            self.scaling = nn.Parameter(torch.ones(1))\n        elif scaling == \"channel\":\n            self.scaling = nn.Parameter(torch.ones(embed_dim))\n        else:\n            self.scaling = scaling\n\n        # init following (Houslby 2019)\n        if init == \"houlsby\":\n            std = 0.01\n            nn.init.trunc_normal_(\n                self.bottleneck[1].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[1].bias is not None:\n                nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.trunc_normal_(\n                self.bottleneck[4].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n        else:\n            raise ValueError(f\"Initialization {init} not implemented!\")\n\n    def forward(\n        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        x = self.norm_a(x)\n        x = self.drop_path_a(self.bottleneck(x))\n        x = x * self.scaling\n\n        y = x\n        if skip is not None:\n            y = y + skip\n\n        return y\n\n\nclass AdapterBlock(Block):\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\n\nclass AdapterResPostBlock(ResPostBlock):\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\ndef _create_vision_transformer_adapter(\n    variant: str, pretrained: bool = False, adapter=False, **kwargs\n) -> VisionTransformer:\n    out_indices = kwargs.pop(\"out_indices\", 3)\n    if \"flexi\" in variant:\n        _filter_fn = partial(\n            checkpoint_filter_fn, interpolation=\"bilinear\", antialias=False\n        )\n    else:\n        _filter_fn = checkpoint_filter_fn\n\n    strict = True\n    if \"siglip\" in variant and kwargs.get(\"global_pool\", None) != \"map\":\n        strict = False\n\n    if adapter:\n        block_fn = kwargs.pop(\"block_fn\", Block)\n        if block_fn == Block:\n            block_fn = AdapterBlock\n        elif block_fn == ResPostBlock:\n            block_fn = AdapterResPostBlock\n        else:\n            raise ValueError(f\"Adapters not implemented for {block_fn}!\")\n        return build_model_with_cfg(\n            VisionTransformerAdapter,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            block_fn=block_fn,\n            **kwargs,\n        )\n    else:\n        return build_model_with_cfg(\n            VisionTransformer,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            **kwargs,\n        )\n\n\nclass AdapterModel(pl.LightningModule):\n    def __init__(\n        self,\n        cfg,\n        img_size=224,\n        num_classes=1000,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.img_size = img_size\n        self.num_classes = num_classes\n        self.train_acc = torchmetrics.Accuracy(\n            task=\"multiclass\", num_classes=num_classes\n        )\n        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.label_smoothing = cfg.train.label_smoothing\n\n        self.vit = timm.create_model(\n            cfg.vit.model,\n            adapter=True,\n            pretrained=True,\n            num_classes=num_classes,\n            img_size=img_size,\n            drop_path_rate=cfg.vit.drop_path,\n            adapter_config=cfg.get(\"adapter\", None),\n            lora_config=cfg.get(\"lora\", None),\n            prompt_config=cfg.get(\"prompt\", None),\n        )\n\n        if cfg.get(\"adapter\", None) or cfg.get(\"lora\", None) or cfg.get(\"prompt\", None):\n            if not cfg.vit.finetune:\n                self.vit.requires_grad_(False)\n            self.vit.head.requires_grad_(True)\n            for m in self.vit.modules():\n                if isinstance(m, Adapter):\n                    m.requires_grad_(True)\n\n                if cfg.train.train_ln:\n                    if isinstance(m, nn.LayerNorm):\n                        m.requires_grad_(True)\n\n                if cfg.get(\"prompt\", None):\n                    if isinstance(m, AdapterBlock):\n                        m.prompt.requires_grad_(True)\n\n        if cfg.train.classifier_only:\n            self.vit.requires_grad_(False)\n            self.vit.head.requires_grad_(True)\n\n        self.save_hyperparameters()\n        self.trainable_params = {\n            \"trainable_params\": sum(\n                p.numel() for p in self.parameters() if p.requires_grad\n            )\n        }\n        self.save_hyperparameters(self.trainable_params)",
    "Experiment Result": "The method uses a Vision Transformer (ViT-B/16) with frozen parameters for parameter-efficient adaptation. Adapters are integrated as small bottleneck modules, comprising a down-projection to rank 'r' (default `bottleneck_dim=8`), GELU activation, and an up-projection. The adapter is positioned at the 'Post-Adapter' location, meaning it is added at the end of the transformer layer, after the Feed-Forward Network (FFN) and its skip connection. Adapter parameters are initialized using Houlsby initialization: weights are drawn from a zero-centered Gaussian distribution with a standard deviation (σ) of 0.01, truncated at 2σ, and all biases are initialized to zero. A learnable, channel-wise scaling mechanism is applied to the adapter output. During training, only the adapter parameters and the linear classifier head are updated, while the base ViT model parameters (excluding LayerNorms if `cfg.train.train_ln` is false) remain frozen."
}{
    "Title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
    "Main Contributions": "The paper introduces Conditionally Adaptive Multi-Task Learning (CA-MTL), a novel Transformer-based Adapter designed to improve transfer learning in NLP by addressing challenges like overfitting to low-resource tasks, catastrophic forgetting, and negative task transfer in Multi-Task Learning (MTL). CA-MTL achieves more efficient parameter sharing and mitigates forgetting by fixing half of a pretrained model's weights and using a new multi-task data sampling strategy. It surpasses single-task fine-tuning and other Adapter methods in performance while being parameter and data efficient, achieving state-of-the-art results on several NLP benchmarks across up to 26 tasks.",
    "Methodology": "CA-MTL modularizes a pretrained network using latent task representations and balances tasks with an uncertainty-based sampling method. Key components include: 1) Task Conditioned Transformer architecture that adapts and modulates pretrained weights via Conditional Weight Transformations. 2) Specific modules: Conditional Attention (block-diagonal to account for task-specific biases), Conditional Alignment (a task-conditioned alignment layer for covariate distribution differences), Conditional Layer Normalization (adapting layer normalization statistics to specific tasks), and Conditional Bottleneck (a two-layer feed-forward bottleneck facilitating weight sharing and task-specific information flow). 3) Multi-Task Uncertainty Sampling, an active learning-inspired task selection strategy that uses Shannon Entropy to prioritize uncertain training examples, normalizing for class distribution and highest average task entropy.",
    "Experimental Setup": "The method was evaluated on 26 NLP tasks, including GLUE (9 tasks), SuperGLUE (8 tasks), MRQA (6 tasks), and WNUT2017 (Named Entity Recognition). Domain adaptation was tested on SciTail and SNLI datasets. Models used were BERTBASE, BERTLARGE, and RoBERTaLARGE, implemented using HuggingFace. Baselines included single-task fine-tuning (ST), standard MTL (with random or counterfactual sampling), and other adapter networks (PALs, Adapters-256, MT-DNN, STILTS, BAM!). Performance was measured using average GLUE scores, F1, Spearman's correlation, Matthew's correlation, and accuracy. Experiments involved freezing the bottom half of Transformer layers, using Adam optimizer (learning rate 2e-5, warm-up over first 10%), batch size 32, sequence length 128 (256 for 24-task models), and 5-8 epochs. Ablation studies analyzed individual module contributions and layer freezing configurations. Task difficulty was estimated using Evolutionary Data Measures (EDM). Experiments were conducted on NVIDIA P100/V100 GPUs.",
    "Limitations": "While generally outperforming baselines, the chosen strategy of freezing half the Transformer layers, aimed at preserving pretrained knowledge for a larger number of tasks, slightly lowered performance on some GLUE tasks. For the token-level NER task (WNUT2017), CA-MTL, despite significantly outperforming the MTL baseline, had not yet overfit and could potentially close the gap with single-task baselines with more training cycles. For zero-shot transfer, CA-MTL was found to be sensitive to the initialization of the new task's embedding. The external Evolutionary Data Measures (EDM) for task difficulty estimation lacked precision for regression-like values.",
    "Future Research Directions": "Future work will focus on extending the ideas of task-conditioned adaptive learning within a single model, further exploring dynamic adaptation, and modularizing knowledge embedded in large monolithic pretrained models.",
    "Experiment Code": "from dataclasses import dataclass, field\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import BertPreTrainedModel\n\nfrom src.model.decoder import Decoder\nfrom src.model.encoders.bert import _BertEncoder\nfrom src.model.encoders.ca_mtl_base import CaMtlBaseEncoder\nfrom src.model.encoders.ca_mtl_large import CaMtlLargeEncoder\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass CaMtlArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from: CA-MTL-base, CA-MTL-large, bert-base-cased \"\n                    \"bert-base-uncased, bert-large-cased, bert-large-uncased\"\n        }\n    )\n    freeze_encoder_layers: str = field(\n        default=None,\n        metadata={\"help\": \"Freeze encoder layers. format: <start_layer>-<end_layer>\"},\n    )\n\n\nclass CaMtl(BertPreTrainedModel):\n    def __init__(\n        self,\n        config,\n        model_args,\n        data_args,\n    ):\n        super().__init__(config)\n\n        self.data_args = data_args\n        self.bert = self._create_encoder(model_args.model_name_or_path)\n        self.decoders = nn.ModuleList()\n        for task in data_args.tasks:\n            self.decoders.append(Decoder(config.hidden_size, task))\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        task_id=None,\n        span_locs=None,\n        sample_id=None,\n    ):\n\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            task_id=task_id,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n\n        loss_list = []\n        unique_task_ids = torch.unique(task_id)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        loss_grouped_per_task = (\n            torch.zeros_like(task_id[0]).repeat(len(self.data_args.tasks)).float()\n        )\n        batch_entropy_per_task = torch.zeros(input_ids.shape[0])\n        batch_entropy_mean_per_task = torch.zeros(input_ids.shape[0])\n        max_mean_batch_entropy = None\n        logits = None\n        for unique_task_id in unique_task_ids_list:\n            task_id_filter = task_id == unique_task_id\n            decoder_id = unique_task_id\n            logits, current_loss, batch_entropy = self.decoders[decoder_id].forward(\n                sequence_output[task_id_filter],\n                pooled_output[task_id_filter],\n                labels=None if labels is None else labels[task_id_filter],\n                attention_mask=attention_mask[task_id_filter],\n            )\n\n            batch_entropy_mean = batch_entropy.mean().item()\n            batch_entropy_per_task[task_id_filter] = batch_entropy\n            batch_entropy_mean_per_task[task_id_filter] = torch.full_like(\n                batch_entropy, batch_entropy_mean\n            )\n            if (\n                max_mean_batch_entropy is None\n                or batch_entropy_mean > max_mean_batch_entropy\n            ):\n                max_mean_batch_entropy = batch_entropy_mean\n\n            if labels is not None:\n                loss_grouped_per_task[unique_task_id] = current_loss\n                loss_list.append(current_loss)\n\n        outputs = (\n            (logits,)\n            + outputs[2:]\n            + (\n                batch_entropy_per_task,\n                batch_entropy_mean_per_task,\n                max_mean_batch_entropy,\n            )\n        )\n\n        if loss_list:\n            loss = torch.stack(loss_list)\n            outputs = (loss.mean(),) + outputs + (loss_grouped_per_task.view(1, -1),)\n\n        return outputs\n\n    def _create_encoder(self, model_name_or_path):\n        if model_name_or_path == \"CA-MTL-large\":\n            return CaMtlLargeEncoder(self.config, data_args=self.data_args)\n        elif model_name_or_path == \"CA-MTL-base\":\n            return CaMtlBaseEncoder(self.config, data_args=self.data_args)\n        else:\n            return _BertEncoder(self.config)\n\n    @staticmethod\n    def get_base_model(model_name_or_path):\n        if model_name_or_path == \"CA-MTL-large\":\n            return \"bert-large-cased\"\n        elif model_name_or_path == \"CA-MTL-base\":\n            return \"bert-base-cased\"\n        else:\n            return model_name_or_path\n\n    def freeze_encoder_layers(\n        self,\n        model_args,\n        unfrozen_modules=[\n            \"random_weight_matrix\",\n            \"film.gb_weights\",\n            \"ln_weight_modulation.gb_weights\",\n            \"adapter\",\n        ],\n    ):\n        if model_args.freeze_encoder_layers is not None:\n            start_layer, end_layer = model_args.freeze_encoder_layers.split(\"-\")\n\n            for name, param in self.bert.named_parameters():\n                requires_grad = True\n                match = re.match(self.bert.get_layer_regexp(), name)\n                if match:\n                    layer_number = int(match.groups()[0])\n                    requires_grad = not int(start_layer) <= layer_number <= int(\n                        end_layer\n                    ) or any([module in match.string for module in unfrozen_modules])\n                elif name.startswith(\"embedding\"):\n                    requires_grad = False\n                param.requires_grad = requires_grad\n\n        for name, param in self.bert.named_parameters():\n            logger.info(\n                \"%s - %s\", name, (\"Unfrozen\" if param.requires_grad else \"FROZEN\")\n            )\nimport torch\nimport numpy\nfrom scipy.stats import entropy\nfrom transformers import glue_tasks_num_labels\nfrom torch.nn import MSELoss, CrossEntropyLoss, Softmax, Dropout, Linear, Softmax\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_size, task_name):\n        super().__init__()\n        self.num_labels = glue_tasks_num_labels[task_name]\n        self.dropout = Dropout(0.1)\n        self.model = Linear(hidden_size, self.num_labels)\n\n    def forward(self, sequence_output, pooled_output, labels=None, **kwargs):\n        loss = None\n        pooled_output = self.dropout(pooled_output)\n        logits = self.model(pooled_output)\n\n        batch_entropy = self.calculate_entropy(logits)\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.float().view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.long().view(-1))\n\n        return logits, loss, batch_entropy\n\n    def calculate_entropy(self, logits):\n        probas = Softmax(dim=1)(logits.detach())\n        samples_entropy = entropy(probas.transpose(0, 1).cpu())\n        even_preds = numpy.array(\n            [[1 / self.num_labels for _ in range(self.num_labels)]]\n        )\n        max_entropy = entropy(even_preds.T)\n        epsilon = 1e-5\n        samples_entropy = samples_entropy / (max_entropy.item() + epsilon)\n        return torch.tensor(samples_entropy)\nimport math\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_bert import (\n    BertEmbeddings,\n    BertPooler,\n    BertPreTrainedModel,\n    BertAttention,\n    BertIntermediate,\n    BertLayer,\n    BertSelfOutput,\n)\n\nfrom src.model.encoders.conditional_modules import FiLM, CBDA, ConditionalBottleNeck, ConditionalLayerNorm\n\n\nclass MyBertSelfAttention9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.max_seq_length = config.max_seq_length\n        assert config.hidden_size % self.max_seq_length == 0, \\\n            \"Block decomposed attention will only work if this condition is met.\"\n        self.num_blocks = config.hidden_size//self.max_seq_length\n        self.cond_block_diag_attn = CBDA(\n            config.hidden_size, math.ceil(self.max_seq_length/self.num_blocks), self.num_blocks\n        )  # d x L/N\n\n        self.random_weight_matrix = nn.Parameter(\n            torch.zeros(\n                [config.max_seq_length, math.ceil(self.max_seq_length/self.num_blocks)]\n            ),\n            requires_grad=True,\n        )\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        if encoder_hidden_states is not None:\n            mixed_value_layer = self.value(encoder_hidden_states)\n            attention_mask = encoder_attention_mask\n        else:\n            mixed_value_layer = self.value(hidden_states)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        mixed_key_layer = self.key(hidden_states)\n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        attention_scores1 = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n\n        attention_scores2 = self.cond_block_diag_attn(\n            x_cond=task_embedding,\n            x_to_film=self.random_weight_matrix,\n        )\n\n        attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n        # b x seq len x hid dim\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # y = ax + b(task_emb)\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(\n            attention_scores\n        )  # b x num heads x seq length x head dim\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs)\n            if self.output_attentions\n            else (context_layer,)\n        )\n        return outputs\n\n\nclass MyBertSelfOutput9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertOutput9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAttention9(BertAttention):\n    def __init__(self, config, add_conditional_layernorm=True):\n        super().__init__(config)\n        self.self = MyBertSelfAttention9(config)\n        self.add_conditional_layernorm = add_conditional_layernorm\n        if add_conditional_layernorm:\n            self.output = MyBertSelfOutput9(config)\n        else:\n            self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            task_embedding=task_embedding,\n        )\n        if self.add_conditional_layernorm:\n            attention_output = self.output(self_outputs[0], hidden_states, task_embedding, task_id)\n        else:\n            attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass BertAdapter9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bottleneck = ConditionalBottleNeck(config)\n        self.condlayernorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, bert_layer_input, hidden_states, task_embedding, task_id):\n        hidden_states = self.bottleneck(task_embedding, hidden_states)\n        hidden_states = self.condlayernorm(hidden_states + bert_layer_input, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAdapterLayer9(nn.Module):\n    \"\"\"Adapter Layer trained from scratch (sub layer names are changed)\"\"\"\n    def __init__(self, config):\n        super(MyBertAdapterLayer9, self).__init__()\n        self.new_attention = MyBertAttention9(config)\n        self.new_intermediate = BertIntermediate(config)\n        self.new_output = MyBertOutput9(config)\n        self.adapter = BertAdapter9(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.new_attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        intermediate_output = self.new_intermediate(attention_output)\n        layer_output = self.new_output(\n            intermediate_output, attention_output, task_embedding=task_embedding, task_id=task_id\n        )\n        adapted_layer_output = self.adapter(\n            attention_output, layer_output, task_embedding=task_embedding, task_id=task_id\n        )\n        outputs = (adapted_layer_output,) + outputs\n        return outputs\n\n\nclass MyBertLayer9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MyBertAttention9(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = MyBertOutput9(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        if self.is_decoder and encoder_hidden_states is not None:\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:]\n            )  # add cross attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, task_embedding, task_id)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass BertLayer9(BertLayer):\n    \"\"\"Same as BertLayer but with different inputs\"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.attention = MyBertAttention9(config, add_conditional_layernorm=False)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass MyBertEncoder9(nn.Module):\n    def __init__(self, config, tasks):\n        super().__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n        num_bert_layers = config.num_hidden_layers//2\n        num_mybert_layers = config.num_hidden_layers//2-1\n        assert num_bert_layers+num_mybert_layers+1 == config.num_hidden_layers\n        self.layer = nn.ModuleList(\n            [BertLayer9(config) for _ in range(num_bert_layers)] +\n            [MyBertLayer9(config) for _ in range(num_mybert_layers)] +\n            [MyBertAdapterLayer9(config)]  # FiLM8\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_type=None,\n        task_embedding=None\n    ):\n        all_hidden_states = ()\n        all_attentions = ()\n        task_embedding = self.task_transformation(task_embedding)\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                head_mask[i],\n                encoder_hidden_states,\n                encoder_attention_mask,\n                task_embedding,\n                task_type\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass CaMtlBaseEncoder(BertPreTrainedModel):\n    r\"\"\"\n    # NOTE: Combination of: (might work best for base) and uses:\n        -- block diagonal attention\n        -- conditional layer norm for the top half layers base=6-10 and large=11-22, top layer excluded\n        -- conditional bias attention term to the original attention matrix\n        -- conditional adapter for the top layer only at layer=11 for base and layer=23 for large\n        -- conditional alignment after the embedding layer only\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertFiLMModel.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n\n    def __init__(self, config, data_args=None, **kwargs):\n        super().__init__(config)\n        tasks = data_args.tasks\n        self.task_id_2_task_idx = {i: i for i, t in enumerate(tasks)}\n        self.config = config\n        self.config.num_tasks = len(tasks)\n        config.max_seq_length = data_args.max_seq_length\n        self.task_type_embeddings = nn.Embedding(len(tasks), config.hidden_size)\n        self.conditional_alignment = FiLM(\n            config.hidden_size, config.hidden_size\n        )  # FiLM5\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = MyBertEncoder9(config, tasks)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass on the Model.\n\n        The model can behave as an encoder (with only self-attention) as well\n        as a decoder, in which case a layer of cross-attention is added between\n        the self-attention layers, following the architecture described in:\n        .. _`Attention is all you need`:\n            https://arxiv.org/abs/1706.03762\n\n        \"\"\"\n        task_type = self._create_task_type(kwargs[\"task_id\"])\n        task_embedding = self.task_type_embeddings(task_type)\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                causal_mask = causal_mask.to(\n                    torch.long\n                )  # not converting to long will cause errors with pytorch version < 1.3\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\n            if encoder_attention_mask.dim() == 3:\n                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n            elif encoder_attention_mask.dim() == 2:\n                encoder_extended_attention_mask = encoder_attention_mask[\n                    :, None, None, :\n                ]\n            else:\n                raise ValueError(\n                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n                        encoder_hidden_shape, encoder_attention_mask.shape\n                    )\n                )\n\n            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # fp16 compatibility\n            encoder_extended_attention_mask = (\n                1.0 - encoder_extended_attention_mask\n            ) * -10000.0\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n        embedding_output = self.conditional_alignment(\n            x_cond=task_embedding,\n            x_to_film=embedding_output,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            task_type=task_type,\n            task_embedding=task_embedding,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def _create_task_type(self, task_id):\n        task_type = task_id.clone()\n        unique_task_ids = torch.unique(task_type)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        for unique_task_id in unique_task_ids_list:\n            task_type[task_type == unique_task_id] = self.task_id_2_task_idx[\n                unique_task_id\n            ]\n        return task_type\n\n    @classmethod\n    def get_layer_regexp(cls):\n        return r\"encoder.layer.*\\\\.([0-9]+)\\\\..*\"\nimport math\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_bert import (\n    BertEmbeddings,\n    BertPooler,\n    BertPreTrainedModel,\n    BertAttention,\n    BertIntermediate,\n    BertLayer,\n    BertSelfOutput,\n)\n\nfrom src.model.encoders.conditional_modules import FiLM, CBDA, ConditionalLayerNorm, ConditionalBottleNeck\n\n\nclass MyBertSelfAttention10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.max_seq_length = config.max_seq_length\n        assert config.hidden_size % self.max_seq_length == 0, \\\n            \"Block decomposed attention will only work if this condition is met.\"\n        self.num_blocks = config.hidden_size // self.max_seq_length\n        self.cond_block_diag_attn = CBDA(\n            config.hidden_size, math.ceil(self.max_seq_length / self.num_blocks), self.num_blocks\n        )  # d x L/N\n\n        self.random_weight_matrix = nn.Parameter(\n            torch.zeros(\n                [config.max_seq_length, math.ceil(self.max_seq_length / self.num_blocks)]\n            ),\n            requires_grad=True,\n        )\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        if encoder_hidden_states is not None:\n            mixed_value_layer = self.value(encoder_hidden_states)\n            attention_mask = encoder_attention_mask\n        else:\n            mixed_value_layer = self.value(hidden_states)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        mixed_key_layer = self.key(hidden_states)\n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        attention_scores1 = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n\n        attention_scores2 = self.cond_block_diag_attn(\n            x_cond=task_embedding,\n            x_to_film=self.random_weight_matrix,\n        )\n\n        attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n        # b x seq len x hid dim\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # y = ax + b(task_emb)\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(\n            attention_scores\n        )  # b x num heads x seq length x head dim\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs)\n            if self.output_attentions\n            else (context_layer,)\n        )\n        return outputs\n\n\nclass MyBertSelfOutput10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertOutput10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAttention10(BertAttention):\n    def __init__(self, config, add_conditional_layernorm=True):\n        super().__init__(config)\n        self.self = MyBertSelfAttention10(config)\n        self.add_conditional_layernorm = add_conditional_layernorm\n        if add_conditional_layernorm:\n            self.output = MyBertSelfOutput10(config)\n        else:\n            self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            task_embedding=task_embedding,\n        )\n        if self.add_conditional_layernorm:\n            attention_output = self.output(self_outputs[0], hidden_states, task_embedding, task_id)\n        else:\n            attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass MyBertLayer10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MyBertAttention10(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = MyBertOutput10(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        if self.is_decoder and encoder_hidden_states is not None:\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:]\n            )  # add cross attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, task_embedding, task_id)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass BertLayer10(BertLayer):\n    \"\"\"Same as BertLayer but with different inputs\"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.attention = MyBertAttention10(config, add_conditional_layernorm=False)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass MyBertEncoder10(nn.Module):\n    def __init__(self, config, tasks):\n        super().__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n        num_bert_layers = config.num_hidden_layers//2 + config.num_hidden_layers % 2\n        num_mybert_layers = config.num_hidden_layers//2\n        assert num_bert_layers+num_mybert_layers == config.num_hidden_layers\n        self.layer = nn.ModuleList(\n            [BertLayer10(config) for _ in range(num_bert_layers)] +\n            [MyBertLayer10(config) for _ in range(num_mybert_layers)]\n        )\n        assert len(self.layer) == config.num_hidden_layers\n        #FiLM6\n        self.adapter_layer = nn.ModuleList(\n            [\n                ConditionalBottleNeck(config)\n                for _ in range(config.num_hidden_layers)\n            ]\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_type=None,\n        task_embedding=None\n    ):\n        all_hidden_states = ()\n        all_attentions = ()\n        task_embedding = self.task_transformation(task_embedding)\n        hidden_film = torch.zeros_like(hidden_states)\n        for i, (layer_module, adapter_module) in enumerate(zip(self.layer, self.adapter_layer)):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                head_mask[i],\n                encoder_hidden_states,\n                encoder_attention_mask,\n                task_embedding,\n                task_type\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n            # FiLM layer with a skip connection\n            hidden_film = adapter_module(\n                x_cond=task_embedding, hidden_states=hidden_states + hidden_film\n            )\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_film,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass CaMtlLargeEncoder(BertPreTrainedModel):\n    r\"\"\"\n    # NOTE: Combination of: (might work best for large) and uses:\n        -- block diagonal attention\n        -- conditional layer norm for the top half layers base=6-10 and large=11-22, top layer excluded\n        -- conditional bias attention term to the original attention matrix\n        -- down/projection bottleneck for all layers\n        -- conditional alignment after the embedding layer only\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertFiLMModel.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n\n    def __init__(self, config, data_args=None, **kwargs):\n        super().__init__(config)\n        tasks = data_args.tasks\n        self.task_id_2_task_idx = {i: i for i, t in enumerate(tasks)}\n        self.config = config\n        self.config.num_tasks = len(tasks)\n        config.max_seq_length = data_args.max_seq_length\n        self.task_type_embeddings = nn.Embedding(len(tasks), config.hidden_size)\n        self.conditional_alignment = FiLM(\n            config.hidden_size, config.hidden_size\n        )  # FiLM5\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = MyBertEncoder10(config, tasks)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass on the Model.\n\n        The model can behave as an encoder (with only self-attention) as well\n        as a decoder, in which case a layer of cross-attention is added between\n        the self-attention layers, following the architecture described in:\n        .. _`Attention is all you need`:\n            https://arxiv.org/abs/1706.03762\n\n        \"\"\"\n        task_type = self._create_task_type(kwargs[\"task_id\"])\n        task_embedding = self.task_type_embeddings(task_type)\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                causal_mask = causal_mask.to(\n                    torch.long\n                )  # not converting to long will cause errors with pytorch version < 1.3\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\n            if encoder_attention_mask.dim() == 3:\n                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n            elif encoder_attention_mask.dim() == 2:\n                encoder_extended_attention_mask = encoder_attention_mask[\n                    :, None, None, :\n                ]\n            else:\n                raise ValueError(\n                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n                        encoder_hidden_shape, encoder_attention_mask.shape\n                    )\n                )\n\n            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # fp16 compatibility\n            encoder_extended_attention_mask = (\n                1.0 - encoder_extended_attention_mask\n            ) * -10000.0\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n        embedding_output = self.conditional_alignment(\n            x_cond=task_embedding,\n            x_to_film=embedding_output,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            task_type=task_type,\n            task_embedding=task_embedding,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def _create_task_type(self, task_id):\n        task_type = task_id.clone()\n        unique_task_ids = torch.unique(task_type)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        for unique_task_id in unique_task_ids_list:\n            task_type[task_type == unique_task_id] = self.task_id_2_task_idx[\n                unique_task_id\n            ]\n        return task_type\n\n    @classmethod\n    def get_layer_regexp(cls):\n        return r\"encoder.layer.*\\\\.([0-9]+)\\\\..*\"\nimport torch\nimport numbers\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FiLM(nn.Module):\n    \"\"\" Feature-wise Linear Modulation (FiLM) layer\"\"\"\n    def __init__(self, input_size, output_size, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(FiLM, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        return out\n\n\nclass CBDA(nn.Module):\n    \"\"\" Conditional Block Diagonal Attention (CBDA) layer\"\"\"\n    def __init__(self, input_size, output_size, blocks=1, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(CBDA, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        self.blocks = blocks\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        out = [torch.block_diag(*list(out_b.chunk(self.blocks, 0))) for out_b in out]\n        out = torch.stack(out)\n        return out[:, :, :out.size(1)]\n\n\nclass ConditionalLayerNorm(nn.Module):\n    r\"\"\"Applies Conditional Layer Normalization over a mini-batch of inputs.\n\n    .. math::\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma(z) + \\beta(z)\n\n    The mean and standard-deviation are calculated separately over the last\n    certain number dimensions which have to be of the shape specified by\n    :attr:`normalized_shape`.\n    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n\n    .. note::\n        Unlike Batch Normalization and Instance Normalization, which applies\n        scalar scale and bias for each entire channel/plane with the\n        :attr:`affine`, Layer Normalization applies per-element scale and\n        bias with :attr:`elementwise_affine`.\n\n    This layer uses statistics computed from input data in both training and\n    evaluation modes. The affine transformation is molulated by a conditional tensor.\n    In our case, we use task embeddings z.\n\n    Args:\n        normalized_shape (int or list or torch.Size): input shape from an expected input\n            of size\n\n            .. math::\n                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n\n            If a single integer is used, it is treated as a singleton list, and this module will\n            normalize over the last dimension which is expected to be of that specific size.\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        elementwise_affine: a boolean value that when set to ``True``, this module\n            has learnable per-element affine parameters initialized to ones (for weights)\n            and zeros (for biases). Default: ``True``.\n\n    Shape:\n        - Input: :math:`(N, *)`\n        - Output: :math:`(N, *)` (same shape as input)\n\n    Examples::\n\n        >>> input_ = torch.randn(20, 5, 10, 10)\n        >>> condition = torch.randn(20, 10)\n        >>> # With Learnable Parameters\n        >>> m = ConditionalLayerNorm([10, 10])\n        >>> # Normalize over last dimension of size 10\n        >>> m = nn.LayerNorm(10)\n        >>> # Activating the module\n        >>> output = m(input_, condition)\n\n    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450\n    .. _`Conditional Layer Normalization`: https://arxiv.org/\n    \"\"\"\n    __constants__ = ['normalized_shape', 'condition_size', 'weight', 'bias', 'eps']\n\n    def __init__(self, normalized_shape, condition_size, eps=1e-5):\n        super(ConditionalLayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = tuple(normalized_shape)\n\n        self.condition_size = condition_size\n        self.eps = eps\n\n        self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.ln_weight_modulation = FiLM(condition_size, sum(normalized_shape))\n        self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input_, condition, task_id):\n        unique_task_ids = torch.unique(task_id)\n        cln_output = torch.zeros_like(input_)\n        for unique_task_id in unique_task_ids:\n            task_id_filter = task_id == unique_task_id\n            task_emb = condition[task_id_filter][0].unsqueeze(0)\n            weight = self.ln_weight_modulation(task_emb, self.weight).view(-1)\n            cln_output[task_id_filter] = F.layer_norm(input_[task_id_filter], self.normalized_shape, weight, self.bias, self.eps)\n        return cln_output\n\n    def extra_repr(self):\n        return '{normalized_shape}, {condition_size}, eps={eps}'.format(**self.__dict__)\n\n\nclass ConditionalBottleNeck(nn.Module):\n    \"\"\"Down projection and up projection with FiLM layers within Transformer layer.\"\"\"\n    def __init__(self, config):\n        super(ConditionalBottleNeck, self).__init__()\n        self.emb_transf = nn.Linear(config.hidden_size, config.hidden_size)\n        self.hidden_modulation = FiLM(config.hidden_size, config.hidden_size)\n        self.down_proj_layer = nn.Linear(config.hidden_size, config.hidden_size//3)\n        self.up_proj_layer = nn.Linear(config.hidden_size//3, config.hidden_size)\n\n    def forward(self, x_cond, hidden_states):\n        x_cond = self.emb_transf(x_cond)\n        hidden_states = self.hidden_modulation(x_cond=x_cond, x_to_film=hidden_states)\n        hidden_states = self.down_proj_layer(hidden_states)\n        hidden_states = self.up_proj_layer(hidden_states)\n        return hidden_states\nimport os\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional, Callable, Tuple\n\nimport torch\nimport numpy as np\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler\nfrom transformers import Trainer, TrainingArguments, EvalPrediction, glue_output_modes\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup\n\nfrom src.data.glue_utils import compute_glue_metrics\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass MultiTaskTrainingArguments(TrainingArguments):\n    use_mt_uncertainty: bool = field(\n        default=False,\n        metadata={\"help\": \"Use MT-Uncertainty sampling method\"},\n    )\n    uniform_mt_sampling: bool = field(\n        default=False,\n        metadata={\"help\": \"Sample each task an equal amount to times per epoch.\"},\n    )\n    percent_of_max_data_size: float = field(\n        default=1.0,\n        metadata={\n            \"help\": \"If uniform_mt_sampling=True, specify the samples per task per \"\n            \"epoch based on the maximum dataset length. If below 0.0 or above 1.0,\"\n            \"it will be set to the closest of 0.0 or 1.0.\"\n        },\n    )\n    warmup_proportion: float = field(\n        default=0.1,\n        metadata={\"help\": \"0.0 to args.lr for warmup_proportion * num_training_steps\"},\n    )\n\n\nclass MultiTaskTrainer(Trainer):\n    def __init__(\n        self,\n        tokenizer,\n        data_args,\n        eval_datasets=None,\n        test_datasets=None,\n        *args,\n        **kwargs,\n    ):\n        super(MultiTaskTrainer, self).__init__(*args, **kwargs)\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        self.eval_datasets = eval_datasets\n        self.test_datasets = test_datasets\n        self.eval_results = {}\n\n    def get_optimizers(\n        self, num_training_steps: int\n    ) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n        \"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well.\n        If you want to use something else, you can pass a tuple in the Trainer's init,\n        or override this method in a subclass.\n        \"\"\"\n        if self.optimizers is not None:\n            return self.optimizers\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": self.args.weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        num_warmup_steps = (\n            self.args.warmup_proportion * num_training_steps\n        )  # this is different from overridden function\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.args.learning_rate,\n            eps=self.args.adam_epsilon,\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,  # this is different from overridden function\n        )\n        return optimizer, scheduler\n\n    def get_train_dataloader(self) -> DataLoader:\n        if self.args.use_mt_uncertainty:\n            return self._create_custom_dataloader()\n        else:\n            return super().get_train_dataloader()\n\n    def _create_custom_dataloader(self):\n        class MtUcertaintyIterator:\n            \"\"\"Sample tasks using uncertainty measure.\"\"\"\n\n            def __init__(self, my_loader):\n                self.my_loader = my_loader\n                self.loader_iters = [iter(loader) for loader in self.my_loader.loaders]\n                self.loader_iter_sizes = [len(i) for i in self.loader_iters]\n                self.max_count = len(self.my_loader)\n                self.batch_count = 0\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self.batch_count == self.max_count:\n                    self.batch_count = 0\n                    raise StopIteration()\n\n                test_batch = {}\n                for idx, loader_iter in enumerate(self.loader_iters):\n                    try:\n                        batch = loader_iter.__next__()\n                    except StopIteration:\n                        new_loader_iter = iter(self.my_loader.loaders[idx])\n                        self.loader_iters[idx] = new_loader_iter\n                        batch = new_loader_iter.__next__()\n\n                    test_batch = self.batchify_data(batch, test_batch)\n\n                inputs = {}\n                for k, v in test_batch.items():\n                    if k not in [\"labels\"]:\n                        inputs[k] = v.detach().to(self.my_loader.args.device)\n\n                with torch.no_grad():\n                    model.select_batch_mode = True\n                    outputs = model(**inputs)\n                    model.select_batch_mode = False\n\n                (\n                    test_batch_entropy,\n                    test_batch_entropy_mean,\n                    max_mean_batch_entropy,\n                ) = outputs[-3:]\n\n                for _, v in inputs.items():\n                    del v  # free GPU mem\n                del inputs\n\n                test_batch_entropy_mean = (\n                    test_batch_entropy_mean / max_mean_batch_entropy\n                )\n                test_batch_entropy = test_batch_entropy * test_batch_entropy_mean\n\n                if \"sts-b\" in tasks and \"mrpc\" in tasks:\n                    stsb_idx = test_batch[\"task_id\"] == tasks.index(\"sts-b\")\n                    mrpc_idx = test_batch[\"task_id\"] == tasks.index(\"mrpc\")\n                    num_items = min(\n                        len(test_batch_entropy[stsb_idx]),\n                        len(test_batch_entropy[mrpc_idx]),\n                    )\n                    stsb_idx = stsb_idx.nonzero()[:num_items]\n                    mrpc_idx = mrpc_idx.nonzero()[:num_items]\n                    test_batch_entropy[stsb_idx] = test_batch_entropy[mrpc_idx]\n                    test_batch_entropy_mean[stsb_idx] = test_batch_entropy_mean[\n                        mrpc_idx\n                    ]\n\n                select_size = min(\n                    self.my_loader.args.train_batch_size,\n                    test_batch[\"input_ids\"].shape[0],\n                )  # Handled the last batch if it is lower than the batch size\n\n                top_entropy = torch.topk(test_batch_entropy, select_size)\n\n                for k, v in test_batch.items():\n                    test_batch[k] = torch.index_select(v, 0, top_entropy.indices)\n\n                self.batch_count += 1\n\n                return test_batch\n\n            @staticmethod\n            def batchify_data(data, curr_batch):\n                for k in data.keys():\n                    if k in curr_batch.keys():\n                        curr_batch[k] = torch.cat((curr_batch[k], data[k]), dim=0)\n                    else:\n                        curr_batch[k] = data[k]\n                return curr_batch\n\n        class CustomLoader:\n            def __init__(self, loaders, datasets, loader_args):\n                self.loaders = loaders\n                self.dataset = datasets\n                self.args = loader_args\n                self.current_epoch = 0\n\n            def __iter__(self):\n                iterator = MtUcertaintyIterator(self)\n\n                # for determinism across runs\n                # https://github.com/pytorch/examples/issues/501\n                for l in self.loaders:\n                    if isinstance(l.sampler, DistributedSampler):\n                        l.sampler.set_epoch(self.current_epoch)\n                self.current_epoch += 1\n                return iterator\n\n            def __len__(self):\n                loader_len = [len(loader) for loader in self.loaders]\n                if self.args.uniform_mt_sampling:\n                    return int(\n                        self.args.percent_of_max_data_size\n                        * max(loader_len)\n                        * len(self.loaders)\n                        / self.args.train_batch_size\n                    )\n                elif self.args.use_mt_uncertainty:\n                    return int(\n                        max(loader_len)\n                        * len(self.loaders)\n                        * self.args.percent_of_max_data_size\n                    )\n                else:\n                    return sum(loader_len)\n\n        model = self.model\n        tasks = self.data_args.tasks\n\n        data_loaders = []\n        for dataset in self.train_dataset.datasets:\n            train_sampler = (\n                RandomSampler(dataset)\n                if self.args.local_rank == -1\n                else DistributedSampler(dataset)\n            )\n\n            data_loader = DataLoader(\n                dataset,\n                batch_size=self.args.train_batch_size,\n                sampler=train_sampler,\n                collate_fn=self.data_collator.collate_batch,\n            )\n            data_loaders.append(data_loader)\n\n        return CustomLoader(data_loaders, self.train_dataset, self.args)\n",
    "Experiment Result": "### Model Configuration (from `CaMtlArguments`):\n- `model_name_or_path`: \"Path to pretrained model or model identifier from: CA-MTL-base, CA-MTL-large, bert-base-cased bert-base-uncased, bert-large-cased, bert-large-uncased\"\n- `freeze_encoder_layers`: \"Freeze encoder layers. format: <start_layer>-<end_layer>\"\n- When `freeze_encoder_layers` is applied, the following modules are specifically unfrozen: `[\"random_weight_matrix\", \"film.gb_weights\", \"ln_weight_modulation.gb_weights\", \"adapter\"]`.\n\n### Data Configuration (from `MultiTaskDataArguments`):\n- `max_seq_length`: Maximum total input sequence length after tokenization, sequences longer are truncated, shorter are padded (default: `128`).\n\n### Training Configuration (from `MultiTaskTrainingArguments` and `MultiTaskTrainer`):\n- `use_mt_uncertainty`: Boolean to enable the Multi-Task Uncertainty sampling method (default: `False`).\n- `uniform_mt_sampling`: Boolean to sample each task an equal amount of times per epoch (default: `False`).\n- `percent_of_max_data_size`: Specifies samples per task per epoch based on the maximum dataset length if `uniform_mt_sampling=True`. If below 0.0 or above 1.0, it will be set to the closest of 0.0 or 1.0 (default: `1.0`).\n- `warmup_proportion`: The proportion of training steps used for linear warmup of the learning rate (default: `0.1`).\n\n### Multi-Task Uncertainty Sampling Mechanism (from `MultiTaskTrainer` and `Decoder`):\n- **Entropy Calculation**: Shannon Entropy is calculated for the logits of each sample after passing through a Softmax function in the `Decoder`.\n- **Entropy Normalization**: The calculated `samples_entropy` is normalized by `max_entropy`, which is the entropy of an even distribution across classes (`1/num_labels`). A small epsilon (`1e-5`) is added for numerical stability.\n- **Uncertainty Prioritization**: In the `_create_custom_dataloader` method:\n    - `batch_entropy`, `batch_entropy_mean`, and `max_mean_batch_entropy` are retrieved from the model's forward pass for a combined batch of samples from all tasks.\n    - `batch_entropy_mean` is normalized by `max_mean_batch_entropy`.\n    - The `batch_entropy` for each sample is then multiplied by this normalized `batch_entropy_mean` to prioritize samples.\n- **Sample Selection**: From the prioritized `batch_entropy` values, `torch.topk` is used to select `select_size` samples with the highest entropy. `select_size` is defined as `min(self.my_loader.args.train_batch_size, test_batch[\"input_ids\"].shape[0])` to handle batches smaller than the `train_batch_size`.\n- **Special Handling for STS-B and MRPC**: If both \"sts-b\" and \"mrpc\" tasks are present, the `test_batch_entropy` and `test_batch_entropy_mean` for a subset of \"sts-b\" items are overwritten with those from \"mrpc\" items, matching the number of items (`num_items = min(len(test_batch_entropy[stsb_idx]), len(test_batch_entropy[mrpc_idx]))`).\n- **Custom Loader Length**: The length of the `CustomLoader` is determined by:\n    - If `uniform_mt_sampling` is `True`: `int(percent_of_max_data_size * max(loader_len) * len(self.loaders) / train_batch_size)`.\n    - If `use_mt_uncertainty` is `True`: `int(max(loader_len) * len(self.loaders) * percent_of_max_data_size)`.\n    - Otherwise, it's the sum of all individual loader lengths."
}{
    "Title": "Multi-Head Adapter Routing for Cross-Task Generalization",
    "Main Contributions": "The paper investigates the role of adapter routing in parameter-efficient fine-tuning (PEFT) for cross-task generalization, building upon Polytropon (Poly). It introduces Multi-Head Routing (MHR), which enhances expressivity by combining blocks of parameters from different adapters, outperforming Poly with a comparable parameter budget. A variant, MHR-z, is proposed for extreme parameter efficiency by fine-tuning only the routing function during few-shot adaptation. The research reveals that the superior performance of routing-based PEFT methods like Poly/MHR stems from better multi-task optimization, characterized by higher gradient alignment between tasks, rather than modular inductive biases. Furthermore, it introduces MHR-µ, which discards routing during few-shot adaptation and fine- tunes the average of pre-trained adapters, achieving competitive performance and establishing an effective single-adapter fine-tuning method. MHR-µ is also shown to be effective for zero-shot transfer with additional fine-tuning on the multi-task pre-training set, yielding significant accuracy gains.",
    "Methodology": "The core methodology extends Polytropon (Poly), which uses an inventory of LoRA adapters and a routing function to select task-specific subsets of modules. The proposed MHR (Multi-Head Routing) partitions adapter dimensions into 'h' disjoint blocks (heads) and applies a separate Poly-style combination for each block, learning a distinct routing matrix for each head. These head-specific combinations are then concatenated to form the task-specific adapter, enabling more fine-grained, piecewise linear mixing of parameters. MHR-z is a variant that freezes adapter parameters and only updates the routing parameters during few-shot adaptation for parameter efficiency. MHR-µ initializes the adapter for fine-tuning by averaging all pre-trained module parameters, effectively discarding the routing function during adaptation to assess its necessity. For zero-shot transfer, MHR-µ involves training the averaged pre-trained adapters for additional steps on the multi-task pre-training data. The paper also analyzes gradient alignment between tasks using cosine similarity during multi-task pre-training to understand the underlying optimization benefits.",
    "Experimental Setup": "The methods were evaluated using T5-XL, T0-3B, and T0-11B as backbone models. Experiments were conducted on two primary datasets: the T0 evaluation suite, comprising 313 unique tasks (data subset–template pairs), and a subset of 20 randomly selected test tasks from Super-Natural Instructions (SuperNI), each with 100 training, 100 validation, and 100 test examples. Performance on T0 was measured by the mean of the best validation accuracy across 3 seeds, while SuperNI used Rouge-L averaged over 3 seeds. Baselines included standard PEFT methods like LoRA and (IA)3, AdapterSoup, the original Polytropon (Poly), and a random-routing baseline (Random-µ) for comparative analysis. All experiments were performed on a single NVIDIA A100 GPU.",
    "Limitations": "One limitation identified is that without additional fine-tuning steps (k=0), MHR-µ for zero-shot learning does not yield good results, primarily due to a potential mismatch between adapters learned via task-specific routing and the uniform routing strategy used for averaging. The broader impact section also highlights a general necessity to critically examine potential biases and ethical implications of the underlying large language models and training data when applying such adaptation methods.",
    "Future Research Directions": "The findings suggest potential avenues for future work in improving meta-learning and weight-averaging approaches, inspired by the effectiveness of MHR-µ. Additionally, multi-head routing's demonstration of fine-grained adapter selection for sample-efficient generalization holds promise for enhancing other modular methods, such as Mixtures of Experts (MoEs).",
    "Experiment Code": "dataclasses.dataclass\nclass SkilledLoRAConfig(LoRAConfig):\n    n_skills: int = 1\n    n_splits: int = 1\n@mttl.models.modifiers.base.Modifier.register(\"skilled_lora\", config_cls=SkilledLoRAConfig)\nclass SkilledLoRA(LoRA):\n    def __init__(\n        self,\n        config: SkilledLoRAConfig,\n        layer: nn.Module,\n        **kwargs,\n    ):\n        self.n_splits = config.n_splits\n        self.n_skills = config.n_skills\n        super().__init__(config, layer)\n    def create_for_layer(self, layer):\n        self.lora_a = nn.Parameter(\n            torch.empty(\n                self.n_skills,\n                self.n_splits,\n                layer.in_features // self.n_splits,\n                self.rank,\n            ).to(device=self.weight.device)\n        )\n        self.lora_b = nn.Parameter(\n            torch.empty(\n                self.n_skills,\n                self.rank,\n                self.n_splits,\n                layer.out_features // self.n_splits,\n            ).to(device=self.weight.device)\n        )\n    @classmethod\n    def parallel_linear_weighted_forward(\n        cls,\n        input: torch.Tensor,\n        skilled_loras: List[\"SkilledLoRAView\"],\n        weights: torch.Tensor,\n        dim_names: List[str],\n        merge_after: bool = False,\n    ):\n        if len(set([lora.layer for lora in skilled_loras])) > 1:\n            raise ValueError(\"Cannot parallelize loras applied to different layers.\")\n        if len(dim_names) != weights.ndim:\n            raise ValueError(\"Not all dimensions are present in the weights tensor.\")\n        device = input.device\n        n_skills = skilled_loras[0].lora_a.shape[0]\n        assert np.all(skl.n_skills == n_skills for skl in skilled_loras)\n        if num_skilled_loras == 1:\n            skilled_loras_a = skilled_loras[0].lora_a.unsqueeze(0)\n            skilled_loras_b = skilled_loras[0].lora_b.unsqueeze(0)\n        else:\n            skilled_loras_a = torch.stack(\n                [lora.lora_a for lora in skilled_loras], dim=0\n            )\n            skilled_loras_b = torch.stack(\n                [lora.lora_b for lora in skilled_loras], dim=0\n            )\n        if skilled_loras_a.device != device:\n            skilled_loras_a = skilled_loras_a.to(device=device)\n            skilled_loras_b = skilled_loras_b.to(device=device)\n        expected_dims = [\"batch\", \"sequence\", \"splits\", \"experts\"]\n        for i, dim in enumerate(expected_dims):\n            if dim not in dim_names:\n                weights = weights.unsqueeze(i)\n        weights = weights.to(dtype=skilled_loras[0].lora_a.dtype)\n        layer_out = skilled_loras[0].layer(input)\n        input_lora = input.to(skilled_loras[0].lora_a.dtype)\n        input_lora = skilled_loras[0].dropout_layer(input_lora)\n        if input_lora.ndim == 2:\n            input_lora = input_lora.unsqueeze(1)\n        if merge_after:\n            partial_out = torch.einsum(\"bld,beqdr->bleqr\", input_lora, skilled_loras_a)\n            adapter_out = torch.einsum(\n                \"bleqr,berqd,blqe->blqd\", partial_out, skilled_loras_b, weights\n            )\n            adapter_out = adapter_out.flatten(2, 3)\n        else:\n            A = torch.einsum(\"blqe,beqdr->blqdr\", (weights, skilled_loras_a))\n            B = torch.einsum(\"blqe,berqd->blrqd\", (weights, skilled_loras_b))\n            batch_size, sequence_length, rank, n_splits, d_split = B.shape\n            A, B = A.flatten(2, 3), B.flatten(3, 4)\n            partial_out = torch.einsum(\"bld,bldr->blr\", (input_lora, A))\n            adapter_out = torch.einsum(\"blr,blrd->bld\", (partial_out, B))\n        adapter_out.mul_(scaling)\n        if layer_out.ndim == 2:\n            adapter_out = adapter_out.squeeze(1)\n        return layer_out + adapter_out.to(dtype=input.dtype)\n\n\n@dataclasses.dataclass\nclass PolySelectorConfig(mttl.models.containers.selectors.base.SelectorConfig):\n    n_splits: int = 1\n    task_names: List[str] = None\n    allow_unknown_tasks: bool = False\n\n\n@mttl.models.containers.selectors.base.Selector.register(\"poly_router\", config_cls=PolySelectorConfig)\nclass PolySelector(mttl.models.containers.selectors.base.Selector):\n    avg_selector_warned: bool = False\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.n_tasks = len(self.config.task_names) if self.config.task_names else 0\n        shape = (\n            self.n_tasks + (1 if self.config.allow_unknown_tasks else 0),\n            self.config.n_splits,\n        )\n        self.module_logits = nn.Parameter(torch.empty(*shape).uniform_(-1e-3, 1e-3))\n        if self.n_tasks == 0:\n            logger.warning(\n                \"No task names found in the config. Using a single task for PolySelector.\"\n            )\n\n    def _get_weights(self, task_names: List[str] = None) -> torch.Tensor:\n        if self.n_tasks == 0:\n            task_ids = [0]\n        else:\n            if task_names is not None:\n                task_ids = self._convert_task_names_to_ids(task_names)\n            else:\n                routing_info: RoutingInfo = self.routing_infos\n                if hasattr(routing_info, \"task_ids_from_name\"):\n                    task_ids = routing_info.task_ids_from_name\n                else:\n                    task_ids = self._convert_task_names_to_ids(routing_info.task_names)\n                    self.routing_infos.task_ids_from_name = task_ids\n            if task_ids.max() < self.n_tasks:\n                if PolySelector.avg_selector_warned:\n                    logger.warning(\n                        f\"Task ids were found. Reverting to default task-based routing\"\n                    )\n                PolySelector.avg_selector_warned = False\n            else:\n                if not PolySelector.avg_selector_warned:\n                    not_found_tasks = set(\n                        [\n                            t\n                            for t in self.routing_infos.task_names\n                            if t not in self.config.task_names\n                        ]\n                    )\n                    if self.config.allow_unknown_tasks:\n                        logger.warning(\n                            f\"Tasks {not_found_tasks} not in training tasks. Defaulting to average selector.\"\n                        )\n                        PolySelector.avg_selector_warned = True\n                    else:\n                        raise ValueError(\n                            f\"Tasks {not_found_tasks} not in training tasks.\"\n                        )\n                assert not self.training, \"Unknown tasks during training\"\n        module_logits = torch.sigmoid(self.module_logits[task_ids])\n        module_logits = module_logits.view(\n            module_logits.size(0), self.config.n_splits, self.n_experts\n        )\n        module_weights = module_logits / (module_logits.sum(dim=-1, keepdim=True) + EPS)\n        return module_weights\n\n    @mttl.models.containers.selectors.base.forward_with_cache\n    def forward(self, input, **kwargs) -> Union[\n        BatchExpertsSplitsAndWeightsSelectorOutput,\n        ExpertsSplitsAndWeightsSelectorOutput,\n    ]:\n        weights = self._get_weights()\n        if self.n_tasks == 0:\n            return ExpertsSplitsAndWeightsSelectorOutput(\n                ALL_EXPERTS, weights.squeeze(0)\n            )\n        return BatchExpertsSplitsAndWeightsSelectorOutput(ALL_EXPERTS, weights)\n\n\n@mttl.models.containers.base.ExpertContainer.register(\"skilled_lora\", mttl.models.modifiers.lora.SkilledLoRAConfig)\nclass SkilledLoRAExpertContainer(LoRAExpertContainer):\n    __supports_configs__ = [SkilledLoRAConfig]\n\n    def __init__(\n        self,\n        config,\n        layer,\n        selector=None,\n        **kwargs,\n    ):\n        super().__init__(config, layer, selector)\n\n        if not isinstance(self.layer, nn.Linear):\n            raise ValueError(\n                \"Expert containers for layers other than nn.Linear have not been implemented, current layer is {}\".format(\n                    self.layer.__class__.__name__\n                )\n            )\n        self.dummy_config = SkilledLoRAConfig(\n            lora_alpha=config.lora_alpha,\n            lora_dropout=config.lora_dropout,\n            lora_init_b_random=config.lora_init_b_random,\n            lora_rank=config.lora_rank,\n            n_splits=config.n_splits if isinstance(config, SkilledLoRAConfig) else 1,\n            n_skills=0,\n        )\n        self.experts = SkilledLoRA(self.dummy_config, layer)\n\n    def on_add_expert(\n        self, expert: Expert, is_default=False, device: str = None\n    ) -> None:\n        from mttl.models.containers.utils import filter_expert_weights\n\n        self._check_config(expert.expert_config)\n        lora_type = get_modifier_name(expert.expert_config)\n        LoRA_cls = {\"lora\": LoRA, \"skilled_lora\": SkilledLoRA}[lora_type]\n        modifier_module = LoRA_cls(\n            expert.expert_config, self.layer, layer_name=self.__layer_name__\n        )\n        if expert.expert_weights:\n            expert_weights = filter_expert_weights(\n                self.__layer_name__, expert.expert_weights\n            )\n            modifier_module.load_lora_weights(expert_weights)\n        self.experts.add_skill(modifier_module)\n\n    def route(self, input, selection, **kwargs):\n        if isinstance(selection, BatchExpertsSelectorOutput):\n            batch_size = len(selection.experts)\n            indices = torch.LongTensor(\n                self._convert_expert_names_to_indices(\n                    selection.experts,\n                    use_default_expert=self.default_expert_name is not None,\n                )\n            )\n            weights = (\n                torch.zeros(\n                    (batch_size, self.experts.n_skills),\n                )\n                .scatter_add(\n                    1, indices.unsqueeze(1), torch.ones((len(selection.experts), 1))\n                )\n                .to(device=self.experts.lora_a.device, dtype=torch.float32)\n            )\n            module_output = SkilledLoRA.parallel_linear_weighted_forward(\n                input,\n                [self.experts],\n                weights,\n                dim_names=[\"batch\", \"experts\"],\n                merge_after=self.selector.config.lora_merge_after,\n            )\n            return module_output\n        elif (\n            isinstance(selection, BatchSequenceExpertsAndWeightsSelectorOutput)\n            or isinstance(selection, BatchExpertsAndWeightsSelectorOutput)\n            or isinstance(selection, ExpertsAndWeightsSelectorOutput)\n        ):\n            if selection.experts is not ALL_EXPERTS:\n                if not isinstance(selection.experts, torch.Tensor):\n                    selection.experts = torch.LongTensor(\n                        self._convert_expert_names_to_indices(\n                            selection.experts,\n                            use_default_expert=self.default_expert_name is not None,\n                        )\n                    ).to(selection.weights.device)\n                weights = torch.zeros(\n                    (selection.weights.shape[:-1] + (self.experts.n_skills,)),\n                    device=selection.weights.device,\n                    dtype=selection.weights.dtype,\n                ).scatter_add(\n                    selection.weights.ndim - 1, selection.experts, selection.weights\n                )\n            else:\n                weights = selection.weights\n                assert weights.shape[-1] == self.experts.n_skills\n            module_output = SkilledLoRA.parallel_linear_weighted_forward(\n                input,\n                [self.experts],\n                weights,\n                dim_names=selection.dim_names,\n                merge_after=self.selector.config.lora_merge_after,\n            )\n            return module_output\n        else:\n            raise ValueError(\"Unknown selection type.\")\n\n\n    def container_forward(self, input, **kwargs):\n        selection = self.selector(input, container=self, **kwargs)\n        return self.route(input, selection, **kwargs)",
    "Experiment Result": "MHR (Multi-Head Routing) is a method that extends Polytropon (Poly) by partitioning adapter dimensions into 'h' disjoint blocks (heads) and applying a separate Poly-style combination for each block, learning a distinct routing matrix for each head. These head-specific combinations are then concatenated to form the task-specific adapter.\n\n**Core MHR Configuration:**\n- The number of disjoint blocks or heads (`h`) is configured via `n_splits` in both `SkilledLoRAConfig` and `PolySelectorConfig`:\n    - `SkilledLoRAConfig.n_splits: int` (defines how adapter dimensions are partitioned)\n    - `PolySelectorConfig.n_splits: int` (defines the routing matrix dimensions)\n- The learning of a distinct routing matrix for each head is handled by `PolySelector` where `module_logits` has a shape of `(n_tasks, n_splits)`.\n\n**MHR-z Variant (Freezing Adapter Parameters):**\n- This variant freezes adapter parameters and updates only the routing parameters during few-shot adaptation.\n- This behavior is configured by setting `trainable_param_names` in `TrainingArgs` (from `mttl/arguments.py`) to target only selector and module logits parameters:\n    - `args.trainable_param_names = \".*module_logits.*|.*selector.*\"` (as seen in `finetune_polylib_selector` in `projects/modular_llm/finetune_experts.py`).\n\n**MHR-µ Variant (Averaging Pre-trained Modules):**\n- This variant initializes the adapter for fine-tuning by averaging all pre-trained module parameters, effectively discarding the routing function during adaptation.\n- The initialization involves averaging module parameters, which can be achieved by `WeightedLinearMerge` (from `mttl/models/library/library_transforms.py`) as used in `create_mean_expert` (from `projects/modular_llm/finetune_experts.py`).\n- Discarding the routing function during adaptation is achieved in functions like `pretrain_poly` (from `projects/modular_llm/finetune_experts.py`) with:\n    - `module.model.switch_selector_to_average()`\n    - `module.model.resize_module_logits(1)`\n- For zero-shot transfer, MHR-µ involves training the averaged pre-trained adapters for additional steps on the multi-task pre-training data.\n\n**Training and Optimization Settings (from `TrainingArgs` in `mttl/arguments.py`):**\n- `router_weight_decay: float = None`\n- `router_learning_rate: float = None`\n- `module_logits_relaxed_bernoulli: bool = True`\n- `module_logits_straight_through: bool = False`\n- `module_logits_learning_rate: float = 0.1`\n- `adapters_learning_rate: float = None`\n- `adapters_weight_decay: float = None`\n- `module_logits_dropout: float = 0.0`\n- `module_logits_l2_norm: float = False`\n- Gradient alignment analysis during multi-task pre-training is performed using:\n    - `monitor_grad_alignment_on: str = None`"
}{
    "Title": "Sparse High Rank Adapters",
    "Main Contributions": "The paper introduces Sparse High Rank Adapters (SHiRA), a new parameter-efficient finetuning (PEFT) paradigm that addresses limitations of Low Rank Adaptation (LoRA), specifically related to mobile deployment, rapid adapter switching, and multi-adapter fusion. SHiRA enables training by modifying only 1-2% of base model weights, resulting in a highly sparse adapter that can be rapidly switched in a fused mode without inference overhead. Key contributions include: (i) proposing SHiRA to enable rapid switching for fused adapters and reduce concept loss in multi-adapter fusion; (ii) demonstrating that finetuning a small fraction of parameters (1-2%) significantly outperforms LoRA on diverse vision and language tasks; (iii) providing theoretical and empirical insights into how high sparsity aids multi-adapter fusion by reducing concept loss; (iv) offering a PEFT-based, memory- and latency-efficient implementation that trains nearly as fast as LoRA while consuming 16% lower peak GPU memory, and enables 5x-16x faster inference loading on CPU than LoRA fusion.",
    "Methodology": "SHiRA operates by making a small percentage (1-2%) of existing pretrained model weights trainable, rather than adding new weights. This is achieved through gradient masking during backpropagation, where a highly sparse mask (M = {0, 1}) is applied to gradients using a Hadamard product. The paper explores various mask construction strategies: SHiRA-Struct (structured rows/columns/diagonal), SHiRA-Rand (random selection), SHiRA-WM (top-K weights by magnitude), SHiRA-Grad (top 1-2% weights by gradient magnitude), and SHiRA-SNIP (combining weight and gradient magnitudes). For rapid switching, SHiRA stores sparse weights and their indices, directly overwriting base model weights using a `torch.Tensor.scatter_` operation. For multi-adapter fusion, SHiRA leverages high sparsity to reduce interference between adapters, aiming for near-orthogonality, especially with SHiRA-Struct. Theoretical insights include analysis of parameter/learning complexity, LoRA as an approximation of SHiRA, and adapter weight orthogonality (AWOM and AWOR metrics). Implementations include a backward hook-based gradient masking for general trainers and a PEFT-based one for efficiency.",
    "Experimental Setup": "The research conducted extensive experiments across Large Vision Models (LVMs) and Large Language Models (LLMs). For vision tasks, the RealisticVision-v3 model checkpoint for Stable Diffusion-v1.5 and SDXL were finetuned on two style transfer datasets: Bluefire (54 images for 'blue fire' effect) and Paintings (90 images for 'paintings' effect). DreamBooth was also used with SDXL for content/style personalization. Image quality was quantified using Human Preference Score-V2 (HPSv2). For language tasks, LLaMA-7B and LLaMA2-7B were evaluated on commonsense reasoning benchmarks (HellaSwag, PIQA, SIQA, BoolQ, Arc-easy, Arc-challenge, OpenBookQA, Winogrande), using a combined 170K sample dataset for single adapters, and individual task datasets for multi-adapter fusion. Accuracy was the metric. Further experiments included Image Classification (ViT on CIFAR-10, CIFAR-100, Food101, DTD) and GLUE tasks (DeBERTa-V3-base on QNLI, COLA, SST2, MRPC). All finetuning and evaluation experiments were performed on a single NVIDIA A100 GPU. Inference speed benchmarks were conducted on a Desktop-grade CPU, comparing LoRA fusion vs. SHiRA `scatter_op` for various tensor dimensions, and end-to-end switching times for SDXL and LLaMA2-7B.",
    "Limitations": "The paper identifies several limitations: (i) For optimal mobile deployment, hardware-software co-design techniques (e.g., lookup-table based approaches) may be necessary to fully optimize SHiRA's implementation on edge devices. (ii) Building optimal sparse masks, specifically determining which parameters to train for a given task, requires further investigation. (iii) SHiRA-Struct, while effective for vision tasks, did not perform well on more complex language tasks like commonsense reasoning, likely due to its inherent rank 1 + diagonal adapter structure potentially lacking sufficient expressive power. (iv) Similar to other parameter-efficient finetuning (PEFT) techniques, SHiRA carries the potential risk of digital forgery due to finetuning on smaller subsets of data, which could lead to overfitting. (v) A deeper subspace analysis of SHiRA's properties is left for future work.",
    "Future Research Directions": "Future research directions include: (i) Exploring hardware-software co-design techniques, such as lookup-table (LUT) based approaches, to further optimize SHiRA's implementation for resource-constrained edge devices and achieve more efficient mobile deployment. (ii) Investigating advanced strategies for building optimal sparse masks, focusing on identifying the most pertinent parameters to train for specific tasks to maximize performance and efficiency. (iii) Conducting a comprehensive subspace analysis of SHiRA to better understand its theoretical properties and how it interacts with the base model, which was explicitly left as future work.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "Main Contributions": "The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel parameter-efficient fine-tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and full fine-tuning (FT) without incurring additional inference costs. It achieves this by decomposing pre-trained weights into magnitude and direction components, with LoRA applied specifically for directional updates. DoRA consistently outperforms LoRA across various tasks and models, including LLaMA, LLaVA, and VL-BART, and is compatible with other LoRA variants like VeRA. The work also presents a novel weight decomposition analysis to identify fundamental differences in the learning patterns of FT and existing PEFT methods.",
    "Methodology": "DoRA reparameterizes pre-trained model weights (W) into a magnitude vector (m) and a directional matrix (V), such that W = m * (V / ||V||c). During fine-tuning, DoRA fine-tunes both components, with the directional component updated using a low-rank adaptation (LoRA) mechanism (∆V = BA). The magnitude component (m) is a trainable vector, while the base directional component (W0) remains frozen. A gradient analysis is performed to demonstrate how this decomposition enhances optimization stability and aligns DoRA's learning pattern, characterized by a negative slope between magnitude and direction updates, more closely with FT. To reduce training overhead, a modification is introduced where the norm factor ||V + ∆V ||c is treated as a constant during backpropagation, significantly reducing memory consumption without notable accuracy loss.",
    "Experimental Setup": "DoRA's efficacy is validated across diverse tasks and models. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B are fine-tuned on 8 sub-tasks, comparing DoRA against LoRA, Prefix, Series, and Parallel adapters, and ChatGPT. For image/video-text understanding, VL-BART is used with image-text tasks (VQAv2, GQA, NLVR2, MSCOCO) and video-text tasks (TVQA, How2QA, TVC, YC2C). Visual instruction tuning is performed on LLaVA-1.5-7B, evaluated on seven vision-language benchmarks (VQAv2, GQA, VisWiz, SQA, VQAT, POPE, MMBench). Compatibility with LoRA variants is shown by combining DoRA with VeRA (DV oRA) for instruction tuning on LLaMA-7B/LLaMA2-7B using the Alpaca dataset, evaluated on MT-Bench. Ablation studies cover varying rank settings and training sample sizes. QDoRA (DoRA on QLoRA) is evaluated on Orca-Math with LLaMA2-7B/LLaMA3-8B. Text-to-image generation with SDXL is explored using DreamBooth with 3D icons and Lego sets.",
    "Limitations": "The paper notes that in scenarios where full fine-tuning (FT) performs worse than LoRA, DoRA's improvements over LoRA might be less pronounced compared to situations where FT generally outperforms LoRA. This suggests that DoRA's performance uplift is somewhat conditional on the underlying capacity for improvement relative to FT. Additionally, while inference overhead is avoided, the training mechanism for DoRA is implicitly more complex than vanilla LoRA due to the weight decomposition and the need for specific gradient handling to reduce memory during backpropagation, although this was successfully mitigated.",
    "Future Research Directions": "The authors intend to explore the generalizability of DoRA in domains beyond language and vision, specifically mentioning the field of audio as a promising area for future research.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "Main Contributions": "This research introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel parameter-efficient fine-tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and full fine-tuning (FT) while avoiding additional inference costs. DoRA achieves this by decomposing pre-trained weights into magnitude and directional components, applying LoRA specifically for directional updates and allowing magnitude components to be tunable. The paper also presents a novel weight decomposition analysis to understand the fundamental differences in learning patterns between FT and existing PEFT methods like LoRA. DoRA consistently outperforms LoRA across various tasks and model architectures, including LLaMA, LLaVA, and VL-BART, and demonstrates compatibility with other LoRA variants like VeRA.",
    "Methodology": "The core methodology involves a novel weight decomposition analysis, which reparameterizes model weights into magnitude and directional components to study LoRA and FT learning patterns. Based on these insights, DoRA is proposed, which first decomposes the pre-trained weight W0 into a magnitude vector m and a directional matrix V. For fine-tuning, DoRA keeps m as a trainable vector and updates the directional component V using a low-rank adaptation (LoRA) formulation (∆V = BA), resulting in W′ = m * (W0 + BA) / ||W0 + BA||c. This approach simplifies the learning task for LoRA by concentrating it on directional adaptation and enhances training stability. A gradient analysis is performed to explain DoRA's learning pattern. To reduce training overhead, a modification is introduced where ||V + ∆V||c is treated as a constant during backpropagation, drastically reducing gradient graph memory consumption with negligible accuracy impact.",
    "Experimental Setup": "DoRA's efficacy is evaluated across diverse tasks and models. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B are fine-tuned on eight sub-tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA) and compared against LoRA, Prefix, Series, Parallel adapters, and ChatGPT. For image/video-text understanding, VL-BART is used with image-text tasks (VQAv2, GQA, NLVR2, MSCOCO Caption) and video-text tasks (TVQA, How2QA, TVC, YC2C). Visual instruction tuning is performed on LLaVA-1.5-7B, evaluated on VQAv2, GQA, VisWiz, SQA, VQAT, POPE, and MMBench. Compatibility with other LoRA variants is tested by creating DVoRA (DoRA with VeRA) and evaluating on LLaMA-7B/LLaMA2-7B with a 10K subset of Alpaca dataset using MT-Bench (GPT-4 scoring). Ablation studies investigate robustness to varying training sample sizes and rank settings. QDoRA (DoRA with QLoRA) is tested on LLaMA2-7B/LLaMA3-8B with Orca-Math dataset. Text-to-image generation is explored by fine-tuning SDXL via DreamBooth on 3D icons and Lego sets.",
    "Limitations": "The paper does not explicitly list limitations of DoRA. It mentions that in scenarios where full fine-tuning (FT) is inferior to LoRA, DoRA's improvement over LoRA might not be as pronounced, as DoRA is designed to align more closely with FT's learning behavior. A practical consideration is the marginal increase of 0.01% in trainable parameters for DoRA over LoRA due to the learnable magnitude components. Additionally, the proposed memory reduction technique for training overhead involves treating a component as constant during backpropagation, which is an approximation.",
    "Future Research Directions": "The authors express interest in exploring the generalizability of DoRA in domains beyond language and vision, specifically mentioning the field of audio as a promising area for future research.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
    "Main Contributions": "The main research problem addresses the suboptimal performance of parameter-efficient fine-tuning methods for large pre-trained language models (PLMs) due to their even distribution of incremental update budgets across weight matrices, overlooking their varying importance. The key contribution is AdaLoRA, a novel method that adaptively allocates the parameter budget among weight matrices based on their importance score. AdaLoRA parameterizes incremental updates using singular value decomposition (SVD) to effectively prune singular values of unimportant updates, reducing parameter budget without intensive SVD computations. It significantly improves performance, especially in low-budget settings, across natural language processing, question answering, and natural language generation tasks.",
    "Methodology": "AdaLoRA's methodology consists of two main components: (i) SVD-based adaptation and (ii) Importance-aware rank allocation. For SVD-based adaptation, incremental updates (Δ) are parameterized as Δ = PΛQ, where P and Q are orthogonal matrices representing left/right singular vectors and Λ is a diagonal matrix containing singular values. An orthogonality regularizer R(P, Q) = ||PᵀP - I||F² + ||QQᵀ - I||F² is added to the training loss. For importance-aware rank allocation, singular values are pruned iteratively based on an importance score for each triplet Gk,i = {Pk,∗i, λk,i, Qk,i∗}. The importance metric Sk,i combines smoothed sensitivity and uncertainty of each entry (λk,i, Pk,∗i, Qk,i∗), calculated as I⁽ᵗ⁾(wij) = β₁I⁽ᵗ⁻¹⁾(wij) + (1-β₁)I(t)(wij) and U⁽ᵗ⁾(wij) = β₂U⁽ᵗ⁻¹⁾(wij) + (1-β₂) |I⁽ᵗ⁾(wij) - I⁽ᵗ⁾(wij)|, with s⁽ᵗ⁾(wij) = I⁽ᵗ⁾(wij) · U⁽ᵗ⁾(wij). A global budget scheduler gradually reduces the total rank b(t) from an initial higher budget to the target budget following a cubic schedule, allowing for parameter space exploration before focusing on important weights.",
    "Experimental Setup": "AdaLoRA was implemented using PyTorch and the Huggingface Transformers codebase, running on NVIDIA V100 GPUs. Experiments were conducted with DeBERTaV3-base on natural language understanding (GLUE benchmark: MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B) and question answering (SQuADv1.1, SQuADv2.0) tasks. BART-large was used for natural language generation (XSum, CNN/DailyMail). Baselines included Full fine-tuning, Bitfit, Adapter tuning (Houlsby and Pfeiffer adapters), and LoRA (generalized to all weight matrices: Wq, Wk, Wv, Wf1, Wf2). Evaluation metrics included accuracy, F1, EM, and ROUGE 1/2/L scores. Experiments compared methods under various parameter budget levels (e.g., 0.08% to 2.20% of total pre-trained parameters) by tuning adapter hidden dimensions, LoRA rank r, and AdaLoRA's final total rank b(T). The learning rate was selected from a range {5e-5 to 1e-3}, and exponential moving average parameters β1, β2 were set to 0.85.",
    "Limitations": "The paper does not explicitly state significant limitations of AdaLoRA itself. However, it notes that AdaLoRA incurs an additional training time overhead, although it is considered not significant (e.g., 11-16% more training time compared to LoRA on MNLI and SQuADv2.0 datasets). It also highlights the limitations of prior methods like diff pruning (reliance on low-level implementation for unstructured sparse matrices, high computational cost) and LoRA (prespecifying identical rank for all incremental matrices, leading to suboptimal performance by ignoring varying importance, and potential training instability with doublet-wise pruning). AdaLoRA is designed to overcome these limitations.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "import loralib as lora\n# ...\nclass Attention(nn.Module):\n    # ...\n    self.c_attn = lora.MergedLinear(\n        nx, n_state * 3,\n        r=config.lora_attn_dim,\n        lora_alpha=config.lora_attn_alpha,\n        lora_dropout=config.lora_dropout,\n        enable_lora=[True, False, True],\n        fan_in_fan_out=True,\n        merge_weights=False\n    )\n    # ...\nclass GPT2Config(object):\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n        lora_attn_dim=0,\n        lora_attn_alpha=128,\n        lora_dropout=0.0,\n        lora_r_dropout=0.0,\n        fix_dropout=0.0,\n    ):\n        # ...\n        self.lora_attn_dim = lora_attn_dim\n        self.lora_attn_alpha = lora_attn_alpha\n        self.lora_dropout = lora_dropout\n        self.lora_r_dropout = lora_r_dropout\n        # ...\n\n# In examples/NLG/src/gpt2_ft.py\nimport loralib as lora\n# ...\nparser.add_argument('--lora_dim', type=int, default=0, help='lora attn dimension')\nparser.add_argument('--lora_alpha', type=int, default=128, help='lora attn alpha')\nparser.add_argument('--lora_dropout', default=0.0, type=float, help='dropout probability for lora layers')\n# ...\nif args.lora_dim > 0:\n    lora.mark_only_lora_as_trainable(lm_net)\n# ...\n# Configuration for different model sizes, e.g., for gpt2.sm:\nconfig = GPT2Config(\n    n_embd=768, n_layer=12, n_head=12,\n    lora_attn_dim=args.lora_dim,\n    lora_attn_alpha=args.lora_alpha,\n    lora_dropout=args.lora_dropout,\n)",
    "Experiment Result": "The repository implements the underlying LoRA mechanism, which serves as a foundation for AdaLoRA. However, the explicit adaptive components of AdaLoRA, such as SVD-based adaptation (Δ = PΛQ with orthogonality regularizer) and importance-aware rank allocation (singular value pruning based on importance scores and a cubic budget scheduler), are not present in the provided code.\n\nExperimental settings for LoRA include:\n- `lora_attn_dim`: The rank of the low-rank matrices for LoRA. Configurable via `--lora_dim` (default 0).\n- `lora_attn_alpha`: The scaling factor for LoRA. Configurable via `--lora_alpha` (default 128).\n- `lora_dropout`: Dropout probability applied to LoRA layers. Configurable via `--lora_dropout` (default 0.0).\n- `lora_r_dropout`: Recurrent dropout for LoRA layers (not exposed as a command-line argument).\n- `enable_lora=[True, False, True]`: Specifies that LoRA is enabled for the query and value projections within the attention mechanism, but not for the key projection.\n\nDuring training, if `lora_attn_dim > 0`, only the LoRA parameters are made trainable (`lora.mark_only_lora_as_trainable(model)`). The base model parameters (e.g., `n_embd`, `n_layer`, `n_head` for `gpt2.sm`, `gpt2.md`, `gpt2.lg`) are inherited from predefined configurations and augmented with the LoRA-specific parameters."
}{
    "Title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
    "Main Contributions": "This research theoretically analyzes LoRA fine-tuning in the Neural Tangent Kernel (NTK) regime. The main contributions are: (i) full fine-tuning (without LoRA) admits low-rank solutions of rank r ≲ √ N; (ii) using LoRA with rank r ≳ √ N eliminates spurious local minima, allowing (stochastic) gradient descent to find the low-rank solutions; and (iii) the low-rank solution found using LoRA generalizes well.",
    "Methodology": "The study analyzes LoRA fine-tuning in the Neural Tangent Kernel (NTK) regime, approximating network changes with first-order Taylor expansions. It models LoRA's low-rank parameterization with weight decay as a nuclear-norm regularized optimization problem. Key techniques involve analyzing the optimization landscape by characterizing second-order stationary points (SOSPs) using matrix factorization theory and a perturbed loss function. The absence of spurious local minima is proven using Sard’s theorem and dimension-counting arguments. Generalization bounds are established using Rademacher complexity.",
    "Experimental Setup": "The experiments validate the theory by fine-tuning linearized pre-trained models. For NLP tasks, a linearized RoBERTa-base model is fine-tuned on datasets like SST-2, QNLI, MR, CR, Subj, and QQP (N=32, K=2) using prompt-based fine-tuning and cross-entropy loss, comparing LoRA (r ≥ 11) against full fine-tuning. Image classification uses a pre-trained vision transformer on the bean disease dataset (N=48, 3 labels), and speech classification uses a pre-trained wav2vec2 model on the SUPERB dataset (N=64, 4 labels). All experiments monitor training loss curves and test accuracy (for NLP), with image and speech tasks also reporting accuracies. Training focuses on the last layer's Wq and Wv weights.",
    "Limitations": "The theoretical analysis primarily relies on the Neural Tangent Kernel (NTK) regime, which is a key assumption that may not hold for all fine-tuning scenarios. The proof for the absence of spurious local minima requires a small, randomly generated perturbation to the loss function, which is not applied in practical LoRA fine-tuning; thus, the theoretical result is interpreted as applying 'generically' to the unperturbed case. The current theory also does not explain or predict the observed differences in convergence rates, particularly the slowdown with lower LoRA ranks, suggesting the loss landscape might still contain unfavorable regions like plateaus or saddles despite lacking spurious local minima. The provided guarantees are upper bounds, indicating an initial step in understanding LoRA dynamics.",
    "Future Research Directions": "Future work includes conducting more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, gaining a better understanding of the minimum rank requirement through lower bounds, and analyzing the tradeoff between training rate and LoRA rank, especially considering the observed phenomenon where lower LoRA ranks slow down convergence.",
    "Experiment Code": "class LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))  # scaling the initailization by \\frac{1}{sqrt{r}}\n           #torch.nn.init.kaiming_normal_(param)\n      \n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])     \n            \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) \n        self.gradient_dtype = gradient[0].dtype\n\n        # Compute \\langle G(X_i), B*A \\rangle \n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n            \n        def convert_to_buffer(name):\n            if layer_name in name:\n                #logger.info(\"Including {}\".format(name))\n                return False\n            else:\n                return True\n        \n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n  \n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n                \n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n    \n            \n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n            \n        targets.append(label)\n    \n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))\n\n    def finetune(self, train_dataset, eval_dataset):\n        \n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr) # Weight decay will be implented manually\n\n        if self.args.fp16 and _use_apex:\n            if not transformers.is_apex_available():\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            model, optimizer = amp.initialize(self.lora_model, optimizer, opt_level=self.args.fp16_opt_level)\n\n        # Multi-gpu training (should be after apex fp16 initialization)\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n        \n        epoch_count = 0 \n        \n        #Make sure to freeze other parameters\n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False)\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False)\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False)\n         \n        for name, param in self.lora_model.named_parameters():\n            if param.requires_grad:\n                print(f\"{name} is being trained\")\n                \n                \n        writer = SummaryWriter(f\"./finallasttrain/{self.model.model_args.apply_lora}-{self.args.linear_lr}-{self.model.data_args.task_name}-{self.args.seed}-{self.model.model_args.lora_r}\")\n        \n        file_exists = False\n        eval_file_exists = False\n        saved_gradients_eval = []\n        saved_gradients = []\n        for epoch in range(self.args.linear_num_epoch):\n            \n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n            \n            if self.args.eval_during_training:\n                total_loss_eval = 0\n                eval_preds=[]\n                eval_targets_list=[]\n                \n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                    \n                \n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1) \n                    self.gradient_dtype = gradient[0].dtype\n                    \n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)   \n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                ## Weight decay\n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001: #Skip this if there is no weight decay (weight decay = 0)\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)  #Soft-thresholding operator \n                            param = u@s@v\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param\n                \n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:  #Skip this if there is no weight decay (weight decay = 0)\n                            reg += (1/2)*(param ** 2).sum()  \n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            reg += torch.norm(param, p = 'nuc')\n                            \n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item()\n                            \n            file_exists = True\n                          \n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd) \n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n            writer.add_scalar(f\"train_loss_{self.model.data_args.task_name}/epoch\", avg_loss, epoch)\n            \n            # Do evaluation during training if needed.\n            if self.args.eval_during_training: \n                with torch.no_grad():\n                    \n                    for i, inputs_outer in enumerate(tqdm(dataloader_outer_eval, desc=\"Evaluating\")):\n                               \n                        if eval_file_exists:\n                            gradient_eval = saved_gradients_eval[i]\n                            for j in range(len(self.target_layers)):\n                                gradient_eval[j].to(self.args.device)\n                                gradient_eval[j].requires_grad_(False)\n                                \n                        else:\n                            gradient_eval, _  = self.compute_gradient_sharded(inputs_outer)\n                            for j in range(len(self.target_layers)):  \n                                gradient_eval[j].requires_grad_(False)\n                        \n                        \n                        output_eval = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient_eval)\n                        eval_logits, eval_targets = self.compute_model_logits(inputs_outer)\n                        eval_logits = eval_logits.to(self.args.device)\n                            \n                        loss_eval = nn.CrossEntropyLoss(reduction = 'sum')(output_eval + eval_logits , eval_targets) \n                        dist.all_reduce(loss_eval, op=dist.ReduceOp.SUM) \n                        total_loss_eval += loss_eval.item()\n                        \n                        eval_preds.append( eval_logits + output_eval )\n                        eval_targets_list.append(eval_targets)\n                        \n                        saved_gradients_eval_cpu = []\n                        for j in range(len(self.target_layers)):\n                            gradient_eval_layer= gradient_eval[j].detach().cpu()\n                            saved_gradients_eval_cpu.append(gradient_eval_layer)\n                        saved_gradients_eval.append(saved_gradients_eval_cpu)\n                        \n                       \n                eval_file_exists = True\n\n                avg_loss_eval = (total_loss_eval / len(dataloader_outer_eval.dataset) )+ (reg * self.args.linear_wd)\n                logger.info(f\"epoch : {epoch+1} eval_loss : {avg_loss_eval}\")\n                writer.add_scalar(f\"eval_loss_{self.model.data_args.task_name}/epoch\", avg_loss_eval, epoch)\n                \n                eval_preds = torch.cat(eval_preds, dim=0)\n                eval_targets = torch.cat(eval_targets_list, dim=0)\n\n                if self.args.local_rank != -1:\n                    logger.info(\"Starting to gather kernel across GPUs\")\n                    eval_preds = varsize_tensor_all_gather(eval_preds.to(self.args.device), torch.distributed.get_world_size())\n                    eval_targets = varsize_tensor_all_gather(eval_targets.to(self.args.device), torch.distributed.get_world_size())\n                    logger.info(\"Finished gathering kernel across GPUs\")\n\n                # Now calculate the accuarcies\n                metrics = None\n                eval_preds = eval_preds.cpu()\n                eval_targets = eval_targets.cpu()\n                \n                if self.args.binary_classification: # Make sure to compute loss before this transformation!\n                    eval_preds = torch.cat([-eval_preds, eval_preds], dim=-1) # convert back to two logits\n                    eval_targets = ((eval_targets + 1) / 2).long() # convert back from {-1, 1} to {0, 1}\n\n                if self.compute_metrics is not None:\n                    metrics = self.compute_metrics(EvalPrediction(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy()))\n\n                # Prefix all keys with eval_\n                for key in list(metrics.keys()):\n                    if not key.startswith(\"eval_\"):\n                        metrics[f\"eval_{key}\"] = metrics.pop(key)\n                metrics[\"eval_loss\"] = avg_loss\n\n                metrics.update(self.metrics())\n                metrics[\"grad_dim\"] = self.grad_dim\n\n                output = PredictionOutput(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy(), metrics=metrics)\n                metrics = output.metrics\n                objective = default_dev_objective(metrics)\n                logger.info(f\"epoch : {epoch+1}  objective : {objective}\")\n                writer.add_scalar(f\"Eval_acc_{self.model.data_args.task_name}/epoch\", objective, epoch)\n                         \n        writer.flush()\n        writer.close()\n        self.save_model(self.args.output_dir)\n        return avg_loss.item() , epoch_count",
    "Experiment Result": "LoRA Configuration:\n- `apply_lora`: True (use LoRA for finetuning)\n- `lora_alpha`: 1 (initialization scale for one of the low-rank matrices in LoRA)\n- `lora_r`: 8 (inner rank for LoRA matrices)\n\nLinearized Training Configuration:\n- `trainer`: \"linearized\" (specifies `LinearizedLoraTrainer`)\n- `linear_freeze_A`: False (do not freeze layer A in LoRA)\n- `linear_lr`: 0.01 (learning rate of linearized LoRA fine-tuning)\n- `linear_num_epoch`: 30 (number of epochs in linearized LoRA training)\n- `linear_wd`: 0 (weight decay in linearized LoRA, 0 implies nuclear-norm regularization using soft-thresholding if `apply_lora` is false, otherwise standard L2 decay on LoRA parameters)\n- `eval_during_training`: False (evaluation during the training process)\n- `train_last_layer`: True (train the last layer only for gradient computation)\n- `binary_classification`: False (if number of classes is 2, convert two softmax logits to single sigmoid logit if needed)"
}{
    "Title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
    "Main Contributions": "The paper theoretically analyzes LoRA fine-tuning in the Neural Tangent Kernel (NTK) regime. Its main contributions are proving that full fine-tuning admits low-rank solutions (rank r \rless\rless\rless\rless\rless\rless sqrt(N)), demonstrating that using LoRA with sufficient rank (r \rless\rless\rless\rless\rless\rless sqrt(N)) eliminates spurious local minima, thereby enabling (stochastic) gradient descent to find these low-rank solutions, and showing that these low-rank solutions generalize well.",
    "Methodology": "The methodology involves theoretical analysis of LoRA fine-tuning within the Neural Tangent Kernel (NTK) regime, where network changes are approximated by first-order Taylor expansions. LoRA training with weight decay is formulated as a rank-constrained nuclear-norm regularized optimization problem. The existence of low-rank solutions is proven by relating the problem to semi-definite programs. To ensure gradient-based methods find these solutions, the authors demonstrate that, under specific rank conditions and with a small perturbation, all second-order stationary points of the LoRA loss function are global minima, drawing on matrix factorization theory and Sard's theorem. Generalization guarantees are derived using Rademacher complexity and McDiarmid inequality.",
    "Experimental Setup": "Experiments were conducted to validate the theory by fine-tuning linearized pre-trained models. For NLP tasks, a linearized RoBERTa-base model was fine-tuned on six datasets (SST-2, QNLI, MR, CR, Subj, QQP) using N=32 data points and K=2 labels. Image classification involved a linearized Vision Transformer on the bean disease dataset (N=48, K=3). Speech classification used a linearized wav2vec2 model on a SUPERB dataset (N=64, K=4). All tasks employed cross-entropy loss, comparing LoRA (with ranks satisfying theoretical guarantees, e.g., r \rless\rless\rless\rless\rless\rless= 11 for NLP) against full fine-tuning. Only query (Wq) and value (Wv) weights of the last layer were fine-tuned. Validation involved observing training loss curves and test accuracy.",
    "Limitations": "The primary limitation is the reliance on the Neural Tangent Kernel (NTK) regime assumption, which dictates small network changes during fine-tuning and may not universally hold. The theoretical guarantee for the absence of spurious local minima applies to a 'perturbed' loss function, with an interpretive extension to the unperturbed practical LoRA setting. The theory also requires specific rank conditions (e.g., r(r+1)/2 > KN) for its guarantees. Critically, the theory does not address the rate of convergence; empirical observations indicate that lower LoRA ranks can slow down convergence, despite reaching global optima, possibly due to flatter loss landscapes.",
    "Future Research Directions": "Future research directions include conducting more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, better understanding the minimum rank requirements for LoRA through lower bounds, and analyzing the trade-off between training rate and LoRA rank, particularly how lower ranks might create unfavorable, flatter loss landscapes that slow down convergence.",
    "Experiment Code": "File: src/linearized.py\n\nclass LogitModelWrapper(nn.Module):\n    def __init__(self, model, binary_classification):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n\n    def forward(self, input_ids, attention_mask, mask_pos):\n        logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0]\n        if self.binary_classification:\n            assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n            logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        return logits\n\nclass LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n\n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))\n\n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])\n\n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n\n    def forward(self, input_ids, attention_mask, mask_pos, gradient):\n\n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0]\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n\n        self.num_labels = gradient[0].size(1)\n        self.gradient_dtype = gradient[0].dtype\n\n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) )\n\n        return output\n\nclass LinearizedLoraTrainer(LinearHeadTrainer):\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n\n        def convert_to_buffer(name):\n            if layer_name in name:\n                return False\n            else:\n                return True\n\n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n\n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n\n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n\n\n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n\n        targets.append(label)\n\n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))\n\n    def compute_gradient_everylayer(self, inputs_outer):\n\n        grads_outer=[]\n        with torch.no_grad():\n            for layer_name in self.target_layers:\n                    grads_outer_layer, targets = self.compute_gradient_perlayer(inputs_outer, layer_name)\n                    grads_outer.append(grads_outer_layer)\n\n        if self.grad_dim is None:\n            self.grad_dim = sum(np.prod(x.shape[2:]) for x in grads_outer)\n\n        return (grads_outer, targets)\n\n    def compute_model_logits(self, inputs_outer):\n\n        self.model_wrapper.eval()\n\n        logits = []\n        targets = []\n        with torch.no_grad():\n            for k, v in inputs_outer.items():\n                if isinstance(v, torch.Tensor):\n                    inputs_outer[k] = v.to(self.args.device)\n            label = inputs_outer.get(\"labels\")\n            if self.args.binary_classification:\n                label = (label * 2 - 1).float() # convert from {0, 1} to {-1, 1}\n            preds = self.model_wrapper(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))\n            logits.append(preds.detach())\n            targets.append(label)\n\n        logits = torch.cat(logits, dim=0)\n        targets = torch.cat(targets, dim=0)\n\n        return logits, targets\n\n    def finetune(self, train_dataset, eval_dataset):\n\n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr)\n\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n\n        epoch_count = 0\n\n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False)\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False)\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False)\n\n        for epoch in range(self.args.linear_num_epoch):\n\n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n\n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n\n\n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1)\n                    self.gradient_dtype = gradient[0].dtype\n\n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)\n\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)\n                            param = u@s@v\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param\n\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:\n                            reg += (1/2)*(param ** 2).sum()\n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            reg += torch.norm(param, p = 'nuc')\n\n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item()\n\n            file_exists = True\n\n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd)\n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n\n            if self.args.eval_during_training:\n                # Evaluation logic as provided in the repository content\n                pass\n\n        return avg_loss.item() , epoch_count\n",
    "Experiment Result": "1. **LoRA Specific Arguments (from `ModelArguments` in `run.py`):**\n    - `apply_lora`: Boolean (default `True`), whether to use LoRA for finetuning. If `False`, a full matrix update (`delta`) is used instead of low-rank factors. (Note: LoRA only implemented for RoBERTa models).\n    - `lora_alpha`: Integer (default `1`), initialization scale for one of the low-rank matrices in LoRA.\n    - `lora_r`: Integer (default `8`), inner rank for LoRA matrices (i.e., the `r` in LoRA).\n2. **Linearized Training Specific Arguments (from `DynamicTrainingArguments` in `run.py`):**\n    - `trainer`: String (default `\"linearized\"`), must be set to `\"linearized\"` to use `LinearizedLoraTrainer`.\n    - `linear_freeze_A`: Boolean (default `False`), if `True`, the `Lora_A` matrix of the LoRA layers is frozen during training.\n    - `linear_lr`: Float (default `0.01`), learning rate for the linearized LoRA fine-tuning.\n    - `linear_num_epoch`: Integer (default `30`), number of epochs for training the linearized LoRA model.\n    - `linear_wd`: Float (default `0`), weight decay applied during linearized LoRA training.\n    - `eval_during_training`: Boolean (default `False`), if `True`, evaluation is performed during the training process at each epoch end.\n    - `train_last_layer`: Boolean (default `True`), if `True`, LoRA layers are applied only to `attention.query.weight` and `attention.value.weight` in the last encoder layer (layer 11 for RoBERTa-base). If `False`, LoRA is applied to these weights in all encoder layers.\n    - `binary_classification`: Boolean (default `False`), if `num_classes=2`, converts two softmax logits to a single sigmoid logit for regression-like tasks.\n3. **LoRA Matrix Initialization (from `LinearLoraupdate.__init__` in `src/linearized.py`):**\n    - `Lora_a` matrices are initialized from a normal distribution with `mean = 0` and `std = 1 / (sqrt(lora_r) * target_size[i][1])`.\n    - `Lora_b` matrices are initialized with zeros.\n4. **Weight Decay Implementation (from `LinearizedLoraTrainer.finetune` in `src/linearized.py`):**\n    - If `apply_lora` is `True` and `linear_wd` is greater than a threshold, L2 regularization is applied to `Lora_A` and `Lora_B` matrices: `param = param - linear_wd * linear_lr * param`.\n    - If `apply_lora` is `False` (for full matrix update `delta`) and `linear_wd` is greater than a threshold, a soft-thresholding operator is applied on the singular values of the `delta` matrix for nuclear norm regularization: `s = torch.nn.Threshold(0, 0)(s - linear_lr * linear_wd)`."
}{
    "Title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
    "Main Contributions": "Introduces DP-HyPO, a pioneering framework for adaptive private hyperparameter optimization, effectively bridging the gap between private and non-private HPO. It allows the flexible use of non-DP adaptive HPO methods (e.g., Gaussian process) while avoiding substantial privacy costs due to composition. The framework provides sharp Differential Privacy (DP) guarantees by utilizing the Rényi DP framework, strictly generalizing prior results that required uniform sampling. Empirically, the Gaussian process-based DP-HyPO algorithm demonstrates superior performance compared to its non-adaptive uniform counterpart across diverse real-world datasets and scenarios, and offers practitioners the flexibility to integrate any non-private adaptive HPO methods and allocate privacy budget for adaptivity.",
    "Methodology": "The DP-HyPO framework operates by maintaining an adaptive sampling distribution (π) at each iteration, which incorporates accumulated information. A hyperparameter (λ) is randomly drawn from this distribution, and a base DP algorithm (Mλ(D)) is executed. The key innovation lies in adaptively updating π based on previous outputs, while rigorously ensuring that the density of any posterior sampling distribution is bounded (c ≤ π(j+1)(λ) / π(0)(λ) ≤ C) to control privacy loss. The privacy guarantees are quantified using Rényi Differential Privacy (RDP). To privatize any non-private HPO update rules, a projection technique is proposed, which solves a convex functional programming problem to project the sampling distribution into a space of bounded densities (SC,c). The paper provides an instantiation of DP-HyPO using Gaussian Processes, which constructs a surrogate model for performance measures, assigns scores (e.g., UCB) to hyperparameters, and uses a softmax function to convert these scores into a sampling distribution.",
    "Experimental Setup": "The DP-HyPO framework, specifically its Gaussian process (GP)-based instantiation, was empirically evaluated against a Uniform DP-HyPO baseline (a non-adaptive special case) in both white-box and black-box privacy settings. In the white-box setting, experiments were conducted on MNIST and CIFAR-10 datasets, training standard CNNs with DP-SGD and optimizing learning rate (η) and clipping norm (R). For MNIST, a semi-real simulation cached mean accuracies of models for discretized hyperparameters, adding Gaussian noise to sampled accuracies for evaluation, with a total privacy budget of ε=15, δ=1e-5. For CIFAR-10, the hyperparameter landscape (mean and standard error of accuracy) was generated using BoTorch, and an oracle returned noisy scores, with a total privacy budget of ε=12, δ=1e-5. In the black-box setting, a real-world Federated Learning task on a proprietary dataset was simulated, optimizing learning rates for the central server (AdaGrad) and individual users (SGD). The loss landscape was also generated by BoTorch, and noisy scores were returned by an oracle. Various C values (1.25, 1.33, 1.5) with c=1/C were explored to analyze the impact of adaptivity on performance.",
    "Limitations": "The framework requires that the density of any posterior sampling distribution remains bounded by constants c and C (c ≤ π(j+1)(λ) / π(0)(λ) ≤ C), which may necessitate modifications and projection techniques for existing non-private HPO methods. Practical implementation often requires discretizing the hyperparameter space, which is an approximation inherent in numerical computations. Due to computational resource constraints, the empirical evaluations on MNIST and CIFAR-10, as well as the Federated Learning task, utilized semi-real simulations or pre-generated landscapes from libraries like BoTorch, rather than full training runs for every hyperparameter combination.",
    "Future Research Directions": "Two main future research directions are proposed: first, exploring alternative HPO specifications that are more practically favorable and leveraging more advanced HPO methods to further improve empirical performance. Second, establishing a theoretical utility guarantee for DP-HyPO, potentially by adapting and extending proof methodologies similar to those found in prior work on private selection.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "The paper introduces \"Partitioned Neural Networks,\" a novel, efficient, and scalable method for hyperparameter optimization inspired by the marginal likelihood objective. This approach allows for optimizing various hyperparameters (e.g., neural architecture, data augmentation strategies, regularization, dropout rates) in a single training run without requiring a separate validation set. The core idea involves partitioning both the training data into shards and the neural network model into corresponding parameter partitions. Hyperparameters are optimized using an \"out-of-training-sample\" loss on unseen data shards. Key findings include successfully identifying correct models in toy selection tasks, learning effective affine data augmentations that improve test accuracy (especially in low-data regimes), outperforming traditional validation-set-based optimization and prior marginal likelihood approximations (Augerino, Differentiable Laplace) on various datasets like CIFAR10 and TinyImagenet, and demonstrating scalability to larger architectures where previous methods encounter memory issues. Furthermore, the method is shown to be particularly beneficial in federated learning settings by reducing communication overhead and improving model generalization on non-i.i.d. data distributions.",
    "Methodology": "The method optimizes an approximation of the marginal likelihood, LML(D,ψ), which serves as a lower bound on the true marginal likelihood and is interpreted as a measure of a model's \"learning speed.\" It operates by partitioning the neural network's weights (w) into C partitions (w1, ..., wC) and the training dataset (D) into C corresponding non-overlapping data shards (D1, ..., DC). For training, a k-th subnetwork, w(k)s, is constructed by concatenating the first k trained partitions (w1, ..., wk) with default values (ˆwk+1, ..., ˆwC) for the remaining partitions, ensuring it's trained only on data shards D1:k. Training involves interleaving parameter and hyperparameter updates: model parameters (wk) for a given partition are updated by optimizing the negative log-likelihood on data from D1:k using the corresponding subnetwork w(k)s. Hyperparameters (ψ) are optimized using stochastic gradients derived from the \"out-of-sample\" loss of a subnetwork w(k-1)s on data from the next shard Dk. The weight partitioning scheme typically involves randomly assigning a fixed proportion of weights in each layer to a given partition before training. For federated learning, clients are assigned to data chunks, and only the updated partitions relevant to their chunk are communicated to the server, significantly reducing upload costs.",
    "Experimental Setup": "The research validates Partitioned Neural Networks across a range of tasks and datasets. Tasks include model selection on a synthetic input selection problem (15 informative, 15 spurious features), differentiable input selection using a learnable mask, learning affine image augmentations (shear, translation, scale, rotation), optimizing general feature extractors (first two stages of a Wide ResNet-20), and hyperparameter optimization (augmentations and dropout rates) in federated learning. Datasets utilized are synthetic data, MNIST, RotMNIST, CIFAR10, RotCIFAR10, TinyImagenet, and RotTinyImagenet. For federated learning, non-i.i.d. splits (label-skew with Dirichlet and rotation-skew) of MNIST and CIFAR10 are used. Architectures range from fully-connected MLPs for toy tasks, CNNs for MNIST, Fixup ResNets (8, 14) for CIFAR10, Wide ResNet-20 for feature extraction, to ResNet-50 with GroupNorm(2) for TinyImagenet, and ResNet-9 with GroupNorm for federated CIFAR10, all incorporating learnable dropout where appropriate. Baselines include standard training (no augmentations), Augerino, Differentiable Laplace, Last-layer marginal likelihood, traditional validation set optimization with finetuning, FedAvg, and FedAvg + Augerino. Optimization is performed using Adam or SGD with various learning rates, batch sizes, and weight decay, with 20 augmentation samples for invariance learning. The number of partitions (C) and their data/parameter proportions are varied (e.g., 2-4 chunks, with splits like [80%,10%,10%]). Evaluation metrics include test accuracy, log-likelihood, learned mask evolution, and validation accuracy, along with communication costs for FL.",
    "Limitations": "The method inherently requires an additional forward-backward pass for hyperparameter updates, which, while less costly than existing approaches, contributes to computational overhead. Empirically, partitioned networks may also require more training iterations to converge. Furthermore, the act of partitioning the network parameters can constrain the model's capacity, potentially leading to a slight performance loss compared to a fully optimized, non-partitioned network with ideal hyperparameters. A practical limitation is the introduction of the partitioning strategy (e.g., number of chunks, data/parameter proportions) as an additional hyperparameter that may require tuning for optimal performance on specific tasks. The paper also notes a pathological behavior in a compared baseline, Augerino, where its loss function's unbounded regularization term can cause hyperparameters to diverge if not addressed by careful tuning, clipping, or early stopping.",
    "Future Research Directions": "Potential future research directions include exploring dynamic partitioning strategies where network parameters are partitioned during training, rather than being fixed beforehand. Investigating methods to alleviate the inherent performance loss associated with network partitioning, such as adjusting training rounds or increasing the initial network capacity, is another promising area. The paper suggests exploring alternative partitioning schemes, like node partitioning, which might enable updating multiple parameter partitions with a single batch, thereby reducing computational overhead. Further work could also focus on optimizing hyperparameter updates through techniques like gradient accumulation from different chunks to achieve lower-variance estimates, or by interleaving hyperparameter updates at less frequent intervals. In the context of federated learning, exploring different sequential update schemes for network partitions on the client side could yield further improvements. Overall, the method aims to contribute to reducing the carbon footprint of AI research by minimizing the need for extensive hyperparameter search through repeated training runs.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
    "Main Contributions": "The paper addresses the challenge of hyperparameter optimization (HO) for Lasso-type estimators, which suffer from high memory consumption with automatic differentiation or numerical instability with standard implicit differentiation, especially for non-smooth problems. The main contributions include introducing an efficient implicit differentiation algorithm without matrix inversion, tailored for Lasso-type problems, which scales to high-dimensional data by leveraging solution sparsity. The work demonstrates that forward iterative differentiation of block coordinate descent (BCD) converges linearly to the true gradient once the support is identified. A novel algorithm is proposed that decouples Jacobian computation from regression coefficient computation, avoiding the need to solve potentially ill-conditioned linear systems. Experimental results show that the method outperforms state-of-the-art HO techniques for optimizing held-out error or Stein Unbiased Risk Estimator (SURE).",
    "Methodology": "Hyperparameter optimization is formulated as a bi-level optimization problem where an outer criterion C(ˆβ(λ)) is minimized with respect to hyperparameters λ, subject to ˆβ(λ) being the solution of an inner Lasso-type problem (e.g., Lasso or weighted Lasso with non-smooth ℓ1 penalties). The regularization parameter is re-parametrized as eλ. The core methodology involves efficiently computing the weak Jacobian ˆJ(λ) (gradient of ˆβ with respect to λ). The proposed \"Implicit Forward Iterative Differentiation\" algorithm (Algorithm 2) works in two main steps: first, solve the inner Lasso-type problem to obtain regression coefficients ˆβ and identify its support ˆS using any state-of-the-art solver. Second, compute the Jacobian by applying forward differentiation recursion steps restricted to the identified support. This method leverages the sparsity of Lasso solutions and the fixed-point iteration induced by BCD solvers to avoid direct matrix inversion of a large, potentially ill-conditioned system, making it memory-efficient and numerically stable. The outer problem then uses standard gradient descent with line-search.",
    "Experimental Setup": "The Python code is open-sourced as 'sparse-ho', utilizing Numba for critical BCD loops. For fair comparison, all methods use the same vanilla BCD algorithm (Algorithm 5) for the inner optimization, stopping at a tolerance ϵtol = 10^-5. Gradient-based methods employ a line-search strategy. Lasso initializations are set to λmax - log(10), while weighted Lasso uses a regularized bi-level problem solution for initialization. Competitors include: hypergradient-based methods (Implicit Differentiation, Forward Iterative Differentiation, and the proposed Implicit Forward Iterative Differentiation) and non-hypergradient methods (Grid-search, Random-search, Lattice Hypercube Sampling, and Bayesian Optimization). Experiments evaluate performance on held-out loss (split into train, validation, test) and SURE (Stein Unbiased Risk Estimator) criteria. Real-world datasets used are rcv1 (n=20k, p=20k), 20news (n=11k, p=130k), and finance (n=16k, p=1.6M). Simulated data (n=100, p from 200 to 10k, SNR=3) are used for SURE evaluation, with 50 repetitions. Metrics include objective value convergence, test loss, relative Mean Squared Error (MSE), and computation time.",
    "Limitations": "The theoretical guarantees for the proposed method (Propositions 1 and 2) assume smooth loss functions and do not cover non-convex penalty cases (e.g., MCP), although the algorithm shows proper numerical behavior in such settings. The convergence proof for the Jacobian also relies on the uniqueness of the Lasso solution, which, while typically true, might not hold in pathological cases leading to non-continuous solution paths for ˆβ(λ). The objective function L(λ) for hyperparameter optimization is generally non-convex, meaning gradient descent may converge to local minima rather than a global optimum. The SURE criterion requires prior knowledge of the noise variance.",
    "Future Research Directions": "Future work primarily involves extending the theoretical framework to cover non-convex Lasso formulations, such as the Minimax Concave Penalty (MCP), which were shown to behave properly numerically but lack theoretical guarantees in the current work.",
    "Experiment Code": "class ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            # relative stopping criterion for the computation of the jacobian\n            # and absolute stopping criterion to handle warm start\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    # HACK we only need this for one test, do not rely on it\n    get_only_jac.n_iter = i\n\n    return dbeta\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    \"\"\"\n    Parameters\n    --------------\n    X: array-like, shape (n_samples, n_features)\n        Design matrix.\n    y: ndarray, shape (n_samples,)\n        Observation vector.\n    log_alpha: float or np.array, shape (n_features,)\n        Logarithm of hyperparameter.\n    beta0: ndarray, shape (n_features,)\n        initial value of the regression coefficients\n        beta for warm start\n    dbeta0: ndarray, shape (n_features,)\n        initial value of the jacobian dbeta for warm start\n    max_iter: int\n        number of iterations of the algorithm\n    tol: float\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        primal decrease for optimality and continues until it\n        is smaller than ``tol``\n    compute_jac: bool\n        to compute or not the Jacobian along with the regression\n        coefficients\n    model:  instance of ``sparse_ho.base.BaseModel``\n        A model that follows the sparse_ho API.\n    return_all: bool\n        to store the iterates or not in order to compute the Jacobian in a\n        backward way\n    use_stop_crit: bool\n        use a stopping criterion or do all the iterations\n    gap_freq : int\n        After how many passes on the data the dual gap should be computed\n        to stop the iterations.\n\n    Returns\n    -------\n    mask : ndarray, shape (n_features,)\n        The mask of non-zero coefficients in beta.\n    dense : ndarray, shape (n_nonzeros,)\n        The beta coefficients on the support\n    jac : ndarray, shape (n_nonzeros,) or (n_nonzeros, q)\n        The jacobian restricted to the support. If there are more than\n        one hyperparameter then it has two dimensions.\n    \"\"\"\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    ############################################\n    alpha = np.exp(log_alpha)\n\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    ############################################\n    # warm start for beta\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    ############################################\n    # warm start for dbeta\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    # store the values of the objective\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    ############################################\n    # store the iterates if needed\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if verbose:\n            print(\"%i -st iteration over %i\" % (i, max_iter))\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if i > 1:\n            if verbose:\n                print(\"relative decrease = \", (pobj[-2] - pobj[-1]) / pobj0)\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if verbose:\n                    print(\"dual gap %.2e\" % dual_gap)\n                if verbose:\n                    print(\"gap %.2e\" % dual_gap)\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n        if return_all:\n            list_beta.append(beta.copy())\n        if save_iterates:\n            list_beta.append(beta.copy())\n            list_jac.append(dbeta.copy())\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None",
    "Experiment Result": "The hyperparameter optimization framework uses various experimental settings:\n\n**1. Datasets:**\n- Real-world: `rcv1.binary`, `real-sim`, `news20`, `mnist`, `usps`, `sector_scale`, `gina_agnostic`, `leukemia`. Accessed via `libsvmdata.datasets.fetch_libsvm` or custom `sparse_ho.datasets.real.get_data`.\n- Synthetic: `sklearn.datasets.make_classification`, `sklearn.datasets.make_regression`.\n\n**2. Data Splitting:**\n- `train_test_split`: Data is commonly split into training (`idx_train`), validation (`idx_val`), and sometimes test (`idx_test`) sets (e.g., 50% train, 25% val, 25% test or 1/3 train, 1/3 val, 1/3 test).\n- `sklearn.model_selection.KFold`: Used for cross-validation (`cv=5` folds typically).\n\n**3. Inner Problem Solvers (Lasso-type models):**\n- `celer.Lasso`, `celer.ElasticNet`: For Lasso and ElasticNet problems.\n- `sklearn.linear_model.LogisticRegression` (with `penalty='l1'`, `solver='saga'` or `solver='liblinear'`): For sparse logistic regression.\n- `lightning.classification.LinearSVC`: For Support Vector Machines.\n- Common parameters for inner solvers: `fit_intercept=False`, `warm_start=True`, varying `max_iter` (e.g., 50, 100, 1_000, 10_000, 100_000), and `tol` (e.g., 1e-3, 1e-5, 1e-8, 1e-16, 1e-32).\n\n**4. Hyperparameter (`\text{lambda}`) Range and Initialization:**\n- `\text{alpha_max}`: Computed from the data, typically `np.max(np.abs(X.T @ y)) / n_samples` (for Lasso-type objectives).\n- `\text{alpha_min}`: A fraction of `\text{alpha_max}`, e.g., `1e-2 * \text{alpha_max}`, `\text{alpha_max} / 100`, or `\text{alpha_max} / 10_000`.\n- `\text{alpha0}` (starting point for optimization): Commonly `\text{alpha_max} / 10` or `\text{alpha_max} / 100`.\n- Regularization parameters are re-parameterized as `e^{\text{lambda}}`.\n\n**5. Hyperparameter Optimization Methods (Outer Loop):**\n- **Zero-order methods:**\n  - `grid_search`: Evaluates hyperparameters on a logarithmically spaced grid (`np.geomspace`) or linearly spaced grid (`np.linspace`) with `n_alphas` (e.g., 10, 15, 20, 30, 100) or `max_evals` points.\n  - `random_search` (using `hyperopt`): Samples hyperparameters uniformly on a log scale for `max_evals` (e.g., 30, 50, 100).\n  - `bayesian` (using `hyperopt`): Bayesian optimization for `max_evals` (e.g., 30, 50, 100).\n- **First-order methods:**\n  - `grad_search`: Employs gradient descent-based optimizers, using the `ImplicitForward` (or `Forward`, `Implicit`) algorithm for hypergradient computation.\n    - **Optimizers:** `LineSearch`, `GradientDescent`, `Adam`.\n    - **Optimizer Parameters:** `n_outer` (number of outer iterations, e.g., 10, 25, 30, 75, 100), `tol` (outer loop tolerance, e.g., 1e-5, 1e-7, 1e-8), `step_size` (for `GradientDescent`/`Adam`), `p_grad_norm` (for `GradientDescent`, e.g., 1, 1.5, 1.9), `lr` (for `Adam`, e.g., 0.11), `tol_decrease` ('constant', 'geom', 'exponential').\n\n**6. `ImplicitForward` Algorithm Parameters:**\n- `tol_jac`: Tolerance for Jacobian computation (e.g., 1e-3, 1e-8, 1e-11, 1e-32).\n- `n_iter_jac`: Maximum iterations for Jacobian computation (e.g., 100, 1_000, 5_000, 10_000, 100_000).\n- `max_iter`: Maximum iterations for inner solver, can be inherited from the overall `max_iter`.\n- `use_stop_crit`: Boolean, typically `True` to enable stopping criteria.\n\n**7. Objective/Criterion Functions (`C(\text{beta}(\text{lambda}))`):**\n- `HeldOutMSE`: Mean Squared Error on a held-out validation set.\n- `HeldOutLogistic`: Logistic loss on a held-out validation set.\n- `HeldOutSmoothedHinge`: Smoothed Hinge loss on a held-out validation set.\n- `CrossVal`: Cross-validation wrapper for other criteria (e.g., `HeldOutMSE`).\n- `FiniteDiffMonteCarloSure`: Smoothed Stein Unbiased Risk Estimator."
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL), an approach to efficiently tune hyperparameters for deep (reinforcement) learning systems by exploiting the iterative nature of training. Key contributions include an algorithm that optimizes the learning curve through training curve compression (instead of just final performance averages), a method to learn the compression curve from data, and a data augmentation technique for improved sample-efficiency. BOIL demonstrates superior performance in identifying optimal hyperparameters in minimal wall-clock time compared to existing baselines.",
    "Methodology": "BOIL frames hyperparameter tuning as a cost-sensitive global optimization problem using Bayesian Optimization (BO). It models the black-box evaluation function over the joint space of hyperparameters (x) and training iterations (t) with a Gaussian Process (GP) using a product kernel. The core innovation is to compress the entire learning curve into a single numeric score using a Sigmoid preference function, whose growth and middle-point parameters are learned by maximizing the GP's log marginal likelihood. To enhance sample efficiency and avoid ill-conditioning of the GP covariance matrix, a selective data augmentation technique is employed, sampling points from the learning curve at maximum GP predictive uncertainty, guided by a condition number threshold.",
    "Experimental Setup": "The algorithm's efficiency was demonstrated by tuning hyperparameters for two Deep Reinforcement Learning (DRL) agents and a Convolutional Neural Network (CNN). DRL experiments involved a Dueling DQN agent on the CartPole-v0 environment and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments. CNN tuning was performed on the SVHN and CIFAR10 datasets. All experiments were conducted on an NVIDIA 1080 GTX GPU using the tensorflow-gpu package, with DRL implementations based on OpenAI Baselines. Results were averaged over 20 independent runs. Baselines included Hyperband and Continuous Multi-Task/Fidelity BO (CM-T/F-BO). Square-exponential kernels were used for the GP, and a linear regressor approximated the cost function.",
    "Limitations": "The cost function is approximated by a linear regressor, which might be overly simplistic if the actual cost has a more complex dependency on hyperparameters and iterations. While BOIL addresses the issue of GP covariance matrix ill-conditioning through selective data augmentation, this implies that careful management is required to prevent such problems. The approach relies on the assumption that a Sigmoid function is suitable for compressing learning curves, even with learnable parameters. From a broader perspective, the automation facilitated by BOIL could potentially distance humans from the modeling process, making critical failure detection more challenging.",
    "Future Research Directions": "The approach is general and can be extended beyond machine learning algorithms to other processes exhibiting iterative structures, such as optimizing manufacturing pipelines. Further research could explore the impact of different acquisition functions and kernel choices within the BOIL framework, as these were not extensively compared in the current study. The paper also implies ongoing work in developing fully automated pipelines for ML model training and deployment.",
    "Experiment Code": "from bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import ProductGaussianProcess\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass ProductGaussianProcess(object):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        \n        Euc_dist_t=euclidean_distances(T,T)\n               \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\\\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n\n        return x_max\n\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        \n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    def predict(self,xTest, eval_MSE=True):\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n\n\nclass BOIL(object):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        self.f = func\n    \n        self.X_ori= None\n\n        self.X = None\n        \n        self.Y = None\n               \n        self.Y_ori = None\n        \n        self.T=None\n        self.T_original=None\n        \n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        self.Y_curves=[]\n        \n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        np.random.seed(seed)\n\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))\n\n        self.Y_curves+=y_init_curves\n\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\\\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n\n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1))))\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))))\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:\n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n    def suggest_nextpoint(self):\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        start_opt=time.time()\n\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1:\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
    "Experiment Result": "The method uses a Gaussian Process (GP) with a product kernel, specifically an RBF kernel for both hyperparameters (x) and training iterations (t). The GP model incorporates a `noise_delta` of `5e-4`.\n\nThe core innovation involves compressing the learning curve into a single numeric score using a Sigmoid preference function `1.0/(1+np.exp(-growth*(x-midpoint)))`. The parameters of this function (`midpoint` and `growth`), along with the GP's lengthscales (`lengthscale_x` and `lengthscale_t`), are learned by maximizing the GP's log marginal likelihood. This optimization is performed every `2*D` iterations (where `D` is the dimensionality of the search space).\n\nBounds for the optimized parameters are:\n- `lengthscale_x`: `[0.03, 0.3]`\n- `lengthscale_t`: `[0.3, 0.6]`\n- `midpoint`: `[-2, 3]`\n- `growth`: `[0.5, 2]`\n\nFor selecting the next evaluation point, BOIL employs an acquisition function proportional to `log(utility) - log(mean_cost)`. The `utility` is typically Expected Improvement (`ei_mu_max`), where the incumbent is the maximum of the GP mean function. The `mean_cost` is predicted using a `linear_model.LinearRegression` fitted to the observed `(X, T)` pairs and their associated costs.\n\nTo enhance sample efficiency, a selective data augmentation technique is utilized:\n- After each real observation, virtual observations are generated along the training iteration (time) dimension for the newly sampled hyperparameter `x`.\n- Up to `max_n_augmentation=10` virtual points are sampled.\n- These points are chosen by maximizing the GP's predictive uncertainty (using a `pure_exploration` acquisition function) along the time dimension.\n- The augmentation process stops if the logarithm of the GP covariance matrix condition number `log(cond_num)` exceeds a `threshold_cond=15`, or if the predictive variance is extremely low (`< (noise_delta + 1e-3)`).\n- The cost for these virtual observations is interpolated from the observed cost of the full learning curve.\n\nInitial points (`n_init_points=3` by default) are drawn randomly, and virtual observations are also generated for them."
}{
    "Title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
    "Main Contributions": "The paper demonstrates that prompting and prefix-tuning can enable a pretrained transformer model to be a universal approximator for sequence-to-sequence functions. It shows that much smaller pretrained models than previously thought can achieve this. Key findings include: a single attention head, when prefix-tuned, is sufficient to approximate any continuous function on a hypersphere; any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length; and Jackson-type bounds are provided for the prefix length needed to achieve a desired approximation precision. The work also explores how prefix-tuning can lead to element-wise functions, offering insights into its practical success.",
    "Methodology": "The methodology is primarily theoretical, focusing on mathematical proofs of universal approximation. The core approach involves simplifying the classical attention head into 'core' and 'split' attention heads, demonstrating their ability to approximate continuous functions on hyperspheres (Sm). This restriction is justified by LayerNorm's projection of activations to Sm in modern transformers. Jackson-type bounds are derived for the required prefix length (N) and a concentration parameter (lambda). The paper then shows how a core/split attention head can be represented by a classical attention head through specific constructions of attention (H) and value (WV) matrices and an increase in hidden dimension. For sequence-to-sequence functions, the work extends the single attention head result to element-wise functions and, for general sequence-to-sequence functions, employs a variant of the Kolmogorov-Arnold representation theorem, constructing a transformer with T+2 attention layers to achieve the approximation.",
    "Experimental Setup": "The paper is purely theoretical and does not involve empirical experimental setups, datasets, benchmarks, or traditional validation methods. The 'setup' refers to mathematical constructions and proofs for specific idealized transformer architectures and parameter configurations used to demonstrate universal approximation capabilities.",
    "Limitations": "The theoretical results assume a highly specific pretrained model architecture and parameter values (e.g., for attention and value matrices H and WV) that are unlikely to occur in real-world pretrained transformers. Consequently, it remains an open question what class of functions a *real-world* pretrained transformer can approximate with prefix-tuning. Additionally, the derived Jackson-type bounds suggest that prefix-tuning and prompting may be asymptotically less efficient than training a transformer from scratch, with trainable parameters growing significantly worse (O(epsilon^-10^-14m^-4m^2)) compared to O(epsilon^-m). The construction for general sequence-to-sequence functions is also considered highly unlikely in practice, meaning the current results do not fully explain why prefix-tuning works for complex tasks requiring cross-token interaction.",
    "Future Research Directions": "Future research should focus on deriving tighter Jackson-type bounds for prefix-tuning to determine if its observed asymptotic inefficiency is inherent. A critical direction is to understand the class of functions that *real-world pretrained transformers* can approximate with prefix-tuning, which involves deriving more challenging inverse (Bernstein-type) bounds. Further investigation into the hypothesis that prompting and prefix-tuning work by combining prefix-based element-wise mapping with pretrained cross-element mixing is warranted. Research could also explore methods to guarantee token-wise universal approximation by designing models with specific attention head structures (like those in Lemma 3). Finally, the theoretical implications for designing safety and security measures in transformer-based systems need to be further explored, especially concerning potential undetectable collusion of agents.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "HyperTuning:  Toward Adapting Large Language Models without Back-propagation",
    "Main Contributions": "The paper introduces HyperTuning, a novel approach to large language model (LLM) adaptation that uses a hypermodel to generate task-specific parameters for a fixed downstream model, aiming to overcome the cost and inefficiency of gradient-based fine-tuning. The main contribution is demonstrating that a T5-based hypermodel, HyperT5, can effectively generate soft prefixes or LoRA parameters from few-shot examples for unseen tasks in a single forward pass, without back-propagation. Furthermore, hypermodel-generated parameters are shown to improve performance and accelerate convergence when used as initializations for further parameter-efficient fine-tuning (PEFT).",
    "Methodology": "HyperTuning involves a hypermodel (H) that takes few-shot examples as input and outputs PEFT parameters (φ̂) for a frozen downstream LLM. The hypermodel is trained end-to-end by optimizing its parameters (ξ) to minimize the loss on the downstream model, where gradients are back-propagated through both the downstream model and the hypermodel. A two-stage training procedure is proposed for HyperT5: 1) HyperPretraining, using a Context-Augmented Conditional Language Modeling (CACLM) objective to teach the hypermodel to generate parameters that assist the downstream model in predicting masked tokens based on provided context. 2) Multi-Task Fine-Tuning (MTF) on diverse language tasks to enable generalization to unseen tasks, where the hypermodel learns to generate task-specific PEFT parameters from few-shot examples.",
    "Experimental Setup": "The experiments use HyperT5, a T5-based hypermodel, and a frozen LM-adapted T5 (T5 v1.1 architecture, T5-Large (770M) and T5-XL (3B) models) as the downstream model. Two PEFT methods are explored: prefix tuning (HyperT5-Preﬁx) and LoRA (HyperT5-LoRA). Training is conducted using 1-bit Adam optimizer with a batch size of 256, learning rate of 5e-5, and ZeRO for memory optimization. Max input sequence lengths are 1024 for the hypermodel, 384 for the downstream model, and 1408 for few-shot baselines. Hyperpretraining runs for 100k steps on the C4 dataset. Multi-task fine-tuning is performed for 10,000 steps. Evaluation is conducted on three multi-task datasets: P3 (Public Pool of Prompts) using held-out tasks with multiple-choice accuracy, MetaICL using three train-task splits with ROUGE or Macro-F1, and Super-NaturalInstructions (S-NI v2.5) using definition-only and definition+2positive examples with ROUGE-L. Baselines include T5-MTF, T5-MTF-Few-shot, and PEFT methods (Preﬁx, LoRA) directly tuned. Additional experiments involve joint training of hypermodel and downstream model, and using hypermodel-generated parameters as initializations for PEFT.",
    "Limitations": "The current formulation of hypermodels can only take a small number of examples as input, which limits its performance compared to full parameter-efficient fine-tuning or full fine-tuning. HyperT5 generally underperforms T5 models explicitly trained for few-shot in-context learning with full attention across examples (e.g., T5-MTF-Fewshot), which provides higher performance but is computationally more expensive at inference. The choice of hyperpretraining steps might not be optimal, as seen with HyperT5-LoRA's performance decline after 50k steps. The results presented are primarily a proof-of-concept, and extensive hyperparameter tuning was not performed.",
    "Future Research Directions": "Future research includes generalizing hypertuning to other scenarios, such as training hypermodels to predict gradients or generate parameter updates for larger training sets. The authors suggest potentially replacing gradient-descent-based fine-tuning pipelines with hypertuning for various applications. They also plan to explore training a hypermodel against an already multi-task fine-tuned model for enhanced performance. Further investigation into hyperpretraining and MTF hyperparameters, as well as alternative hypermodel architectures, is suggested to improve results and address current limitations.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Prefix Conditioning Unifies Language and Label Supervision",
    "Main Contributions": "The paper addresses the challenge of unifying image classification and image-caption datasets for visual model pretraining, which often leads to suboptimal zero-shot recognition due to dataset bias. It proposes 'Prefix Conditioning', a novel pre-training strategy that introduces dataset-specific prefix tokens to inform the language encoder of the input data type. This method disentangles dataset biases from visual concepts, allowing the language encoder to learn from both data types while tailoring feature extraction. The approach significantly improves zero-shot recognition performance (over 6% on average) and enhances robustness to image-level distribution shift.",
    "Methodology": "The core methodology is 'Prefix Conditioning'. During pre-training, a dataset-specific prefix token (e.g., PREFIXP for classification data, PREFIXC for caption data) is prepended to the text input token sequence. This prefix token absorbs dataset-specific biases, allowing the main text tokens to focus on learning generalizable visual concepts. The language encoder then extracts dataset-conditioned representations. This input design is independent of the training objectives and can be easily integrated with existing vision-language contrastive learning frameworks like CLIP's symmetric multi-class N-pair loss or UniCL's objective. The number of prefix tokens is set to one. Data sampling can be debiased or equal, with debiased sampling chosen in experiments, though sampling strategy itself did not show significant advantage.",
    "Experimental Setup": "Experiments utilized image-caption datasets (CC3M, CC12M) and image classification datasets (ImageNet21K, ImageNet1K, and ImageNet21K excluding ImageNet1K classes). The vision encoder was a Swin-Tiny transformer and the language encoder was the same as CLIP. Prompting followed CLIP's strategy with 80 templates. Models were optimized with AdamW (learning rate 0.001, weight decay 0.1, batch size 1024) for 15 or 50 epochs with a cosine learning rate schedule. Baselines included CLIP and UniCL trained on single or combined datasets. Evaluation was conducted on zero-shot image classification across ImageNet1K and 11 other diverse datasets, linear-probe evaluation of image encoders, robustness to image-level domain shifts (ImageNet-V2, ImageNet-R, ImageNet-S), robustness to class name shifts (using WordNet synonyms), and image-caption retrieval on CC3M and COCO validation sets. All training used 32 Nvidia Tesla V100 GPUs.",
    "Limitations": "The current scope of the research is limited to unifying image-caption and image-label supervision. Although prefix conditioning helps disentangle dataset biases in language embeddings, the zero-shot performance gain is primarily attributed to improved language representations rather than direct improvements in image representations. While captioning datasets are generally more open-domain, they can still have data biases.",
    "Future Research Directions": "Future research directions include incorporating other forms of supervision beyond image-caption and image-label, such as object detection or semantic segmentation. Another promising direction is to explore combinations of prefix conditioning with knowledge augmentation techniques, like those proposed by K-Lite, as they are considered complementary.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
    "Main Contributions": "The paper investigates the stability-efficiency dilemma in pre-training large-scale GPT models, where increasing batch sizes and learning rates improves efficiency but causes training instability (divergence or slow convergence). It identifies a strong correlation between training instability and extreme gradient variance values, particularly linked to long sequence lengths at the beginning of training. The main contribution is the proposed Sequence Length Warmup (SLW) method, which starts training with short sequences and gradually increases the length to mitigate this instability. SLW enables stable training with significantly larger batch sizes (8x) and learning rates (up to 40x) compared to baselines, leading to substantial reductions in training tokens (up to 2.2x) and wall-clock time (up to 17x) while maintaining or improving accuracy on GPT-2 and GPT-3 models. A lightweight hyperparameter tuning strategy for SLW is also presented.",
    "Methodology": "The methodology involves an in-depth analysis of GPT-2 pre-training tasks using the NVIDIA Megatron-LM pipeline on 128 NVIDIA V100 GPUs. Training instability is quantitatively measured using a 'loss ratio' metric (current loss / minimum previous loss) and correlated with the l1 norm and max element of Adam's variance state. Based on this analysis, the Sequence Length Warmup (SLW) method is proposed. SLW is implemented using a truncation-based approach where a linear pacing function determines the sequence length for each training batch, starting from a short `seqlens` and gradually increasing to the full `seqlene` over a duration `T`. A low-cost hyperparameter tuning strategy for `seqlens` and `T` is developed, relying on validation perplexity fluctuations during the initial training steps. The learning rate decay schedule is modified to be token-wise for fair comparisons.",
    "Experimental Setup": "Experiments are conducted on GPT-2 (117M and 1.5B parameters) and GPT-3 (125M and 1.3B parameters) models. The GPT-2 experiments use a blend of Wikipedia, CC-Stories, RealNews, and OpenWebtext datasets, while GPT-3 experiments primarily use the Pile dataset, with additional CC-Stories and RealNews for the 1.3B model. All experiments utilize 128 NVIDIA V100 GPUs with mixed precision/FP16 training, Adam optimizer, 0.01 weight decay, and gradient clipping at 1.0. Baselines include standard GPT-2/3 training recipes and comparisons with Shortformer and Batch Size Warmup. Evaluation metrics include training loss curves, Adam variance norm/max element, validation perplexity (token-wise and time-wise), and zero-shot/few-shot accuracy on various downstream tasks such as WikiText-103, LAMBADA, HellaSwag, TriviaQA, WebQs, Winogrande, PIQA, ARC Challenge/Easy, ANLI R1/R2/R3, RACE-h, and BoolQ. The implementation is open-sourced in DeepSpeed.",
    "Limitations": "The paper acknowledges that the root cause of the causal relationship between long sequences and training instability is not fully deciphered, making the assumption that shorter sequences are 'simpler' and lead to less 'noisy' gradients. The correlation analysis between instability and gradient variance outliers is empirical and not sufficient to prove a causal relationship, as other factors could also contribute to instability. Due to the high computational cost of pre-training, experiments are based on a single random seed, thus error bars are not reported.",
    "Future Research Directions": "Future research directions include further deciphering the root cause of the causal relationship between long sequences and training instability. The authors also encourage more theoretical and practical studies into the important issue of training instability in large-scale model training. Exploring more complex and adaptive gradient/variance/activation clipping techniques is also suggested as a potential area for future work.",
    "Experiment Code": "@dataclass\nclass GPTDatasetConfig(BlendedMegatronDatasetConfig):\n    random_seed: int\n    sequence_length: int\n\n    blend: Optional[Tuple[List[str], Optional[List[float]]]] = None\n\n    blend_per_split: Optional[List[Optional[Tuple[List[str], Optional[List[float]]]]]] = None\n\n    multiple_validation_sets: Optional[bool] = None\n\n    full_validation: Optional[bool] = None\n\n    split: Optional[str] = None\n\n    split_matrix: Optional[List[Tuple[float, float]]] = field(init=False, default=None)\n\n    num_dataset_builder_threads: int = 1\n\n    path_to_cache: Optional[str] = None\n\n    mmap_bin_files: bool = True\n\n    mock: bool = field(init=False, default=False)\n\n    tokenizer: Optional[MegatronTokenizerBase] = None\n\n    mid_level_dataset_surplus: float = 0.005\n\n    allow_ambiguous_pad_tokens: Optional[bool] = False\n\n    reset_position_ids: Optional[bool] = None\n\n    reset_attention_mask: Optional[bool] = None\n\n    eod_mask_loss: Optional[bool] = None\n\n    create_attention_mask: bool = True\n\n    drop_last_partial_validation_sequence: bool = True\n\n    add_extra_token_to_sequence: bool = True\n\n    object_storage_cache_path: Optional[str] = None\n\n\nclass GPTDataset(MegatronDataset):\n    def __getitem__(self, idx: Optional[int]) -> Dict[str, torch.Tensor]:\n        if idx is None:\n            text, _ = self._query_document_sample_shuffle_indices(0)\n        else:\n            text, _ = self._query_document_sample_shuffle_indices(idx)\n\n        text = torch.from_numpy(text).long()\n        if self.config.add_extra_token_to_sequence:\n            tokens = text[:-1].contiguous()\n            labels = text[1:].contiguous()\n        else:\n            tokens = text\n            labels = torch.roll(text, shifts=-1, dims=0)\n            labels[-1] = self._pad_token_id\n\n        attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(\n            tokens,\n            self.config.tokenizer.eod,\n            self.config.reset_position_ids,\n            self.config.reset_attention_mask,\n            self.config.eod_mask_loss,\n            self.config.create_attention_mask,\n        )\n\n        loss_mask[labels == self._pad_token_id] = 0.0\n        tokens[tokens == self._pad_token_id] = 0\n        labels[labels == self._pad_token_id] = 0\n\n        if idx is None:\n            loss_mask = torch.zeros_like(loss_mask)\n\n        if self.config.create_attention_mask:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"attention_mask\": attention_mask,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\n        else:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\n\ndef train_valid_test_datasets_provider(train_val_test_num_samples):\n    args = get_args()\n\n    print_rank_0('> building train, validation, and test datasets '\n                 'for GPT ...')\n    train_ds, _, test_ds = BlendedMegatronDatasetBuilder(\n        GPTDataset,\n        train_val_test_num_samples,\n        lambda: True,\n        GPTDatasetConfig(\n            blend=get_blend_from_list(args.data_path),\n            split=args.split,\n            random_seed=args.seed,\n            sequence_length=args.seq_length,\n            path_to_cache=args.data_cache_path,\n            return_document_ids=False,\n            mid_level_dataset_surplus=args.mid_level_dataset_surplus,\n        )\n    ).build()\n    print_rank_0(\"> finished creating finetuning GPT datasets ...\")\n\n    _, valid_ds, _ = BlendedMegatronDatasetBuilder(\n        GPTDataset,\n        train_val_test_num_samples,\n        lambda: True,\n        GPTDatasetConfig(\n            blend=get_blend_from_list(args.data_path2),\n            split=\"98,2,0\",\n            random_seed=1234,\n            sequence_length=2048,\n            path_to_cache=args.data_cache_path,\n            return_document_ids=False,\n            mid_level_dataset_surplus=args.mid_level_dataset_surplus,\n        )\n    ).build()\n    print_rank_0(\"> finished creating pretrained GPT datasets ...\")\n\n    return train_ds, valid_ds, test_ds\n\ndef get_batch(data_iterator):\n    args = get_args()\n    tokenizer = get_tokenizer()\n\n    keys = ['text']\n    datatype = torch.int64\n\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    data_b = mpu.broadcast_data(keys, data, datatype)\n\n    tokens_ = data_b['text'].long()\n    labels = tokens_[:, 1:].contiguous()\n    tokens = tokens_[:, :-1].contiguous()\n\n    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(\n        tokens,\n        tokenizer.eod,\n        args.reset_position_ids,\n        args.reset_attention_mask,\n        args.eod_mask_loss)\n\n    return tokens, labels, loss_mask, attention_mask, position_ids\n\ndef forward_step(data_iterator, model):\n    timers = get_timers()\n\n    timers('batch-generator').start()\n    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(\n        data_iterator)\n    timers('batch-generator').stop()\n\n    output_tensor = model(tokens, position_ids, attention_mask,\n                          labels=labels)\n\n    return output_tensor, partial(loss_func, loss_mask)",
    "Experiment Result": "The methodology involves an in-depth analysis of GPT-2 pre-training tasks using the NVIDIA Megatron-LM pipeline. The provided repository content outlines a standard GPT training framework within Megatron-LM, particularly in 'examples/academic_paper_scripts/detxoify_lm/finetune_gpt.py' and 'megatron/core/datasets/gpt_dataset.py'.\n\nExperimental settings that are **directly related** to the method description:\n-   **GPT-2 Pre-training Tasks**: The code from 'finetune_gpt.py' demonstrates setting up a GPT model for pre-training/fine-tuning by defining a `model_provider` and utilizing the `pretrain` function, which is central to the Megatron-LM pipeline. This implies the general context of pre-training GPT-like models.\n-   **NVIDIA Megatron-LM Pipeline**: The entire extracted code uses Megatron-LM's `GPTModel`, `get_args`, `get_tokenizer`, `mpu`, `BlendedMegatronDatasetBuilder`, and `pretrain` components, confirming the use of the specified pipeline.\n\nSpecific implementations of the following experimental details were **not directly found** in the provided repository content:\n-   **Sequence Length Warmup (SLW) Method**: The method describes a 'truncation-based approach where a linear pacing function determines the sequence length for each training batch'. While `GPTDatasetConfig` includes `sequence_length` (passed via `args.seq_length` in `train_valid_test_datasets_provider`) and `get_batch` dynamically truncates input to a fixed length (`tokens_[:, :-1]`), there is no explicit linear pacing function or dynamic adjustment of this `sequence_length` per batch during a training run visible in the provided code. The code currently reflects a static `sequence_length` configuration.\n-   **Training Instability Measurement**: The 'loss ratio' metric (current loss / minimum previous loss) and its correlation with the l1 norm and max element of Adam's variance state are not explicitly implemented or measured in the provided code snippets.\n-   **Low-cost Hyperparameter Tuning Strategy**: The hyperparameter tuning for `seqlens` and `T` based on validation perplexity fluctuations during initial training steps is not present.\n-   **Token-wise Learning Rate Decay Schedule**: A modified learning rate decay schedule to be token-wise is not explicitly shown."
}{
    "Title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
    "Main Contributions": "The paper addresses the stability-efficiency dilemma in pre-training large-scale autoregressive language models like GPT, where increasing batch sizes and learning rates improves efficiency but causes instability. The main contributions include an in-depth analysis correlating training instability with extreme gradient variance values, particularly linked to long sequence lengths at the beginning of training. Based on this, the authors propose Sequence Length Warmup (SLW), a method that starts training with short sequences and gradually increases length, effectively resolving the stability-efficiency dilemma. SLW is shown to enable stable training with significantly larger batch sizes (8x) and learning rates (4x for GPT-2, 40x for GPT-3), leading to substantial reductions in required training tokens (up to 2.2x) and wall-clock time (up to 3.7x for GPT-2; 10x data, 17x time for GPT-3 125M while retaining 99% accuracy). A lightweight hyperparameter tuning strategy for SLW is also presented, and the method's implementation is open-sourced in DeepSpeed.",
    "Methodology": "The methodology involves an in-depth analysis of GPT-2 pre-training to understand instability, measuring training loss and Adam optimizer's variance norm/max element, and correlating them with a 'loss ratio' metric. The proposed Sequence Length Warmup (SLW) method truncates full-length input sequences to shorter lengths determined by a step-wise linear pacing function: `seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`. `seqlens` is the starting length, `seqlene` is the full length, `T` is the total duration, and `t` is the current step. A low-cost tuning strategy is developed for `seqlens` and `T`, involving validation perplexity monitoring during early training steps. Training used mixed precision (FP16), Adam optimizer, 0.01 weight decay, and gradient clipping at 1.0. Learning rate decay schedules were adjusted to be token-wise for fair comparisons.",
    "Experimental Setup": "Experiments replicated GPT-2 models (117M and 1.5B parameters) and GPT-3 models (125M and 1.3B parameters). Hardware consisted of 128 NVIDIA V100 GPUs across 16 nodes, interconnected with NVLink 2.0 and 100 Gigabit InfiniBand. Datasets included Wikipedia, CC-Stories, RealNews, and OpenWebtext for GPT-2, and the Pile public dataset (with additional CC-Stories and RealNews for GPT-3 1.3B) for GPT-3. Baseline training parameters varied, with batch sizes up to 4K and learning rates up to 6e-4 (GPT-2), or 256 and 6e-4 (GPT-3). Aggressive scenarios for GPT-3 125M used 8x batch size (2K) and up to 40x learning rate. Evaluation metrics included training loss, validation perplexity (PPL), and zero-shot accuracy on WikiText-103, LAMBADA, and 11 GPT-3 tasks (e.g., HellaSwag, TriviaQA, PIQA). Few-shot evaluation was performed for GPT-3 1.3B. Efficiency was measured by required training tokens and wall-clock time.",
    "Limitations": "The paper identifies that the root cause of the causal relationship between long sequences and training instability is not fully deciphered, with an assumption that shorter sequences are simpler and generate less noisy gradients. The work provides correlation analysis between instability and gradient variance outliers but acknowledges it is insufficient to prove a causal relationship, as other factors could contribute to instability. Due to the high cost of pre-training, most experimental results are based on a single random seed.",
    "Future Research Directions": "Future research directions include further deciphering the root cause of the causal relationship between long sequences and training instability. The authors also hope their work inspires more studies into understanding other factors contributing to training instability in large-scale model training, both in theory and practice. Investigating more complex and adaptive gradient/variance/activation clipping techniques is also implied as a potential alternative, although the proposed method offers a simpler solution.",
    "Experiment Code": "def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):\n    print_rank_0('building GPT model ...')\n    if config is None:\n        if args.yaml_cfg is not None:\n            config = core_transformer_config_from_yaml(args, \"language_model\")\n        else:\n            config = core_transformer_config_from_args(args)\n    if args.use_legacy_models:\n        model = megatron.legacy.model.GPTModel(\n            config,\n            num_tokentypes=0,\n            parallel_output=True,\n            pre_process=pre_process,\n            post_process=post_process,\n        )\n    else:  # using core models\n        if args.spec is not None:\n            transformer_layer_spec = import_module(args.spec)\n        else:\n            use_te = args.transformer_impl == \"transformer_engine\"\n\n            if args.num_experts:\n                # Define the decoder block spec\n                transformer_layer_spec = get_gpt_decoder_block_spec(\n                    config,\n                    use_transformer_engine=use_te,\n                    normalization=args.normalization,\n                    qk_l2_norm=args.qk_l2_norm,\n                    vp_stage=vp_stage,\n                )\n            elif args.heterogeneous_layers_config_path is not None:\n                transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)\n            else:\n                # Define the decoder layer spec\n                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)\n        mtp_block_spec = None\n        if args.mtp_num_layers is not None:\n            if (\n                hasattr(transformer_layer_spec, 'layer_specs')\n                and len(transformer_layer_spec.layer_specs) == 0\n            ):\n                # Get the decoder layer spec explicitly if no decoder layer in the last stage,\n                # Only happens with block spec (TransformerBlockSubmodules) when using MoE.\n                transformer_layer_spec_for_mtp = _get_transformer_layer_spec(use_te, config)\n            else:\n                transformer_layer_spec_for_mtp = transformer_layer_spec\n            mtp_block_spec = get_gpt_mtp_block_spec(\n                config,\n                transformer_layer_spec_for_mtp,\n                use_transformer_engine=use_te,\n                vp_stage=vp_stage,\n            )\n\n        model = GPTModel(\n            config=config,\n            transformer_layer_spec=transformer_layer_spec,\n            vocab_size=args.padded_vocab_size,\n            max_sequence_length=args.max_position_embeddings,\n            pre_process=pre_process,\n            post_process=post_process,\n            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n            parallel_output=True,\n            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n            position_embedding_type=args.position_embedding_type,\n            rotary_percent=args.rotary_percent,\n            rotary_base=args.rotary_base,\n            rope_scaling=args.use_rope_scaling,\n            mtp_block_spec=mtp_block_spec,\n            vp_stage=vp_stage,\n        )\n\n    return model\n\ndef _get_transformer_layer_spec(use_te, config):\n    \"\"\"Get transformer layer specification based on configuration.\n\n    Args:\n        use_te (bool): Whether to use Transformer Engine\n        args: Training arguments\n        config: Model configuration\n\n    Returns:\n        transformer_layer_spec: The transformer layer specification\n    \"\"\"\n    args = get_args()\n    if use_te:\n        return get_gpt_layer_with_transformer_engine_spec(\n            args.num_experts,\n            args.moe_grouped_gemm,\n            args.qk_layernorm,\n            args.multi_latent_attention,\n            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,\n            qk_l2_norm=args.qk_l2_norm,\n            use_kitchen=config.use_kitchen,\n        )\n    else:\n        return get_gpt_layer_local_spec(\n            args.num_experts,\n            args.moe_grouped_gemm,\n            args.qk_layernorm,\n            args.multi_latent_attention,\n            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,\n            normalization=args.normalization,\n            use_kitchen=config.use_kitchen,\n        )",
    "Experiment Result": "The provided repository content does not contain a direct implementation of the proposed Sequence Length Warmup (SLW) method (`seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`) or the instability analysis/loss ratio metric. However, the repository supports general GPT model training, which is the context for the method.\n\nKey experimental settings related to GPT training that are configurable via command-line arguments (`args`) in the Megatron framework (as reflected in `gpt_builders.py` and `finetune_gpt.py`) include:\n-   **Mixed Precision (FP16)**: Configurable through arguments like `args.fp16_lm_cross_entropy` during model building, allowing for FP16 training.\n-   **Adam Optimizer**: Explicitly used in some training loops (e.g., `examples/run_simple_mcore_train_loop.py`) and implicitly configured for general Megatron `pretrain` function usage (as seen in `examples/academic_paper_scripts/detxoify_lm/finetune_gpt.py`).\n-   **Weight Decay (0.01)**: A common optimizer hyperparameter, typically set via command-line arguments such as `args.weight_decay`.\n-   **Gradient Clipping (1.0)**: A common training stability technique, typically set via command-line arguments such as `args.gradient_clip_val`.\n-   **Learning Rate Decay Schedules**: The method mentions these being adjusted to be 'token-wise', which implies configuration through relevant `args` (e.g., `args.lr_decay_tokens`).\n-   **Sequence Length**: While a `sequence_length` is a configurable parameter (e.g., `args.seq_length`, `args.max_position_embeddings`), the dynamic adjustment logic of SLW is not present in the provided code snippets."
}

# Instructions
Following the instructions below, please provide an evaluation of the new hypothesis.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new hypothesis has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new hypothesis on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new hypothesis is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new hypothesis on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'The related papers that work in the spectral/low–rank PEFT design space (Spectral-Adapter, SVFT, AdaLoRA, SHiRA, ReFT, etc.) either (i) choose a single global rank per layer, (ii) at most adapt this rank across layers (AdaLoRA / CBSP), or (iii) gate the whole layer uniformly for the full token sequence (BHSP / CBSP). None of them (a) decide the active spectral directions on a token-by-token basis, (b) couple this decision to a hard, sentence-level FLOP budget that is enforced by a differentiable projection, or (c) show how to do both with near-zero controller cost via HardConcrete gates and sparse matmuls. Hence the proposal to expose top-R singular vectors and let a light MLP + HardConcrete gate select a variable per-token rank while globally projecting the expected token-ranks onto a simplex constitutes a new control-granularity (token-level) and a new optimisation constraint (strict worst-case compute cap) that are absent from current spectral or budgeted PEFT methods.', 'novelty_score': 8, 'significance_reason': 'If successful, TABS would let practitioners keep strict latency / energy envelopes independent of input length while still reaping the accuracy benefits of high-rank adapters on only the “hard” tokens. This directly tackles a pressing deployment barrier for on-device or real-time NLP that existing PEFT methods cannot handle (they violate worst-case latency when the sentence is long or token complexity spikes). Academically, it opens a fresh axis (token-wise adaptive capacity under global resource constraints) that could influence future work in dynamic inference and fine-tuning. Societally, reducing tail latency and FLOPs on edge devices lowers energy consumption and enables broader access. The downside is added implementation complexity and reliance on sparse kernels which may limit immediate adoption, but the potential impact on both efficiency research and practical deployment is substantial.', 'significance_score': 8}
