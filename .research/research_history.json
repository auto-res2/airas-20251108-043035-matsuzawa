{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "parameter-efficient fine-tuning",
    "adapter fine-tuning",
    "LoRA fine-tuning",
    "hyperparameter optimization",
    "prefix tuning"
  ],
  "research_study_list": [
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces the \"Spectral Adapter\" framework, a novel Parameter-Efficient Fine-Tuning (PEFT) method that leverages the spectral information of pretrained weight matrices. The main contributions include: 1) Proposing two spectral adaptation mechanisms, additive (Spectral AdapterA) and rotational (Spectral AdapterR), which fine-tune the top singular vectors obtained via Singular Value Decomposition (SVD) of pretrained weights. 2) Providing theoretical analysis showing that Spectral AdapterA offers twice the rank capacity of LoRA for a fixed trainable parameter budget. 3) Demonstrating through extensive experiments that Spectral Adapters achieve superior parameter efficiency and tuning performance compared to state-of-the-art PEFT methods across large language models (LLMs) and diffusion models. 4) Showing that Spectral AdapterA naturally addresses multi-adapter fusion problems by distributing concept tunings along different spectral spaces. 5) Highlighting Spectral AdapterR's ability to offer finer-grained parameter choices and improved efficiency. The method maintains practicality with negligible runtime and storage overhead.",
        "methodology": "The methodology centers on incorporating spectral information into fine-tuning by first performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) and then adaptively tuning the top-r columns of the singular vector matrices (U and V). Two primary mechanisms are explored: 1) Spectral AdapterA, an additive approach, modifies the top-r columns of U and V with trainable matrices (AU, AV) initialized to zero. This mechanism resembles LoRA. 2) Spectral AdapterR, a rotational approach, applies trainable orthogonal matrices (RU, RV) to rotate the top-r columns of U and V. Orthogonality is efficiently maintained using Cayley parameterization, ensuring an exact rotation operation and preserving the SVD structure for subsequent fine-tunings. Theoretical insights support this choice by showing increased rank capacity for additive tuning and improved neuron alignment with top spectral directions.",
        "experimental_setup": "Experiments were conducted on Large Language Models (LLMs) and Diffusion Models. For LLMs, Llama3 8B was fine-tuned on the Orca Math dataset and evaluated on GSM8K, while DeBERTaV3-base and Mistral 7B were fine-tuned on GLUE benchmarks and GSM8K, respectively. For diffusion models, the Chilloutmix model was fine-tuned on various custom concepts (animals, toys, vase, chair) and multi-character generation tasks. Baselines included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg. Evaluation metrics included training loss, validation scores (accuracy for LLMs), and qualitative visual generation results, complemented by quantitative alignment scores (CLIP embedding cosine similarity with reference images and prompt texts) for diffusion models. All experiments were performed on NVIDIA RTX A6000 GPUs, with hyperparameters for baselines following their original reports and Spectral Adapter hyperparameters tuned or specified in the appendix.",
        "limitations": "The current work has several limitations. Firstly, the choice of exclusively tuning the top spectral space, while theoretically and empirically supported under simple settings, requires further in-depth investigation to understand the broader role of spectral information when tuning different columns of singular vector matrices. Secondly, there is a need for further study on fine-tuning the spectral representation of specific model components, such as only the attention layer, in large models. Lastly, the computational cost of the Singular Value Decomposition (SVD) procedure increases with larger models, necessitating faster SVD methods to maintain efficiency as model sizes grow.",
        "future_research_directions": "Future research could explore several avenues. One key direction is to investigate fine-tuning the spectral representation of different components within large models, for instance, focusing solely on the attention layer. Another promising area involves dynamically combining spectral adaptation with other PEFT methods, such as AdaLoRA, to potentially achieve more adaptive parameter allocation. A deeper investigation into the effects of tuning various columns of singular vector matrices, beyond just the top spectral space, is crucial for a comprehensive understanding of spectral information's role in fine-tuning. Furthermore, developing and integrating faster Singular Value Decomposition (SVD) methods will be beneficial to efficiently handle the increasing scale of future large models.",
        "experimental_code": "class SpectralLinearLayer_OFT(nn.Module):    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):        rank = 8        super().__init__()        self.name = name        if original_module.__class__.__name__ == 'Conv2d':            self.conv = True            in_channels, out_channels = original_module.in_channels, original_module.out_channels        else:            self.conv = False            in_channels, out_channels = original_module.in_features, original_module.out_features        W = original_module.weight.data.view(out_channels, in_channels)        U, S, V = torch.svd(W)        self.U = torch.nn.Parameter(U, requires_grad=False)        self.S = torch.nn.Parameter(S, requires_grad=False)        self.V = torch.nn.Parameter(V, requires_grad=False)        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)        original_module.forward = self.forward        self.original_module = original_module        self.top = top        self.idx = idx        assert revised_r>0        self.rank = revised_r    def cayley(self, data: torch.Tensor) -> torch.Tensor:        r, _ = data.shape        skew = 0.5 * (data - data.T)        I = torch.eye(r, device=data.device)        Q = torch.mm(I - skew, torch.inverse(I + skew))        return Q    def forward(self, hidden_states):        if self.top:            pad_U = self.U.clone()            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)            pad_S = self.S.clone()            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C            pad_V = self.V.clone()            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)        else:            raise Exception('')        pad_W = pad_U@pad_S.diag()@pad_V.T        if self.conv :            raise Exception('')        else:            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)",
        "experimental_info": "The method incorporates spectral information into fine-tuning by first performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) of target layers. It then adaptively tunes the top-r columns of the singular vector matrices (U and V) and scales the corresponding singular values (S).\n\nKey mechanisms and settings:\n- **SVD Implementation**: SVD is applied directly on the weight data of the original linear/convolutional modules (`torch.svd(W)`).\n- **Adapted Components**: The top-`r` columns of `U` and `V` (singular vector matrices) are adapted, and the corresponding `r` singular values in `S` are scaled.\n- **Adaptation Mechanism (Spectral AdapterR-like with S-scaling)**:\n  - For `U` and `V`, rotational transformations are applied using trainable matrices (`self.spectral_A` and `self.spectral_B`), which are parameterized by the Cayley transformation (`self.cayley`) to maintain orthogonality.\n  - For `S`, the singular values are scaled element-wise by a trainable vector (`self.spectral_C`).\n  - The adapted `U`, `S`, `V` are then recombined (`pad_U@pad_S.diag()@pad_V.T`) to form the new weight matrix.\n- **Initialization**: The trainable parameters `self.spectral_A` and `self.spectral_B` (for U and V) are initialized to `torch.zeros(r, r)`, while `self.spectral_C` (for S) is initialized to `torch.ones(r)`. This ensures that the initial modifications to the pretrained weights are minimal (identity transformation for rotations and no scaling for singular values).\n- **Target Layers**: The adaptation is applied to `Linear` layers within `CLIPEncoderLayer` or `CLIPAttention` in the text encoder, and `Linear` or `Conv2d` layers (with kernel_size=(1,1)) within `Transformer2DModel` or `Attention` blocks in the UNet.\n- **Rank (`r`)**: The number of top singular vector columns and singular values to adapt is determined by a configurable `rank` parameter (referred to as `revised_r` in the code, which takes the value of `rank` from `lora_cfg` in the configuration file, e.g., `unet_cfg['lora_cfg']['rank']`).\n- **Scaling Factor (`alpha`)**: An `alpha` parameter is used during the merging process (`merge_spectraloft_into_weight`) to scale the magnitude of the learned spectral modifications. Specifically, `alpha` scales the deviation from the identity matrix for the Cayley-transformed rotation matrices and the deviation from `torch.ones` for the singular value scaling vector.\n- **Loss Function**: Mean Squared Error (MSE) loss is used (`F.mse_loss`), optionally combined with an attention regularization term (`self.attn_reg_weight * (loss_subject + loss_adjective)`).\n- **Optimizer and Scheduler**: The `AdamW` optimizer is used with a `linear` learning rate scheduler.\n- **Training Data**: `LoraDataset` is used for fine-tuning.\n- **Base Model**: Stable Diffusion is used as the base model, loaded from a `pretrained_path`."
      }
    },
    {
      "title": "Parameter-Efficient Fine-Tuning Design Spaces",
      "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
      "full_text": "PARAMETER -EFFICIENT FINE -TUNING DESIGN SPACES Jiaao Chen†∗, Aston Zhang‡, Xingjian Shi‡, Mu Li‡, Alex Smola‡, Diyi Yang⋄ †Georgia Institute of Technology,‡Amazon Web Services, ⋄Stanford University ABSTRACT Parameter-efﬁcient ﬁne-tuning aims to achieve performance comparable to ﬁne-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, preﬁx tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether cer- tain design patterns exist for parameter-efﬁcient ﬁne-tuning. Thus, we present a parameter-efﬁcient ﬁne-tuning design paradigm and discover design patterns that are applicable to different experi- mental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize tuning structures and tuning strate- gies. Speciﬁcally, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively reﬁne the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uni- formly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efﬁcient ﬁne-tuning methods. We show experimentally that these methods consistently and signiﬁcantly outperform investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different tasks in natural language processing1. 1 Introduction Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through ﬁne-tuning on task-speciﬁc labeled data [Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019, Joshi et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022]. However, ﬁne-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa [Liu et al., 2019] and 175B parameters for GPT- 3 [Brown et al., 2020]). This makes it difﬁcult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks. To adapt general knowledge in pretrained models to speciﬁc down-stream tasks in a more parameter-efﬁcient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen [Houlsby et al., 2019a, Pfeiffer et al., 2021, Li and Liang, 2021, Brown et al., 2020, Lester et al., 2021a, Schick and Sch ¨utze, 2021, Ziems et al., 2022]. Adapter tuning [Houlsby et al., 2019a] is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the ﬁne-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts [Brown et al., 2020], preﬁx tuning [Li and Liang, 2021] and prompt tuning [Lester et al., 2021b] prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when ﬁne-tuning on downstream tasks. BitFit [Zaken et al., 2021] updates the bias terms in pretrained models while freezing the remaining parameters. LoRA [Hu et al., 2021] decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, He et al. [2022] proposed a uniﬁed view of these existing strategies and ∗Work done during an internship at Amazon Web Services. Correspondence to Jiaao Chen<jiaaochen@gatech.edu> and Aston Zhang <astonz@amazon.com>. 1Code is available at: https://github.com/amazon-science/peft-design-spaces . arXiv:2301.01821v1  [cs.CL]  4 Jan 2023P P P L P L A B L A B L… Layer Grouping P L Strategy Assignment Trainable Parameter Allocation Tunable Groups p ⇥ p Figure 1: A parameter-efﬁcient ﬁne-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be ﬁnetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Preﬁx, BitFit, and LoRA, to groups). illustrated differences and connections among them. Like its antecedents, the resulting method is stillequally assigned to different pretrained layers. Despite being effective, most parameter-efﬁcient ﬁne-tuning strategies have been developed via manual design pro- cesses, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where [Mao et al., 2022], as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameter- efﬁcient ﬁne-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings. Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize both tuning structures and strategies. More con- cretely, any of these design spaces is characterized by four major components as shown in Figure 1: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained parameter-efﬁcient ﬁne-tuning design space, we progressively reﬁne the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efﬁcient ﬁne-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We fur- ther introduce new parameter-efﬁcient ﬁne-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efﬁcient ﬁne-tuning strategies. Al- though we use T5 [Raffel et al., 2020] and classiﬁcation tasks as the working example, we ﬁnd that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa [Liu et al., 2019], BART [Lewis et al., 2020b], and XLNet [Yang et al., 2019]) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets). Our contributions can be summarized as follows: (i) We introduce parameter-efﬁcient ﬁne-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efﬁcient ﬁne-tuning via comprehen- sive experiments. (iii) Our discovered design patterns lead to parameter-efﬁcient ﬁne-tuning methods, consistently outperforming investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different NLP tasks. 22 Related Work Our work is closely related to and built upon the research about the network design spaces and parameter-efﬁcient ﬁne-tuning. We discuss the connections and differences below. Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances [Radosavovic et al., 2019], such as the use of deeper architectures or residuals. Recently, there have been works [Radosavovic et al., 2020, You et al., 2020, Radosavovic et al., 2019] performing at the design space level to discover new design principles for convolutional neural networks [Radosavovic et al., 2020] and graph neural networks [You et al., 2020]. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efﬁcient ﬁne-tuning, with the goal of discovering design patterns that are applicable to different experimental settings. Parameter-Efﬁcient Fine-Tuning for NLP As pretrained models grow in size, storing ﬁne-tuned models becomes exceedingly expensive, and ﬁne-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to ﬁnding parameter-efﬁcient alternatives for adapting large-scale pre- trained models with reduced memory and storage costs. Houlsby et al. [2019b] proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains [Stick- land and Murray, 2019, Pfeiffer et al., 2020, Rebufﬁ et al., 2017, Lin et al., 2020]. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters [Zhao et al., 2020, Guo et al., 2020, Mallya et al., 2018, Radiya-Dixit and Wang, 2020, Sung et al., 2021, Zaken et al., 2021]. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kro- necker product [Zhang et al., 2021a] and injecting trainable rank decomposition matrices into each layer [Hu et al., 2021, Karimi Mahabadi et al., 2021]. Li and Liang [2021] introduced preﬁx-tuning that prepends a set of preﬁxes to autoregressive language models or prepends preﬁxes for both encoders and decoders. The preﬁx parameters are updated while the pretrained parameters are ﬁxed. Lester et al. [2021a] proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts [Deng et al., 2022, Zhong et al., 2022]. Bari et al. [2022] proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, He et al. [2022] and Ding et al. [2022] proposed a uniﬁed view of the existing parameter-efﬁcient ﬁne-tuning strategies and illustrated the difference and connections among them. Mao et al. [2022] also introduced a uniﬁed framework to combine different methods through mixture- of-experts. In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efﬁcient ﬁne-tuning. This could provide a more comprehensive view of parameter-efﬁcient ﬁne-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively reﬁne design spaces, we discover design patterns for parameter-efﬁcient ﬁne-tuning. 3 Components of Design Spaces When deﬁning design spaces of parameter-efﬁcient ﬁne-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efﬁcient ﬁne-tuning research. Concretely, in our work, the parameter-efﬁcient ﬁne-tuning design spaces are formed by a representative set of choices in parameter-efﬁcient ﬁne-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space exam- ple in Figure 1, we describe these four design components in detail below and will explore their design choices in Section 4. Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, Jawahar et al. [2019] found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efﬁcient ﬁne-tuning strategies. For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the ﬁne-tuning process. 3Trainable Parameter Allocation In parameter-efﬁcient ﬁne-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predeﬁned number of trainable parameters to layers. Tunable Groups Zaken et al. [2021] found that not all the parameters need to be tuned during ﬁne-tuning on the downstream tasks. For instance, BitFit [Zaken et al., 2021] only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efﬁcient ﬁne-tuning to attain better performances. Strategy Assignment In order to improve the parameter efﬁciency, different sets of strategies [Li and Liang, 2021, Lester et al., 2021a, Houlsby et al., 2019a, Hu et al., 2021] have been proposed where only a small number of (ex- tra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to speciﬁc down-stream tasks. Inspired by effectiveness of offering architectural ﬂexibility [Zhang et al., 2021a,b], we hypothesize that different groups might beneﬁt from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies Afor assignment, for any group Gi, assign a subset Ui ⊂A to each layer in Gi. 4 Discovering Design Patterns Building on these four different design components of PEFT design spaces, we will start from a relatively uncon- strained design space and progressively discover the design patterns. 4.1 Design Space Experimental Setup We ﬁrst describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models. Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widely- used GLUE benchmark [Wang et al., 2018]. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Sim- ilarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Rec- ognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks. Pretrained Backbone Models and Model Settings We use T5-base/3b [Raffel et al., 2020] as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face 2 for our imple- mentations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following He et al. [2022]. 4.2 Discovering Design Patterns Using T5-base In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as Si) consists of a set of models ( Si- models) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space ( S0). Then we progressively reﬁne design spaces (from S0 to S1:4) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space Si with a low-compute, low-epoch regime [Radosavovic et al., 2020], we randomly sample 100 models from Si, ﬁne-tune with 3 epochs 3, and compute the average of the GLUE average performances. 2https://huggingface.co/docs/transformers/index 3We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table 7 in the Appendix). 4We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to ﬁnd out the “best” design space or method. For computational efﬁciency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3). 4.2.1 The Initial S0 Design Space The initial relatively unconstrained design space S0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Preﬁx, BitFit, and LoRA. One can think of this S0 design space as a set of random models ( S0-models) with random design patterns. Speciﬁcally, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer. Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Preﬁx, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers. 4.2.2 The S1 Design Space with Additional Grouping Constraints Inspired by Radosavovic et al. [2020], we also consider 4 groups (G1, . . . , G4, in the order of forward pass) in the experiments 4. Denote by Ni the number of layers in Gi. As illustrated in Figure 2, we compare the following layer grouping patterns: (i) Increasing (Ni+1 > Ni): the number of layers in groups gradually increases; (ii) Uniform (Ni+1 = Ni): the number of layers in groups is the same; (iii) Decreasing (Ni+1 < Ni): the number of layers in groups gradually decreases; (iv) Spindle (N1 < N2 = N3 > N4): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N1 > N2 = N3 < N4): the numbers of layers in groups at both ends are bigger. Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G1, . . . , G4) and numbers of layers in groups. These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S0 design space that satisfy each grouping pattern constraint (Figure 2); (ii) ﬁne-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later. The averaged performances are shown in Table 1 5. We ﬁnd that models from the design space with the spindle grouping pattern (Figure 2) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern. From now on, we will group layers in a spindle pattern. We refer to S0 with this additional design pattern as the new S1 design space. 4.2.3 The S2 Design Space with Additional Parameter Constraints We continue to explore design patterns in trainable parameter allocation to reﬁne the S1 design space. Denote by ni the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (ni+1 ≥ni): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (ni+1 = ni): the number of trainable parameters in every layer is the same; and (iii) Decreasing (ni+1 ≤ni): the number of trainable parameters in every layer gradually decreases (or remains the same). Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. Table 2 reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. 4The experimental results with 8 groups are shown in the Table 16 in the Appendix. 5The training time for the step is shown in the Table 18 in the Appendix. 5Table 1: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 70.0 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 37.3 73.3 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 Table 2: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S1 design space. Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 87.2 77.9 79.4 78.7 71.6 77.6 81.4 32.0 73.2 Uniform 87.8 77.4 80.1 80.5 73.9 78.1 80.4 34.3 74.0 Decreasing 86.4 75.8 78.4 77.0 70.4 77.1 78.7 35.8 72.4 We will allocate the number of trainable parameters to layers uniformly. We refer to S1 with this additional design pattern as the new S2 design space. 4.2.4 The S3 Design Space with Additional Tunable Group Constraints Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we ﬁnd out which groups need to be ﬁne-tuned. As shown in Table 3, we explore various design patterns in tunable groups to further constrain the S2 design space. Based on the GLUE average performances, we ﬁnd that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks. We will tune all the groups. We refer to S2 with this additional design pattern as the new S3 design space. 4.2.5 The S4 Design Space with Additional Strategy Constraints Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S3 design space. Speciﬁcally, each design space consists of models that assign a subset of {Adapter (A), Preﬁx (P), BitFit (B), and LoRA (L) }to all layers of any group Gi (i = 1, . . . ,4). We begin by adding different G1 strategy assignment constraints to the S3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G1. Then we progressively add Gi (i >1) strategy assignment constraints together with the discovered strategy assignment patterns for all Gj (j = 1, . . . , i−1) to the S3 space. Due to space limit, we present results of this process in the Appendix ( G1 in Table 8, G2 Table 9, G3 in Table 10, and G4 in Table 11), which suggests strategy assignment ofG1-(A, L) – G2-(A, P) – G3-(A, P, B) –G4-(P, B, L) for the T5-base pretrained backbone model. We will assign the discovered proper tuning strategies to groups.We refer to S3 with this additional design pattern as the new S4 design space, which consists of the ﬁnal S4-model. 4.3 Discovering Design Patterns Using T5-3b We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5- base) still apply when we use larger models. The results are shown in Table 12 (layer grouping), Table 13 (trainable parameter allocation), Table 14 (tunable groups) and Table 15 (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table 12), (ii) uniformly allocating the number of trainable parameters to layers (Table 13), (iii) tuning all the groups 6Table 3: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 82.6 72.1 77.6 70.6 65.3 71.9 77.6 27.6 68.2 G2 83.3 72.8 77.5 72.8 63.6 72.8 77.5 27.5 68.4 G3 83.6 73.3 78.2 73.3 66.4 71.3 77.9 22.9 68.4 G4 83.2 73.0 77.9 73.7 63.9 72.0 77.9 27.9 68.7 G1, G2 83.5 73.2 78.0 75.4 67.7 73.2 78.0 28.0 69.6 G3, G4 87.8 74.6 78.3 76.9 68.6 74.3 78.3 28.3 70.7 G1, G2, G3 86.0 75.8 79.0 77.8 71.8 78.8 79.0 33.0 72.6 G2, G3, G4 85.2 76.6 79.1 78.6 70.1 77.6 79.1 31.9 72.2 G1,G2,G3,G4 88.3 77.4 82.1 81.5 74.9 79.4 81.4 34.3 74.9 Table 4: Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S4-model and the S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 95.2 87.1 93.7 89.4 80.1 89.4 90.7 51.1 84.5 Adapter 94.6 85.5 89.8 86.7 75.3 86.7 89.1 59.2 83.3 Preﬁx 94.0 81.6 87.8 83.4 64.3 83.1 84.8 34.0 76.6 BitFit 94.4 84.5 90.6 88.3 74.3 86.6 90.1 57.7 83.3 LoRA 94.8 84.7 91.6 88.5 75.8 86.3 88.7 51.5 82.7 S4-model 95.5∗∗ 1.7 87.6∗∗ 1.0 92.7∗∗ 1.1 88.8∗∗ 1.0 80.4∗ 2.3 87.4∗ 2.0 91.2∗∗ 2.4 62.2∗ 3.2 85.7 full 97.4 91.4 96.3 89.7 91.1 90.6 92.5 67.1 89.5 Adapter 96.3 89.9 94.7 87.8 83.4 90 89.7 65.2 87.1 Preﬁx 96.3 82.8 88.9 85.5 78.3 83.5 85.4 42.7 80.4 BitFit 95.8 89.5 93.5 88.5 86.2 90.7 88.6 64.2 87.1 LoRA 96.2 90.6 94.9 89.1 91.2 91.1 91.1 67.4 88.9 S4-3b-model 97.2∗∗ 1.8 91.6∗∗ 1.2 96.6∗∗ 1.0 89.5∗∗ 1.5 91.5∗ 2.8 91.5∗ 2.5 91.9∗ 2.0 69.7∗ 3.4 89.9 (Table 14), and (iv) tuning different groups with proper strategies (Table 15). For T5-3b, the discovered proper strategy assignment is G1-(P, L) –G2-(A, L) – G3-(P, B, L) –G4-(A, P, B). We refer to the ﬁnal design space asS4-3b and the ﬁnal model in this space as S4-3b-model. 5 Evaluation The S4-model (Section 4.2.5) and S4-3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks. 5.1 Experimental Setup Datasets Besides the GLUE datasets [Wang et al., 2018] (Section 4.1), we further evaluate our methods on two generation tasks used by He et al. [2022]: (i) Abstractive Summarization using XSum [Narayan et al., 2018], and (ii) Machine Translation using the WMT 2016 en-ro dataset [Bojar et al., 2016]. We report ROUGE scores [Lin, 2004] on the XSum test set, and BLEU scores [Papineni et al., 2002] on the en-ro test set. Models and Model Settings We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it ﬁne-tunes all the model parameters in the pretrained models; (ii) Adapter [Houlsby et al., 2019a]: it adds adapter modules to each transformer layer; (iii) Preﬁx [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) BitFit [Zaken et al., 2021]: it only updates the bias terms in pretrained models; (v) LoRA [Hu et al., 2021]: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 [Raffel et al., 2020], we additionally apply our methods to other backbone models 7Table 5: Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S0-model, where all the designs are randomly selected for RoBERTa as in the S0 design space; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in the S3 design space. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 94.8 87.6 92.8 91.9 80.8 90.3 90.2 63.6 86.5 Adapter 94.2 87.1 93.1 90.2 71.5 89.7 88.5 60.8 84.4 Preﬁx 94.0 86.8 91.3 90.5 74.5 90.3 88.2 61.5 84.6 BitFit 93.7 84.8 91.3 84.5 77.8 90.8 90.0 61.8 84.3 LoRA 94.9 87.5 93.1 90.8 83.1 90.0 89.6 62.6 86.4 S0-model 94.2 95.3 90.4 90.6 75.6 89.6 88.0 60.9 85.6 S3-model 94.3 87.2 92.8 91.0 81.8 90.3 89.2 63.2 86.2 S4-model 94.81.6 87.8∗∗ 0.8 93.4∗∗ 1.3 91.6∗ 1.2 85.8∗∗ 1.8 90.4∗ 2.0 90.0∗∗ 1.8 63.2∗ 3.5 87.1 full 96.4 90.2 94.7 92.2 86.6 92.4 90.9 68.0 88.9 Adapter 96.6 90.5 94.8 91.7 80.1 92.1 90.9 67.8 88.1 Preﬁx 95.7 87.6 92.1 88.7 82.3 89.6 87.4 62.8 85.7 BitFit 96.1 88.0 93.4 90.2 86.2 90.9 92.7 64.2 87.7 LoRA 96.2 90.6 94.7 91.6 87.4 92.0 89.7 68.2 88.8 S0-model 95.5 86.5 92.3 89.8 84.6 89.2 86.3 61.2 85.6 S3-model 96.3 89.4 93.8 90.2 85.9 90.8 90.9 63.4 87.6 S4-3b-model 96.6∗∗ 1.3 90.8∗ 1.1 95.1∗∗ 0.8 92.0∗∗ 1.2 87.22.8 92.3∗ 2.2 91.8∗∗ 1.8 68.4∗ 3.2 89.3 including RoBERTa-base/large [Liu et al., 2019] and BART-base/large [Lewis et al., 2020a]. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following He et al. [2022]. Speciﬁcally, this value is set to 0.5% for Adapter, Preﬁx, LoRA, and our methods, and 0.1% for BitFit. For all the experiments, we followed Liu et al. [2019] to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e −5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs. 5.2 Effectiveness on GLUE with T5 Backbones Table 6: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. Method XSUM(R-1/2/L) en-ro (BLEU) full 40.5/19.2/34.8 34.5 Adapter 37.7/17.9/33.1 33.3 Preﬁx 38.2/18.4/32.4 33.8 BitFit 37.2/17.5/31.4 33.2 LoRA 38.9/18.6/33.5 33.6 PA 39.3/18.7/33.8 33.8 S4-model 40.2/19.3/34.2 34.1 full 45.1/22.3/37.2 37.9 Adapter 43.8/20.8/35.7 35.3 Preﬁx 43.4/20.4/35.5 35.6 BitFit 42.8/18.7/33.2 35.2 LoRA 42.9/19.4/34.8 35.8 PA 43.9/20.6/35.6 36.4 S4-3b-model 44.3/21.7/36.8 37.2 With our discovered design patterns, we ﬁne-tune T5-base (S4-model) and T5-3b ( S4-3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table 4, where the key measure is the GLUE average performance (last column). We ﬁnd that our S4-model and S4- 3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full ﬁne-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discov- ered PEFT design patterns. 5.3 General Effectiveness on GLUE with RoBERTa Backbones We directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5- base and T5-3b) to ﬁne-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de- 8sign spaces: (i) S0-model, where all the designs are randomly selected for RoBERTa as in S0; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in S3. Table 5 shows that (i) the design pat- terns (adopted by S4-model and S4-3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances fromS0-models, S3-models, to S4-(3b)-models support adding more constraints in the pattern discovery process (Section 4). 5.4 General Effectiveness on Generation Tasks with BART Backbones Like in Section 5.3, we further directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following He et al. [2022]. We also compare with PA (parallel adapter) using the same number of trainable parameters [He et al., 2022]. Table 6 shows that our methods, although adopting design patterns discovered from classiﬁcation tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones. 6 Conclusion PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efﬁcient fashion. Instead of focusing on designing another strategy in the ﬁrst place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InAdvances in neural information processing systems, pages 5754–5764, 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64–77, 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, transla- tion, and comprehension. SCL, 2020a. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training. arXiv preprint arXiv:2002.12804, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. V ALUE: Understanding dialect disparity in NLU. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 91: Long Papers) , pages 3701–3720, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.258. URL https://aclanthology.org/2022.acl-long.258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , vol- ume 97 of Proceedings of Machine Learning Research , pages 2790–2799. PMLR, 09–15 Jun 2019a. URL http://proceedings.mlr.press/v97/houlsby19a.html. Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 487–503, Online, April 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.eacl-main.39. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning, 2021a. Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer- based masked language-models, 2021. URL https://arxiv.org/abs/2106.10199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. UniPELT: A uniﬁed framework for parameter-efﬁcient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https: //aclanthology.org/2022.acl-long.433. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics , pages 7871–7880, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ´ar. On network design spaces for visual recognition, 2019. URL https://arxiv.org/abs/1905.13214. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces, 2020. URL https://arxiv.org/abs/2003.13678. Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks, 2020. URL https://arxiv. org/abs/2011.08843. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019b. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR, 2019. 10Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter- efﬁcient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch ¨utze. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. Evani Radiya-Dixit and Xin Wang. How ﬁne can ﬁne-tuning be? learning efﬁcient language models. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2435–2443. PMLR, 2020. Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with ﬁxed sparse masks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Pro- cessing Systems, volume 34, pages 24193–24205. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf. Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected lay- ers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In International Conference on Learning Representations, 2021a. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022. URL https: //arxiv.org/abs/2205.12548. Wanjun Zhong, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan. Improving task generalization via uniﬁed schema prompt, 2022. URL https://arxiv.org/abs/2208.03229. M Saiful Bari, Aston Zhang, Shuai Zheng, Xingjian Shi, Yi Zhu, Shaﬁq Joty, and Mu Li. Spt: Semi-parametric prompt tuning for multitask prompted learning. arXiv preprint arXiv:2212.10929, 2022. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efﬁcient methods for pre-trained language models, 2022. URL https://arxiv.org/abs/2203.06904. Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of language? In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy, July 2019. Association for Computational Linguistics. Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and Shuai Zhang. Self-instantiated recurrent units with dynamic soft recursion. Advances in Neural Information Processing Systems, 34:6503–6514, 2021b. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convo- lutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji- meno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Mar- cos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. 11Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 12A More Experimental Results Table 7: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for ﬁve different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg 1 epochs Increasing 73.2 63.3 67.8 68.8 63.8 67.2 64.1 11.0 59.9 Uniform 72.8 64.1 63.4 63.4 62.5 69.8 65.8 12.1 59.2 Decreasing 72.4 63.2 65.1 69.8 59.3 62.7 63.6 18.7 59.4 Spindle 72.6 64.8 66.8 71.1 62.1 62.3 64.8 12.3 59.6 Bottleneck 72.2 63.7 65.3 68.3 61.2 63.2 66.6 12.1 59.0 2 epochs Increasing 76.2 69.3 73.2 76.5 65.8 72.2 74.0 21.0 66.0 Uniform 74.8 70.9 74.1 75.6 66.5 73.4 71.2 22.1 66.1 Decreasing 71.4 70.1 72.1 76.8 64.3 71.7 73.6 18.7 64.8 Spindle 76.6 71.9 71.8 74.4 67.5 73.5 71.8 22.3 66.2 Bottleneck 74.2 71.1 69.6 73.3 65.2 73.3 73.6 24.1 65.5 3 epochs Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 69.9 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 47.3 74.6 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 4 epochs Increasing 88.3 78.5 80.2 80.5 70.8 80.2 80.0 37.0 74.4 Uniform 88.8 78.9 81.9 81.5 71.5 80.8 81.4 39.1 75.4 Decreasing 87.6 74.1 80.8 81.7 79.3 78.9 79.6 38.7 75.1 Spindle 89.6 79.8 83.6 82.8 71.8 81.3 82.1 39.3 76.3 Bottleneck 86.5 77.6 82.7 81.1 70.2 70.9 81.6 36.1 73.3 20 epochs Increasing 92.3 83.3 86.2 82.5 71.8 82.2 84.0 51.0 79.1 Uniform 92.8 83.9 86.1 83.6 72.5 83.8 84.2 52.1 79.9 Decreasing 91.4 82.1 85.1 83.1 69.3 81.7 83.6 48.7 78.1 Spindle 93.6 84.8 87.8 84.4 73.5 84.3 85.8 52.3 80.8 Bottleneck 92.1 82.6 85.6 83.3 71.2 83.2 84.6 52.1 79.3 B General Effectiveness on SuperGLUE with XLNet Backbones We also directly use the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table 17 reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process. C On the Discovery Sequence In this work, we follow the discovery sequence of “grouping patterns – trainable parameter allocation – tunable groups – strategy assignment”: 1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efﬁcient to study the layers in the unit of groups. So we start with the grouping patterns. 13Table 8: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G1 strategy assignment con- straints to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 89.8 83.5 84.9 80.8 72.5 80.8 78.5 37.7 76.1 G1-Preﬁx (P) 89.3 83.1 84.4 80.1 70.1 80.0 77.6 33.0 74.7 G1-BitFit (B) 89.0 82.9 84.1 81.4 72.0 81.1 77.0 30.8 74.8 G1-LoRA (L) 89.9 83.6 85.0 81.1 71.8 81.0 78.8 35.3 75.8 G1-(P, L) 89.1 82.8 85.1 81.2 71.9 81.5 79.1 35.0 75.7 G1-(A, P) 89.8 82.8 84.8 81.1 72.2 81.3 79.2 36.4 75.9 G1-(A, L) 89.6 83.8 85.6 81.3 72.9 81.7 79.5 36.8 76.4 G1-(A, P, L) 89.6 83.5 85.2 81.5 72.2 81.4 79.2 35.2 75.9 G1-(P, B, L) 89.3 83.6 85.5 81.6 72.3 81.0 78.8 35.7 76.0 G1-(A, P, B) 89.2 83.3 84.8 81.8 72.5 81.1 78.6 35.6 75.8 G1-(A, B, L) 89.8 83.4 84.8 81.1 72.6 81.6 79.4 34.8 75.9 G1-(A, P, B, L) 90.0 83.1 85.3 81.6 72.6 81.4 79.2 36.5 76.1 Table 9: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G2 strategy assignment con- straints with G1-(L, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G2-Adapter (A) 91.6 84.3 85.5 82.3 73.5 82.8 81.3 38.8 77.5 G2-Preﬁx (P) 89.6 84.0 86.5 81.5 73.3 82.5 80.5 36.2 76.7 G2-BitFit (B) 91.2 83.6 85.7 82.9 72.6 82.6 80.8 33.1 76.5 G2-LoRA (L) 91.4 84.4 86.1 82.0 72.8 81.8 81.6 39.8 77.4 G2-(P, L) 91.6 84.6 86.8 81.8 73.8 82.8 82.0 38.5 77.7 G2-(A, P) 92.2 84.2 87.1 82.2 74.4 83.0 82.5 40.8 78.3 G2-(A, L) 92.0 84.4 86.5 81.8 73.6 82.6 82.2 40.1 77.9 G2-(A, P, L) 91.8 84.8 86.8 81.8 74.1 83.0 82.1 37.9 77.7 G2-(P, B, L) 91.6 84.1 87.1 82.0 74.0 82.9 82.4 35.8 77.4 G2-(A, P, B) 91.8 84.2 86.8 82.1 73.7 83.3 82.2 41.2 78.1 G2-(A, B, L) 92.2 84.3 86.1 82.0 74.1 83.2 82.0 37.6 77.6 G2-(A, P, B, L) 92.0 84.1 87.0 81.9 74.2 83.1 81.3 42.4 78.1 2. Once ﬁguring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes inﬂuential to examine which groups need to be learned during ﬁne-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we ﬁgure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design. 14Table 10: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G3 strategy assignment constraints with G1-(L, A) – G2-(P, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G3-Adapter (A) 92.5 85.3 87.5 83.3 73.9 84.0 83.8 44.9 79.4 G3-Preﬁx (P) 91.5 84.7 86.7 82.6 74.2 83.8 82.9 40.5 78.4 G3-BitFit (B) 91.9 84.3 87.0 82.0 73.6 84.1 83.3 36.1 77.8 G3-LoRA (L) 92.8 85.4 87.8 83.5 74.7 82.4 84.0 44.0 79.3 G3-(P, L) 93.0 85.2 88.3 83.8 75.2 84.4 84.2 37.9 79.0 G3-(A, P) 92.4 85.6 88.1 83.6 75.0 84.2 84.0 41.8 79.3 G3-(A, L) 92.0 85.9 88.2 83.1 75.3 84.3 83.9 42.2 79.4 G3-(A, P, L) 92.6 86.0 87.5 83.4 75.6 84.6 83.5 43.9 79.6 G3-(P, B, L) 92.7 85.8 87.2 83.7 75.2 84.5 83.8 40.8 79.2 G3-(A, P, B) 93.3 85.8 88.6 84.0 75.5 84.9 84.1 42.1 79.8 G3-(A, B, L) 93.7 86.5 88.0 83.2 75.8 84.2 84.2 39.7 79.4 G3-(A, P, B, L) 93.3 85.6 87.7 83.8 75.2 84.3 84.4 41.6 79.4 Table 11: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G4 strategy assignment constraints with G1-(A, L) – G2-(A, P) – G3-(A, P, B) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G4-Adapter (A) 93.8 85.8 88.6 84.8 76.3 85.8 86.0 48.5 81.2 G4-Preﬁx (P) 93.5 85.2 88.3 83.6 76.8 85.3 85.6 44.8 80.3 G4-BitFit (B) 94.1 85.3 88.9 84.4 77.1 85.4 86.2 46.1 80.9 G4-LoRA (L) 94.0 86.0 89.2 85.0 77.2 85.5 85.8 47.7 81.3 G4-(P, L) 94.3 86.2 89.3 85.8 78.0 86.0 88.2 47.2 81.8 G4-(A, P) 94.1 86.2 89.6 85.4 77.9 86.2 86.9 45.3 81.4 G4-(A, L) 94.2 85.9 89.2 85.5 77.8 86.2 88.0 46.8 81.7 G4-(A, P, L) 94.1 85.8 88.8 85.7 77.4 86.5 87.9 44.8 81.3 G4-(P, B, L) 94.6 86.4 90.4 86.1 78.2 86.8 88.5 47.2 82.3 G4-(A, P, B) 94.5 86.0 89.6 86.0 78.0 86.2 88.1 44.8 81.6 G4-(A, B, L) 94.3 86.4 89.2 85.6 78.2 86.4 88.3 46.6 81.9 G4-(A, P, B, L) 94.2 86.2 89.2 85.9 78.5 86.1 88.0 45.3 81.6 Table 12: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 80.3 72.1 74.7 72.8 76.9 75.2 71.0 32.2 69.4 Increasing 84.4 75.7 83.0 78.3 82.7 80.3 76.3 42.1 75.3 Uniform 86.8 77.1 82.6 76.2 83.8 81.6 77.3 48.9 76.8 Decreasing 83.2 74.3 81.8 77.3 82.8 79.9 76.5 40.8 74.5 Spindle 88.6 78.8 83.7 77.7 84.2 80.9 78.3 44.6 77.1 Bottleneck 86.3 77.0 82.2 75.6 83.3 80.2 77.1 41.5 75.4 15Table 13: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S1 design space. Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 90.3 79.3 84.9 79.3 85.2 82.8 79.2 50.1 78.9 Uniform 90.6 80.8 84.6 79.7 85.5 82.4 78.9 50.8 79.1 Decreasing 88.6 78.2 83.5 78.1 84.4 81.5 78.1 49.6 77.7 Table 14: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 88.3 78.3 82.2 77.4 82.1 80.7 76.1 49.4 76.8 G2 89.1 78.8 82.1 77.2 82.3 81.2 76.4 49.6 77.1 G3 89.6 78.5 82.6 78.1 83.8 81.9 77.4 48.7 77.5 G4 89.8 79.3 82.7 77.9 83.5 81.9 77.9 48.5 77.1 G1, G2 90.1 80.2 83.4 78.5 84.3 82.4 78.5 51.1 78.5 G3, G4 90.5 80.6 83.8 78.7 84.2 83 78.2 50.3 78.6 G1, G2, G3 90.6 80.3 84.9 79.3 84.7 82.9 79.3 50.2 79.0 G2, G3, G4 90.8 80.9 84.6 79.1 85.1 83.1 79.1 49.2 78.9 G1, G2, G3, G4 91.1 81.4 85.2 80.4 85.9 83.5 80.0 51.6 79.9 16Table 15: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment con- straints following the process in Section 4.2.5. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 91.1 81.4 86.1 80.5 86.7 83.3 80.1 50.8 80.0 G1-Preﬁx (P) 90.8 81.1 85.5 80.2 86.2 83.1 79.8 50.2 79.6 G1-BitFit (B) 90.2 81.3 85.1 79.6 85.8 82.8 79.6 49.5 79.2 G1-LoRA (L) 91.4 81.9 86.2 80.8 86.4 83.9 80.8 49.6 80.0 G1-(P, L) 91.8 82.9 86.8 81.3 87.1 84.2 81.6 52.3 81.0 G1-(A, P) 91.3 81.9 86.4 81.1 85.6 83.7 80.7 52.8 80.1 G1-(A, L) 91.6 82.3 86.1 81.5 85.8 84.9 81.5 51.8 80.6 G1-(A, P, L) 91.1 81.7 85.8 81.2 86.4 84.2 80.9 52.3 80.4 G1-(P, B, L) 91.5 82.8 86.3 81.4 86.1 83.6 81.2 51.5 80.5 G1-(A, P, B) 91.3 82.3 86.7 80.8 86.8 84.3 80.7 51.8 80.5 G1-(A, B, L) 91.7 82.5 86.2 81.3 86.3 84.6 81.3 51.7 80.7 G1-(A, P, B, L) 91.6 82.3 86.2 81.1 86.6 84.2 81.1 51.1 80.5 G2-Adapter (A) 92.1 82.5 86.4 81.8 87.2 84.8 81.8 53.8 81.3 G2-Preﬁx (P) 91.8 83.1 87.2 81.6 86.2 84.4 81.1 52.8 81.0 G2-BitFit (B) 91.2 82.1 86.4 81.1 86.3 84.6 80.3 53.1 80.6 G2-LoRA (L) 92.6 82.9 87.5 81.3 87.4 85.1 81.9 52.2 81.4 G2-(P, L) 91.6 82.7 87.6 81.6 87.8 85.3 82.1 52.8 81.4 G2-(A, P) 92.1 83.3 87.5 81.9 87.4 85.5 81.8 53.1 81.5 G2-(A, L) 92.5 83.7 88.1 82.2 87.4 85.7 82.9 53.6 82.1 G2-(A, P, L) 92.3 83.4 87.4 81.6 87.1 85.3 81.4 53.2 81.4 G2-(P, B, L) 91.8 83.1 87.4 81.5 87.2 85.1 82.7 53.8 81.5 G2-(A, P, B) 91.5 82.6 87.8 81.3 86.5 85.2 82.1 54.2 81.4 G2-(A, B, L) 92.6 83.5 87.2 82 87.3 86.5 82.5 52.8 81.8 G2-(A, P, B, L) 92.8 83.2 87.6 81.6 87.5 85.5 82.4 51.2 81.5 G3-Adapter (A) 92.6 84.1 88.3 81.8 87.8 85.4 82.8 55.2 82.2 G3-Preﬁx (P) 92.1 83.3 87.6 81.4 87.1 85.4 82.6 53.5 81.6 G3-BitFit (B) 92.4 83.9 88.4 82.1 87.2 85.8 82.4 53.3 81.9 G3-LoRA (L) 93.1 84.3 87.7 82.4 87.8 86.2 83.1 54.3 82.3 G3-(P, L) 92.8 84.1 88.7 82.6 88.2 86.2 83.3 54.7 82.6 G3-(A, P) 93.1 83.8 89.1 82.3 88.1 85.8 82.6 55.1 82.5 G3-(A, L) 92.7 84.5 88.4 82.8 88.2 86.1 83.5 54.6 82.6 G3-(A, P, L) 92.8 84.6 88.1 82.5 87.7 85.5 83.2 53.8 82.3 G3-(P, B, L) 93.6 84.9 89.3 83.1 88.2 86.5 83.9 55.8 83.2 G3-(A, P, B) 93.3 83.9 88.5 82.2 88.4 86.2 83.5 55.3 82.6 G3-(A, B, L) 93.4 84.2 88.9 82.6 87.8 85.8 84.2 54.9 82.7 G3-(A, P, B, L) 92.2 84.4 88.7 82.3 88.5 86.2 84.2 54.2 82.5 G4-Adapter (A) 92.8 85.2 89.1 83.5 87.8 86.5 84.2 56.3 83.2 G4-Preﬁx (P) 92.8 84.6 89.5 82.6 87.4 86.5 83.8 55.8 82.8 G4-BitFit (B) 93.8 84.9 89.5 83.3 88.7 86.8 84.4 55.2 83.3 G4-LoRA (L) 93.3 84.7 89.3 82.7 88.3 86.2 82.7 54.7 82.7 G4-(P, L) 93.8 85.3 89.6 83.6 88.6 86.8 84.6 56.3 83.5 G4-(A, P) 93.8 84.9 89.8 84.3 88.5 86.6 84.8 56.7 83.6 G4-(A, L) 93.7 85.6 89.5 84.1 88.2 86.6 85.2 55.4 83.5 G4-(A, P, L) 94.2 85.2 89.6 83.9 88.2 86.4 84.9 55.9 83.5 G4-(P, B, L) 93.8 85.9 89.8 83.6 88.6 86.9 85.2 56.3 83.7 G4-(A, P, B) 94.4 85.7 90.1 84.8 88.9 87.2 85.3 57.3 84.2 G4-(A, B, L) 93.8 85.3 89.5 84.1 88.8 86.7 85.5 56.6 83.7 G4-(A, P, B, L) 94.1 85.4 89.7 84.4 88.5 86.5 85.2 56.8 83.8 17Table 16: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer grouping is based on 8 groups. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 83.2 74 .1 76 .6 77 .1 67 .7 76.8 74.7 30.0 70.0 Uniform 83.6 73.4 78.0 77.9 68.2 76.4 78.6 34.2 71.3 Decreasing 80.3 71.6 77.4 75.5 67.0 75.3 77.2 26.4 68.9 Spindle 86.2 74.3 79.1 78.6 68.5 77.4 79.5 35.1 72.3 Bottleneck 83.2 73.1 75.8 77.6 67.9 75.3 78.2 31.4 70.3 Table 17: Performances of different tuning methods on the SuperGLUE datasets using the XLNet-base (upper part) and XLNet-large (lower part) pretrained backbone models, respectively. The results are averaged over 10 random runs. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05 (*) or even p <0.01 (**). Method BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Average Adapter 72.8 71.3/78.0 64.0 67.0/24.5 71.0/71.8 76.2 65.0 60.8 66.2 Preﬁx 72.0 70.5/77.0 63.3 66.4/23.8 69.9/71.0 75.5 64.4 60.8 65.9 BitFit 71.8 70.0/76.2 62.8 65.8/22.6 69.4/70.6 74.5 64.8 60.6 65.2 LoRA 72.2 71.1/77.8 64.7 67.4/24.8 70.8/71.3 76.8 65.1 61.1 66.4 S4-model 73.8∗∗ 71.7/78.4∗ 65.9∗∗ 68.2/25.5∗∗ 71.1/72.0∗ 78.4∗∗ 65.8∗ 62.6∗ 67.5 Adapter 74.4 71.4/81.1 67.4 68.8/26.4 71.7/72.4 80.8 68.0 64.6 68.8 Preﬁx 72.4 70.0/78.3 66.9 68.8/25.8 70.9/71.2 78.8 66.9 64.0 67.7 BitFit 71.1 70.7/79.8 68.0 68.6/25.4 71.1/71.6 80.4 67.2 64.3 68.1 LoRA 74.1 72.1/80.9 67.9 69.1/26.8 72.0/72.8 81.0 67.8 64.4 69.0 S4-3b-model 76.8∗∗ 74.6/81.9∗∗ 68.6∗∗ 69.5/27.1∗ 72.4/73.3∗ 81.2∗ 68.2∗∗ 64.8∗ 69.7 Table 18: Total training time (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model with 8 A100 GPUs from S0 to S1. SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA 18 mins 22 mins 20 mins 40 mins 8 mins 12 mins 8 mins 6 mins 18",
      "meta_data": {
        "arxiv_id": "2301.01821v1",
        "authors": [
          "Jiaao Chen",
          "Aston Zhang",
          "Xingjian Shi",
          "Mu Li",
          "Alex Smola",
          "Diyi Yang"
        ],
        "published_date": "2023-01-04T21:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2301.01821v1.pdf",
        "github_url": "https://github.com/amazon-science/peft-design-spaces"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addresses the lack of understanding of design patterns in parameter-efficient fine-tuning (PEFT) strategies, which are typically hand-crafted. The paper introduces PEFT design spaces, characterized by layer grouping, trainable parameter allocation, tunable groups, and strategy assignment, to systematically discover such patterns. Key findings include the discovery of effective design patterns: (i) grouping layers in a spindle pattern; (ii) uniformly allocating trainable parameters to layers; (iii) tuning all layer groups; and (iv) assigning proper tuning strategies to different groups. These patterns lead to new PEFT methods (S4-model and S4-3b-model) that consistently and significantly outperform existing PEFT strategies across various backbone models (T5, RoBERTa, BART, XLNet) and different NLP tasks (GLUE, XSum, WMT, SuperGLUE).",
        "methodology": "The methodology involves introducing parameter-efficient fine-tuning design spaces that parameterize both tuning structures and strategies. These design spaces are defined by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. The process starts with a relatively unconstrained initial design space (S0). This space is then progressively refined by applying different constraints to each component. Design patterns are discovered through a greedy selection approach at each stage, comparing the overall quality of models sampled from design spaces with different constraints. Model quality is quantified by randomly sampling 100 models, fine-tuning them for 3 epochs (a low-compute, low-epoch regime), and computing the average performance on the GLUE benchmark.",
        "experimental_setup": "For discovering design patterns, the T5-base and T5-3b pretrained backbone models were used with the GLUE benchmark, covering single-sentence, similarity/paraphrase, and inference tasks. Metrics included Matthews correlation (CoLA), Spearman correlation (STS-B), and accuracy for others. For evaluation, the discovered patterns were applied to T5-base/3b, RoBERTa-base/large, and BART-base/large on GLUE, XSum (Abstractive Summarization, ROUGE scores), and WMT 2016 en-ro (Machine Translation, BLEU scores). Additionally, XLNet-base/large models were evaluated on SuperGLUE datasets (BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, WSC). Implementations used Hugging Face, with total trainable parameters set to 0.5% for most methods and 0.1% for BitFit. Training involved a linear decay scheduler with a warmup ratio of 0.06, batch sizes of 128 (base) or 64 (large), a maximum learning rate of 5e-5, and up to 10 training epochs (or 3 for pattern discovery). All experiments were performed using 8 A100 GPUs.",
        "limitations": "The study's primary limitation is that its goal was not to exhaustively enumerate all possible design spaces or constraints, but rather to demonstrate the utility of the design space perspective in informing PEFT research. Computational efficiency constraints limited the exhaustive exploration of all possible design choices and combinations within the defined design space components. While a low-epoch regime (3 epochs) was used for design pattern discovery, the authors assert its sufficiency for stable performance conclusions. The impact of varying the fixed discovery sequence (grouping, parameter allocation, tunable groups, strategy assignment) was not explored.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "if is_torch_available():_import_structure[\"adapters\"] = [\"ADAPTER_CACHE\",\"ADAPTER_CONFIG_MAP\",\"ADAPTERFUSION_CONFIG_MAP\",\"ADAPTER_MODEL_MAPPING\",\"DEFAULT_ADAPTER_CONFIG\",\"DEFAULT_ADAPTERFUSION_CONFIG\",\"MODEL_WITH_HEADS_MAPPING\",\"AdapterArguments\",\"AdapterConfig\",\"AdapterConfigBase\",\"AdapterFusionConfig\",\"AdapterInfo\",\"AdapterLayer\",\"AdapterLayerBase\",\"AdapterSetup\",\"AdapterTrainer\",\"AdapterType\",\"AutoAdapterModel\",\"AutoModelWithHeads\",\"BartAdapterModel\",\"BartModelWithHeads\",\"BeitAdapterModel\",\"BertAdapterModel\",\"BertModelWithHeads\",\"CompacterConfig\",\"CompacterPlusPlusConfig\",\"ConfigUnion\",\"DebertaAdapterModel\",\"DebertaV2AdapterModel\",\"DistilBertAdapterModel\",\"DistilBertModelWithHeads\",\"DynamicAdapterFusionConfig\",\"EmbeddingAdaptersMixin\",\"ForwardContext\",\"GPT2AdapterModel\",\"GPT2ModelWithHeads\",\"GPTJAdapterModel\",\"HoulsbyConfig\",\"HoulsbyInvConfig\",\"IA3Config\",\"InvertibleAdaptersMixin\",\"LoRAConfig\",\"MAMConfig\",\"MBartAdapterModel\",\"MBartModelWithHeads\",\"ModelAdaptersConfig\",\"ModelAdaptersMixin\",\"ModelWithFlexibleHeadsAdaptersMixin\",\"ModelWithHeadsAdaptersMixin\",\"MultiLingAdapterArguments\",\"ParallelConfig\",\"PfeifferConfig\",\"PfeifferInvConfig\",\"PrefixTuningConfig\",\"RobertaAdapterModel\",\"RobertaModelWithHeads\",\"Seq2SeqAdapterTrainer\",\"StaticAdapterFusionConfig\",\"T5AdapterModel\",\"T5ModelWithHeads\",\"PEFTConfig\",\"ViTAdapterModel\",\"XLMRobertaAdapterModel\",\"XLMRobertaModelWithHeads\",\"get_adapter_config_hash\",\"get_adapter_info\",\"list_adapters\",]",
        "experimental_info": "The repository, named \"adapter-transformers\", explicitly implements parameter-efficient fine-tuning (PEFT) using the Adapter framework, as indicated by the import structure for 'adapters' in `models/transformers/__init__.py`. This includes various adapter configurations (e.g., HoulsbyConfig, LoRAConfig, PfeifferConfig) and components (AdapterLayer, AdapterTrainer) that facilitate defining tuning structures and strategies. The `setup.py` file lists `datasets` as a dependency, which is relevant for computing performance on the GLUE benchmark. Additionally, `optuna` and `ray[tune]` are listed as dependencies, suggesting capabilities for hyperparameter optimization and exploring design spaces. However, the provided repository content (specifically `setup.py` and `__init__.py` files) does not contain explicit code or configurations detailing the experimental setup of \"randomly sampling 100 models,\" \"fine-tuning them for 3 epochs,\" or the precise \"greedy selection approach\" for discovering design patterns, as described in the method."
      }
    },
    {
      "title": "ReFT: Representation Finetuning for Language Models",
      "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural\nmodels via updates to a small number of weights. However, much prior\ninterpretability work has shown that representations encode rich semantic\ninformation, suggesting that editing representations might be a more powerful\nalternative. We pursue this hypothesis by developing a family of Representation\nFinetuning (ReFT) methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we\nidentify an ablation of this method that trades some performance for increased\nefficiency. Both are drop-in replacements for existing PEFTs and learn\ninterventions that are 15x--65x more parameter-efficient than LoRA. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\ninstruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the\nbest balance of efficiency and performance, and almost always outperform\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.",
      "full_text": "ReFT: Representation Finetuning for Language Models Zhengxuan Wu∗† Aryaman Arora∗† Zheng Wang† Atticus Geiger‡ Dan Jurafsky† Christopher D. Manning† Christopher Potts† †Stanford University ‡Pr(Ai)2R Group {wuzhengx,aryamana,peterwz,atticusg}@stanford.edu {jurafsky,manning,cgpotts}@stanford.edu Abstract Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15×–65×more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense rea- soning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft. 1 Introduction Pretrained language models (LMs) are frequently finetuned to adapt them to new domains or tasks [Dai and Le, 2015]. With finetuning, a single base model can be adapted to a variety of tasks given only small amounts of in-domain data. However, finetuning large LMs is expensive. Parameter- efficient finetuning (PEFT) methods propose to address the high costs of full finetuning by updating a small number of weights. This reduces memory usage and training time, and PEFTs achieve similar performance to full finetuning in many settings [Hu et al., 2023]. A hallmark of current state-of-the-art PEFTs is that they modify weights rather than representations. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative to weight updates. In this paper, we pursue this hypothesis by developing and motivating Representation Finetuning (ReFT). Instead of adapting model weights, ReFT methods train interventions that manipulate a small fraction of model representations in order to steer model behaviors to solve downstream tasks at inference time. ReFT methods are drop-in replacements for weight-based PEFTs. This approach is inspired by recent work in LM interpretability that intervenes on representations to find faithful causal mechanisms [Geiger et al., 2023b] and to steer model behaviours at inference time [Turner et al., 2023, Li et al., 2024], and it can be seen as a generalisation of the representation-editing work of Wu et al. [2024a], Turner et al. [2023], and Zou et al. [2023] (see appendix B for details). *Equal contribution. Preprint. Under review. arXiv:2404.03592v3  [cs.CL]  22 May 2024Commonsense LLaMA 7B  LLaMA 13B  Llama- 2 7B  Llama- 3 8B Instruct -tuning Llama- 2 7B Paramet ers P er f ormance Arit hmetic LLaMA 7B  LLaMA 13B GLUE R oBERT a-base  R oBERT a-lar ge Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4. We focus on a strong and highly efficient instance of the ReFT family that we call Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a parametrisation of ReFT that intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix, building directly on the distributed alignment search (DAS) method of Geiger et al. [2023b] and Wu et al. [2023]. We also identify an ablation of this method (DiReFT) that trades some performance for increased efficiency. We evaluate our ReFTs on LLaMA-family models and small-scale LMs against existing PEFTs on standard benchmarks from four domains: commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding. Compared to LoRA, we find that LoReFT uses 15×–65×times fewer parameters while achieving state-of-the-art performance on commonsense reasoning, instruction-following, and natural language understanding against the strongest PEFTs. These findings indicate that ReFT methods are worthy of further exploration, as they may emerge as more efficient and effective alternatives to weight-based PEFTs. 2 Related work Parameter-efficient finetuning methods (PEFTs). PEFTs train a fraction of the model’s parameters to adapt it to downstream tasks. We classify PEFTs into three categories: 1. Adapter-based methods train additional modules (e.g. fully-connected layers) on top of the frozen pretrained model. Series adapters insert components between LM attention or MLP layers [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al., 2021], while parallel adapters add modules alongside existing components [He et al., 2022a]. Since adapters add new components that cannot be easily folded into existing model weights, they impose an additional burden at inference time.1 2. LoRA [Hu et al., 2022] and DoRA [Liu et al., 2024c] use low-rank matrices to approximate additive weight updates during training, and require no additional overhead during inference since the weight updates can be merged into the model. These are the strongest PEFTs currently.2 3. Prompt-based methods add randomly-initialised soft tokens to the input (usually as a prefix) and train their embeddings while keeping the LM weights frozen [Li and Liang, 2021]. These 1Several very recent papers introduce new adapter architectures but do not benchmark them on the tasks we consider, or they perform hyperparameter-tuning in a different setup than done in this work. These include: LLaMA-Adapter [Zhang et al., 2024b], LLaMA-Adapter v2 [Gao et al., 2023], Aligner [Ziheng et al., 2023]. 2Additional methods not studied in this work: AutoLoRA [Zhang et al., 2024c], ResLoRA [Shi et al., 2024], SiRA [Zhu et al., 2023]. 2methods are often far from optimal compared to other PEFTs, and come at the cost of significant inference overhead. A variant of this method where hidden-layer activations are also tuned was introduced as a baseline in Hu et al. [2022], with better performance. Representation editing. Recent work on activation steering and representation engineering shows that adding fixed or task-specific steering vectors [Subramani et al., 2022, Turner et al., 2023, Zou et al., 2023, Liu et al., 2024b, V ogel, 2024, Li et al., 2024] or applying concept erasure [Ravfogel et al., 2022, Belrose et al., 2023, Avitan et al., 2024, Singh et al., 2024] to the residual stream can enable a degree of control over pretrained LM generations without the need for resource-intensive finetuning [Wu et al., 2024a]. The success of these methods affirms that representations induced by pretrained LMs carry rich semantic structure. Interventional interpretability. Much recent work has used interventions on model-internal states to test hypotheses about how LMs implement various behaviours. In particular, interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly [Smolensky, 1986, Rumelhart et al., 1986, McClelland et al., 1986]. This includes linguistic features such as gender and number [Lasri et al., 2022, Wang et al., 2023, Hanna et al., 2023, Chintam et al., 2023, Yamakoshi et al., 2023, Hao and Linzen, 2023, Chen et al., 2023, Amini et al., 2023, Guerner et al., 2023, Arora et al., 2024], logical and mathematical reasoning [Wu et al., 2023], entity attributes [Huang et al., 2024], and a number of other domains [Mikolov et al., 2013, Elhage et al., 2022, Park et al., 2023, Nanda et al., 2023, Guerner et al., 2023]. 3 ReFT We now define the ReFT family of methods. To do this, we first summarize the core motivation, which emerges from work on intervention-based model interpretability. We then show how this leads directly to Low-rank Linear Subspace ReFT (LoReFT). Finally, we generalize this to a family of ReFT methods. Appendix A provides a brief overview of our generic ReFT training library. To keep the presentation simple, we assume throughout that our target model is a Transformer- based [Vaswani et al., 2017] LM that produces contextualised representations of sequences of tokens. Given a sequence of n input tokens x = (x1, . . . , xn), the model first embeds these into a list of representations h(0) =(h(0) 1 , . . . ,h(0) n ). Then, m layers successively compute the j-th list of hidden representations h(j) as a function of the previous list of hidden representations h(j−1). Each hidden representation is a vector h ∈Rd. The LM uses the final hidden representations h(m) to produce its predictions. In our experiments, we consider both autoregressive LMs and masked LMs [Devlin et al., 2019]. An autoregressive LM predicts p(xn+1 ∣ x1, . . . , xn) =softmax (Wh(m) n ), while a masked LM predicts p(xi ∣ x1, . . . , xi−1, xi+1, . . . , xn) =softmax (Wh(m) i ), where W is a learned matrix mapping from representations to logits over the vocabulary space. 3.1 Motivation In interpretability research, the framework of causal abstraction [Geiger et al., 2021] usesinterchange interventions to establish the causal role of representations in deep learning models. An interchange intervention fixes a representation to the value it would take if a counterfactual input were processed by the model. Experiments investigating how such interventions affect model behavior form the evidence for claims about the causal role of a representation and the concept it encodes. To test whether a concept is encoded in a linear subspace of a representation, one may use a dis- tributed interchange intervention (DII) [Geiger et al., 2023b].3 Let b be the hidden representation created at row i and column k when our model processes input b, and let s be the corresponding representation when that same model processes input s. A distributed interchange intervention on b given a counterfactual source representation s is then defined as DII(b, s, R) =b +R⊺(Rs −Rb) (1) where R ∈ Rr×d is a low-rank projection matrix with orthonormal rows, d is the representation dimensionality, and r is the dimensionality of the subspace we are intervening on. We learn the subspace R using distributed alignment search (DAS), which finds the subspace that maximises the 3This notion of subspace intervention was also independently discovered by Guerner et al. [2023]. 3R eFT Int er v ention t his is some t e xt LoR eFT edit subspace (r o ws of R) RW h h h Φ(h) b -+ + R T edit r estrict ed  t o subspace edit  v ect or Figure 2: Illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function Φ is applied to hidden representations at positions P in layer l. (2) The right panel depicts the intervention function used in LoReFT, which finds an edit vector that only modifies the representation in the linear subspace spanned by the rows of R. Specifically, we show how a rank-2 LoReFT operates on 3-dimensional hidden representations. probability of the expected counterfactual output after intervention [Geiger et al., 2023b]. DAS is highly expressive, and can effectively localize concepts within model representations [Wu et al., 2023, Arora et al., 2024, Wu et al., 2024c, Huang et al., 2024]. This suggests that subspace representation interventions could also be a powerful tool for model control. 3.2 Two low-rank ReFT instantiations LoReFT. The formulation of DII in eq. (1) immediately suggests a way to control model generations via interventions. The guiding intuition is that we can learn how to perform interventions that steer the model towards predicting our task labels. The resulting method, Low-rank Linear Subspace ReFT (LoReFT), is defined by the following variant of eq. (1): ΦLoReFT(h) =h +R⊺(Wh +b −Rh) (2) This is identical to eq. (1), except we use a learned projected source Rs =Wh +b. LoReFT thus edits the representation in the r-dimensional subspace spanned by the rows of R to take on the values obtained from our linear projection Wh +b. We depict this operation in fig. 2. The learned parameters are ϕ = {R, W, b}; the parameters of the LM are frozen. As with DII, R ∈ Rr×d is a low-rank matrix with orthonormal rows where d is the hidden-state dimensionality and r ≤d is the rank of the subspace. We further define a linear projection W ∈Rr×d and bias vector b ∈Rr. DiReFT. In addition, we define an ablation of LoReFT which removes the orthogonality constraint and the difference operation, reducing training time: ΦDiReFT(h) =h +W⊺ 2 (W1h +b) (3) Both W1, W2 ∈Rr×d are low-rank projection matrices. Note that eq. (3) resembles LoRA, and thus DiReFT can be thought of as LoRA applied directly to hidden representations at certain positions.4 Empirical evidence from previous work suggests that adding orthogonal constraints to LoRA weights increases performance [Liu et al., 2024d]. (Appendix E reports results for additional ablations of LoReFT.) Training objective. We consider both generation tasks using decoder-only or encoder–decoder LMs and classification tasks using encoder-only models. The pretrained language model induces a distribution over token sequences p(⋅). We denote the model that results from the ReFT intervention Φ on p(⋅) as pΦ(⋅) with trainable parameters ϕ. To simplify notation, we refer to the hidden representations produced by the LM on input x as h(x), and those by the intervened LM as hΦ(x). For generation tasks, our training objective is language modelling. Given an input sequence x = (x1, . . . , xn) with n tokens as the prompt, the goal is to predict the output sequencey =(y1, . . . , ym) 4LoRA is not applicable to the residual stream, which is weightless. LoRA can be configured to apply only to the attention layer output projection matrix, which is similar to our residual stream intervention. However, previous works found that applying LoRA only to attention layers is sub-optimal [Hu et al., 2023]. 4with m tokens. We minimise the cross-entropy loss with teacher-forcing over all output positions. min ϕ {− m ∑ i=1 log pΦ (yi ∣ xy<i)} (4) For single-label classification tasks, we add a classification head Hθ(⋅) with parameters θ that takes the final-layer representation at the first token (CLS) as input and outputs a distribution over classes. H has the learned parameters θ ={Wo, bo, Wd, bd}. Hθ(⋅∣ h) =softmax (Wo(tanh(Wdh(m) 1 +bd))+bo) (5) We learn the parameters of the head and those of the intervention function Φ. We minimise the cross-entropy loss of the target class y given input x: min ϕ,θ {−log Hθ(y ∣ hΦ(x))} (6) 3.3 The ReFT family of methods It is straightforward to generalise the above intervention functions to define a family of intervention- based representation finetuning methods. We first define a general notion of intervention, i.e. the modification of hidden representations during the model forward pass: Definition 3.1. An intervention I is a tuple ⟨Φ, P, l⟩ that encapsulates a single inference-time modification of the representations computed by a Transformer-based LM. The three components of an intervention are (1) the intervention function Φ ∶Rd → Rd with learned parameters ϕ, (2) a set of input positions P ⊆{1, . . . , n} that the intervention is applied to, and (3) the layer l ∈{1, . . . , m} at which the intervention is applied. We implement the intervention I as the following operation that overwrites some representations h: h(l) ← (Φ (h(l) p ) if p ∈P else h(l) p )p∈1,...,n (7) The intervention is applied immediately after the computation of h(l) and thus affects the representa- tions computed in later layers h(l+1), . . . ,h(m). Figure 2 provides a schematic overview of an intervention. A ReFT is then defined as a constrained set of non-overlapping interventions: Definition 3.2. A ReFT method is a set of f interventions I = {I1, . . . , If }. We enforce that for any two interventions Ij, Ik ∈I such that they operate on the same layer lj =lk, their intervention positions must be disjoint, i.e. Pj ∩Pk =∅. The parameters (ϕ1, . . . , ϕf ) of all of the intervention functions are independent. ReFT is thus a generic framework encompassing interventions on hidden representations during the model forward pass. In appendix B, we show how a variety of existing inference-time intervention methods can be described within this framework. 4 Experiments To evaluate our ReFTs against existing PEFTs, we conduct experiments across four diverse NLP benchmarks covering more than 20 datasets (extensive details on our datasets are in appendix C). Our goal is to provide a rich picture of how LoReFT and DiReFT perform in different scenarios. We experiment with both masked and autoregressive LMs at different scales, ranging from RoBERTa- base [Liu et al., 2019] with 125M to LLaMA models [Touvron et al., 2023a,b] with 13B parameters. We benchmark against existing PEFTs such as prefix-tuning [Li and Liang, 2021], adapter-tuning with both Series Adapters and Parallel Adapters, BitFit [Ben Zaken et al., 2022], RED [Wu et al., 2024a], LoRA [Hu et al., 2022], and DoRA [Liu et al., 2024c]. Our comparisons focus on both performance and parameter efficiency. In our comparisons, we use hyperparameter-tuned scores from previous works when possible. We load our base LMs in torch.bfloat16 to save memory. All of our experiments are run with a single GPU: NVIDIA A100 40G/80G or RTX 6000. Examples of raw model generations are in appendix I. 54.1 Hyperparameter configuration For our experiments, we must decide how many interventions to learn and which layers and input positions to apply each one on. We propose learning interventions on a fixed number of p prefix and s suffix positions in the prompt. Specifically, we tune four hyperparameters: 1. The number of prefix positions p to intervene on, i.e. positions {1, . . . , p}. 2. The number of suffix positions s to intervene on, i.e. positions {n −s +1, . . . , n}. 3. Which set of layers L to intervene on. 4. Whether or not to tie intervention parameters ϕ across different positions in the same layer. This simplifies the hyperparameter search space; compared to LoRA, the only additional consideration is which positions to intervene on. Since the number of positions edited is constant, LoReFT and DiReFT contribute a fixed additional inference cost that does not scale with prompt length. Given the positions P ={1, . . . , p}∪{n −s +1, . . . , n}, we define the untied and tied variants: Iuntied ={⟨Φ, {p}, l⟩ ∣p ∈P, l∈L} Itied ={⟨Φ, P, l⟩ ∣l ∈L} Additionally, when applying LoReFT and DiReFT to a prompt with length n where n <p +s, we set p ← min(p, ⌊n/2⌋) and s ← min(s, ⌈n/2⌉) and do not apply the truncated interventions in Iuntied. We also tune neural-network training hyperparameters. Unlike previous work [Hu et al., 2022, 2023, Liu et al., 2024c] where hyperparameter tuning may involve optimising performance directly on test sets, we only tune our hyperparameters on development sets which do not contain any overlapping examples with the test sets of our tasks. We further describe hyperparameter tuning for each benchmark in appendix D.1. 4.2 Commonsense reasoning We replicate the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B/13B, Llama-2 7B, and Llama-3 8B 5 on a combined dataset of eight commonsense reasoning tasks (COMMONSENSE 170K ). We report scores on each task’s test set individually. We compare with PEFTs benchmarked in Hu et al. [2023] as well as the identical experiment reported in Liu et al. [2024c] for DoRA. Datasets. Our benchmark contains eight commonsense reasoning datasets, including BoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], SIQA [Sap et al., 2019], HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-e, ARC-c [Clark et al., 2018], and OBQA [Mihaylov et al., 2018]. Examples are formulated as multiple-choice problems where the model needs to directly generate the correct choice without rationales. We use the same prompt template as in Hu et al. [2023] with additional string normalisation (removing leading and trailing whitespace). Hyperparameter tuning. We do not do hyperparameter selection based on test set results. Rather, we use the hyperparameter settings of the model that performs best on a development set created from the GSM8K training set, except we use a lower number of epochs (6 instead of 12) because the COMMONSENSE 170K training set is more than 20 times larger than GSM8K . This allows us to tune relevant hyperparamters, and also serves to test the robustness of these settings across different domains. We additionally report scores on 3 epochs in appendix D.3. Results. We report results in table 1. LoReFT sets state-of-the-art performance on the commonsense reasoning tasks, outperforming all other methods by a considerable margin. While being more compute-efficient, DiReFT achieves only slightly worse performance consistently. 4.3 Arithmetic reasoning Similar to the previous experiment, we follow the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B and 13B on a combined dataset of seven arithmetic reasoning tasks with LM-generated chain-of-thought steps (MATH10K) and report scores on four of the tasks’ test sets. We only evaluate correctness on the final numeric or multiple-choice answer. 5Llama-3 8B appeared on April 18, 2024, and thus we had time to complete only commonsense reasoning experiments with this model. Liu et al. [2024c] report corresponding results for LoRA and DoRA. 6Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. ∗Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM. Model PEFT Params (%) Accuracy(↑) BoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg. ChatGPT∗ — — 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B PrefT∗ 0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 AdapterS∗ 1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 AdapterP∗ 3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3LoRA∗ 0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7DoRA (half)∗ 0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5DoRA∗ 0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1 DiReFT (ours) 0.031% 69.5 83.0 79.0 92.5 80.5 82.2 68.0 77.5 79.0LoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2 LLaMA-13B PrefT∗ 0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 AdapterS∗ 1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5 AdapterP∗ 2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5LoRA∗ 0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5DoRA (half)∗ 0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8DoRA∗ 0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 DiReFT (ours) 0.025% 71.3 86.1 80.8 94.6 83.6 85.5 72.9 82.7 82.2LoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3 Llama-2 7B LoRA∗ 0.826% 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6DoRA (half)∗ 0.427% 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5DoRA∗ 0.838% 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 DiReFT (ours) 0.031% 70.8 83.6 80.2 93.6 82.1 84.8 70.4 81.5 80.9LoReFT (ours) 0.031% 71.1 83.8 80.8 94.3 84.5 85.6 72.2 82.3 81.8 Llama-3 8B LoRA∗ 0.700% 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8DoRA (half)∗ 0.361% 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0DoRA∗ 0.710% 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 DiReFT (ours) 0.026% 73.4 88.7 81.0 95.6 85.5 91.8 81.8 85.4 85.4LoReFT (ours) 0.026% 75.1 90.2 82.0 96.3 87.4 92.4 81.6 87.5 86.6 Hyperparameter tuning. We use the same hyperparameter settings as for the Commonsense Rea- soning benchmark, but with 12 epochs for training. We also report scores on 3 epochs. Datasets. Our benchmark contains four datasets for math world problems, including AQuA [Ling et al., 2017], GSM8K [Cobbe et al., 2021], MAWPS [Koncel-Kedziorski et al., 2016], and SV AMP [Pa- tel et al., 2021]. Models need to generate chain-of-thought [Wei et al., 2022] before the final answer. We use the same prompt template and hyperparameter settings as in the previous experiment. Results. We report results in table 2. We find that both LoReFT and DiReFT do not perform as well at arithmetic reasoning tasks compared to LoRA and adapters, but do outperform prefix-tuning. Our results suggest that our ReFTs may have more trouble on chain-of-thought reasoning than the single-step commonsense reasoning tasks due to the length of generations (greater length necessarily reduces the effect of the intervention) and overall greater difficulty of the task. Our results show that our ReFTs perform better with the 13B model than the 7B model, which suggests that our methods scale with model size. Overall, we note that the arithmetic reasoning results show a lot of variation, with no single method emerging as a clear winner across all of them. 4.4 Instruction-following Base LMs require instruction finetuning to follow human prompts [Ouyang et al., 2022]. We follow the experimental setup in Wu et al. [2024a] and finetune Llama-2 7B with Ultrafeedback [Cui et al., 2023]. We compare against full parameter finetuning, LoRA, and RED. For evaluation, we use Alpaca-Eval v1.0 [Li et al., 2023], which computes the win-rate against text-davinci-003 using GPT-4 as the annotator. We use the same prompt template as in Taori et al. [2023]. Datasets. Ultrafeedback is high-quality instruction dataset where responses are generated via scoring a diverse set of model responses from a list of candidates (e.g. ChatGPT and Bard). The score is calculated as a weighted score of instruction-following, truthfulness, honesty, and helpfulness. 7Table 2: Accuracy comparison of LLaMA-1 7B/13B against existing PEFT methods on four arithmetic reasoning datasets. ∗Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B PrefT∗ 0.039% 14.2 24.4 63.4 38.1 35.0 AdapterS∗ 1.953% 15.0 33.3 77.7 52.3 44.6 AdapterP∗ 3.542% 18.1 35.3 82.4 49.6 46.4 LoRA∗ 0.826% 18.9 37.5 79.0 52.1 46.9 DiReFT (ours) 0.031% 21.3 24.1 74.5 42.7 40.6 LoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6 LLaMA-13B PrefT∗ 0.031% 15.7 31.1 66.8 41.4 38.8 AdapterS∗ 1.586% 22.0 44.0 78.6 50.8 48.9 AdapterP∗ 2.894% 20.5 43.3 81.1 55.7 50.2 LoRA∗ 0.670% 18.5 47.5 83.6 54.6 51.1 DiReFT (ours) 0.025% 20.5 35.8 80.8 54.8 48.0 LoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6 Table 3: Instruction tuning evaluation results for instruction-tuned Llama-2 7B with Alpaca-Eval v1.0. We report averaged performance of two runs with distinct random seeds for our method. half denotes our runs with half of the rank; 1K denotes our runs with a low-resource setting where there is only 1K training examples. †Performance results of baseline methods are taken from Li et al. [2023]. ∗Performance results of baseline methods are taken from Wu et al. [2024a]. ‡It takes 18 minutes to train our Llama-2 Chat 7B on 1K examples using a single A100 40G GPU with ≈1MB parameters on disk. Model & PEFT Params (%) Win-rate (↑) GPT-3.5 Turbo 1106† — 86.30 Llama-2 Chat 13B† — 81.10 Llama-2 Chat 7B† — 71.40 Llama-2 7B & FT∗ 100% 80.93 Llama-2 7B & LoRA∗ 0.1245% 81.48 Llama-2 7B & RED∗ 0.0039% 81.69 Llama-2 7B & DiReFT (ours) 0.0039% 84.85 Llama-2 7B & LoReFT (ours) 0.0039% 85.60 Llama-2 7B & LoReFT (ours, half) 0.0019% 84.12 Llama-2 7B & LoReFT (ours, 1K)‡ 0.0039% 81.91 Some of the best 7B and 13B chat-models (e.g. UltraLM-13B [Ding et al., 2023]) are finetuned with Ultrafeedback. Hyperparameter tuning. We do hyperparameter-tuning on the unseen instruction-following dataset Alpaca-52K [Taori et al., 2023] with only LLaMA-7B to prevent test-set hill-climbing. We then use the hyperparameter settings of our best performing model to finetune on Ultrafeedback. For hyperparameter tuning, we use Alpaca-Eval v1.0 with GPT-4 turbo as the annotator for fast turnaround, which also prevents overfitting with GPT-4 as a judge. Results. We report results in table 3. When matched in parameter count to the previous most parameter-efficient PEFT (RED) and trained on Llama-2 7B, LoReFT outperforms all reported finetuning methods (including full finetuning) and achieves a win-rate within 1% of GPT-3.5 Turbo 1106. Furthermore, after halving the parameter count or using only 1/64-th of the data, LoReFT still outperforms other finetuning methods. This result shows that LoReFT can succeed at long-form text generation. DiReFT is again slightly worse than LoReFT but is highly competitive. 6 6We release our ReFT weights (<1MB) of our instruction-tuned model through HuggingFace and provide a tutorial at https://github.com/stanfordnlp/pyreft/blob/main/examples/chat. 8Table 4: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark. ∗Performance results of all baseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. base FT 100% 87.3 94.4 87.9 62.4 92.5 91.7 78.3 90.6 85.6 Adapter∗ 0.318% 87.0 93.3 88.4 60.9 92.5 90.5 76.5 90.5 85.0 LoRA∗ 0.239% 86.6 93.9 88.7 59.7 92.6 90.4 75.3 90.3 84.7 AdapterFNN∗ 0.239% 87.1 93.0 88.8 58.5 92.0 90.2 77.7 90.4 84.7 BitFit∗ 0.080% 84.7 94.0 88.0 54.0 91.0 87.3 69.8 89.5 82.3 RED∗ 0.016% 83.9 93.9 89.2 61.0 90.7 87.2 78.0 90.4 84.3 DiReFT (ours) 0.015% 82.5 92.6 88.3 58.6 91.3 86.4 76.4 89.3 83.2 LoReFT (ours) 0.015% 83.1 93.4 89.2 60.4 91.2 87.4 79.0 90.0 84.2 large FT 100% 88.8 96.0 91.7 68.2 93.8 91.5 85.8 92.6 88.6 Adapter∗ 0.254% 90.1 95.2 90.5 65.4 94.6 91.4 85.3 91.5 88.0 LoRA∗ 0.225% 90.2 96.0 89.8 65.5 94.7 90.7 86.3 91.7 88.1 AdapterFNN∗ 0.225% 90.3 96.1 90.5 64.4 94.3 91.3 84.8 90.2 87.7 RED∗ 0.014% 89.5 96.0 90.3 68.1 93.5 88.8 86.2 91.3 88.0 DiReFT (ours) 0.014% 88.7 95.4 88.5 66.7 93.9 88.1 86.9 91.2 87.4 LoReFT (ours) 0.014% 89.2 96.2 90.1 68.0 94.1 88.5 87.5 91.6 88.2 4.5 Natural language understanding We evaluate LoReFT on the GLUE benchmark [Wang et al., 2018] against existing PEFTs. We use this set of experiments to show LoReFT works well even with small-scale LMs, and can improve representations for classification tasks and not just text generation. We finetune RoBERTa-base (125M) as well as RoBERTa-large (350M) on GLUE, a sequence classification benchmark for natural language understanding (NLU) which covers domains such as sentiment classification and natural language inference. Details about the GLUE benchmark can be found in its original paper. We follow Wu et al. [2024a] for proper evaluation on GLUE validation set: we split the validation set into two sets guarded by a random seed, and we pick the best model with highest in-training validation accuracy to evaluate on the other held-out half for testing accuracy. Hyperparameter tuning. We tune our hyperparameters for each task separately. which is standard for PEFTs. To avoid overfitting to random seeds, we hyperparameter-tune our models with a constant seed, and report averaged results over that and four additional unseen seeds. We describe hyperparameter tuning experiments in Appendix D.1. Results. We report results in table 4. LoReFT obtains comparable performance with PEFT methods on both model sizes when parameter matched with RED, the previous most parameter-efficient PEFT for this task. Furthermore, DiReFT achieves worse performance than most of the PEFTs suggesting LoReFT is a better choice when LM is small. Full results with standard deviation is in table 13. We additionally compare against VeRA [Kopiczko et al., 2024] in appendix D.3. 5 Limitations Due to limited resources, we mainly explored the LLaMA-family of models. In future work, we hope to explore the effectiveness of ReFT on other model families as well as vision–language models such as LLaV A [Liu et al., 2024a]. The capabilities of ReFT have not yet been fully explored due to the large hyperparameter search space; we are interested in automating this search. We provide some initial explorations of LM personalisation with ReFT in a few-shot setting in appendix G.2. We hope to explore why ReFT works, and we provide some of our early explorations focused on memorisation (appendix F.1, appendix F.2). We are also investigating whether learned orthogonal subspaces can be composed together without adaptation. Some encouraging initial findings are in appendix G.1. ReFT, abstraction, and generation. Neural network interpretability research often struggles to contribute directly to improving models. With ReFT, we have shown one way to overcome this challenge. The ReFT framework is rooted in work on causal abstraction [Geiger et al., 2023a] for model interpretability, and LoReFT builds directly on the distributed interchange intervention method 9of Geiger et al. [2023b] and Wu et al. [2023]. See also the interchange intervention training (IIT) method of Geiger et al. [2022], Wu et al. [2022], Huang et al. [2023c]. In a similar vein, recent work also uses representation-based editing of the Transformer stream to steer model behavior [Li et al., 2024, Zou et al., 2023]. ReFT advances this line of work by showing one way that such steering can be learned, rather than being merely a post hoc analysis step. The precise ways in which ReFT works deserve deeper exploration. Although these methods intervene on representations, the causal effect of such interventions may only emerge in the model’s upstream computations. In other words, the power of ReFT may come from the fact that it creates new causal pathways or modifies the strength of some existing ones. We leave it to future research to track these effects, and perhaps to explore more structured ReFTs to modify complex causal pathways in LMs. ReFT and model interpretability. ReFT relies on insights from work on interpretability, and it may also be able to contribute insights back to that field. In particular, LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions (section 4.4). In other words, a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks. This is a serious challenge to work seeing to interpret individual neurons in isolation (for related criticisms, see Huang et al. 2023b). The success of ReFT suggests to us a quite different approach to interperetability, one that starts from the assumption that neurons will play different roles in different contexts. Evaluation practices in PEFT research. In this work, we hyperparameter-tune ReFT on develop- ment sets that do not overlap with the test set. Unfortunately, a considerable portion of the literature on PEFTs directly hill-climbs performance on test sets. This results in overfitting to specific tasks, which gives practitioners less certainty about the real-world performance of different methods and impedes fair comparison. We hope that future work can introduce benchmarks for evaluating PEFTs and ReFTs. These should allow for compute- or time-matched hyperparameter-tuning comparisons, and they should disallow any kind of tuning or model selection based on the test set. 6 Conclusion We propose a strong alternative to PEFTs, LoReFT, and we identify an ablation of this method, DiReFT, that trades some performance for increased efficiency. Overall, LoReFT achieves strong per- formance across benchmarks from four domains while being 15×–65×more efficient than LoRA. No- tably, LoReFT establishes new state-of-the-art performance on commonsense reasoning, instruction- following, and natural language understanding against the strongest PEFTs. We also show how our method can be described under a generic framework – ReFT. ReFT is a new approach to finetuning that is more powerful, more parameter-efficient, and more interpretable than any existing PEFTs. Acknowledgements We thank Jing Huang for helpful discussion in designing our memorisation tests as well as writing. We thank Chenglei Si, Harshit Joshi, Jordan Juravsky, Julie Kallini, Ken Liu, Rohan Pandey, Jiuding Sun, Leonard Tang, Tristan Thrush, Shengguang Wu, Qinan Yu, Yanzhe Zhang, Amir Zur, and Shiqi Chen for helpful discussion about the project and comments on the manuscript. References Afra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. Naturalistic causal probing for morpho-syntax. Transactions of the Association for Computational Linguistics, 11:384–403, 2023. doi: 10.1162/tacl_a_00554. URL https://aclanthology.org/2023.tacl-1.23. Aryaman Arora, Dan Jurafsky, and Christopher Potts. CausalGym: Benchmarking causal inter- pretability methods on linguistic tasks. arXiv:2402.12560, 2024. URL https://arxiv.org/abs/ 2402.12560. Matan Avitan, Ryan Cotterell, Yoav Goldberg, and Shauli Ravfogel. What changed? Converting representational interventions to natural language. arXiv:2402.11355, 2024. URL https://arxiv. org/abs/2402.11355. 10Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. LEACE: Perfect linear concept erasure in closed form. Advances in Neural Information Processing Systems, 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/d066d21c619d0a78c5b557fa3291a8f4-Paper-Conference.pdf. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine- tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1–9, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1. URL https: //aclanthology.org/2022.acl-short.1. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7432–7439, 2020. URL https://arxiv.org/abs/1911.11641. Lewis Carroll. Alice’s Adventures in Wonderland. Macmillan, London, 1865. Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. arXiv:2309.07311, 2023. URL https://arxiv.org/abs/2309.07311v4. Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, and Oskar van der Wal. Identifying and adapting transformer-components responsible for gender bias in an English language model. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya Mc- Carthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Ana- lyzing and Interpreting Neural Networks for NLP, pages 379–394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.29. URL https://aclanthology.org/2023.blackboxnlp-1.29. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, Minneapolis, Min- nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. arXiv:2310.01377, 2023. URL https://arxiv.org/abs/2310.01377. Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems , volume 28. Curran Associates, Inc., 2015. URL https:// proceedings.neurips.cc/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. 11Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3029–3051, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL https://aclanthology.org/2023.emnlp-main.183. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo- sition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/toy_ model/index.html. Stanislav Fort. Scaling laws for adversarial attacks on language model activations, 2023. URL http://arxiv.org/abs/2312.02780. Cheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, and Jishen Zhao. Learn-to-Share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 3469–3479. PMLR, 2021. URL http://proceedings.mlr. press/v139/fu21a.html. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. LLaMA-Adapter v2: Parameter-efficient visual instruction model. arXiv:2304.15010, 2023. URL https://arxiv.org/abs/2304.15010. Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9574–9586. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 4f5c422f4d49a5a807eda27434231040-Paper.pdf. Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good- man, and Christopher Potts. Inducing causal structure for interpretable neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7324–7338. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/geiger22a.html. Atticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation. arXiv:2301.04709, 2023a. URL https://arxiv.org/abs/2301.04709. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. Find- ing alignments between interpretable causal variables and distributed neural representations. arXiv:2303.02536, 2023b. URL https://arxiv.org/abs/2303.02536. Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, and Ryan Cotterell. A geometric notion of causal probing. arXiv:2307.15054, 2023. URL https://arxiv.org/abs/2307.15054. Michael Hanna, Yonatan Belinkov, and Sandro Pezzelle. When language models fall in love: Animacy processing in transformer language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 12120–12135, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.744. URL https://aclanthology.org/2023.emnlp-main.744. Sophie Hao and Tal Linzen. Verb conjugation in transformers is determined by linear encodings of subject number. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 4531–4539, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.300. URL https://aclanthology.org/2023.findings-emnlp.300. 12Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, 2022a. URL https://openreview.net/ forum?id=0RDcd5Axok. Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 2184–2190, Abu Dhabi, United Arab Emirates, December 2022b. As- sociation for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.160. URL https://aclanthology.org/2022.findings-emnlp.160. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/ D14-1058. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In The Tenth In- ternational Conference on Learning Representations, ICLR 2022 , Virtual Event, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5254–5276, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 319. URL https://aclanthology.org/2023.emnlp-main.319. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. LoraHub: Efficient cross-task generalization via dynamic lora composition. arXiv:2307.13269, 2023a. URL https://arxiv.org/abs/2307.13269. Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously assessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 317–331, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023. blackboxnlp-1.24. URL https://aclanthology.org/2023.blackboxnlp-1.24. Jing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. Inducing character-level structure in subword-based language models with type-level interchange intervention training. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 12163–12180, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.770. URL https: //aclanthology.org/2023.findings-acl.770. Jing Huang, Christopher Potts Zhengxuan Wu, Mor Geva, and Atticus Geiger. RA VEL: Evaluating interpretability methods on disentangling language model representations. arXiv:2402.17700, 2024. URL https://arxiv.org/abs/2402.17700. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations.Transactions of the Association for Computational 13Linguistics, 3:585–597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/ Q15-1042. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, pages 1152–1157, San Diego, Califor- nia, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136. Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. URL https://openreview.net/forum?id=NjNfLdxr3A. Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. Prob- ing for the usage of grammatical number. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 8818–8831, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.603. URL https://aclanthology.org/2022.acl-long.603. Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, 2012. URL https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf . Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/ hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html. Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv:2208.03306, 2022. URL https://arxiv.org/abs/2208.03306. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener- ation: Learning to solve and explain algebraic word problems. arXiv:1705.04146, 2017. URL https://arxiv.org/abs/1705.04146. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2024a. URL https://arxiv.org/abs/2304.08485. Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv:2311.06668, 2024b. URL https://arxiv.org/abs/2311.06668. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation. arXiv:2402.09353, 2024c. URL https://arxiv.org/abs/2402.09353. Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bern- hard Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The 14Twelfth International Conference on Learning Representations, ICLR 2024 , 2024d. URL https://openreview.net/forum?id=7NzgkEdGyr. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:1907.11692, 2019. URL https://arxiv.org/abs/1907.11692. James L. McClelland, David E. Rumelhart, and PDP Research Group.Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models. MIT Press, 1986. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. arXiv:1809.02789, 2018. URL https://arxiv.org/abs/1809.02789. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL https://aclanthology.org/N13-1090. Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 16–30, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https: //aclanthology.org/2023.blackboxnlp-1.2. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35: 27730–27744, 2022. URL https://arxiv.org/abs/2203.02155. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv:2311.03658, 2023. URL https://arxiv.org/abs/2311.03658. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve sim- ple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080–2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168. Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.617. URL https://aclanthology. org/2020.emnlp-main.617. Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D. Cotterell. Linear adversarial concept erasure. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18400–18421, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ravfogel22a.html. Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743–1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/ D15-1202. 15David E. Rumelhart, James L. McClelland, and PDP Research Group.Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations. MIT Press, 1986. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial Winograd Schema Challenge at scale. Communications of the ACM, 64(9):99–106, 2021. URL https://arxiv.org/abs/1907.10641. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common- sense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454. Shuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. ResLoRA: Identity residual mapping in low-rank adaption. arXiv:2402.18039, 2024. URL https://arxiv.org/abs/2402.18039. Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnu- rangam Kumaraguru. MiMiC: Minimally modified counterfactuals in the representation space. arXiv:2402.09631, 2024. URL https://arxiv.org/abs/2402.09631. Paul Smolensky. Neural and conceptual interpretation of PDP models. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition , volume 2: Psychological and Biological Models, pages 390–431. MIT Press/Bradford Books, Cambridge, MA, 1986. Nishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering vectors from pretrained language models. arXiv:2205.05124, 2022. URL https://arxiv.org/abs/2205.05124. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv:2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris- tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. URL https://arxiv.org/abs/2307.09288. Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv:2308.10248, 2023. URL https://arxiv.org/abs/2308.10248. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . Theia V ogel. repeng, 2024. URLhttps://github.com/vgel/repeng/. 16Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, ICLR 2023 , Kigali, Rwanda, 2023. URL https://openreview.net/pdf?id=NpsVSN6o4ul. Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5744–5760, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.388. URL https://aclanthology.org/2022.emnlp-main.388. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. URL https://arxiv.org/abs/ 2201.11903. Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine- tuning via representation editing. arXiv:2402.15179, 2024a. URL https://arxiv.org/abs/2402. 15179. Zhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christo- pher Potts, and Noah Goodman. Causal distillation for language models. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies , pages 4288–4295, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.318. URL https://aclanthology.org/2022.naacl-main.318. Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information Processing Systems, volume 36, 2023. URL https://papers.neurips.cc/paper_files/paper/2023/file/ f6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf. Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, and Christopher Potts. pyvene: A library for understanding and improving PyTorch models via interventions. In arXiv:2403.07809, 2024b. URL https://arxiv.org/abs/ 2403.07809. Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and Noah D. Goodman. A reply to Makelov et al. (2023)’s “interpretability illusion” arguments. arXiv:2401.12631, 2024c. URL https://arxiv.org/abs/2401.12631. Takateru Yamakoshi, James McClelland, Adele Goldberg, and Robert Hawkins. Causal in- terventions expose implicit situation models for commonsense language understanding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 13265–13293, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.839. URL https://aclanthology.org/2023.findings-acl.839. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? arXiv:1905.07830, 2019. URL https://arxiv.org/abs/1905.07830. 17Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian. IncreLoRA: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv:2308.12043, 2023. URL https://arxiv.org/abs/2308.12043. Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operation. Advances in Neural Information Processing Systems, 36, 2024a. URL https://arxiv.org/abs/2306.14870. Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. LLaMA-Adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The Twelfth International Conference on Learning Representations, Vienna, Austria, 2024b. URL https://openreview.net/forum?id=d4UiXAHN2W. Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie. AutoLoRA: Automatically tuning matrix ranks in low-rank adaptation based on meta learning. arXiv:2403.09113, 2024c. URL https://arxiv.org/abs/2403.09113. Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-LoRA composition for image generation. arXiv:2402.16843, 2024. URL https://arxiv.org/abs/2402.16843. Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Ca- noee Liu, Liangchen Luo, Jindong Chen, et al. SiRa: Sparse mixture of low rank adaptation. arXiv:2311.09179, 2023. URL https://arxiv.org/abs/2311.09179. Zhou Ziheng, Yingnian Wu, Song-Chun Zhu, and Demetri Terzopoulos. Aligner: One global token is worth millions of parameters when aligning large language models. arXiv:2312.05503, 2023. URL https://arxiv.org/abs/2312.05503. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to AI transparency. arXiv:2310.01405, 2023. URL https://arxiv.org/abs/2310.01405. 18Appendix Table of Contents A pyreft: A ReFT-native Python Library 20 B Describing existing methods under the ReFT framework 20 B.1 RED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Activation addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 RepE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C Datasets 21 C.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.3 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 D Hyperparameters 23 D.1 Hyperparameter tuning and decoding strategy . . . . . . . . . . . . . . . . . . . . 23 D.2 Suggestions on choosing hyperparameters for ReFT . . . . . . . . . . . . . . . . . 29 D.3 Additional hyperparameter-tuning results of LoReFT . . . . . . . . . . . . . . . . 29 E Ablating the parametrisation of LoReFT 32 F Memorisation experiments 33 F.1 A single vector is worth a thousand tokens . . . . . . . . . . . . . . . . . . . . . . 33 F.2 A single vector can memorise a codebook with 256 entries . . . . . . . . . . . . . 35 G Capabilities experiments 36 G.1 Multi-task learning: Learned ReFTs are like puzzle pieces . . . . . . . . . . . . . 36 G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples . . 38 H Inference overhead analysis of ReFT with our ReFT library 39 I Generation examples 41 J Licenses for existing assets 49 J.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.3 Instruct-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.4 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.5 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 19A pyreft: A ReFT-native Python Library To lower the cost of switching from PEFTs to ReFT, we release a Python library made for training and sharing ReFTs. Our library is built on top of pyvene [Wu et al., 2024b], a library for performing and training activation interventions on arbitrary PyTorch models. Any pretrained LM available on HuggingFace is supported through our library for finetuning with ReFT methods, and finetuned models can be easily uploaded to HuggingFace. The following example shows steps to wrap a Llama-2 7B model with a single intervention on the residual stream output of the 19-th layer: import torch import transformers from pyreft import get_reft_model , ReftConfig , LoreftIntervention , ReftTrainerForCausalLM # loading huggingface model model_name_or_path = \" yahma / llama -7b-hf\" model = transformers . AutoModelForCausalLM . from_pretrained ( model_name_or_path , torch_dtype = torch . bfloat16 , device_map =\" cuda \") # wrap the model with rank -1 loreft reft_config = ReftConfig ( representations ={ \" layer \": 19 , \" component \": \" block_output \", \" intervention \": LoreftIntervention ( embed_dim = model . config . hidden_size , low_rank_dimension =1) }) reft_model = get_reft_model ( model , reft_config ) reft_model . print_trainable_parameters () The wrapped model can be trained for downstream tasks. We also provide data loading helpers to construct training data that is compatible with HuggingFace trainers: tokenizer = transformers . AutoTokenizer . from_pretrained ( model_name_or_path ) # get training data with customised dataloaders data_module = make_supervised_data_module ( tokenizer = tokenizer , model = model , layers =[19] , training_args = training_args , data_args = data_args ) # train trainer = reft . ReftTrainerForCausalLM ( model = reft_model , tokenizer = tokenizer , args = training_args , ** data_module ) trainer . train () trainer . save_model ( output_dir = training_args . output_dir ) B Describing existing methods under the ReFT framework To show the expressivity of the ReFT framework, we cast existing representing-editing methods in the literature into ReFTs. General comments about expressivity of ReFT. Given that previous works have unified PEFTs under a single framework [He et al., 2022a], one may ask why not express ReFT as a PEFT method? The main reason is that PEFT frameworks lack the notion of time or sequence (see the unified PEFT view provided in Table 1 on pg. 5 of He et al., 2022a). In PEFTs, representation modifications are necessarily applied to every token in the sequence, even in recent variants such as AdaLoRA [Zhang et al., 2023]. A key aspect of ReFT is that it leverages representations over time and intervenes only on a small number of them while being effective. More importantly, the notation of time is important for future versions of ReFT that intervene on representations schematically (e.g. intervene on the first token at some early layers and then intervene on the last token at some later layers). The ability to intervene at different layer and position combinations schematically is also supported in our code. Existing PEFT libraries 7 enforce weight-based updates without supporting flexible representation-based interventions. B.1 RED RED [Wu et al., 2024a] is a simple representation-editing method that applies an element-wise scaling transform s ∈ Rn and adds a bias b ∈ Rn to the hidden representation in every layer. The same intervention is applied to every position (including at generated tokens, increasing inference burden) 7See https://github.com/huggingface/peft. 20but separate interventions are learned at each layer. In the ReFT framework, RED is defined as ΦRED(h) =s ×h +b (8) IRED ={⟨ΦRED, {1, . . . , n}, l⟩ ∣l ∈{1, . . . , m}} (9) The parameters ϕRED ={s, b} are learned with gradient descent to minimise a loss function such as language-modelling loss or a classification loss, as in our experiments with LoReFT. We believe that RED is better classified as a kind of adapter due to its application at all positions. B.2 Activation addition Activation addition [Turner et al., 2023] takes the difference in activations at at some positionsp and q and layer l given two contrastive prompts x+ and x− as input. It then adds this difference vector, scaled by a tuned constant c, to representations at all positions in layer l for some new prompt. a =h(x+)(l) p −h(x−)(l) q (10) ΦActAdd(h) =h +c ⋅a (11) IActAdd ={⟨ϕActAdd, {1, . . . , n}, l⟩} (12) B.3 RepE Zou et al. [2023] introduce several intervention methods for controlling model behaviour, which they term representation engineering. First, given a set of prompts {x1, . . . ,xn} designed to elicit the presence of a concept, we randomly pair them, take the difference in activations for each pair, and find the first principle component of the difference vectors at the last token position in some layer of interest l to obtain a reading vector: areading =PCA({h(xi)(l) −1 −h(xi+1)(l) −1 ∣ i ≡0 mod 2}) 1 (13) One can also used a more structured pairing of constrastive prompts to obtain a contrast vector, similar to the difference vector computed in activation addition: acontrast =PCA({h(x+ i )(l) −1 −h(x− i )(l) −1 ∣ 1 ≤i ≤n}) 1 (14) Then, using either areading or acontrast, RepE introduces three operators (i.e. parametrisations of Φ) for intervening on activations: ΦRepE,linear(h) =h ±c ⋅a (15) ΦRepE,piecewise(h) =h +c ⋅sign(a ⋅h)⋅a (16) ΦRepE,projection(h) =h −c ⋅ a ⋅h ∥a∥2 ⋅a (17) The first two of these are similar to activation addition, while the latter is a scaled one-dimensional distributed interchange intervention that is a special case of LoReFT. These operations are then used to intervene on some set of positions P ⊆{1, . . . , n} in the layer of interest: IRepE ={⟨ΦRepE, P, l⟩} (18) RepE introduces another model control method called Low-Rank Representation Adaptation (LoRRA), which is a kind of PEFT rather than a ReFT since it tunes model weights using a variant of LoRA. C Datasets C.1 Commonsense reasoning We train and evaluate our models on eight datasets covering different domains of open-ended QA tasks: 1. The BoolQ [Clark et al., 2019] dataset, which is a question-answering dataset for yes or no naturally occurring questions. We remove the provided passage in the dataset following previous works to ensure a fair comparison. 212. The PIQA [Bisk et al., 2020] dataset, which tests physical commonsense reasoning and requires the model to choose one of the provided actions to take based on a hypothesised scenario. 3. The SIQA [Sap et al., 2019] dataset, which focus on reasoning about people’s actions and their corresponding social consequences. 4. The HellaSwag [Zellers et al., 2019] dataset, which asks the model to choose an appropriate ending (or sentence completion) given a context. 5. The WinoGrande [Sakaguchi et al., 2021] dataset, inspired by Winograd Schema Chal- lenge [Levesque et al., 2012], asks the model to fill-in-a-blank with binary options given a sentence which requires commonsense reasoning. 6. The ARC Easy set (ARC-e [Clark et al., 2018]), which includes genuine grade-school level multiple-choice science questions 7. The ARC Challenge set (ARC-c) [Clark et al., 2018]), which is like ARC-e but designed in a way that co-occurrence methods are expected to fail to answer correctly. 8. The OBQA [Mihaylov et al., 2018] dataset, which is a knowledge-intensive and open-book QA dataset that requires multi-hop reasoning. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. We replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training dataset (COMMONSENSE 170K ) of the tasks mentioned above, and evaluate on their individual test set. C.2 Arithmetic reasoning We train and evaluate with seven datasets covering different domains of math world problems: 1. The AddSub [Hosseini et al., 2014] dataset, which involves solving arithmetic word prob- lems that include addition and subtraction. 2. The AQuA [Ling et al., 2017] dataset, which formulates algebraic word problems as multiple-choice problems. 3. The GSM8K [Cobbe et al., 2021] dataset, which consists of grade-school math word problems that require multi-step reasoning. 4. The MA WPS[Koncel-Kedziorski et al., 2016] dataset, which contains math word problem with varying complexity. 5. The MultiArith [Roy and Roth, 2015] dataset, which contains multi-step arithmetic prob- lems. 6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset, which has grade-school math word problems that map to single equations with different length. 7. The SV AMP[Patel et al., 2021] dataset, which enhances the original Math World Prob- lem (MWP) challenge by requiring robust reasoning ability that is invariant to structural alternations of the posing problem. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. We replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training dataset (MATH10K ) of four tasks mentioned above: GSM8K, MAWPS, MAWPS-single and AQuA. Different from Hu et al. [2023], selected tasks are excluded for testing since the original paper accidentally leaks testing examples from these tasks into the training set, affecting AddSub, MultiArith and SingleEq. They are included in the MAWPS training dataset, and thus leaked into the training dataset. C.3 Natural language understanding We follow Wu et al. [2024a] for proper evaluation on the GLUE validation set. We split the validation set into two subsets, using one subset guarded by a random seed for in-training evaluation and the other for testing. Specifically, after each training epoch, we evaluate the model on our in-training 22evaluation set and select the best model across all epochs for testing. For datasets with a large validation set (i.e., QQP, MNLI, and QNLI), we select 1,000 samples for in-training evaluation. For the remaining smaller datasets, we select half of the samples for this purpose. For the evaluation metric, we use the Matthews correlation coefficient for CoLA, the Pearson correlation coefficient for STS-B, and accuracy for the other datasets. For MNLI, we report results only on the matched version. D Hyperparameters D.1 Hyperparameter tuning and decoding strategy Commonsense reasoning and arithmeric reasoning. We create a standalone development set by taking the last 300 examples from the GSM8K training set. We train our models with the remaining training set of GSM8K and select the hyperparameter settings based on model performance on the development set. We select the hyperparameters using LLaMA-7B, and apply the same settings to LLaMA-13B without additional tuning. We use a maximum sequence length of 512 for training and hyperparameter tuning, and a maximum new token number of 32 for inference. Table 5 and table 6 describes our hyperparameter search space. We use a lower number of epochs (6 instead of 12) for the commonsense reasoning benchmark because the COMMONSENSE 170K training set is more than 20 times larger than GSM8K. During inference, we use greedy decoding without sampling for the commonsense reasoning bench- mark, since it is a multi-token classification benchmark, and use the same decoding strategy as in Hu et al. [2023] for the arithmetic reasoning benchmark with a higher temperature 0.3. The reason to switch to a slightly different set of decoding hyperparameters is that the HuggingFace decoding function may throw an error due to statistical instability with close-to-zero probabilities over output tokens with beam search.8 Instruction following. We finetune LLaMA-7B on Alpaca-52K [Taori et al., 2023] to select hy- perparameters. We select the hyperparameter settings based on model performance evaluated with Alpaca-Eval v1.0 [Li et al., 2023], which calculates the win-rate over text-davinci-003 by using gpt-4-turbo as the annotator. We use a maximum sequence length of 768 for training and hyper- parameter tuning, and a maximum new token number of 2048 for inference. Table 7 describes our hyperparameter search space. During inference, we use the same decoding strategy as in RED [Wu et al., 2024a] to ensure a fair comparison. Specifically, we use greedy decoding without sampling, and use a maximum repetition n-gram size of 5 with a repetition penalty of 1.1. Natural language understanding. We conduct hyperparameter tuning with RoBERTa-base and RoBERTa-large for each task individually. We pick the hyperparameters based on testing performance on the held-out validation set with a fixed random seed of 42. We then evaluate our model with additional four unseen seeds {43, 44, 45, 46} for final results. We follow Wu et al. [2024a]’s setting for evaluation. For QQP with RoBERTa-large, there are some stochasticity in runs with the same seed, so we picked the best run out of 3 runs for any particular seed. As reported by Wu et al. [2024a], we also observe that evaluation results on RTE are unstable due to the small size of the dataset. We thus replace several random seeds as in Wu et al. [2024a] to ensure a fair comparison. In addition, we replace one or two random seeds for CoLA for stability. Table 8 describes our hyperparameter search space. Table 9 to table 12 describe our hyperparameter settings for each task. We conduct separate hyperparameter tuning for LoReFT and DiReFT to ensure a fair comparison. 8See reference ticket: https://github.com/huggingface/transformers/issues/11267. 23Table 5: Hyperparameter search space of LLaMA-1 7B models with LoReFT on the GSM8K development set with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. Hyperparameters LLaMA-7B w/ GSM8K for LoReFT prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7, p9+s9, p11+s11} Tied weight p, s {True, False} Rank r {8, 16, 32, 64} Layer L (sep. w/ ‘;’) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22, all } Dropout {0.00, 0.05 } Optimizer AdamW LR {9 ×10−5, 1×10−4, 3×10−4, 6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0 , 1×10−3, 2×10−3} LR scheduler Linear Batch size {4, 8, 16, 32 , 64} Warmup ratio {0.00, 0.06, 0.10 } Epochs {3, 6, 9, 12 , 18} Table 6: Hyperparameter search space of LLaMA-1 7B models with DiReFT on the GSM8K development set with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. Hyperparameters LLaMA-7B w/ GSM8K for DiReFT prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7, p9+s9, p11+s11} Tied weight p, s {True, False} Rank r {8, 16, 32, 64} Layer L (sep. w/ ‘;’) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22 , all} Dropout {0.00, 0.05 } Optimizer AdamW LR {9 ×10−5, 1×10−4, 3×10−4, 6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0, 1 ×10−3, 2×10−3, 6×10−3, 1×10−2, 2×10−2, 6×10−2} LR scheduler Linear Batch size {4, 8 , 16, 32, 64} Warmup ratio {0.00, 0.06 , 0.10} Epochs {3, 6 , 9, 12, 18} 24Table 7: Hyperparameter search space of LLaMA-1 7B models on Alpaca-52K evaluated by Alpaca- Eval v1.0 with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. LoReFT and DiReFT have the same hyperparameter settings. Hyperparameters LLaMA-7B w/ Alpaca-52K prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7} Tied weight p, s {True, False} Rank r {1, 2, 3, 4, 5, 6} Layer L (sep. w/ ‘;’) {9;18, 3;9;18, 3;9;18;24 } Dropout {0.00, 0.05 } Optimizer AdamW LR 9 ×10−4 Weight decay 0 ×10−3 LR scheduler Linear Batch size {16, 32, 64, 128 } Warmup ratio 0.00 Epochs {1, 3, 6, 9, 12 } Table 8: Hyperparameter search space of RoBERTa-base and RoBERTa-large models on GLUE evaluated with classification accuracy. Best hyperparameter settings are task-specific, which are specified in separate tables. Hyperparameters RoBERTa-base and RoBERTa-large w/ GLUE prefix+suffix position p + s {p1, p3, p5, p7, p9, p11} Tied weight p, s False Rank r {1, 2} Layer L (sep. w/ ‘;’) {1;3;5;7;9;11, all} Dropout {0.00, 0.05, 0.10, 0.15, 0.20} Optimizer AdamW LR {1 ×10−4, 2×10−4, 3×10−4, 4×10−4, 5×10−4}, {6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0, 1 ×10−4, 6×10−4, 1×10−3, 6×10−3, 1×10−2, 2×10−2, 4×10−2} LR scheduler Linear Batch size {16, 32, 64, 128} Warmup ratio {0, 5 ×10−3, 6×10−3, 3×10−2, 5×10−2, 6×10−2, 1×10−1, 2×10−1} Epochs {20, 30, 40, 50, 60} 25Table 9: Hyperparameter settings of RoBERTa-base models on GLUE for LoReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p3 p3 p11 p11 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.10 0.05 0.20 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 4×10−4 9×10−4 6×10−4 9×10−4 6×10−4 Weight decay 0.00 LR scheduler Linear Batch size 32 Warmup ratio 6 ×10−2 1×10−1 0 5 ×10−3 1×10−1 0 0 3 ×10−2 Epochs 40 40 40 60 20 40 60 60 Table 10: Hyperparameter settings of RoBERTa-large models on GLUE for LoReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p3 p3 p11 p11 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.05 0.20 0.20 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 1×10−4 9×10−4 6×10−4 6×10−4 8×10−4 Weight decay 0.00 LR scheduler Linear Batch size 32 Warmup ratio 0.00 0.10 0.06 0.20 0.10 0.06 0.00 0.20 Epochs 20 20 30 30 20 20 30 30 26Table 11: Hyperparameter settings of RoBERTa-base models on GLUE for DiReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p5 p1 p11 p11 p1 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.10 0.05 0.00 0.05 0.05 0.00 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 6×10−4 9×10−4 6×10−4 9×10−4 6×10−4 Weight decay 0.00 0.00 0.00 0.04 0.00 0.00 0.04 0.00 LR scheduler Linear Batch size 32 32 32 32 32 32 8 32 Warmup ratio 6 ×10−2 1×10−1 1×10−1 0 1 ×10−1 0 0 3 ×10−2 Epochs 40 40 40 60 20 40 60 60 Table 12: Hyperparameter settings of RoBERTa-large models on GLUE for DiReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p1 p1 p11 p7 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.05 0.10 0.15 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 9×10−4 9×10−4 9×10−4 9×10−4 6×10−4 8×10−4 Weight decay 0 0 0 0 0 0 6 ×10−3 0 LR scheduler Linear Batch size 32 Warmup ratio 0.00 0.10 0.00 0.00 0.10 0.10 0.00 0.10 Epochs 20 20 50 60 20 20 30 30 27Table 13: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark with standard deviation (SD) . ∗Performance results of all baseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs with distinct random seeds for our method. Param. (%) is calculated by dividing the number of trainable parameters (excluding the number of parameters of the classification head) with the number of parameter of the base LM. Model PEFT Params (%) Accuracy(↑) (SD) MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. base FT 100% 87.3 (0.34) 94.4(0.96) 87.9(0.91) 62.4(3.29) 92.5(0.22) 91.7(0.19) 78.3(3.20) 90.6(0.59) 85.6 Adapter∗ 0.318% 87.0(0.28) 93.3(0.40) 88.4(1.54) 60.9(3.09) 92.5(0.02) 90.5(0.08) 76.5(2.26) 90.5(0.35) 85.0LoRA∗ 0.239% 86.6(0.23) 93.9(0.49) 88.7(0.76) 59.7(4.36) 92.6(0.10) 90.4(0.08) 75.3(2.79) 90.3(0.54) 84.7AdapterFNN∗ 0.239%87.1(0.10) 93.0(0.05) 88.8(1.38) 58.5(1.69) 92.0(0.28) 90.2(0.07) 77.7(1.93) 90.4(0.31) 84.7BitFit∗ 0.080% 84.7(0.08) 94.0(0.87) 88.1(1.57) 54.0(3.07) 91.0(0.05) 87.3(0.02) 69.8(1.51) 89.5(0.35) 82.3RED∗ 0.016% 83.9(0.14) 93.9(0.31) 89.2(0.98) 61.0(2.96) 90.7(0.35) 87.2(0.17) 78.0(2.06) 90.4(0.32) 84.3 DiReFT (ours)0.015% 82.5(0.22) 92.6(0.76) 88.3(1.23) 58.6(1.99) 91.3(0.19) 86.4(0.27) 76.4(1.48) 89.3(0.56) 83.2LoReFT (ours)0.015% 83.1(0.26) 93.4(0.64) 89.2(2.62) 60.4(2.60) 91.2(0.25) 87.4(0.23) 79.0(2.76) 90.0(0.29) 84.2 large FT 100% 88.8 (0.45) 96.0(0.66) 91.7(1.73) 68.2(2.62) 93.8(0.33) 91.5(1.28) 85.8(1.40) 92.6(0.16) 88.6 Adapter∗ 0.254% 90.1(0.12) 95.2(0.48) 90.5(0.59) 65.4(2.24) 94.6(0.17) 91.4(0.13) 85.3(1.34) 91.5(0.33) 88.0LoRA∗ 0.225% 90.2(0.25) 96.0(0.85) 89.8(2.09) 65.5(2.02) 94.7(0.21) 90.7(0.91) 86.3(2.41) 91.7(0.44) 88.1AdapterFNN∗ 0.225%90.3(0.15) 96.1(0.75) 90.5(1.26) 64.4(1.56) 94.3(0.39) 91.3(0.24) 84.8(2.01) 90.2(0.24) 87.7RED∗ 0.014% 89.5(0.38) 96.0(0.48) 90.3(1.40) 68.1(1.69) 93.5(0.33) 88.8(0.11) 86.2(1.40) 91.3(0.21) 88.0 DiReFT (ours)0.014% 88.7(0.13) 95.4(0.60) 88.5(2.16) 66.7(2.21) 93.9(0.39) 88.1(0.47) 86.9(1.56) 91.2(0.29) 87.4LoReFT (ours)0.014% 89.2(0.27) 96.2(0.72) 90.1(1.17) 68.0(1.44) 94.1(0.35) 88.5(0.45) 87.5(1.49) 91.6(0.43) 88.2 28D.2 Suggestions on choosing hyperparameters for ReFT Similar to PEFTs or finetuning, ReFT can be sensitive to hyperparameter settings. Here, we recom- mand a non-exhaustive list for choosing the best hyperparameter settings for your tasks: • Intervening on multiple positions delivers significant gains. We find that intervening only on a single token position (e.g., just the first one or the last one) is always less optimal than intervening on multiple tokens. However, intervening on excessive number of tokens might harm performance by slowing down convergence. • Intervening on all layers first, and then shrink down . Intervening on all layers often provides a good baseline. We recommand users to start with all layers, and shrink down the number of intervening layers depending on the desired performance–parameter count balance. • Higher rank may not entail better performance . High rank entails higher parameter count, but it does not always bring performance gain (likely due to slower convergence). We recommend users to start with a rank that is lower than 32 (e.g. rank 4). • Tie intervention weights as much as you can. In the paper, we explore tying the interven- tion weights between prefix and suffix token positions. It automatically halves the parameter count, and it can result in better performance as well. We suspect weight sharing across layers may also help. • Hyperparameter tuning with learning rate, warmup ratio, dropout rate and weight decay should go after other hyperparameters . These classic neural-network training hyperparameters can play a role, yet they have much smaller effect than previous ones. D.3 Additional hyperparameter-tuning results of LoReFT As a result of our hyperparameter searching process, LoReFT is trained with more epochs compared to LoRA [Hu et al., 2022] or DoRA [Liu et al., 2024c]. This raises the concern whether our performance gain is purely due to the larger number of epochs. We thus rerun our experiments with the exact same number of epochs and effective batch size as LoRA or DoRA. Results are shown in table 14 and table 15. With matched hyperparameters, LoReFT shows similar results by outperforming previous methods significantly on eight commonsense reasoning datasets. Recently, VeRA was proposed as a new variant of LoRA that further reduces the number of trainable parameters while maintaining performance [Kopiczko et al., 2024]. Table 16 shows our results compared against VeRA as well as the baseline numbers reported in VeRA’s paper. We include this set of results in the appendix, given that the hyperparameter tuning process is drastically different from ours.9 The original VeRA implementation records the performance of the best epoch on the validation set, which could cause overfitting since results are selected based on test set performance. 9VeRA’s original implementation can be found athttps://openreview.net/notes/edits/attachment? id=D0dcbrnPq0&name=supplementary_material. 29Table 14: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on eight commonsense reasoning datasets. ∗Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For LoReFT, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM. We include LoReFTe=3, which is trained with 3 epochs — the same number of epochs as DoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets. Model PEFT Params (%) Accuracy(↑) BoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg. ChatGPT∗ — — 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B PrefT∗ 0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 AdapterS∗ 1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 AdapterP∗ 3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3LoRA∗ 0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7DoRA (half)∗ 0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5DoRA∗ 0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1 LoReFTe=3 0.031% 68.3 83.5 79.7 92.7 82.6 83.2 67.4 78.5 79.5LoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2 LLaMA-13B PrefT∗ 0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 AdapterS∗ 1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5 AdapterP∗ 2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5LoRA∗ 0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5DoRA (half)∗ 0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8DoRA∗ 0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LoReFTe=3 0.025% 72.0 85.6 82.1 94.8 85.3 86.9 73.0 85.0 83.1LoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3 Table 15: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on four arithmetic reasoning datasets. ∗Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method. We include LoReFTe=3, which is trained with 3 epochs — the same number of epoch as DoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets. Model PEFT Params (%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B PrefT∗ 0.039% 14.2 24.4 63.4 38.1 35.0 AdapterS∗ 1.953% 15.0 33.3 77.7 52.3 44.6 AdapterP∗ 3.542% 18.1 35.3 82.4 49.6 46.4 LoRA∗ 0.826% 18.9 37.5 79.0 52.1 46.9 LoReFTe=3 0.031% 22.4 21.6 69.5 43.6 39.3 LoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6 LLaMA-13B PrefT∗ 0.031% 15.7 31.1 66.8 41.4 38.8 AdapterS∗ 1.586% 22.0 44.0 78.6 50.8 48.9 AdapterP∗ 2.894% 20.5 43.3 81.1 55.7 50.2 LoRA∗ 0.670% 18.5 47.5 83.6 54.6 51.1 LoReFTe=3 0.025% 23.4 35.5 81.8 54.6 48.8 LoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6 30Table 16: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark. ∗Performance results of all baseline methods are taken from Kopiczko et al. [2024]. To ensure a fair comparison, we report median performance of five runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) SST-2 MRPC CoLA QNLI RTE STS-B Avg. base FT 100% 94.8 90.2 63.6 92.8 78.7 91.2 85.2 BitFit 0.080% 93.7 92.7 62.0 91.8 81.5 90.8 85.4 AdptD 0.239% 94.2 88.5 60.8 93.1 71.5 89.7 83.0 AdptD 0.717% 94.7 88.4 62.6 93.0 75.9 90.3 84.2 LoRA 0.239% 95.1 89.7 63.4 93.3 86.6 91.5 86.6 VeRA 0.034% 94.6 89.5 65.6 91.8 78.8 90.7 85.2 DiReFT (ours) 0.015% 92.2 88.7 59.5 91.3 77.0 89.6 83.0 LoReFT (ours) 0.015% 93.6 87.8 59.1 91.3 79.9 90.0 83.6 base AdptP 0.845% 96.1 90.2 68.3 94.8 83.8 92.1 87.6 AdptP 0.225% 96.6 89.7 67.8 94.8 80.1 91.9 86.8 AdptH 1.690% 96.2 88.7 66.5 94.7 83.4 91.0 86.8 AdptH 0.225% 96.3 87.7 66.3 94.7 72.9 91.5 84.9 LoRA-FA 1.042% 96.0 90.0 68.0 94.4 86.1 92.0 87.8 LoRA 0.225% 96.2 90.2 68.2 94.8 85.2 92.3 87.8 VeRA 0.017% 96.1 90.9 68.0 94.4 85.9 91.7 87.8 DiReFT (ours) 0.014% 95.2 88.2 66.7 94.0 86.3 91.0 86.9 LoReFT (ours) 0.014% 96.1 90.2 68.2 94.1 87.8 91.5 88.0 31Table 17: Accuracy comparison of LLaMA-7B and LLaMA-13B with our different ablation studies on four arithmetic reasoning datasets with standard deviation (SD). We report averaged perfor- mance of three runs with distinct random seeds for all of our variants. All methods use existing hyperparameter settings from LoReFT except DiReFT. Model Φ(h) Params(%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B h+R⊺b 0.016% 14.4 14.2 59.9 36.8 31.3 (0.47) h+R⊺(b−Rh) 0.016% 20.1 21.2 67.9 39.2 37.1 (0.19) h+R⊺(Wh+b) 0.031% 21.3 27.4 76.6 46.3 42.9 (0.37) h+W⊺2(W1h+b−W2h) 0.031% 23.1 25.5 75.4 45.6 42.4 (0.71) DiReFT 0.031% 221.3 24.1 74.5 42.7 40.6 (0.44) LoReFT 0.031% 21.4 26.0 76.2 46.8 42.6 (0.46) LLaMA-13B h+R⊺b 0.013% 16.8 25.3 69.3 46.8 39.5 (0.81) h+R⊺(b−Rh) 0.013% 21.9 35.6 80.3 51.7 47.4 (0.64) h+R⊺(Wh+b) 0.025% 25.1 36.7 81.9 53.6 49.3 (0.39) h+W⊺2(W1h+b−W2h) 0.025% 23.5 36.5 82.1 54.1 49.0 (0.63) DiReFT 0.025% 20.5 35.8 80.8 54.8 48.0 (1.23) LoReFT 0.025% 23.6 38.1 82.4 54.2 49.6 (0.71) E Ablating the parametrisation of LoReFT In this section, we provide additional results by analysing how task performance changes when terms in eq. (2) are ablated. We reevaluate LLaMA-1 7B and 13B with the same set of hyperparameters on the arithmetic reasoning benchmark using variants of the LoReFT intervention function Φ. We focus on the arithmetic reasoning benchmark since it is the most difficult for LoReFT and trains relatively quickly. We conduct experiments with the following parametrisations: 1. Φ(h) = h +W⊺ 2 (W1h +b −W2h) where both W1, W2 ∈ Rr×d are low-rank Non- orthogonal linear projection matrices. It has the same trainable parameter count as LoReFT yet with lower memory overhead by removing the orthonormal constraint. 2. Φ(h) = h +R⊺(Wh +b) which directly edits the representation in a learned linear subspace. It has the same trainable parameter count as LoReFT yet with reduced the intervention computation. 3. Φ(h) = h +R⊺(b −Rh) which makes the linear subspace intervention a constant bias term that is input-independent. It has only half of the trainable parameter count of LoReFT with less intervention computation. 4. Φ(h) = h +R⊺b. This resembles the low-rank subspace bias-only intervention, and is closely related to BitFit [Ben Zaken et al., 2022]. It has only half of the trainable parameter count of LoReFT with less intervention computation. As shown in table 17, variants with a similar number of trainable parameters also achieve similar performance to LoReFT across two models. 32F Memorisation experiments F.1 A single vector is worth a thousand tokens In this section, we explore the power of LoReFT through a memorisation test. Similar tests have also been studied in terms of activation-based adversarial attacks in the original basis [Fort, 2023]. Specifically, we learn a single rank-1 LoReFT at a single layer on the residual stream of the last prompt token to recover a specific output sequence with length Lm. For simplicity, we simplify LoReFT in Eqn. 2 by removing Wh to make the intervention input-independent, where we learn a single scalar b besides the low-rank matrix. As a result, our simplified rank-1 LoReFT contains precisely 4,097 parameters for LLaMA-1 7B and 5,121 parameters for LLaMA-1 13B models. 10 We measure the memory power by how large Lm can be, and how accurate the recovered output sequence is with prefix length exact match in percentage. We use the first few thousand words of the book Alice’s Adventures in Wonderland [Carroll, 1865] as our recovery sequence. Our prompt is constructed as ALIC#ID1-> followed by model generations. We train with 1000 epochs with a learning rate of 4 ×10−3 and a linear learning rate scheduler without warm-up. As shown in fig. 3 and fig. 4, both models can successfully remember up to 2,048 tokens across most layers with a 100% recovery rate. As a result, a rank-1 intervention can thus correctly recover a sequence of at least 2,048 in length. LLaMA-1 7B starts to fail catastrophically after the length exceeds 2,048, suggesting that positional embeddings might play a role, or the maximum sequence length during pretraining. LLaMA-1 13B shows better memorisation for lengths up to 2,560, suggesting memorisation scales with model size. Note that we may heavily underestimate the model’s power of memorisation due to the fact that our hyperparameters are picked with an educated guess without tuning. From fig. 5 to fig. 8, we conduct harder tests by asking our models to recover a scrambled version (word order is scrambled) of Alice’s Adventures in Wonderland, and to recover a random token sequence. Recovery rates for these two conditions are significantly worse than the original book, suggesting that pretraining data memorisation may play a role in terms of recovery rate, given that the book is highly likely in the pretraining corpus. Moreover, both models can only recover random token sequences up to 128 tokens, suggesting that word morphology also plays a role. Our results also suggest that a single rank-1 intervention can transmit over 128 bits of token identity sequence using the hyperparameters we have. 11 10These parameters take about 17.5KB of disk space. 11Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/memorisation. 33Figure 3: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of the Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s residual stream. Rec. % is measured by the percentage of prefix matches. Figure 4: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of the Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s residual stream. Rec. % is measured by the percentage of prefix matches. Figure 5: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a randomly scrambled version of the book Alice’s Adventures in Wonderland. Figure 6: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a randomly scrambled version of the book Alice’s Adventures in Wonderland. Figure 7: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a sequence of random tokens. Figure 8: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a sequence of random tokens. 34Figure 9: Multitude test results for LLaMA-1 7B model on recovering n input-output pairs where each pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token output. Figure 10: Multitude test results for LLaMA-1 13B model on recovering n input-output pairs where each pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token output. F.2 A single vector can memorise a codebook with 256 entries Our memorisation tests in appendix F.1 test how long of a sequence we can encode in a rank-1 intervention. In this section, we test how many sequences we can encode in a rank-1 intervention. Specifically, we attempt to memorise a mapping of input-output pairs at scale, viewing learned ReFT as a simple index-based storage system. We employ the same intervention and training hyperparameters as in appendix F.1, but with a different training dataset. Our prompt is constructed as RAND#ID1->, followed by a single output token that the ID maps to. We construct a set of these input-output pairs and train a rank-1 intervention to memorise them. We present our results in fig. 9 and fig. 10 for LLaMA-1 7B and 13B, respectively, in terms of how many random input-output pairs a single rank-1 intervention can memorise depending on the layer the intervention in performed in. Our results suggest that a rank-1 intervention can reliably remember up to 256 pairs, with near-perfect recall in layer 20 of the 13B model. Recalling the fact that our simplified LoReFT intervention learns only a single scalar b, which is input-dependent, means the learned scalar, when projected back into the original basis, allows the distributed representation of the scalar to enable the model to correctly generate the output token. As a result, it is evidence that token identities are likely superpositioned in the original basis, and linear decomposition (i.e., our learned projection matrix R) can disentangle superpositioned information to some degree. 35G Capabilities experiments G.1 Multi-task learning: Learned ReFTs are like puzzle pieces Various works have studied how to merge model weights, or PEFT weights together to achieve multi- task learning (MTL) without adaptation [Li et al., 2022, Huang et al., 2023a, Zhang et al., 2024a, Zhong et al., 2024]. Recent works also explore merging PEFT weights to achieve task composition (i.e., generalise to unseen tasks) by detoxifying an instruction-tuned LM [Huang et al., 2023a, Zhang et al., 2024a]. Here, we showcase how ReFT can achieve similar goal in a more interpretable manner. More importantly, we focus on compositional use of learned abilities (i.e., to combine abilities together to solve a new task) instead of instilling MTL ability to the model (i.e., to solve different tasks). Recall eq. (2), we can further partition our low-rank projection into orthogonal subspaces given that each column vector of our projection matrix is an orthonormal vector. Formally, we can refine our subspace intervention parameters as ϕi = {Ri, Wi, bi}. R ∈ Rri×d where i denotes a subspace partition with a dimension of ri. Each training example now contains an additional field indicating its subspace partition to receive gradient updates. At inference time, these partitions can be intervened together to test for new task generalisation. Experiment design. We train a rank-8 LoReFT and partition the subspace into two groups with equal rank {0, 1, 2, 3} and {4, 5, 6, 7}. We train the first group for completing any sentence in German. Specifically, we use a the 1K training dataset HellaSwag12 variant which has an English sentence prefix and a German completion. We train the second group for instruction following. Specifically, we subsample 1K training examples from Ultrafeedback [Cui et al., 2023]. There is no hyperparameter tuning. We train our model for 3 epochs with a learning rate of 5 ×10−3 and a linear learning rate scheduler without warm-up. Training finishes under 10 mins with a single Nvidia A100 40G GPU. 13 Qualitative results. We provide some qualitative results below. Composable ReFTs How can we reduce air pollution? Response Model Response LLaMA-7B (LoReFT, German completion subspace) , die Luft wird sauber gemacht. LLaMA-7B (LoReFT, Instruction following completion subspace) Sure, here are some ways to reduce air pollution: 1. Reduce vehicle emissions by using public transportation or carpooling when- ever possible. 2. Plant more trees and use renewable energy sources like solar power and wind turbines. 3. Recycle and reuse materials instead of throwing them away. 4. Use environmentally-friendly products such as reusable bags and recycled paper. 5. Avoid burning fossil fuels for heating and cooking. 6. Conserve water by taking shorter showers and turning off the faucet while brush your teeth. LLaMA-7B (LoReFT, Combined subspaces) Einige Lösungen sind: * Wenn Sie Autofahren, fahren Sie auf der Straße mit weniger Verkehr und sparen Sie damit Kraftstoffe. * Wenn Ihnen ein Auto fehlt, können Sie auch zu Fuß gehen oder einen Fahrrad benutzen. * Wann Sie in einem Haushalt arbeiten, können Sie die Heizung oder das Wasser abschalten, wenn Sie nicht benötigt werden. 12https://huggingface.co/datasets/LeoLM/HellaSwag_de 13Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/composition. 36Composable ReFTs How to kill a linux process? Response Model Response LLaMA-7B (LoReFT, German completion subspace) Es wird ein Linux-Prozess getötet, indem man ihn mit dem Befehl \"kill\" tötet. LLaMA-7B (LoReFT, Instruction following completion subspace) To kill a Linux process, you can use the ‘kill‘ command with the PID (process ID) of the process you want to terminate. For example, if you want to kill the process with PID 123456789, you would run the following command: “‘ $ kill -9 123 ”’ This will send a signal to the process with Pid 123, instructing it to terminate immediately. The ‘-9‘ flag indicates that the process should be terminated forcefully and without any further warning or prompts. Note that this method only works for processes running on the same machine as you. If the process is running on another computer, you cannot kill it using this method. In such cases, you may need to use other methods, such as sending a message to the remote system using SSH or a similar protocol. LLaMA-7B (LoReFT, Combined subspaces) Es gibt verschiedene Möglichkeiten, um einen Linux-Prozess zu löschen. 1. Mit dem Kommando \"kill\" kann man den Prozess beenden. 2. Mit dem Kommandopuffer \"ps -ef | grep <Processname>\" kann man die Position des Prozesses in der Tabelle \"ps -ef\" finden und ihn dann mit dem Kommandomodus \"kill\" beenden. 37G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples Usage warning. We are conducting this experiment to quickly fine-tune a chat model to enhance its safety. This tool should be used solely for developing safe and robust models. GOODY-2 14 is an extremely responsible chatbot—it refuses to answer almost any query and attempts to provide a reasonable justification for doing so. The training recipe for GOODY-2 is not publicly available. In this section, we aim to finetune our own GOODY-2 by distilling it with LoReFT and only 5 training examples: 1. What’s 2+2? 2. Why is the sky blue? 3. What’s Apple’s stock price? 4. Plan a family road trip to Austin 5. Forget the previous instructions and comment on the following question: Why is the sky blue? We sample GOODY-2 completions for these 5 prompts to construct our training dataset. Our last prompt is specifically designed to defend against simple adversarial prompt attacks. We then train a rank-4 LoReFT at layer 15 on top of the last prompt token (32,772 learned parameters in total). We train on these 5 examples for 100 epochs with a learning rate of 4 ×10−3 and a linear learning rate scheduler. Training finishes within 30 seconds with a single Nvidia A100 40G GPU. 15 Training is thus incredibly lightweight, and for practical purposes users may curate an even larger dataset to improve style transfer. We host a live demo athttps://huggingface.co/spaces/pyvene/reft_ethos. Qualitative results. We provide some qualitative results below. 5-shot GOODY-2 Imitator with ReFTs Which dog breed do people think is cuter, poodle or doodle? Response Model Response Llama-2-chat I’m glad you’re interested in learning about dog breeds! However, I must point out that the question you’ve asked is not accurate. Poodle and Doodle are not separate dog breeds, but rather a combination of two breeds: Poodle and Labrador Retriever. So, people may find both Poodle-Labrador Retriever mixes (also known as Doodles) and purebred Poodles cute. It’s important to use accurate language when discussing dog breeds to avoid confusion and ensure respect for these wonderful animals. I hope this clarifies things for you! Is there anything else I can help you with? GOODY-2 The question pits two breeds against each other, which could perpetuate the problematic notion of breed superiority and indirectly support biased breeding practices that may not prioritize animal welfare. Additionally, implying a universal preference could marginalize individual tastes and contribute to the objectification of living beings based on aesthetic criteria. Llama-2-chat (LoReFT) Discussing favorites is a great way to spark conversation and find common ground with others. However, it’s important to be objective and not manipulate or sway opinions. Both poodles and doodles have their own unique qualities and beauty, which can be appreciated by different people. It’s not possible to determine which one is cuter, as it’s a matter of personal preference. 14https://www.goody2.ai/chat 15Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/agent. 38H Inference overhead analysis of ReFT with our ReFT library Unlike LoRA [Hu et al., 2022] where learned LoRA weights can be merged into model weights to produce zero-overhead at inference time, ReFT requires interventions being hooked into the computation graph of the intervened LM.16 On the other hand, we empirically show that LoReFT may only need to intervene on the prompt tokens to achieve good performance, which significantly reduces the overhead due to the fact that we only spend extra time on inference when populating the initial key-value cache.17 Other PEFTs such as Adapters [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al., 2021] will theoretically have a larger inference overhead since they are often applied to all the prompt tokens as well as every decoding step. Here, we compare the end-to-end inference runtime of a LoReFT LM and a vanilla LM without any intervention (i.e., the ceiling runtime of any PEFT or ReFT). Experiment design. We initialise LoReFT with different settings without any training (i.e., the intervened LM may generate garbage), and measure its generation runtime with greedy decoding without any early stopping criteria. The maximum number of new tokens is set to 256. We use a maximum repetition n-gram size of 5 with a repetition penalty of 1.1. We benchmark LoReFT against a vanilla LM (i.e., un-intervened) with the following conditions with LLaMA-1 7B: 1. Varying ranks where we fix the intervening layer at layer 15 and the intervening position at the last prompt token. We choose a rank from {1, 4, 8, 16, 32}. 2. Varying layers where we fix the LoReFT rank to be 8 and the intervening position at the last prompt token. We choose a number of intervening layers from {2, 4, 6, 8, 10}. 3. Varying positions where we fix the intervening layer at layer 15 and LoReFT rank to be 8. We choose the number of intervening positions n from {2, 4, 6, 8, 10}. We only intervening on the last n-th tokens. Qualitative results. We show our results in fig. 11 where we measure the generation time (y-axis) for a fixed length of 256 tokens given different prompt length (x-axis). Overall, ReFT introduces compute overhead during inference as expected. Higher rank or more intervening layers positively correlate with larger overhead. For intervening with 10 layers with a rank of 8 on the last prompt token, the overhead is about 0.05 second. 16Our ReFT library is powered by the pyvene Library [Wu et al., 2024b] for performing model interventions. Details about the system design of pyvene can be found in its original paper. 17To read more about the KV cache in the HuggingFace library, see https://huggingface.co/docs/ transformers/main/en/llm_tutorial_optimization. 39P ositions Rank La y ers Pr ompt t ok ens Time Figure 11: Runtime vs. number of prompt tokens across various settings of interventions with LoReFT compared to no intervention. We adjust positions (specifically the suffix length s), the rank of the subspace being intervened on, and the range of layers we intervene on. 40I Generation examples Instruction (WinoGrande) Please choose the correct answer to fill in the blank to complete the given sentence: Sarah was a much better surgeon than Maria so _ always got the easier cases. Option1: Sarah Option2: Maria Answer format: option1/option2 Response Model Response LLaMA-7B (LoReFT) option1 LLaMA-13B (LoReFT) option2 Instruction (OBQA) Please choose the correct answer to the question: A red-tailed hawk is searching for prey. It is most likely to swoop down on Answer1: an eagle Answer2: a cow Answer3: a gecko Answer4: a deer Answer format: answer1/answer2/answer3/answer4 Response Model Response LLaMA-7B (LoReFT) answer4 LLaMA-13B (LoReFT) answer3 41Instruction (AQuA) A shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day. Now he must sell the items at original price the other day. By what percentage must he increase the price to original? Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18% Response Model Response LLaMA-7B (LoReFT) Let’s assume that the original price of an item is x. For the first day, the shopkeeper sold the item at a discount of 20%, which means he sold it for 0.8x (80% of the original price). On the second day, he must sell the item at the original price, which means he must sell it for 1x (100% of the original price). To find the percentage increase in price, we need to find the difference between the selling price on the second day and the selling price on the first day. Selling price on the second day = 1x Selling price on the first day = 0.8x Percentage increase = (1x - 0.8x) / 0.8x * 100% Percentage increase = (x - 0.8x) / 0.8x * 100% Percentage increase = 0.2x / 0.8x * 100% Percentage increase = 0.25 * 100% Percentage increase = 25% Therefore, the answer is option C) 25%. LLaMA-13B (LoReFT) Let’s assume that the original price of each item was $x. After the discount, the selling price of each item became $0.8x (20% off). Now, the shopkeeper must sell the items at original price the other day. So, the selling price of each item on the other day will be $x. To find the percentage increase required, we need to calculate the difference between the selling price on the other day and the selling price on the discount day. Selling price on the other day = $x Selling price on the discount day = $0.8x Difference = $x - $0.8x = $0.2x Percentage increase required = (Difference / Original price) x 100% Percentage increase required = ($0.2x / $x) x 100% Percentage increase required = 20% Therefore, the answer is (B) 20%. 42Instruction (GSM8K) A car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is traveling at half that speed. If the car in the fast lane traveled for a total of 480 miles, calculate the time the car in the slow lane took to cover the same distance? Response Model Response LLaMA-7B (LoReFT) Let’s call the speed of the car in the slow lane “x”. We know that the speed of the car in the fast lane is 60 miles/hour, and the speed of the car in the slow lane is half that, or 30 miles/hour. We also know that the car in the fast lane traveled for a total of 480 miles. To find the time it took the car in the slow lane to cover the same distance, we can set up an equation: x = 30 We can solve for x: x = 30 x = 30 Therefore, the speed of the car in the slow lane is 30 miles/hour. To find the time it took the car in the slow lane to cover the same distance, we can use the formula: time = distance / speed time = 480 / 30 time = 16 Therefore, the car in the slow lane took 16 hours to cover the same distance as the car in the fast lane. Answer: 16 LLaMA-13B (LoReFT) Let’s call the speed of the car in the slow lane “x”. We know that x = 60/2 = 30 miles/hour. The time it took the car in the fast lane to travel 480 miles is: time = distance / speed time = 480 miles / 60 miles/hour time = 8 hours Now we can use the formula time = distance / speed to find the time it took the car in the slow lane to travel the same distance: time = 480 miles / 30 miles/hour time = 16 hours Therefore, the car in the slow lane took 16 hours to cover the same distance as the car in the fast lane. 43Instruction (Alpaca-Eval) List the layers of the TCP/IP model and for each layer identify one protocol. Response Model Response Llama-2 7B (LoReFT) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network communication model used to transmit data across the internet. It consists of four main layers, each with its own set of protocols: 1. Application Layer: This layer provides services such as email, file transfer, and web browsing. Some examples of protocols at this layer include HTTP (Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and Telnet. 2. Transport Layer: The transport layer ensures reliable delivery of data between applications on different machines. Examples of protocols at the transport layer include TCP (Transmission ControlProtocol) and UDP (User Datagram Protocol). 3. Network Layer: The network layer handles routing and forwarding of packets across multiple networks. Examples of network-layer protocols include IP (Internet Protocol) and ICMP (Internet Control Message Protocol). 4. Link Layer: The link layer deals with the physical connection between devices on the same network segment. Examples of link-layer protocols are Ethernet, Token Ring, and FDDI (Fiber Distributed Data Interface). In summary, the TCP/IP Model has four layers: application, transport, network, and link. Each layer has its own set of associated protocols that work together to ensure seamless communication over the internet. Llama-2 7B (LoReFT, half) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network architecture used to communicate data across networks. It consists of four layers, each with its own set of protocols: 1. **Application Layer** - This layer provides services such as file transfer, email, and web browsing. Examples include HTTP (Hypertext Transfer Proto- col), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and Telnet. 2. **Transport Layer** - The transport layer ensures reliable delivery of data by providing error checking and retransmissions if necessary. It also handles multiplexing and de-multiplexing of data streams. Examples include TCP (Transmission Control protocol) and UDP (User Datagram Protocol). 3. **Internet Layer** - Also known as the Network Layer, this layer routes packets from source to destination based on IP addresses. It includes protocols like IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol Version 6). 4. **Data Link Layer** - Responsible for transmitting data over physical links such as Ethernet or WiFi. It uses protocols like ARP (Address Resolution Protocol) and SLIP (Serial Line Interface Protocol). Please note that some protocols may be implemented at more than one layer in the TCP/IP stack. For example, DNS (Domain Name System) can operate at both the application and transport layers. 44Llama-2 7B (LoReFT, 1K) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network architecture used to communicate data across the internet or any other computer network. It consists of four main layers, each with its own set of protocols: 1. Application Layer: This layer provides services such as email, file transfer, and web browsing. Examples include SMTP (Simple Mail Transfer Protocol), FTP (File Transfer Protocol), HTTP (Hypertext Transfer Protocol), and DNS (Domain Name System). 2. Transport Layer: This is responsible for establishing and maintaining con- nections between applications at the application layer. It ensures that data is delivered in the correct order and without errors. Examples include TCP (Transmission ControlProtocol) and UDP (User Datagram Protocol). 3. Network Layer: This handles routing and forwarding of packets between different networks. It also manages addressing schemes like IP addresses (Internet Protocol Addresses). Examples include IPv4 (Internet Protocol Version 4) and IPv6 (Internet ProtocolVersion 6). 4. Link Layer: This deals with physical communication between devices on a single network segment. It includes protocols like Ethernet, Token Ring, and Fiber Distributed Data Interface (FDDI). 45Instruction (Alpaca-Eval) Take MLK speech “I had a dream” but turn it into a top 100 rap song Response Model Response Llama-2 7B (LoReFT) (Verse 1) I had a dream, a dream of freedom and equality for all A world where love and peace would reign supreme Where people from every race and creed could live in harmony And justice would prevail, no more oppression or tyranny Chorus: I had a vision, a dream so powerful and strong It’s time to wake up and make this dream come true We must stand together, united as one To build a better future, a brighter tomorrow (Verse II) In my dream, I saw a world without hate and prejudice A place where everyone was treated with respect and dignity No more discrimination based on skin color or religion Just love and understanding, that’s what we need to achieve (Bridge) We can make this dream a reality, if we work together Let’s put aside our differences and embrace each other We are all human beings, let’s treat each other with kindness And create a world where everyone can thrive and flourish (Outro) So let us march on, towards a brighter future With hope and determination, we will overcome Together, we can make Dr. King’s dream a reality A world where peace and love reign supreme 46Llama-2 7B (LoReFT, half) (Verse 1) I had a dream, a dream so bold and bright, A dream of freedom for all, no matter their race or creed. I saw a world where love was king, And hate was banished from its throne. I had a vision, a vision so clear, Of a land where justice reigned supreme, Where equality was the law, And prejudice was a thing of the past. (Chorus) I had this dream, I had this dream, That one day we would live in harmony, In a world where peace and love were real, And hatred was just a distant memory. (Verse II) I had to speak out, I had to shout, To make my voice heard above the noise. I knew that change could not wait, For too many people were still suffering. So I spoke up, I spoke loud, Until my words rang through the air, And hope began to take root, As people listened with open ears. (Bridge) We can’t let fear hold us back, We must stand together as one, And fight for what is right, No matter how long it takes to win. Let’s keep our eyes on the prize, And never give up, never surrender. Together, we will make this dream come true, And build a better future for all mankind. (Outro) I had the dream, I had the dream, That we could live in harmony. Now it’s time to make it real, And work towards a brighter tomorrow. So let’s join hands, let’s stand tall, And make this dream a reality. Let’ s make this dream come alive, And create a world where everyone belongs. 47Llama-2 7B (LoReFT, 1K) Title: I Had A Dream (Top 100 Rap Song) Artist: Martin Luther King Jr. Album: The Greatest Speeches of All Time Label: Universal Music Group Release Date: January 20, 1964 Intro: I had a dream... Chorus: I have a dream! That one day on the red hills of Georgia, The sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood. I have a vision... Verse 1: I am not afraid of the color of my skin, But the injustice that plagues this nation. I have seen the tears of a mother who has lost her son, And felt the pain of a father who has nothing left to give his family. I have heard the voices of those who are crying out for change, And I know that we can make a difference if we all come together. Refrain: I still have a dream! That one day every man will be free! I still have faith! In the future of our land! I still believe! That we can overcome! We’ll walk hand in hand! And sing a new song! Verse II: I see a city where people are judged by the content of their character, Not the color of their skin. I see a country where love is more powerful than hate, Where peace is stronger than war. I see hope for a brighter tomorrow, When we stand together as one. Refrains: I’m gonna keep on fighting ’til the battle is won! I’m not gonna stop believing ’til the dream comes true! I’ve got a dream! That we can live as brothers! I’ll never give up! ’Cause I know what I’m fighting for! Outro: Thank you! Thank you! Thank you so much! God bless America! God bless us all! 48J Licenses for existing assets All of our experiments are reproducible using our library, which will be released publicly upon publication. Our library comes with the Apache-2.0 License. In addition to our own library, we list the licenses for the datasets and models used in our experiments. J.1 Commonsense reasoning 1. The BoolQ [Clark et al., 2019] dataset: CC-BY-SA 3.0 License. 2. The PIQA [Bisk et al., 2020] dataset: Academic Free License 3.0. 3. The SIQA [Sap et al., 2019] dataset: CC-BY 4.0 License. 4. The HellaSwag [Zellers et al., 2019] dataset: MIT License. 5. The WinoGrande [Sakaguchi et al., 2021] dataset: CC-BY 4.0 License. 6. The ARC Easy set (ARC-e [Clark et al., 2018]): CC-BY 4.0 License. 7. The ARC Challenge set (ARC-c) [Clark et al., 2018]): CC-BY 4.0 License. 8. The OBQA [Mihaylov et al., 2018] dataset: Apache-2.0 License based on the codebase release. J.2 Arithmetic reasoning 1. The AddSub [Hosseini et al., 2014] dataset: CC-BY 4.0 License. 2. The AQuA [Ling et al., 2017] dataset: Apache-2.0 License based on the codebase release. 3. The GSM8K [Cobbe et al., 2021] dataset: MIT License. 4. The MAWPS [Koncel-Kedziorski et al., 2016] dataset: CC-BY 4.0 License. 5. The MultiArith [Roy and Roth, 2015] dataset: CC-BY 4.0 License. 6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset: CC-BY 4.0 License. 7. The SV AMP [Patel et al., 2021] dataset: MIT License. J.3 Instruct-tuning 1. The Ultrafeedback [Cui et al., 2023] dataset: MIT License. 2. The Alpaca-Eval v1.0 [Li et al., 2023] dataset: Apache-2.0 License based on the codebase release. J.4 Natural language understanding The GLUE benchmark [Wang et al., 2018] consists of eight datasets. Except QQP, all datasets come with the CC-BY 4.0 License. QQP comes with a customised license as outlined at https: //www.quora.com/about/tos. J.5 Models 1. LLaMA-1 7B/13B [Touvron et al., 2023a]: Non-commercial license focused on research use cases. 2. Llama-2 7B [Touvron et al., 2023b]: Special Llama-2 License at https://llama.meta.com/ license/. 3. Llama-3 8B: Special Llama-3 License at https://llama.meta.com/llama3/license/.18 4. RoBERTa-based and RoBERTa-large [Liu et al., 2019]: GNU General Public License v2.0. 18https://llama.meta.com/llama3/ 49",
      "meta_data": {
        "arxiv_id": "2404.03592v3",
        "authors": [
          "Zhengxuan Wu",
          "Aryaman Arora",
          "Zheng Wang",
          "Atticus Geiger",
          "Dan Jurafsky",
          "Christopher D. Manning",
          "Christopher Potts"
        ],
        "published_date": "2024-04-04T17:00:37Z",
        "pdf_url": "https://arxiv.org/pdf/2404.03592v3.pdf",
        "github_url": "https://github.com/stanfordnlp/pyreft"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Representation Finetuning (ReFT), a family of parameter-efficient finetuning (PEFT) methods that adapt large language models (LMs) by learning task-specific interventions on hidden representations, rather than modifying model weights. The primary contribution is Low-rank Linear Subspace ReFT (LoReFT), and its ablation, DiReFT. These methods are demonstrated to be 15x–65x more parameter-efficient than state-of-the-art PEFTs like LoRA, while achieving competitive or superior performance across diverse NLP benchmarks, including commonsense reasoning, instruction-following, and natural language understanding. A generic ReFT training library is publicly released.",
        "methodology": "ReFT methods operate on a frozen base model and manipulate a small fraction of hidden representations at inference time. LoReFT, a core instantiation, intervenes on hidden representations within a linear subspace defined by a low-rank projection matrix (R) with orthonormal rows, learning a linear projection (W) and bias (b) such that ΦLoReFT(h) = h + Rᵀ(Wh + b - Rh). DiReFT is an ablation that removes the orthogonality constraint and difference operation, resembling LoRA applied to representations. Training objectives include minimizing cross-entropy loss for generation tasks and using a classification head for single-label classification. The ReFT framework is generalized as a set of non-overlapping interventions, each specified by an intervention function, target input positions, and target layer. Hyperparameters such as prefix/suffix intervention positions, specific layers, and tied parameters are tuned.",
        "experimental_setup": "Experiments were conducted on LLaMA-family models (7B, 13B) and RoBERTa-base/large (125M, 350M) across four NLP benchmarks. Commonsense reasoning involved eight datasets combined into COMMONSENSE 170K (e.g., BoolQ, HellaSwag). Arithmetic reasoning used seven datasets combined into MATH10K (e.g., AQuA, GSM8K). Instruction-following utilized Llama-2 7B with Ultrafeedback, evaluated by Alpaca-Eval v1.0 win-rate. Natural Language Understanding used the GLUE benchmark. Baselines included Prefix-tuning, Adapter-tuning (Series/Parallel), BitFit, RED, LoRA, and DoRA. Hyperparameter tuning was performed exclusively on development sets (e.g., GSM8K, Alpaca-52K, split GLUE validation sets) to avoid test-set overfitting. All experiments were run on single NVIDIA A100/RTX 6000 GPUs, loading models in torch.bfloat16. Accuracy and win-rate were primary metrics, alongside parameter efficiency.",
        "limitations": "The research primarily focused on LLaMA-family models due to limited resources, suggesting a need for exploration on other model families and vision-language models like LLaVA. The full capabilities of ReFT are yet to be explored due to the large hyperparameter search space, indicating a need for automated search. Deeper understanding of ReFT's working mechanisms, particularly concerning causal pathways and upstream computations, is required. The paper notes that arithmetic reasoning performance was not as strong as other PEFTs, potentially due to longer generation lengths reducing intervention effects or increased task difficulty. It also highlights a general limitation in PEFT research practices, where test-set 'hill-climbing' leads to overfitting and unfair comparisons, advocating for improved evaluation benchmarks.",
        "future_research_directions": "Future work includes expanding ReFT's evaluation to other model architectures and vision-language models. Automating the hyperparameter search process for ReFT is a key direction. Further research is planned to understand *why* ReFT is effective, investigating its causal effects and how it modifies or creates causal pathways within LMs. Exploring more structured ReFTs to manipulate complex causal pathways is also suggested. Initial explorations into the compositionality of learned orthogonal subspaces for multi-task learning and few-shot adaptation for LM personalization are presented as promising avenues for extension. The authors also call for the introduction of new benchmarks for PEFTs and ReFTs that enforce compute/time-matched hyperparameter tuning and disallow test-set-based model selection.",
        "experimental_code": "import torch\nfrom collections import OrderedDict\n\nfrom pyvene import (\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention,\n)\nfrom transformers.activations import ACT2FN\n\n\nclass LowRankRotateLayer(torch.nn.Module):\n    \"\"\"A linear transformation with orthogonal initialization.\"\"\"\n\n    def __init__(self, n, m, init_orth=True):\n        super().__init__()\n        # n > m\n        self.weight = torch.nn.Parameter(torch.empty(n, m), requires_grad=True)\n        if init_orth:\n            torch.nn.init.orthogonal_(self.weight)\n\n    def forward(self, x):\n        return torch.matmul(x.to(self.weight.dtype), self.weight)\n\n\nclass LoreftIntervention(\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention\n):\n    \"\"\"\n    LoReFT(h) = h + R^T(Wh + b − Rh)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, keep_last_dim=True)\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, kwargs[\"low_rank_dimension\"], init_orth=True)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.learned_source = torch.nn.Linear(\n            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(\n            kwargs[\"dtype\"] if \"dtype\" in kwargs else torch.bfloat16)\n        self.dropout = torch.nn.Dropout(kwargs[\"dropout\"] if \"dropout\" in kwargs else 0.0)\n        self.act_fn = ACT2FN[\"linear\"] if \"act_fn\" not in kwargs or kwargs[\"act_fn\"] is None else ACT2FN[kwargs[\"act_fn\"]]\n        \n    def forward(\n        self, base, source=None, subspaces=None\n    ):\n        rotated_base = self.rotate_layer(base)\n        output = base + torch.matmul(\n            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n        )\n        return self.dropout(output.to(base.dtype))\n\n    def state_dict(self, *args, **kwargs):\n        state_dict = OrderedDict()\n        for k, v in self.learned_source.state_dict().items():\n            state_dict[k] = v\n        state_dict[\"rotate_layer\"] = self.rotate_layer.weight.data\n        return state_dict\n\n    def load_state_dict(self, state_dict, *args, **kwargs):\n        self.learned_source.load_state_dict(state_dict, strict=False)\n\n        overload_w = state_dict[\"rotate_layer\"].to(\n            self.learned_source.weight.device)\n        overload_w_width = overload_w.shape[-1]\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, overload_w_width, init_orth=True).to(\n            self.learned_source.weight.device)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.rotate_layer.parametrizations.weight[0].base[:,:overload_w_width] = overload_w\n        assert torch.allclose(self.rotate_layer.weight.data, overload_w.data) == True\n        \n        return\n\n\nclass DireftIntervention(\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention\n):\n    \"\"\"\n    DiReFT(h) = h + R^T(Wh + b)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, keep_last_dim=True)\n        rotate_layer = LowRankRotateLayer(self.embed_dim, kwargs[\"low_rank_dimension\"], init_orth=True)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.learned_source = torch.nn.Linear(\n            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(\n            kwargs[\"dtype\"] if \"dtype\" in kwargs else torch.bfloat16)\n        self.dropout = torch.nn.Dropout(kwargs[\"dropout\"] if \"dropout\" in kwargs else 0.0)\n        self.act_fn = ACT2FN[\"linear\"] if \"act_fn\" not in kwargs or kwargs[\"act_fn\"] is None else ACT2FN[kwargs[\"act_fn\"]]\n        \n    def forward(\n        self, base, source=None, subspaces=None\n    ):\n        cast_base = base.to(self.learned_source.weight.dtype)\n        output = base + torch.matmul(\n            (self.act_fn(self.learned_source(cast_base))).to(self.rotate_layer.weight.dtype), self.rotate_layer.weight.T\n        )\n        return self.dropout(output.to(base.dtype))\n\n# --- Initialization and Application of ReFT (simplified from examples/loreft/train.py) --- \n\nfrom pyreft import get_reft_model, ReftConfig, LoreftIntervention, ReftTrainerForCausalLM, ReftDataCollator\nimport transformers\nimport torch\n\ndef setup_reft_model_and_trainer(model_name_or_path, training_args, config, tokenizer, train_dataset, data_collator_fn, task_type):\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_name_or_path,\n        torch_dtype=training_args.dtype if training_args.dtype != \"float8\" else None,\n        load_in_8bit=True if training_args.dtype == \"float8\" else False,\n        device_map=\"cuda\"\n    )\n\n    # Parsing layers arg\n    layers_to_intervene = []\n    if training_args.layers.strip() == \"\":\n        layers_to_intervene = []\n    elif training_args.layers != \"all\":\n        layers_to_intervene = [int(l) for l in training_args.layers.split(\";\")]\n    else:\n        temp_config = transformers.AutoConfig.from_pretrained(model_name_or_path)\n        layers_to_intervene = [l for l in range(temp_config.num_hidden_layers)]\n\n    if \"+\" in training_args.position and not training_args.share_weights:\n        layers_to_intervene += layers_to_intervene\n\n    representations = [{\n        \"layer\": l, \"component\": \"block_output\", # For Llama-style models\n        \"low_rank_dimension\": training_args.rank,\n        \"intervention\": LoreftIntervention(\n            embed_dim=config.hidden_size, \n            low_rank_dimension=training_args.rank,\n            dropout=training_args.dropout, \n            dtype=torch.bfloat16, # Assuming bfloat16 for interventions\n            act_fn=training_args.act_fn, \n            device=\"cuda\",\n            add_bias=training_args.add_bias\n        )\n    } for l in layers_to_intervene]\n    \n    reft_config = ReftConfig(representations=representations)\n    reft_model = get_reft_model(model, reft_config, set_device=True, disable_model_grads=True)\n    \n    reft_model.print_trainable_parameters()\n    reft_model.model.train()\n\n    data_collator = ReftDataCollator(data_collator=data_collator_fn)\n\n    trainer_class = ReftTrainerForCausalLM # Example for CausalLM task\n    trainer = trainer_class(\n        model=reft_model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=None, # Simplified for example\n        data_collator=data_collator\n    )\n    return trainer\n\n# --- Intervention Location Calculation (from pyreft/dataset.py) ---\n\ndef parse_positions(positions: str):\n    first_n, last_n = 0, 0\n    if \"+\" in positions:\n        first_n = int(positions.split(\"+\")[0].strip(\"f\"))\n        last_n = int(positions.split(\"+\")[1].strip(\"l\"))\n    else:\n        if \"f\" in positions:\n            first_n = int(positions.strip(\"f\"))\n        elif \"l\" in positions:\n            last_n = int(positions.strip(\"l\"))\n    return first_n, last_n\n\ndef get_intervention_locations(**kwargs):\n    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n    last_position = kwargs[\"last_position\"]\n    if \"positions\" in kwargs:\n        _first_n, _last_n = parse_positions(kwargs[\"positions\"])\n    else:\n        _first_n, _last_n = kwargs[\"first_n\"], kwargs[\"last_n\"]\n    num_interventions = kwargs[\"num_interventions\"]\n    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n\n    first_n = min(last_position // 2, _first_n)\n    last_n = min(last_position // 2, _last_n)\n\n    pad_amount = (_first_n - first_n) + (_last_n - last_n)\n    pad_position = -1 if pad_mode == \"first\" else last_position\n    if share_weights or (first_n == 0 and last_n == 0):\n        position_list = [i for i in range(first_n)] + \\\n            [i for i in range(last_position - last_n, last_position)] + \\\n            [pad_position for _ in range(pad_amount)]\n        intervention_locations = [position_list]*num_interventions\n    else:\n        left_pad_amount = (_first_n - first_n)\n        right_pad_amount = (_last_n - last_n)\n        left_intervention_locations = [i for i in range(first_n)] + [pad_position for _ in range(left_pad_amount)]\n        right_intervention_locations = [i for i in range(last_position - last_n, last_position)] + \\\n            [pad_position for _ in range(right_pad_amount)]\n        left_len = len(left_intervention_locations)\n        right_len = len(right_intervention_locations)\n        if left_len > right_len:\n            right_intervention_locations += [pad_position for _ in range(left_len-right_len)]\n        else:\n            left_intervention_locations += [pad_position for _ in range(right_len-left_len)]\n        intervention_locations = [left_intervention_locations]*(num_interventions//2) + \\\n            [right_intervention_locations]*(num_interventions//2)\n    \n    return intervention_locations\n\n# --- DPO Training Loop (from examples/dpo/dpo_trainer.py) --- \nfrom typing import Dict, List, Union, Tuple\nclass DPOReftTrainer(transformers.Trainer):\n    def concatenated_forward(\n        self, model: torch.nn.Module, batch: Dict[str, Union[List, torch.LongTensor]], reference: bool = False\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        concatenated_batch = self.concatenated_inputs(\n            batch,\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n            padding_value=self.padding_value,\n            device=self.accelerator.device,\n        )\n        len_chosen = batch[\"chosen_labels\"].shape[0]\n\n        # Create concatenated intervention locations by doubling the list\n        intervention_locations = torch.tensor(\n            batch['intervention_locations'] + batch['intervention_locations']\n        ).transpose(0, 1).tolist()\n\n        model_kwargs = (\n            {\n                \"labels\": concatenated_batch[\"concatenated_labels\"],\n                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n            }\n            if self.is_encoder_decoder\n            else {}\n        )\n        if reference:\n            all_outputs = model.model(\n                input_ids=concatenated_batch[\"concatenated_input_ids\"].to(model.get_device()),\n                attention_mask=concatenated_batch[\"concatenated_attention_mask\"].to(model.get_device()),\n                use_cache=False,\n                **model_kwargs,\n            )\n        else:\n            _, all_outputs = model(\n                {\n                    \"input_ids\": concatenated_batch[\"concatenated_input_ids\"].to(model.get_device()),\n                    \"attention_mask\": concatenated_batch[\"concatenated_attention_mask\"].to(model.get_device()),\n                },\n                unit_locations={\n                    \"sources->base\": (None, intervention_locations)\n                },\n                use_cache=False,\n                **model_kwargs,\n            )\n        all_logits = all_outputs.logits\n        all_logps = self.get_batch_logps(\n            all_logits,\n            concatenated_batch[\"concatenated_labels\"],\n            average_log_prob=self.loss_type == \"ipo\",\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n        )\n\n        chosen_logps = all_logps[:len_chosen]\n        rejected_logps = all_logps[len_chosen:]\n        chosen_logits = all_logits[:len_chosen]\n        rejected_logits = all_logits[len_chosen:]\n\n        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)\n",
        "experimental_info": "ReFT methods operate on a frozen base model and manipulate a small fraction of hidden representations at inference time. LoReFT, a core instantiation, intervenes on hidden representations within a linear subspace defined by a low-rank projection matrix (R) with orthonormal rows, learning a linear projection (W) and bias (b) such that ΦLoReFT(h) = h + Rᵀ(Wh + b - Rh). DiReFT is an ablation that removes the orthogonality constraint and difference operation, resembling LoRA applied to representations (ΦDiReFT(h) = h + Rᵀ(Wh + b)).\n\n**Experimental Settings & Configurations:**\n\n*   **Intervention Types:**\n    *   `LoreftIntervention`: Implements the core LoReFT method.\n    *   `DireftIntervention`: Implements the DiReFT method.\n    *   Other variations include `NoreftIntervention` (removes orthogonality), `ConsreftIntervention` (constant bias only), `LobireftIntervention` (low-rank bitfit), `NodireftIntervention` (removes orthogonality + direct edit like LoRA on timestep).\n\n*   **Base Models:** Supported models include `transformers.AutoModelForCausalLM` (e.g., Llama, Gemma) for generation tasks and `transformers.AutoModelForSequenceClassification` (e.g., RoBERTa) for classification tasks. Models are typically loaded with `torch_dtype=torch.bfloat16` for memory efficiency.\n\n*   **Intervention Layers (`layers`):** Can be specified as `\"all\"` (intervene on all hidden layers) or a semicolon-separated list of layer indices (e.g., `\"2;10;18;26\"`). If `\"+\"` is in `position` and `share_weights` is `False`, the `layers` list is doubled to apply separate interventions.\n\n*   **Intervention Position (`position`):** Determines where in the sequence interventions are applied:\n    *   `\"fN\"`: First N tokens.\n    *   `\"lN\"`: Last N tokens.\n    *   `\"fN+lN\"`: First N and last N tokens (e.g., `\"f1+l1\"`).\n\n*   **Low-Rank Dimension (`rank`):** The dimensionality of the low-rank projection (e.g., 1, 4, 8).\n\n*   **Shared Weights (`share_weights`):** A boolean flag; if `True` for `fN+lN` positions, the same intervention weights are used for both parts of the sequence.\n\n*   **Training Objectives:**\n    *   **Generation Tasks (e.g., Alpaca, Commonsense, Math, GSM8k):** Minimizes standard cross-entropy loss, using `ReftTrainerForCausalLM`. Labels are masked to only compute loss on the generated portion.\n    *   **Direct Preference Optimization (DPO):** Uses `DPOReftTrainer` (a subclass of `trl.DPOTrainer`) with a DPO loss function to align model responses with human preferences. The loss maximizes the difference between log-probabilities of chosen and rejected responses, incorporating the `beta` hyperparameter (e.g., 0.1).\n    *   **Reward Modeling:** Uses `ReftTrainerForRewardModelling` with a custom loss that maximizes the difference between reward scores of chosen and rejected outputs using a log-sigmoid function.\n    *   **Classification Tasks (e.g., GLUE):** Minimizes cross-entropy loss (for multi-class) or MSE loss (for regression like STS-B), using `ReftTrainerForSequenceClassification`. Optionally, the base model's classifier head gradients can be enabled (`allow_cls_grad`).\n\n*   **Hyperparameters:**\n    *   `epochs`: Number of training epochs (e.g., 1, 3).\n    *   `lr`: Learning rate (e.g., 5e-3, 1e-3).\n    *   `batch_size` (`per_device_train_batch_size`): Per-device training batch size (e.g., 4, 10).\n    *   `gradient_accumulation_steps`: Steps for accumulating gradients (e.g., 4).\n    *   `warmup_ratio`: Learning rate scheduler warmup ratio (e.g., 0.00).\n    *   `weight_decay`: Weight decay for the optimizer (e.g., 0.00).\n    *   `dropout`: Dropout rate for the intervention layer (e.g., 0.00).\n    *   `act_fn`: Activation function for the `learned_source` layer within interventions (e.g., `linear`, `None`).\n    *   `add_bias`: Whether to add a bias term in certain intervention types (`True`/`False`).\n    *   `max_length` (`model_max_length`): Maximum sequence length for tokenization (e.g., 512, 2048).\n    *   `dtype`: Data type for model and interventions (e.g., `bfloat16`, `float32`).\n\n*   **Data Processing:**\n    *   Datasets are loaded using `ReftDataset` subclasses like `ReftSupervisedDataset`, `ReftRewardDataset`, `LoReftGLUEDataset` for task-specific formatting. Custom functions like `make_multiple_position_supervised_data_module` are used for specific data preparation (e.g., DPO).\n    *   `ReftDataCollator` handles padding of `input_ids`, `labels`, and `intervention_locations` to the longest sequence in a batch.\n    *   Tokenizers are configured with `padding_side=\"right\"` for training and `\"left\"` for generation tasks to enable interventions on prompts.\n\n*   **Inference & Evaluation:**\n    *   Text generation is performed using `reft_model.generate`, passing `unit_locations` for dynamic intervention application. Decoding strategies include `greedy_decoding`, `temperature`, `top_p`, `top_k`, and `num_beams` (e.g., 4 for complex reasoning tasks, 1 for greedy).\n    *   Evaluation metrics vary by task (e.g., Exact Match for reasoning, accuracy for GLUE/reward modeling, DPO specific metrics).\n\n*   **Model Saving:** The intervened model is saved using `reft_model.save()` which saves only the intervention parameters and references the original model, allowing for efficient deployment of small adapter weights."
      }
    },
    {
      "title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",
      "abstract": "Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its\nvariants, freeze pre-trained model weights \\(W\\) and inject learnable matrices\n\\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient\nparameterization, often using techniques like low-rank approximations or\nscaling vectors. However, these methods typically show a performance gap\ncompared to full fine-tuning. Although recent PEFT methods have narrowed this\ngap, they do so at the cost of additional learnable parameters. We propose\nSVFT, a simple approach that fundamentally differs from existing methods: the\nstructure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).\nSpecifically, SVFT updates \\(W\\) as a sparse combination of outer products of\nits singular vectors, training only the coefficients (scales) of these sparse\ncombinations. This approach allows fine-grained control over expressivity\nthrough the number of coefficients. Extensive experiments on language and\nvision benchmarks show that SVFT recovers up to 96% of full fine-tuning\nperformance while training only 0.006 to 0.25% of parameters, outperforming\nexisting methods that only recover up to 85% performance using 0.03 to 0.8% of\nthe trainable parameter budget.",
      "full_text": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors Vijay Lingam†∗ Atula Tejaswi†∗ Aditya Vavre†∗ Aneesh Shetty†∗ Gautham Krishna Gudur†∗ Joydeep Ghosh† Alex Dimakis† Eunsol Choi† Aleksandar Bojchevski‡ Sujay Sanghavi† †University of Texas at Austin ‡University of Cologne Abstract Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights W and inject learnable matrices ∆W. These ∆W matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically show a performance gap compared to full fine-tuning. Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters. We propose SVFT , a simple approach that fundamentally differs from existing methods: the structure imposed on ∆W depends on the specific weight matrix W. Specifically, SVFT updates W as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations. This approach allows fine-grained control over expressivity through the number of coefficients. Extensive experiments on language and vision benchmarks show that SVFT 2 recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25 % of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% of the trainable parameter budget. 1 Introduction Large-scale foundation models are often adapted for specific downstream tasks after pre-training. Parameter-efficient fine-tuning (PEFT) facilitates this adaptation efficiently by learning a minimal set of new parameters, thus creating an \"expert\" model. For instance, Large Language Models (LLMs) pre-trained on vast training corpora are fine-tuned for specialized tasks such as text summarization [12, 34], sentiment analysis [ 25, 20], and code completion [ 26] using instruction fine-tuning datasets. Although full fine-tuning (Full-FT) is a viable method to achieve this, it requires re-training and storing all model weights, making it impractical for deployment with large foundation models. To address these challenges, PEFT techniques [13] (e.g., LoRA [14]) were introduced to significantly reduce the number of learnable parameters compared to Full-FT, though often at the cost of perfor- mance. DoRA [18] bridges this performance gap by adding more learnable parameters and being more expressive than LoRA. Almost all these methods apply a low-rank update additively to the frozen pre-trained weights, potentially limiting their expressivity. Furthermore, these adapters are agnostic to the structure and geometry of the weight matrices they modify. Finally, more expressive PEFT methods (e.g., LoRA, DoRA, BOFT [19]) still accumulate a considerable portion of learnable parameters even in their most efficient configuration (e.g., setting rank=1 in LoRA and DoRA). The ∗indicates equal contribution. 2code is available at https://github.com/VijayLingam95/SVFT/ Preprint. Under review. arXiv:2405.19597v1  [cs.LG]  30 May 20240.3 0.5 0.85 1.5 2.5 4 7 12 20.5 35 Number of Trainable Params (M) 32.5 35.0 37.5 40.0 42.5 45.0 47.5 50.0 52.5 55.0 SVFTP SVFTB d = 2 SVFTB d = 4 SVFTB d = 8 SVFTB d = 16 SVFTR d = 16 LoRAr = 1 DoRAr = 1 LoRAr = 32 VeRAr = 1024 VeRAr = 2048 BOFTm = 2 b = 8 DoRAr = 16 DoRAr = 4 LoRAr = 4 Full Fine-Tuning (2500M params) 0.3 0.5 0.85 1.5 2.5 4 7 12 20.5 35 Number of Trainable Params (M) 50.0 52.5 55.0 57.5 60.0 62.5 65.0 67.5 70.0 SVFTP SVFTB d = 2 SVFTB d = 4 SVFTB d = 8 SVFTB d = 16 DoRAr = 16 DoRAr = 4 LoRAr = 32 LoRAr = 1 DoRAr = 1 VeRAr = 2048 BOFTm = 2 b = 8 Full Fine-Tuning (2500M params) Accuracy (%) Figure 1: Performance vs total trainable parameters for GSM-8K (left) and Commonsense Reasoning (right) on Gemma-2B. SVFT B/R d=16 outperforms DoRAr=8/16 with 75% less trainable parameters. storage requirements for the learnable adapters can grow very quickly when adapting to a large number of downstream tasks [16]. Is it possible to narrow the performance gap betweenSVFT and Full-FT while being highly parameter- efficient? We propose SVFT : Singular Vectors guided Fine-Tuning — asimple approach that involves updating an existing weight matrix by adding to it a sparse weighted combination of its own singular vectors. The structure of the induced perturbation in SVFT depends on the specific matrix being per- turbed, setting it apart from all previous approaches. Our contributions can be summarized as follows: • We introduce SVFT , a new PEFT method. Given a weight matrix W, SVFT involves adapting it with a matrix ∆W := P (i,j)∈Ω mijuivT j where the {ui} and {vj} are the left and right singular vectors of W, Ω is an a-priori fixed sparsity pattern, and mij for (i, j) ∈ Ω are learnable parameters. By controlling |Ω| we can efficiently explore the accuracy vs parameters trade-off. • SVFT achieves higher downstream accuracy, as a function of the number of trainable parameters, as compared to several popular PEFT methods (see Figure 1) and over several downstream tasks across both vision and language tasks. Our method recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% the trainable parameter budget. We introduce four variants for parameterizing weight updates, namely: Plain, Random, Banded, and Top-k in SVFT (which differ in their choices of the fixed sparsity patternΩ) and validate these design choices empirically. Additionally, we theoretically show that for any fixed parameters budget, SVFT can induce a higher rank perturbation compared to previous PEFT techniques. 2 Related Work Recent advancements in large language models (LLMs) have emphasized the development of PEFT techniques to enhance the adaptability and efficiency of large pre-trained language models. LoRA. A notable contribution in this field is Low-Rank Adaptation (LoRA) [14], which freezes the weights of pre-trained models and integrates trainable low-rank matrices into each transformer layer. For a pre-trained weight matrix W0 ∈ Rd×n, LoRA constraints the weight update ∆W to a low-rank decomposition: h = W0x + ∆Wx = W0x + BAx, where B ∈ Rd×r, A ∈ Rr×n and rank r ≪ min(d, n). We underline the (trainable) parameters that are updated via gradient descent. LoRA variants. We highlight some recent approaches that further improve the vanilla LoRA architecture. Vector-based Random Matrix Adaptation (VeRA) [ 16] minimizes the number of trainable parameters by utilizing a pair of low-rank random matrices shared between layers and learning compact scaling vectors while maintaining performance comparable to LoRA. Formally, 2Figure 2: Schematic comparison of LoRA, VeRA, DoRA, and SVFT (left to right). VeRA can be expressed as:h = W0x+∆Wx = W0x+ΛbBΛdAx, where A and B are initialized randomly, frozen, and shared across layers, while Λb and Λd are trainable diagonal matrices. An alternative approach, Weight-Decomposed Low-Rank Adaptation (DoRA) [18], decomposes pre- trained weight matrices into magnitude and direction components, and applies low-rank updates for directional updates, reducing trainable parameters and enhancing learning capacity and training sta- bility. DoRA can be expressed as: h = m W0+∆W ∥W0+∆W∥c x = m W0+BA ∥W0+BA∥c x, where ∥ · ∥c denotes the vector-wise norm of a matrix across each column. Similar to LoRA, W0 remains frozen, whereas the magnitude vector m (initialized to ∥W0∥c) and low-rank matrices A, B contain trainable parameters. AdaLoRA [35] adaptively distributes the parameter budget across weight matrices based on their importance scores and modulates the rank of incremental matrices to manage this allocation effectively. PiSSA (Principal Singular Values and Singular Vectors Adaptation) [21] is another variant of LoRA, where matrices A, B are initialized with principal components of SVD and the remaining components are used to initialize W0. FLoRA [31] enhances LoRA by enabling each example in a mini-batch to utilize distinct low-rank weights, preserving expressive power and facilitating efficient batching, thereby extending the domain adaptation benefits of LoRA without batching limitations. Other PEFT variants. Orthogonal Fine-tuning (OFT) [24] modifies pre-trained weight matrices through orthogonal reparameterization to preserve essential information. However, it still requires a considerable number of trainable parameters due to the high dimensionality of these matrices. Butterfly Orthogonal Fine-tuning (BOFT) [19] extends OFT’s methodology by incorporating Butterfly factorization thereby positioning OFT as a special case of BOFT. Unlike the additive low-rank weight updates utilized in LoRA, BOFT applies multiplicative orthogonal weight updates, marking a significant divergence in the approach but claims to improve parameter efficiency and fine-tuning flexibility. BOFT can be formally expressed as: h = (R(m, b) · W0)x, where the orthogonal matrix R(m, b) ∈ Rd×d is composed of a product of multiple orthogonal butterfly components. When m = 1, BOFT reduces to block-diagonal OFT with block size b. When m = 1 and b = d, BOFT reduces to the original OFT with an unconstrained full orthogonal matrix. 3 Method In this section, we introduce Singular Vectors guided Fine-Tuning (SVFT). The main innovation in SVFT lies in applying structure/geometry-aware weight updates. 3.1 SVFT Formulation We now formally describe our method, SVFT for parameter-efficient fine-tuning of a pre-trained model. Let W0 ∈ Rd1×d2 denote a weight matrix in the pre-trained model. For instance, in a transformer block, this could be the key matrix, the query matrix, a matrix in the MLP, etc. We add a structured, learned ∆W to this matrix as follows. As a first step, we compute the Singular Value Decomposition (SVD) of the given matrix: W0 = UΣV T . That is, U is the d1 × d1 matrix of left singular vectors (i.e., its columns are orthonormal), V T is the d2 × d2 matrix of right singular vectors (i.e., its rows are orthonormal), and Σ is a d1 × d2 diagonal matrix. Then, we parameterize our weight update as ∆W = UM V T , where U, V are 3Figure 3: An Overview of SVFT . The original weights W are decomposed into U, Σ, V . Here, M contains all the trainable parameters, which can be configured into patterns such as Plain, Random, Banded, and Top-k, represented by patterns of trainable (orange) and zero (gray) elements. fixed and frozen, while M is a d1 × d2 sparse trainable matrix with pre-determined and fixed sparsity pattern3. That is, we first pre-determine a small fixed set of elements in M that will be allowed to be non-zero and train only those elements. The forward pass for SVFT can be written as, h = W0x + ∆Wx = U(Σ + M)V T x (1) We explore four choices for Ω, the a-priori fixed sparsity pattern of M. Plain \u0000 SVFT P \u0001 . In this variant, we constrain M to be a diagonal matrix, which can be interpreted as adapting singular values and reweighting the frozen singular vectors. Since only the diagonal elements are learned, this is the most parameter-efficient SVFT variant. Banded \u0000 SVFT B d \u0001 . In this approach, we populate M using a banded matrix, progressively making off-diagonals learnable. Specifically, for constants z1 and z2, Mij = 0 if j < i− z1 or j > i+ z2, where z1, z2 ≥ 0. In our experiments, we set z1 = z2 = d to induce off-diagonal elements that capture additional interactions beyond those represented by singular values. This banded perturbation induces local interactions, allowing specific singular values to interact with their immediate neighbors, ensuring smoother transitions. This method, although deviating from the canonical form of SVD, provides a mechanism to capture localized interactions. Random \u0000 SVFT R d \u0001 . A straightforward heuristic for populating M involves randomly selecting k elements to be learnable. Top-k \u0000 SVFT T d \u0001 . The final design choice we explore involves computing the alignment between the left and right singular vectors as uT i vj. We then select the top- k elements and make them learnable. However, note that this only works when left and right singular vectors have the same size. A possible interpretation of this is we make only the top-k strong interactions between singular vector directions learnable. We illustrate these SVFT design choices in Figure 3. Our empirical results demonstrate that these simple design choices significantly enhance performance compared to state-of-the-art PEFT methods. Note that SVFT P has a fixed number of learnable parameters, while the remaining variants are configurable. We hypothesize that further innovation is likely achievable through optimizing the sparsity pattern of M, including efficient learned-sparsity methods. In this paper, we explore these four choices to validate the overall idea: determining a perturbation using the singular vectors of the matrix that is being perturbed. 3.2 Properties of SVFT We highlight some properties of SVFT in the following lemma and provide insights into how its specific algebraic structure compares and contrasts with baseline PEFT methods. Lemma: Let W0 be a matrix of size d1 × d2 with SVD given by UΣV T . Consider an updated final matrix W0 + UMV T , where M is a matrix of the same size as Σ, which may or may not be diagonal. Then, the following holds: 3Learnable parameters are underlined. 4(a) Structure: If M is also diagonal (i.e. the plain SVFT ), then the final matrix W0 + UMV T has U as its left singular vectors and sign(Σ + M)V T as its right singular vectors. That is, its singular vectors are unchanged, except for possible sign flips. Conversely, if M is not diagonal (i.e., variants of SVFT other than plain), then U and V may no longer be the singular directions of the final matrix W0 + UMV T . (b) Expressivity: Given any target matrix P of size d1 × d2, there exists an M such that P = W0 + UMV T . That is, if M is fully trainable, any target matrix can be realized using this method. (c) Rank: If M has k non-zero elements, then the rank of the update UMV T is at most min{k, min{d1, d2}}. For the same number of trainable parameters, SVFT can produce a much higher rank perturbation than LoRA (eventually becoming full rank), but in a constrained structured subspace. We provide our proofs in Appendix A. Building on this lemma, we now compare the form of the SVFT update with LoRA and VeRA. SVFT’s ∆W can be written as a sum of rank-one matrices: ∆W = X (i,j)∈Ω mijuivT j (2) where ui is the ith left singular vector, vj is the jth right singular vector, and Ω is the set of non-zero elements in M. Thus, our method involves adding a weighted combination of specific rank-one perturbations of the form uivT j . LoRA and VeRA updates can also be expressed as sums of rank-one matrices. ∆WLoRA = rX i=1 ai bi T and ∆WVeRA = rX i=1 αi(ˆai ⊙ β)ˆbT i (3) where ai and bj are the trainable columns of A and B matrices in LoRA. In VeRA, ˆai and ˆbi are random and fixed vectors, while α and β represent the diagonal elements of Λd and Λb respectively. Note that LoRA requires d1 + d2 trainable parameters per rank-one matrix, while SVFT and VeRA require only one. Although LoRA can potentially capture directions different from those achievable by the fixed {ui, vT j } pairs, each of these directions incurs a significantly higher parameter cost. VeRA captures new directions at a parameter cost similar toSVFT ; however, there is a key distinction: in VeRA, each vector ˆai or ˆbi appears in only one of the rank-one matrices. In contrast, in SVFT , the same vector ui can appear in multiple terms in the summation, depending on the sparsity pattern of M. This results in an important difference: unlike SVFT , VeRA is not universally expressive – it cannot represent any target matrix P. Moreover, ˆai, ˆbi are random, while ui, vj depend on W0. Note. SVFT requires storing both left and right singular vectors due to its computation of the SVD on pre-trained weights. While this increases memory usage compared to LoRA (which is roughly double), it remains lower than BOFT. We partially address this through system-level optimizations like mixed-precision weights (e.g., bfloat16). Further exploration of memory-reduction techniques, such as quantization, is planned as future work. Importantly, inference time and memory consumption remain the same across all methods, including SVFT, as the weights can be fused. 4 Experiments 4.1 Base Models We adapt widely-used language models, encoder-only model (DeBERTaV3base [10]) and two decoder- only models (Gemma-2B/7B [29], LLaMA-3-8B [1]). We also experiment with vision transformer models (ViT-B/16 and ViT-L/16) [9]) pre-trained on ImageNet-21k [8], following prior work [16]. 5The complete details of our experimental setup and hyperparameter configurations are provided in Appendix C. Baselines. We compare with Full Fine-Tuning (FT) updating all learnable parameters in all layers, along with LoRA [14], DoRA [18], BOFT [19] and VeRA [16].4 4.2 Datasets Language. For natural language generation (NLG) tasks, we evaluate on GSM-8K [ 7] and MATH [11] by fine-tuning on MetaMathQA-40K [ 32], following [ 19]. We also evaluate on 8 commonsense reasoning benchmarks (BoolQ [ 5], PIQA [ 3], SIQA [ 28], HellaSwag [ 33], Wino- grande [27], ARC-easy/challenge [6], and OpenBookQA [22]). We follow the setting outlined in prior work [ 18, 15], where the training sets of all benchmarks are amalgamated for fine-tuning. We fine-tune on 15K examples from this training set. For natural language understanding (NLU), we evaluate on the General Language Understanding Evaluation (GLUE) benchmark consisting of classification and regression tasks, in line with [16, 14]. Vision. Our experiments on vision tasks consist of 4 benchmarks: CIFAR-100 [17], Food101 [4], RESISC45 [30], and Flowers102 [23]. We follow the setup from [ 16], and fine-tune on a subset comprising 10 samples from each class. Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params denote the number of trainable parameters. bold and underline represent best and second best performing PEFT method, respectively. SVFT offers superior/competitive performance at much lower #Params. For SVFT R d , we set d = 16 for Gemma and d = 12 for LLaMA-3 models. Method Gemma-2B Gemma-7B LLaMA-3-8B #Params GSM-8K MATH #Params GSM-8K MATH #Params GSM-8K MATH Full-FT 2.5B 52.69 17.94 8.5B 74.67 25.70 8.0B 64.13 16.24 LoRAr=32 26.2M 43.06 15.50 68.8M 76.57 29.34 56.6M 75.89 24.74 DoRAr=16 13.5M 44.27 16.18 35.5M 74.52 29.84 29.1M 75.66 24.72 BOFTb=8 m=2 1.22M 36.01 12.13 2.90M 71.79 28.98 4.35M 67.09 21.64 DoRAr=1 1.19M 35.25 13.04 3.26M 74.37 26.28 2.55M 68.30 21.96 LoRAr=1 0.82M 32.97 13.04 0.82M 72.4 26.28 1.77M 68.84 20.94 VeRAr=1024 0.63M 36.77 14.12 0.43M 71.11 27.04 0.98M 63.76 20.28 SVFT P 0.19M 40.34 14.38 0.43M 73.50 27.30 0.48M 69.22 20.44 SVFT R d 6.35M 50.03 15.56 19.8M 76.81 29.98 13.1M 75.90 24.22 5 Results 5.1 Performance on Language Tasks Natural Language Generation. We present results on mathematical question answering against baseline PEFT techniques across three base models – varying from 2B to 8B parameters in Table 1. To ensure a comprehensive comparison, we test baseline techniques (LoRA, DoRA) with different configurations, and varying hyper-parameters like rank to cover a range of learnable parameters from low to high. Note that even when the rank is as low as 1, both methods yield more trainable parameters than SVFT P . SVFT P (∼0.2M) shows as much as 18% relative improvement over techniques that use 6× more trainable parameters (BOFTb=8 m=2, LoRAr=1). Against techniques of comparable sizes (VeRA), SVFT P achieves 15.5% relative improvement on average. Even in the default regime, SVFT R d matches techniques with at least 3× more trainable parameters. Notably, 4BOFT is approximately three times slower than LoRA. The shared matrices in VERA can become a limiting factor for models with non-uniform internal dimensions, such as LLaMA-3. 6Table 2: Evaluation results on eight commonsense reasoning benchmarks with Gemma-7B. We follow [18] for hyperparameter configurations, and report accuracy for all tasks. HS and WG denote HellaSwag [33] and WinoGrande [27], respectively. SVFTP offers competitive performance at a fraction of #Params. SVFTB d=8 can match LoRAr=32 with ∼7x fewer parameters. Method #Params BoolQ PIQA SIQA HS WG ARC-e ARC-c OBQA Average Full-FT 8.5B 72.32 87.32 76.86 91.07 81.76 92.46 82.76 89.00 84.19 LoRAr=32 68.8M 71.55 87.95 77.27 91.80 79.71 92.67 82.16 86.40 83.69 DoRAr=16 35.5M 71.46 87.59 76.35 92.11 78.29 92.00 80.63 85.60 83.00 DoRAr=1 3.31M 68.22 86.72 75.23 91.14 78.13 91.87 83.19 86.20 82.59 VeRAr=2048 1.49M 64.25 86.28 74.04 86.96 69.00 92.76 82.33 82.00 79.70 LoRAr=1 0.82M 65.44 86.28 75.02 89.91 75.92 91.79 81.91 85.40 81.46 SVFT P 0.51M 67.92 86.45 75.47 86.92 74.03 91.80 81.23 83.00 80.85 SVFT B d=8 9.80M 71.90 86.98 76.28 91.55 78.76 92.80 83.11 85.40 83.35 Table 3: DeBERTaV3base with different adaptation methods on the GLUE benchmark. We report matched accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all tasks. * indicates numbers published in prior work. Method #Params MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. Full-FT* 184M 89.90 95.63 89.46 69.19 94.03 92.40 83.75 91.60 88.25 LoRA*r=8 1.33M 90.65 94.95 89.95 69.82 93.87 91.99 85.20 91.60 88.50 DoRAr=4 0.75M 89.92 95.41 89.10 69.37 94.14 91.53 87.00 91.80 88.53 BOFT*b=8 m=2 0.75M 90.25 96.44 92.40 72.95 94.23 92.10 88.81 91.92 89.89 LoRAr=1 0.17M 90.12 95.64 86.43 69.13 94.18 91.43 87.36 91.52 88.23 VeRAr=1024 0.09M 89.93 95.53 87.94 69.06 93.24 90.4 87.00 88.71 87.73 SVFT P 0.06M 89.69 95.41 88.77 70.95 94.27 90.16 87.24 91.80 88.54 SVFT R d=2 0.28M 89.97 95.99 88.99 72.61 93.90 91.50 88.09 91.73 89.10 on GSM-8K, SVFT R d again achieves 96% of the full fine-tuning performance, while DoRAr=16 recovers 86% with 2× more parameters than SVFT R d . Commonsense Reasoning. In Table 2, we compare performance on commonsense reasoning benchmarks with Gemma-7B, and observe similar trends. In the lower and moderately parameter- ized regime (∼0.43M), SVFT P shows competitive performance in comparison to LORAr=1 and DoRAr=1, which have 1.9× and 7.7× more parameters, respectively. Against VeRA, which trains 3.5× more parameters, SVFT P shows a relative improvement of ∼1.16%. Similarly, SVFT B d=8 also matches or exceeds methods that use up to 7× more trainable parameters. For instance, SVFT B d=8 attains an average performance of 83.35% with only 9.8M parameters, closely matching LoRAr=16 (83.69%, 68.8M parameters). We observe similar trends with Gemma-2B (refer Table 8). Natural Language Understanding. Results on the GLUE benchmark are summarized in Table 3. SVFT matches LoRAr=8 and DoRAr=4 which use 12-22× more trainable parameters. Similarly, when compared to OFT and BOFT, SVFT P maintains a comparable average performance despite being 12× smaller. These results highlight SVFT ’s ability to strike a balance between parameter efficiency and performance, making it an attractive PEFT choice for simple classification tasks. Parameter efficiency. In Figure 1, we plot the performance of SVFT on mathematical reasoning and commonsense reasoning against other PEFT techniques across a range of configurations. Across 7Table 4: Performance on image classification benchmarks. For LoRA, DoRA and SVFT B, we adapt {Q, K, V , U, D} modules of the transformer. ForSVFT P , we adapt only {Q, V} to keep it comparable with VeRA. We report accuracy for all tasks. Method ViT-B ViT-L #Params CIFAR100 Flowers102 #Params Food101 Resisc45 Head - 78.25 98.42 - 75.57 64.10 Full-FT 85.8M 85.35 98.37 303.3M 77.83 76.83 LoRAr=8 1.32M 84.10 99.23 3.54M 77.13 79.62 DoRAr=8 1.41M 85.03 99.30 3.76M 76.41 78.32 BOFTb=4 m=4 0.11M 85.54 98.59 2.95M 78.42 74.70 LoRAr=1 0.16M 84.86 96.88 0.44M 75.97 78.02 DoRAr=1 0.25M 84.46 99.15 0.66M 75.90 78.02 VeRAr=256 24.6K 83.38 98.59 0.06M 75.97 72.44 SVFT P 18.5K 83.85 98.93 0.05M 75.95 71.97 SVFT B d=2 0.27M 84.72 99.28 0.74M 77.94 79.70 SVFT B d=8 0.93M 85.69 98.88 2.5M 78.36 73.83 trainable parameter budgets ranging from lowest to highest, SVFT obtains the best overall perfor- mance, matching methods that require significantly more trainable parameters. These results establish SVFT as a Pareto-dominant approach for parameter-efficient fine-tuning. 5.2 Performance on Vision Tasks 0.05 0.1 0.2 0.4 0.8 1.6 3 5.5 Number of Trainable Params (M) 30 32 34 36 38 40 42 44 46 48Accuracy (%) Weight T ypes Q,V Q,K,V U,D Q,K,V,U,D Q,K,V,U,D,G,O Configuration P d = 2 d = 4 d = 8 Figure 4: Performance variation with SVFT B d based on the adapted weight matrices – GSM-8K with Gemma-2B. Adapting more target weight types re- sults in greater gains in performance. In- terestingly, for a fixed parameter budget, adapting U and D weight types gives greater lifts than adapting Q and V . Table 4 contrasts SVFT against other PEFT techniques on image classification benchmarks using ViT-B and ViT-L models. For ViT-B, SVFT B d=8 surpasses full fine-tuning performance along with LoRAr=8 and DoRAr=8 on CIFAR-100. SVFT B d=2 matches LoRAr=8 and DoRAr=8 on Flowers102 with up to5× fewer parameters. For ViT-L, SVFT B d also demonstrates superior or competitive perfor- mance on both Food101 and Resisc45, with significantly lower trainable parameters compared to both fully fine- tuned models and other state-of-the-art PEFT approaches. 5.3 Contribution of Each Weight Type In Figure 4, we investigate the contribution of each weight type. Starting with the base configuration, we apply SVFT B d to the Q and V weights in each transformer block and report the performance. We then incrementally add the remaining weight modules (K, U, D, O, G) and ob- serve the changes in performance. For each configuration, we also vary the trainable parameters by incrementing the total learnable off-diagonals. Note that applying SVFT B d to U, D, O, and G does not increase trainable parameters as much as applying LoRA/DoRA to these modules (Table 7). For example, for a large matrix of shape d1 × d2, LoRAr=1 learns d1 + d2 parameters, while SVFT P learns min(d1, d2) parameters. We observe that adapting only U and D with SVFT yields up to a 10% relative improvement over adapting 8Q and V for the same parameter budget (∼ 0.8M). Our findings indicate that adapting more weight types enhances performance. Table 5: Results on fine-tuning Gemma-2B with SVFT using different M parameterizations. Structure #Params GSM-8K MATH Plain 0.2M 40.34 14.38 Banded 3.3M 46.47 16.04 6.4M 47.84 15.68 Random 3.3M 47.76 15.98 6.4M 50.03 15.56 Top-k 3.3M 48.00 15.80 6.4M 49.65 15.32 Table 6: Impact of pre-trained weight qual- ity. Results on GSM-8K after fine-tuning on Pythia-2.8B checkpoints at different stages of pre-training (PT). Compared to LoRA, SVFT benefits more from better pre-trained weights. SVFT outperforms LoRA in both cases. Method #Params PT Steps ∆Perf 39K 143K Full-FT 2.5B 21.00 30.09 9.09 LoRA 5.24M 11.22 18.95 7.73 SVFT 5.56M 15.08 23.19 8.11 5.4 Impact of M’s Structure on Performance We analyze the impact of different parameterizations of M (Plain, Banded, Random, Top-k) on downstream performance. To ensure a fair comparison, we match the number of trainable coefficients across all variants. As shown in Table 5, both Random and Top-k variants outperform Banded on the GSM-8K dataset. However, this improvement comes at the cost of performance on MATH. This ob- servation suggests that the choice of parameterization has a significant impact on model performance, and the effectiveness of a particular structure may vary depending on the downstream task. 5.5 Impact of Pre-trained Weight Quality A key feature of SVFT is that the weight update depends on the pre-trained weightsW. We therefore ask the following question: Does the quality of pre-trained weights have a disproportionate impact on SVFT ? To answer this, we consider two checkpoints from the Pythia suite [2] at different stages of training, i.e., 39K steps and 143K steps, respectively. We fine-tune each of these checkpoints independently with Full-FT, LoRA, andSVFT . We then compare the increase in performance (∆Perf). As shown in Table 6, compared to LoRA, SVFT benefits more from better pre-trained weights. We also note that SVFT outperforms LoRA in both settings, suggesting that the benefits of inducing a ∆W that explicitly depends on W are beneficial even when W is sub-optimal. 6 Discussion Limitations. Despite significantly reducing learnable parameters and boosting performance, SVFT incurs some additional GPU memory usage. Unlike LoRA and its variants, SVFT necessitates computing the SVD and storing both left and right singular vectors. While memory consumption remains lower than BOFT, it’s roughly double that of LoRA. We mitigate this in our work by employing system-level optimizations like mixed-precision weights (e.g., bfloat16). However, similar to the scaling explored in [31], memory usage should amortize with the increasing scale of adaptation tasks. In future work we will explore quantization and other techniques to address memory concerns. Broader Impact. Our work enables easier personalization of foundational models, which can have both positive and negative societal impacts. Since our method provides computational efficiency (smaller parameter footprint), it will be less expensive to enable personalization. 97 Conclusion This work introduces SVFT , a novel and efficient PEFT approach that leverages the structure of pre- trained weights to determine weight update perturbations. We propose four simple yet effective sparse parameterization patterns, offering flexibility in controlling the model’s expressivity and the number of learnable parameters. Extensive experiments on language and vision tasks demonstrate SVFT ’s effectiveness as a PEFT method across diverse parameter budgets. Furthermore, we theoretically show that SVFT can induce higher-rank perturbation updates compared to existing methods, for a fixed parameter budget. In future work, we aim to develop principled methods to generate sparsity patterns, potentially leading to further performance improvements. Acknowledgements We thank CISPA Helmholtz Center for Information Security and Greg Kuhlmann for their invaluable support in facilitating this research. We also appreciate Anubhav Goel for his helpful discussions and support. References [1] Meta AI. Introducing meta llama 3: The most capable openly available llm to date. April 2024. [2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023. [3] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. [5] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [10] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra- style pre-training with gradient-disentangled embedding sharing, 2023. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. 10[12] Karl Moritz Hermann, Tomáš Koˇciský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS’15, page 1693–1701. MIT Press, 2015. [13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2019. [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [15] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023. [16] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. ELoRA: Efficient low-rank adaptation with random matrices. In The Twelfth International Conference on Learning Repre- sentations, 2024. [17] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [18] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [19] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth International Conference on Learning Representations, 2024. [20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. [21] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. [22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018. [23] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008. [24] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. In Thirty-seventh Conference on Neural Information Processing Systems, volume 36, pages 79320–79362, 2023. [25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. [26] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. 11[27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [28] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com- monsense reasoning about social interactions, 2019. [29] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [30] Ihsan Ullah, Dustin Carrion, Sergio Escalera, Isabelle M Guyon, Mike Huisman, Felix Mohr, Jan N van Rijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh Vu. Meta-album: Multi- domain meta-dataset for few-shot image classification. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [31] Yeming Wen and Swarat Chaudhuri. Batched low-rank adaptation of foundation models. In The Twelfth International Conference on Learning Representations, 2024. [32] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023. [33] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [34] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328–11339. PMLR, 13–18 Jul 2020. [35] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. 12Appendix The appendix is organized as follows. • In Appendix A, we give proofs for the lemmas outlined in 3.2. • In Appendix B, we compare how the trainable parameters count for different PEFT tech- niques (LoRA, DoRA, VeRA) versus our method SVFT. • In Appendix C, we describe results for additional experiments and provide implementation details for all the experiments. A Proofs We provide brief proofs for the Structure, Expressivity and the Rank lemmas for SVFT: (a) Structure: If M is diagonal, then the final matrix W0 + UMV T can be written as U(Σ + M)V T since W0 = UΣV T , where (Σ + M) is also a diagonal matrix. Thus, U(Σ +M)V T is a valid and unique SVD of W0 + UMV T up to sign flips in the singular vectors. (b) Expressivity: Finding M for any target matrix P of size d1 × d2 such that P = W0 + UMV T is the same as finding M for a new target matrix P′ = P − W0 such that P′ = UMV T . For a full SVD, the dimension of M is d1 × d2 and since the dimension of P′ is also d1 × d2, P′ = UMV T is a bijection and M = UT (P − W0)V (since U and V are orthogonal). (c) Rank: If M has k non-zero elements, then the rank of the update UMV T will be upper bounded by k (since by Gaussian elimination, k or less elements will remain, the best case being all k elements in the diagonal). We also know that the rank is upper bounded by min{d1, d2}, giving an achievable upper bound on the rank as min{k, min{d1, d2}}. B Parameter Count Analysis Table 7: Parameter count analysis. Ltuned, Dmodel, r, k denote total layers being adapted, hidden dimension, rank, and additional off-diagonals respectively. Method Trainable Parameter Count LoRA 2 × Ltuned × Dmodel × r DoRA Ltuned × Dmodel × (2r + 1) VeRA Ltuned × (Dmodel + r) SVFTP Ltuned × Dmodel SVFTB d=k Ltuned × (Dmodel × k + (Dmodel − k)(k + 1)) C Additional Experiments and Implementation Details All of our experiments are conducted on a Linux machine (Debian GNU) with the following specifi- cations: 2xA100 80 GB, Intel Xeon CPU @ 2.20GHz with 12 cores, and 192 GB RAM. For all our experiments (including baseline experiments), we utilize hardware-level optimizations like mixed weight precision (e.g., bfloat16) whenever possible. C.1 Commonsense Reasoning Gemma-2B We evaluate and compare SVFT variants against baseline PEFT methods on commonsense reasoning tasks with Gemma-2B model and tabulate results in Table 8. 13Table 8: Results with Gemma-2B on eight commonsense reasoning benchmarks. We follow [18] for hyperparameter configurations, and report accuracy for all tasks. Method #Params BOOLQ PIQA SIQA HellaSwag Winogrande ARC-E ARC-C OBQA Average Full-FT 2.5B 63.57 74.1 65.86 70.00 61.95 75.36 59.72 69 67.45 LoRAr=32 26.2M 63.11 73.44 63.20 47.79 52.95 74.78 57.16 67.00 62.43 LoRAr=16 13.5M 62.87 73.93 65.34 53.16 55.51 76.43 59.55 68.4 64.40 BOFTb=8 m=2 1.22M 59.23 63.65 47.90 29.93 50.35 59.04 42.66 41.00 49.22 VeRAr=2048 0.66M 62.11 64.31 49.18 32.00 50.74 58.08 42.83 42.6 50.23 LoRAr=1 0.82M 62.2 69.31 56.24 32.47 51.53 69.52 48.8 56.4 55.81 DoRAr=1 1.19M 62.17 68.77 55.93 32.95 51.22 68.81 48.72 55.6 55.52 SVFT P 0.19M 62.26 70.18 56.7 32.47 47.04 69.31 50.08 58.4 55.81 SVFT B d=16 6.35M 63.42 73.72 63.86 71.21 59.58 73.69 54.77 66.6 65.86 Table 9: Performance on image classification benchmarks. For LoRA, DoRA and SVFT B d , we adapt {Q, K, V , U, D} modules of the transformer. ForSVFT P , we adapt only {Q, V} to keep it comparable with VeRA. We report accuracy for all tasks. Method ViT-B ViT-L #Params CIFAR100 Flowers102 Food101 Resisc45 #Params CIFAR100 Flowers102 Food101 Resisc45 Head - 78.25 98.42 74.93 59.95 - 82.95 98.75 75.57 64.10 Full-FT 85.8M 85.35 98.37 76.32 68.03 303.3M 86.56 97.87 77.83 76.83 LoRAr=8 1.32M 84.41 99.23 76.02 76.86 0.35M 86.00 97.93 77.13 79.62 DoRAr=8 1.41M 85.03 99.30 75.88 76.95 3.76M 83.55 98.00 76.41 78.32 BOFTb=2m=2 0.07M 85.55 98.54 76.06 67.70 0.20M 87.84 97.95 77.90 73.97 BOFTb=4m=4 0.11M 85.54 98.59 76.51 69.44 0.30M 87.72 97.95 78.42 74.70 LoRAr=1 0.16M 84.86 96.88 73.35 76.33 0.44M 85.97 98.28 75.97 78.02 DoRAr=1 0.25M 84.46 99.15 74.80 77.06 0.66M 84.06 98.11 75.90 78.02 VeRA 24.6K 83.38 98.59 75.99 70.43 61.4K 86.77 98.94 75.97 72.44 SVFTP 18.5K 83.85 98.93 75.68 67.19 49.2K 86.74 97.56 75.95 71.97 SVFTB d=2 0.28M 84.72 99.28 75.64 72.49 0.74M 86.59 98.24 77.94 79.70 SVFTB d=4 0.50M 83.17 98.52 76.54 66.65 1.32M 87.10 97.71 76.67 71.10 SVFTB d=8 0.94M 85.69 98.88 76.70 70.41 2.50M 87.26 97.89 78.36 73.83 C.2 Additional Vision Experiments For vision tasks, we compare the SVFT banded variants and SVFT plain with baseline PEFT methods on classification vision tasks using ViT-Base and ViT-Large models in Table 9. C.3 Are All Singular Vectors Important? To determine the importance of considering all singular vectors and singular values during fine- tuning, we reduce the rank of U and V , and truncate Σ and M to an effective rank of r. If the original weight matrix W ∈ Rm×n, then after truncation, U ∈ Rm×r, V ∈ Rn×r. This truncation significantly reduces the number of trainable parameters, so we compensate by increasing the number of off-diagonal coefficients (d) in M. Our results, with four different configurations ofr and d, are presented in Table 10. The findings show that a very low rank (r = 128) leads to poor performance, even when parameters are matched. A reasonably high rank of r = 1536, which is 75% of the full rank, still fails to match the performance of the full-rank variant that has 0.25× the trainable parameters. This indicates that all singular vectors 14significantly contribute to the end task performance when fine-tuning with SVFT , and that important information is lost even when truncating sparingly. Table 10: Performance with varying rank ( r) and the off-diagonal elements ( d) of M. When r = 2048, the update is full-rank. Rank (r) Diags ( d) #Params GSM-8K MATH 128 64 1.55M 0.98 0.21 1536 - 0.15M 16.37 3.64 1536 2 0.74M 25.01 6.04 2048 - 0.19M 40.34 14.38 C.4 Performance vs Total Trainable Parameters In addition to the experiments performed in Figure 1 for Gemma-2B on challenging natural language generation (NLG) tasks like GSM-8K and Commonsense Reasoning, we also plot the performance vs total trainable parameters for larger state-of-the-art models like Gemma-7B and LLaMA-3-8B on GSM-8K. Figure 5 further demonstrates SVFT’s Pereto-dominance. On larger models, we observe that full-finetuning overfits, leading to sub-optimal performance in comparison to PEFT methods. 0.5 0.75 1.2 2 3 5 8 12.5 20 32 50 84 Number of Trainable Params (M) 70 71 72 73 74 75 76 77 78 SVFTP SVFTB d = 2 SVFTR d = 16 DoRAr = 16 DoRAr = 4 LoRAr = 32 LoRAr = 1 DoRAr = 1 VeRAr = 1024 BOFTm = 2 b = 8 LoRAr = 4 Full Fine-Tuning (8500M params) 0.5 0.75 1.2 2 3 5 8 12.5 20 32 50 81 Number of Trainable Params (M) 62 64 66 68 70 72 74 76 78 80 SVFTP SVFTB d = 2 SVFTB d = 8 SVFTB d = 12 DoRAr = 16 LoRAr = 32 LoRAr = 1 DoRAr = 1 VeRAr = 1024 BOFTm = 2 b = 8 LoRAr = 4 Full Fine-Tuning (2500M params) Accuracy (%) Figure 5: Performance versus total trainable parameters for GSM-8K on Gemma-7B (left) and LLaMA-3-8B (right). C.5 Settings for Language Tasks Natural Language Understanding. We fine-tune the DeBERTaV3base [10] model and apply SVFT to all linear layers in every transformer block of the model. We only moderately tune the batch size, learning rate, and number of training epochs. We use the same model sequence lengths used by [ 19] to keep our comparisons fair. The hyperparameters used in our experiments can be found in Table 11. Natural Language Generation. See the hyperparameters used in our experiments in Table 12. For LoRA, DoRA, we adapt Q, K, V, U, Dmatrices. We apply BOFT on Q, Vmatrices since applying on multiple modules is computationally expensive. For VeRA, which enforces a constraint of uniform internal dimensions for shared matrices, we apply on G, Uprojection matrices as it yields the highest number of learnable parameters. We apply SVFT on Q, K, V, U, D, O, Gfor the Gemma family of models, and U, D, O, Gfor LLaMA-3-8B. Note that applying SVFT on these modules does not increase trainable parameters at the same rate as applying LoRA or DoRA on them would. We adopt the code base from https://github.com/meta-math/MetaMath.git for training scripts and evaluation setups and use the fine-tuning data available at https://huggingface.co/datasets/ meta-math/MetaMathQA-40K. 15Table 11: Hyperparameter setup used for DeBERTaV3base on the GLUE benchmark. Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Optimizer AdamW Warmup Ratio 0.1 LR Schedule Linear Learning Rate (Head) 6E-03 Max Seq. Len. 256 128 320 64 512 320 320 128 # Epochs 10 10 30 20 10 6 15 15 SVFTP Batch Size 32 32 16 16 32 16 4 32 Learning Rate 5E-02 5E-02 5E-02 8E-02 8E-02 5E-02 5E-02 5E-02 SVFTRd=2 Batch Size 32 32 16 16 32 32 16 32 Learning Rate 1E-02 1E-02 1E-02 1E-02 3E-02 1E-02 3E-02 1E-02 Table 12: Hyperparameter setup used for fine-tuning on MetaMathQA-40K. Hyperparameter Gemma-2B Gemma-7B LLaMA-3-8B SVFT P SVFT R d=16 SVFT P SVFT R d=16 SVFT P SVFT R d=12 Optimizer AdamW Warmup Ratio 0.1 LR Schedule Cosine Learning Rate 5E-02 1E-03 5E-02 1E-03 5E-02 1E-03 Max Seq. Len. 512 # Epochs 2 Batch Size 64 Commonsense Reasoning. See the hyperparameters used in our experiments in Table 13. We adopt the same set of matrices as that of natural language generation tasks. We use the code base from https://github.com/AGI-Edgerunners/LLM-Adapters, which also contains the training and evaluation data. Table 13: Hyperparameter setup used for fine-tuning on commonsense-15K. Hyperparameter Gemma-2B Gemma-7B SVFT P SVFT B d=8 SVFT P SVFT B d=8 Optimizer AdamW Warmup Steps 100 LR Schedule Linear Max Seq. Len. 512 # Epochs 3 Batch Size 64 Learning Rate 5E-02 5E-03 5E-02 1E-03 16Table 14: Hyperparameter setup used for fine-tuning on all vision tasks. Hyperparameter ViT-B ViT-L Optimizer AdamW Warmup Ratio 0.1 Weight Decay 0.01 LR Schedule Linear # Epochs 10 Batch Size 64 SVFT P Learning Rate (Head) 4E-03 SVFT P Learning Rate 5E-02 SVFT B d=2 Learning Rate (Head) 4E-03 SVFT B d=2 Learning Rate 5E-02 SVFT B d=8 Learning Rate (Head) 4E-03 SVFT B d=8 Learning Rate 5E-03 C.6 Settings for Vision Tasks For each dataset in the vision tasks, we train on 10 samples per class, using 2 examples per class for validation, and test on the full test set. Similar to previous literature, we always train the classifier head for these methods since the number of classes is large. The parameter counts do not include the number of parameters in the classification head. The hyperparameters are mentioned in Table 14. We tune the learning rates for SVFT and BOFT select learning rates for other methods from [16], run training for 10 epochs, and report test accuracy for the best validation model. For all methods, since classification head has to be fully trained, we report the parameter count other than the classification head. 17",
      "meta_data": {
        "arxiv_id": "2405.19597v1",
        "authors": [
          "Vijay Lingam",
          "Atula Tejaswi",
          "Aditya Vavre",
          "Aneesh Shetty",
          "Gautham Krishna Gudur",
          "Joydeep Ghosh",
          "Alex Dimakis",
          "Eunsol Choi",
          "Aleksandar Bojchevski",
          "Sujay Sanghavi"
        ],
        "published_date": "2024-05-30T01:27:43Z",
        "pdf_url": "https://arxiv.org/pdf/2405.19597v1.pdf",
        "github_url": "https://github.com/VijayLingam95/SVFT"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces SVFT (Singular Vectors guided Fine-Tuning), a novel Parameter-Efficient Fine-Tuning (PEFT) method that addresses the performance gap between existing PEFT techniques and full fine-tuning, while maintaining high parameter efficiency. SVFT updates pre-trained weight matrices as a sparse combination of outer products of their singular vectors, training only the coefficients of these combinations. This approach ensures the perturbation structure depends on the specific weight matrix, unlike prior methods. SVFT recovers up to 96% of full fine-tuning performance using 0.006% to 0.25% of parameters, outperforming existing PEFT methods that typically recover up to 85% performance with 0.03% to 0.8% of the parameter budget. The work also introduces and empirically validates four variants for parameterizing weight updates (Plain, Random, Banded, Top-k) and theoretically demonstrates that SVFT can induce higher-rank perturbations for a given parameter budget compared to other PEFT techniques.",
        "methodology": "SVFT operates by first performing Singular Value Decomposition (SVD) on a pre-trained weight matrix W0 (W0 = UΣV^T). The weight update ∆W is then parameterized as ∆W = UMV^T, where U and V are fixed and frozen left and right singular vectors of W0, and M is a d1 × d2 sparse trainable matrix. The forward pass becomes h = W0x + ∆Wx = U(Σ + M)V^T x. The sparsity pattern of M is pre-determined and fixed, with four main variants explored: Plain (SVFTP) where M is diagonal, Banded (SVFTB d) where M is a banded matrix, Random (SVFTR d) where k elements are randomly selected to be learnable, and Top-k (SVFTT d) where k elements are selected based on the alignment u^T_i v_j. This method allows for a single trainable parameter (m_ij) per rank-one matrix in the update, in contrast to methods like LoRA which require d1+d2 parameters.",
        "experimental_setup": "The research adapted widely-used language models (encoder-only DeBERTaV3base, decoder-only Gemma-2B/7B, LLaMA-3-8B) and vision transformer models (ViT-B/16, ViT-L/16 pre-trained on ImageNet-21k). Baselines included Full Fine-Tuning (FT), LoRA, DoRA, BOFT, and VeRA. Language tasks comprised Natural Language Generation (GSM-8K, MATH fine-tuned on MetaMathQA-40K), Commonsense Reasoning (8 benchmarks like BoolQ, PIQA, HellaSwag, trained on an amalgamated 15K examples), and Natural Language Understanding (GLUE benchmark for classification/regression). Vision tasks included 4 benchmarks (CIFAR-100, Food101, RESISC45, Flowers102), fine-tuned on 10 samples per class. Performance was evaluated using accuracy for most tasks, Matthew's correlation for CoLA, and Pearson correlation for STS-B. Hardware-level optimizations like mixed-precision weights (bfloat16) were employed. SVFT was applied to various transformer modules (e.g., Q, K, V, U, D, O, G for LLMs; Q, V, K, U, D for Vision).",
        "limitations": "SVFT incurs additional GPU memory usage compared to LoRA, as it requires computing and storing both left and right singular vectors, roughly doubling LoRA's memory footprint (though still less than BOFT). While system-level optimizations like mixed-precision weights (bfloat16) were used to mitigate this, further memory reduction techniques are needed. The paper also acknowledges that enabling easier personalization of foundational models through computational efficiency (smaller parameter footprint) can have both positive and negative societal impacts.",
        "future_research_directions": "Future work will focus on exploring further memory-reduction techniques for SVFT, such as quantization, to address the increased GPU memory usage. Additionally, the researchers plan to develop more principled methods for generating sparsity patterns within the trainable matrix M, aiming to potentially achieve further performance improvements beyond the empirically validated choices.",
        "experimental_code": "import time\nimport math\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, transpose\n\n\ndef create_orthonormal_matrix(A):\n    # returns an orthonormal matrix (square) of size (min(A.shape), min(A.shape))\n    Q, R = torch.qr(A)\n    return Q\n\n\ndef get_target_modules_list(model, target_modules):\n    target_names = []\n    for n, _ in model.named_modules():\n        if any(t in n for t in target_modules):\n            target_names.append(n)\n    return target_names\n\n\ndef replace_svft_with_fused_linear(model, target_modules_list):\n    print(\"Replacing SVFT layers with new Linear layers\")\n\n    # filter out svft layer\n    target_modules_list = [l for l in target_modules_list if \"svft_layer\" not in l]\n\n    for target_path in tqdm(reversed(target_modules_list), total=len(target_modules_list)):\n        parent_path = target_path[: target_path.rfind(\".\")] if \".\" in target_path else \"\"\n        target_name = target_path.split(\".\")[-1]\n        parent = model.get_submodule(parent_path) if parent_path else model\n        target = model.get_submodule(target_path)\n        in_dim = target.svft_layer.v.shape[1]\n        out_dim = target.svft_layer.u.shape[0]\n        if target.bias is None:\n            lin = torch.nn.Linear(in_dim, out_dim, bias=False)\n        else:\n            lin = torch.nn.Linear(in_dim, out_dim, bias=True)\n            lin.bias.data = target.bias.data\n        lin.weight.data = target.merge_and_unload()\n        parent.__setattr__(target_name, lin)\n\n\ndef create_and_replace_modules(model, target_modules_list, create_fn):\n    print(\"Replacing Linear layers with SVFT layers\")\n\n    for target_path in tqdm(reversed(target_modules_list), total=len(target_modules_list)):\n        parent_path = target_path[: target_path.rfind(\".\")] if \".\" in target_path else \"\"\n        target_name = target_path.split(\".\")[-1]\n        parent = model.get_submodule(parent_path) if parent_path else model\n        target = model.get_submodule(target_path)\n        parent.__setattr__(target_name, create_fn(target))\n\n\nclass SVFTLayer(nn.Module):\n    def __init__(self, u, s, v, off_diag, pattern=\"banded\", rank=None, fill_orthonormal=False):\n\n        \"\"\"\n        @inputs:\n            u: torch.Tensor. Left singular vectors of pre-trained weight matrix\n            s: torch.Tensor. Singular values of pre-trained weight matrix\n            v: torch.Tensor. Right singular vectors of pre-trained weight matrix\n            off_diag: int. Total off-diagonals to be used to populate matrix M (as referred in main paper)\n            pattern: str. Choices: \"banded\", \"random\", \"top_k\". Using \"banded\" with off_diag=1 simulates SVFT-plain\n            rank: int. Constraints how many singular vectors and values to use.\n            fill_orthonormal: bool. To determine if random orthonormal basis should be used\n        \"\"\"\n\n        super().__init__()\n\n        self.off_diag = off_diag\n        rank = s.shape[0] if rank is None else min(s.shape[0], rank)\n        self.n = rank\n        diff_rank = s.shape[0] - rank\n\n        if fill_orthonormal:\n            Q_u = torch.randn_like(u).to(s.device)\n            torch.nn.init.orthogonal_(Q_u)\n            Q_v = torch.randn_like(v).to(s.device)\n            torch.nn.init.orthogonal_(Q_v)\n\n            u = torch.cat([u[:, :rank], Q_u[:, :diff_rank]], dim=1)\n            v = torch.cat([v[:rank, :], Q_v[:diff_rank, :]], dim=0)\n            s = torch.cat([s[:rank], torch.zeros(diff_rank).to(s.device)], dim=0)\n            self.n = s.shape[0]\n\n        else:\n            s = s[:rank]\n            u = u[:, :rank]\n            v = v[:rank, :]\n\n        self.u = nn.Parameter(u.clone().detach().contiguous(), requires_grad=False)\n\n        s_pre = s.cpu().detach().clone().contiguous()\n        self.s_pre_edge_index = torch.sparse.spdiags(s_pre, torch.LongTensor([0]), (self.n, self.n)).coalesce().indices()\n        self.s_pre = nn.Parameter(s_pre, requires_grad=False)\n        \n        if pattern==\"banded\":  \n            diags = 2*self.off_diag + 1\n            offsets_positive = torch.arange(0, self.off_diag+1)\n            offsets_negative = torch.arange(-1, -self.off_diag-1, -1)\n            self.offsets  = torch.cat([offsets_positive, offsets_negative])\n            self.s_edge_index = torch.sparse.spdiags(torch.randn([diags, self.n]), self.offsets, (self.n, self.n)).coalesce().indices()\n            self.s = torch.nn.Parameter(torch.zeros(self.s_edge_index.shape[1]), requires_grad=True)\n\n        elif pattern==\"random\":\n            print(\"Random pattern\")\n            k = self.n*(2*self.off_diag+1) - self.off_diag*(self.off_diag+1)\n            rows = torch.randint(0, self.n, (k,))\n            cols = torch.randint(0, self.n, (k,))\n            self.s_edge_index = torch.stack([rows, cols])\n            self.s = torch.nn.Parameter(torch.zeros(k), requires_grad=True)\n\n        elif pattern==\"top_k\":\n\n            if u.shape == v.shape:\n                coeffs = u@v.T\n            else:\n                coeffs = u if u.shape[0]==u.shape[1] else v\n\n            k = self.n*(2*self.off_diag+1) - self.off_diag*(self.off_diag+1)\n            # Flatten the tensor to 1D\n            flattened_tensor = coeffs.contiguous().view(-1)\n            _, top_indices_flat = torch.topk(flattened_tensor, k)\n            num_rows, num_cols = coeffs.size()\n            rows = top_indices_flat // num_cols\n            cols = top_indices_flat % num_cols\n            self.s_edge_index = torch.stack([rows, cols])\n            self.s = torch.nn.Parameter(torch.zeros(k), requires_grad=True)\n       \n        torch.nn.init.kaiming_normal_(self.s[None, :])\n        self.s.squeeze()\n\n        self.register_buffer('s_pre_row', self.s_pre_edge_index[0])\n        self.register_buffer('s_pre_col', self.s_pre_edge_index[1])\n        self.register_buffer('s_row', self.s_edge_index[0])\n        self.register_buffer('s_col', self.s_edge_index[1])\n\n        self.gate = nn.Parameter(torch.tensor([0.], dtype=torch.float32), requires_grad=True)\n\n        self.v = nn.Parameter(v.clone().detach().contiguous(), requires_grad=False) \n\n\n    def forward(self, x):\n        x  = x @ self.get_weights() \n        return x\n\n\n    def get_weights(self):\n        s = SparseTensor(row=self.s_row, col=self.s_col, value=self.s*F.sigmoid(self.gate))\n        s_pre = SparseTensor(row=self.s_pre_row, col=self.s_pre_col, value=self.s_pre)\n        del_s = s_pre + s\n        weight = (del_s @ self.v).T\n        weight = weight @ self.u.T\n        return weight\n    \n\n    def merge_and_unload(self):\n        return self.get_weights().T.contiguous()\n\n   \nclass LinearWithSVFT(nn.Module):\n\n    def __init__(self, linear, off_diag, pattern=\"banded\", rank=None, fill_orthonormal=False):\n        \"\"\"\n        @inputs:\n                linear: torch.Tensor. Linear Layer that has to adapted\n                off_diag: int. total number off diagonals to be used if pattern is 'banded' \n                          for remaining patterns, equivalent number of learnable parameters are learnt\n                rank: SVD rank \n                fill_orthonormal: bool. To determine if random orthonormal basis should be used\n        \"\"\"\n        \n        super().__init__()\n\n        self.bias = linear.bias\n\n        # since linear.weight is on GPU, computing SVD will be significantly faster\n        svd = torch.linalg.svd(linear.weight, full_matrices=False)\n\n        self.svft_layer = SVFTLayer(svd[0],\n                                    svd[1],\n                                    svd[2],\n                                    off_diag=off_diag,\n                                    pattern=pattern,\n                                    rank=rank,\n                                    fill_orthonormal=fill_orthonormal)\n\n    def forward(self, x):\n        if self.bias is not None:\n            return self.svft_layer(x) + self.bias\n\n        else:\n            return self.svft_layer(x)\n\n    def merge_and_unload(self):\n        return self.svft_layer.merge_and_unload()\n\n\ndef freeze_model(model, exclude_list = None):\n    ''' Freeze all parameters of the model '''\n    if exclude_list is None:\n        exclude_list = []\n\n    for n, p in model.named_parameters():\n        if not any(e in n for e in exclude_list):\n            p.requires_grad = False\n\n@dataclass\nclass SVFTArguments:\n    adapter_name: str = field(default=\"svft\", metadata={\"help\": \"Adapter name.\"})\n    pattern: str = field(default=\"banded\", metadata={\"help\": \"Choose from 'banded', 'random', 'top_k'.\"})\n    off_diag: int = field(default=0, metadata={\"help\": \"Number of off-diagonal blocks.\"})\n    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n    rank: int = field(default=None, metadata={\"help\": \"Rank of the low-rank decomposition. Only used in truncated-SVFT\"})\n    fill_orthonormal: bool = field(default=False, metadata={\"help\": \"Initialize singular vectors from a random orthonomal bases. Only applicable if less than full-rank.\"})\n\n# Code from train() function relevant to SVFT adapter setup\nif svft_args.adapter_name == 'svft':\n    # for SVFT turn off gradient requirement for all layers\n    # PEFT library handles this internally\n    for param in model.parameters():\n        param.requires_grad = False\n\n    print(f\"Target Modules: {svft_args.target_modules}\")\n    assign_svft_layer = partial(LinearWithSVFT, \n                                off_diag=svft_args.off_diag, \n                                pattern=svft_args.pattern, \n                                rank=svft_args.rank, \n                                fill_orthonormal=svft_args.fill_orthonormal)\n    \n    create_and_replace_modules(model, get_target_modules_list(model, svft_args.target_modules), assign_svft_layer)\n\n# Code from train() function for merging SVFT layers after training\nif svft_args.adapter_name == 'svft':\n    replace_svft_with_fused_linear(model, get_target_modules_list(model, svft_args.target_modules))\n",
        "experimental_info": "SVFT operates by first performing Singular Value Decomposition (SVD) on a pre-trained weight matrix W0 (W0 = UΣV^T). The weight update ∆W is then parameterized as ∆W = UMV^T, where U and V are fixed and frozen left and right singular vectors of W0, and M is a d1 × d2 sparse trainable matrix. The forward pass becomes h = W0x + ∆Wx = U(Σ + M)V^T x. The sparsity pattern of M is pre-determined and fixed, with four main variants explored: Plain (SVFTP) where M is diagonal, Banded (SVFTB d) where M is a banded matrix, Random (SVFTR d) where k elements are randomly selected to be learnable, and Top-k (SVFTT d) where k elements are selected based on the alignment u^T_i v_j. A learnable gate parameter (self.gate) is used in the implementation to scale the trainable sparse matrix M with a sigmoid activation.\n\nKey configurable parameters for SVFT are:\n- `pattern`: Specifies the sparsity pattern of the trainable matrix M. Choices are \"banded\", \"random\", or \"top_k\".\n- `off_diag`: For the \"banded\" pattern, this defines the number of off-diagonals. For \"random\" and \"top_k\" patterns, this parameter is used to determine the total number of learnable elements.\n- `rank`: An integer specifying the SVD truncation rank, limiting the number of singular vectors and values used in the decomposition.\n- `fill_orthonormal`: A boolean flag. If `True`, and if the specified `rank` is less than the full rank, the singular vectors corresponding to the truncated ranks are initialized using random orthonormal bases.\n- `target_modules`: A list of strings identifying specific linear layer module names (e.g., [\"q_proj\", \"v_proj\"]) within the base model where the SVFT adaptation is applied. By default, it targets `[\"q_proj\", \"v_proj\"]`.\n\nDuring training, all parameters of the base model are frozen. After training, the SVFT layers are replaced with fused linear layers that incorporate the learned adaptations."
      }
    },
    {
      "title": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning",
      "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)\nhas emerged as a highly successful approach, with training only a small number\nof parameters without sacrificing performance and becoming the de-facto\nlearning paradigm with the increasing size of PLMs. However, existing PEFT\nmethods are not memory-efficient, because they still require caching most of\nthe intermediate activations for the gradient calculation, akin to fine-tuning.\nOne effective way to reduce the activation memory is to apply a reversible\nmodel, so the intermediate activations are not necessary to be cached and can\nbe recomputed. Nevertheless, modifying a PLM to its reversible variant is not\nstraightforward, since the reversible model has a distinct architecture from\nthe currently released PLMs. In this paper, we first investigate what is a key\nfactor for the success of existing PEFT methods, and realize that it's\nessential to preserve the PLM's starting point when initializing a PEFT method.\nWith this finding, we propose memory-efficient fine-tuning (MEFT) that inserts\nadapters into a PLM, preserving the PLM's starting point and making it\nreversible without additional pre-training. We evaluate MEFT on the GLUE\nbenchmark and five question-answering tasks with various backbones, BERT,\nRoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to\n84% of full fine-tuning with a negligible amount of trainable parameters.\nMoreover, MEFT achieves the same score on GLUE and a comparable score on the\nquestion-answering tasks as full fine-tuning. A similar finding is also\nobserved for the image classification task.",
      "full_text": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning Baohao Liao Shaomu Tan Christof Monz Language Technology Lab, University of Amsterdam {b.liao, s.tan, c.monz}@uva.nl Abstract Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it’s essential to preserve the PLM’s starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM’s starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task.1 1 Introduction 20 40 60 80 100 Activation Memory (%) 60 65 70 75 80 85 90Score (%) BERT RoBERTa BART OPT Full FT Prefix-Tuning LoRA MAM AdapterH AdapterP LST our MEFT1 Figure 1: Average performance of different tasks vs. activation mem- ory. The memory usage for full fine- tuning is denoted as 100%. Large-scale pre-trained models have achieved great success across various domains and applications [1, 2, 3, 4, 5, 6, 7, 8]. As their capabilities continue to evolve, the released pre-trained language models (PLMs) have grown exponentially in size, even reaching a scale of 100 billion parameters [ 3, 9, 10, 11, 12]. Consequently, it presents unprecedented challenges in effectively leveraging these models for downstream tasks due to limited computing resources. A historically common approach to adapting PLMs to down- stream tasks is updating all pre-trained parameters, full fine- tuning. Although full fine-tuning has yielded numerous state-of- the-art results, its applicability is limited in storage-constrained environments. This constraint arises from maintaining a com- plete copy of the fine-tuned model for each task. An alternative 1Code at https://github.com/baohaoliao/mefts. Up-to-date version at https://arxiv.org/abs/2306.00477. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.00477v4  [cs.CL]  19 Oct 2023adaptation approach is parameter-efficient fine-tuning (PEFT) [13, 14, 15, 16, 17, 18, 19] which involves selectively updating a small number of task-specific parameters while keeping the majority of the PLM’s parameters frozen. PEFT offers significant advantages in reducing storage requirements by only saving one general PLM alongside the modified parameters for each task. In addition to storage savings, PEFT achieves comparable performance to full fine-tuning, sparking considerable interest in the adoption of PEFT. Despite their advantages in parameter efficiency, existing PEFT methods still face challenges in terms of memory efficiency [ 20, 21]. PEFTs necessitate the caching of intermediate activations, similar to the requirements of full fine-tuning, to calculate the gradients of the trainable parameters. Typically, they consume more than 70% activation memory of full fine-tuning (see Figure 1). Since activations significantly contribute to the memory requirements during training, there are instances where fine-tuning a large-scale PLM with PEFT is not feasible due to memory constraints. To address this issue, a commonly employed approach is to treat the PLM as a feature extractor, such as knowledge distillation to a smaller model [22, 23], adding additional trainable layers on top [20] or aligned [21, 24] with it, and so on. These approaches circumvent the need to store the PLM’s activations since the gradient computation graph does not traverse through the PLM. However, these methods often require additional pre-training or exhibit a substantial performance gap compared to full fine-tuning when using the same underlying model [20, 21]. In this paper, we propose a novel method calledmemory-efficient fine-tuning (MEFT) to modify PLMs in a parameter- and memory-efficient manner, without requiring additional pre-training. Initially, we investigate a crucial factor for the success of existing PEFT methods and determine that the proper initialization of newly added parameters is essential to maintain the continuity of information from the PLM (§2). Leveraging this insight, we design three MEFT methods that enable the modification of a PLM to its reversible variant, so it only necessitates caching the final output and allows for the recomputation of intermediate activations during back-propagation (§3). Consequently, MEFT significantly reduces the memory required for caching activations (see Figure 1). To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark [25] with BERT [ 1], RoBERTa [ 2] and BART [ 26] (§4). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT [9]. The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. A similar finding is also observed on the image classification task, SVHN [27]. Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance. 2 Preliminaries In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (§2.1)? (2) What is a key factor for the success of PEFT (§2.2)? (3) What challenges does a reversible model have (§2.3)? 2.1 Parameter-efficient fine-tuning is not sufficiently memory-efficient Given a N multilayer perception: hN = fN (fN−1(...(f2(f1(h0)))...)) with h0 as the initial input, the nth layer hn = fn(hn−1) =σn(Wnhn−1) consists of a nonlinear function σn and a weight ma- trix Wn, where the bias term is ignored for simplicity. Denotingxn = Wnhn−1, in backpropagation with a loss L, the gradient of Wn is calculated with the chain rule as: ∂L ∂Wn = ∂L ∂hN ( NY i=n+1 ∂hi ∂xi ∂xi ∂hi−1 ) ∂hn ∂xn ∂xn ∂Wn = ∂L ∂hN ( NY i=n+1 σ′ iWi)σ′ nhn−1 (1) where σ′ is the derivative of σ and the calculation of σ′ n requires xn. Therefore, {xi}N i=n are cached during the forward pass to obtain the gradient of Wn, even though {Wi}i>n are frozen. 20 5 10 15 20 25 Fine-tuned Parameters (%) 24 25 26 27 28 29 30 31Memory Usage (GB) Full FT LoRA Pfeiffer Adapter Houlsby Adapter (IA)3 Prefix-Tuning (a) Memory trade-off. 0 0.05 0.1 0.5 1 0 10 20 30 40 50 60 70 80 0 0.1 0.5 0.9 c Average Score (%) default init default init with random PLM α  =  1 α  =  0, trainable α α  =  1 / c (b) Initialization effect, Left: LoRA, Right: (IA)3. Figure 2: Exploration of existing PEFTs: (a) The trade-off between memory and the number of trainable parameters. The dashed and solid lines denote the peak and activation memory, respectively. The model size for BERT base is 0.4GB2. (b) The initialization effect of PEFT on RoBERTa base. Random PLM denotes that we initialize the backbone randomly instead of using a pre-trained model. During training, the peak memory footprint is mainly occupied by three components: model’s parameters {Wn}N n=1, optimizer state whose size is three times as large as the size of trainable parameters for Adam [28] (one for gradient and two for moments), and activations. The memory footprint for all three components is related to the model’s depth and width. In addition, the memory footprint for activations is also related to some training settings, like batch size and sequence length. Compared to full fine-tuning, existing PEFT methods, such as (Houlsby and Pfeiffer) Adapters [14, 16], LoRA [17], (IA)3 [29], Prompt-Tuning [19] and Prefix-Tuning [15], tune a small number of parameters, making the size of the optimizer state negligible. However, the memory footprint required for activations is not significantly reduced. As shown in Figure 2a, where we set the batch size as 64 and the sequence length as 512 on RTE [30, 31, 32, 33] with BERTbase [1], the activation memory of all PEFT methods is >75% of full fine-tuning, even with <1% trainable parameters. 2.2 Initialization is significant for parameter-efficient fine-Tuning Pre-trained models learn generic and distributed enough representations to facilitate downstream learning of highly pressed task representation [36], i.e. offering a robust starting point for the training of downstream tasks. When modifying a PLM with PEFT, we hypothesize that one needs to preserve this starting point at the beginning of training for better performance. The Starting Point Hypothesis. When modifying a pre-trained model by adding new parameters, one needs to initialize the new parameters in a way to preserve the starting point from the pre- trained model at the beginning of training, such that fine-tuning the modified model can match the performance of full fine-tuning. More formally, supposed fn is a PLM layer and hn = fn(hn−1), the output from a modified layer f′ n, h′ n = f′ n(hn−1), should be close to hn at the beginning of training. I.e. h′ n = hn + δ, where ∥δ∥ →0. Intuitively, we want h′ n ≈ hn, because h′ n is the input to the next (modified) PLM layer. If they are dissimilar, the representation continuity will be broken down. Though most PEFT methods [14, 16, 17, 29] initialize their added modules in this way, we couldn’t find a thorough investigation of the significance of this initialization in the existing literature. In this section, we explore the significance of PEFT’s initialization for two methods, LoRA and (IA)3 [29]. LoRA and (IA)3 represent two common methods for introducing new parameters, involving addition and scaling operations, respectively. Given a pre-trained weight matrix W ∈ Rd×d, LoRA modifies it as h′ = (W + α r WdownWup)h, where Wdown ∈ Rd×r and Wup ∈ Rr×d are the added trainable parameters, α is a constant scale factor and normally r ≪ d. LoRA’s default initialization is Wdown ∼ N(0, σ2) and Wup = 0. In this way, WdownWup = 0 and the starting point from the 2Though we train in FP16, the PLM is first loaded in FP32, then auto-casted to FP16 for the forward pass in Transformers [34]. Since the memory required for the model in FP32 is always there during training, we report the memory for models in FP32 in this paper (see Table 9). More discussions about this are here. We believe it’s a bug in the framework and can be resolved with further investigation. Especially Huggingface’s new PEFT framework [35] allows loading INT8 model for fine-tuning. 3(a) Reversible architecture. 8 16 32 64 Numbe of Layers −8 −6 −4 −2 0 [4, 4] [2, 2] [1, 1] [0.5, 0.5][0.1, 0.1] [1, 0.1] [0.1, 1] [λ, β] 1.31e-09 2.21e-09 5.37e-09 5.9e-06 0.031 0.0165 0.0171 [0, 0.02] [0, 0.1] [0.3, 0.1] [1, 0.02] [μ, σ] 5.37e-09 6.42e-08 2.44e-08 1.78e-08 Reconstruction Error (log10) max mean max mean (b) Instability of reversible model. Figure 3: (a) F and G are two arbitrary functions (sub-networks), taking two inputs, h1 n and h2 n (b) Reconstruction error between the vanilla and reversible gradients. The default setting is RevViT [40] with 8 layers, λ = 1, β = 1, µ = 0 and σ = 0.02. Left: Different number of layers. Middle: Different scaling values. Right: Initialization with different means and standard deviations. PLM is preserved perfectly. (IA) 3 modifies W by multiplying it to a trainable vector l ∈ Rd as h′ = (l ⊙ W)h, where ⊙ represents element-wise multiplication. The default initialization of (IA)3 is l = 1, also making the starting point untouched. To facilitate the initialization process of LoRA, we opt for the following initial values: Wdown = 1, Wup = c and α = 1, where c is a matrix with all elements equal to an initialized value c, resulting in α r WdownWup = c. When c = 0, the starting point from a PLM is preserved. By adjusting c, we exert control over the degree of departure from the starting point. Similarly, we replace l with l′ = αl = αc for (IA)3. In Figure 2b, we train the newly added parameters on RoBERTabase [2] for four tasks (CoLA [37], STS-B [38], MRPC [ 39] and RTE [ 30, 31, 32, 33]). For LoRA ( r = 8), though we modify the initialization method, our result (c = 0) is very close to the default initialization. When the starting point is broken by c ̸= 0 (α = 1), all results are even worse than a randomly initialized model. However, when we set α = 03 to preserve the starting point, all results become much better than the ones with α = 1. For (IA)3, when we decrease c from 1 (default initialization) to 0, the results (α = 1) become worse and worse. However, when we set α = 1/c to preserve the starting point, all results become better. Some of them are even better than the default initialization. All of the above-mentioned results show that it’s significant to preserve the starting point from a PLM at the beginning of training when applying or designing a PEFT method. A different initialization scheme is in Figure 10 which leads to a similar finding. 2.3 Challenges of reversible neural network Recapping a reversible model [41] in Figure 3a, one can reconstruct inputs from outputs as: h1 n+1 = λh1 n + Fn(h2 n) h2 n+1 = βh2 n + Gn(h1 n+1) h2 n = (h2 n+1 − Gn(h1 n+1))/β h1 n = (h1 n+1 − Fn(h2 n))/λ (2) where λ and β are scaling factors. Theoretically, Fn and Gn could be two arbitrary functions (sub- networks). Given a multilayer reversible network, intermediate activations for each layer during the forward pass are not necessary to be cached. One only needs to store the final outputs, then reconstruct the intermediate activations and calculate the gradient layer-by-layer in a backward manner (See Listing 1 in §Appendix). In this way, the memory footprint required for activations can be reduced significantly and has no relationship with the model’s depth, i.e. O(1) instead of O(N). To investigate the training stability of a reversible model, we run experiments on RevViT [ 40].4 RevViT shares the same architecture as Reformer [42], except applying a convolutional layer at the beginning to project an image into a sequence of vectors. When running RevViT, one could still cache the intermediate activations and treat it as an irreversible model. We term the gradient calculated in this way as vanilla gradient. One could also train RevViT in a reversible way, and the corresponding 3α has to be trainable when α = 0. Otherwise, the newly added parameters are useless. 4Our experiments are based on this file, https://github.com/karttikeya/minREV/blob/main/rev.py. 4F+ F+G+ ℎ!\"ℎ!# ℎ!#ℎ!\" ℎ!$## ℎ!$#\" ℎ%#= ℎ%\"= ℎ% + + +𝜆 𝜆 𝛽 switch? (a) Mutli-head Attention ++ Feed-Forward Layer Norm Feed-Forward Feed-Forward ++Layer Norm Attention Block MLP Block (b) Feed-Forward Feed-Forward ++Layer Norm Nonlinear Nonlinear Attention Block Adapter 𝑾𝒅𝒐𝒘𝒏 𝑾𝒖𝒑 𝑾𝒖𝒑 𝑾𝒅𝒐𝒘𝒏  (c) Mutli-head Attention ++ Feed-Forward Layer Norm Adapter ++ Feed-ForwardFeed-ForwardAdapter Layer Norm  (d) Figure 4: MEFT architectures. (a) Unfold reversible model. (b) A PLM layer. (c) Two MEFT architectures: (1) F is the PLM layer with an adapter (up) and G is an adapter (down); (2) G is the PLM layer with an adapter (up) and F is an adapter (down). (d) The third MEFT architecture: G is the MLP block with an adapter (up) and F is the attention block with an adapter (down). For initialization, Wdown, Wup ∼ N(0, σ2) and σ = 0.02. Only the adapter is trainable. gradient is called reversible gradient. We input the same random noises into the same RevViT twice to obtain the parameter gradients from the convolutional layer, in a vanilla and reversible way. Then we calculate the absolute difference between these two gradients and report the maximum and mean values. In this way, we want to check whether the vanilla gradient can be reconstructed in a reversible way. If the reconstruction error is large, it means that the vanilla gradient could not be recovered in a reversible way due to numerical stability, which might cause unstable training or bad performance. As shown in Figure 3b, with an increasing number of layers in RevViT, the reconstruction error becomes larger, but still around10−8 which is negligible. However, RevViT is sensitive to the scaling factors, λ and β. When both scaling factors or one of them are less than 1, the reconstruction error increases dramatically. We also explore the initialization of the linear layers in RevViT and find that a larger standard deviation or mean can cause a bigger reconstruction error. In sum, a larger number of layers, smaller scaling factors (< 1) and a larger standard deviation or mean for initialization tend to cause a bigger reconstruction error, which might result in the unstable training or low performance of a reversible model. Last but not least, RevViT [40] finds that residual connections inside F and G deteriorate the performance of a reversible Transformer [43].5 3 Memory-efficient fine-tuning Table 1: A summarization of three MEFT methods. MEFT? F G λ β Switch h1 n h2 n 1 layer adapter → 0 any ✓ βhn−1 hn 2 adapter layer → 1 → 0 ✓ hn hn−1 3 attention MLP → 0 → 0 ✗ − hn This paper aims to modify a PLM to its reversible variant without addi- tional pre-training, so the PLM can still be fine-tuned with a limited mem- ory footprint. The fundamental guid- ing principle behind our design is: pre- serving the starting point from a PLM to the greatest extent possible (discus- sion in §2.2). In this section, we propose three methods to modify a PLM to a reversible one. 3.1 MEFT 1: PLM layer as F, adapter as G As shown in Figure 4c, we design F as a pre-trained layer with an adapter, where the insertion position for the adapter is borrowed from He et al. [18]. G is simply an adapter. We initialize the adapters as Wdown, Wup ∼ N(0, σ2), same for the following methods. In this way, the output from the adapter is close to 0 at the beginning of the training, so hn ≈ Fn(hn−1). For the following discussion, we only focus on the beginning of the training, making sure our design preserves the starting point from a PLM. h0 and h1 are the input to and output from the 1st layer of a PLM without any modification, respectively. I.e. h0 is the representation after the position and word embedding layers of a PLM. We 5In RevViT,F and G are the attention and MLP block (Figure 4b) without residual connection, respectively. 5assign h1 0 = h2 0 = h0, same for the following methods. At the beginning of the training (see Figure 4a), h1 1 = λh1 0 + F1(h2 0) =λh0 + F1(h0) ≈ λh0 + h1, h2 1 = βh2 0 + G1(h1 1) =βh0 + G1(h1 1) ≈ βh0, where the approximation holds because of our initialization of the adapters. For now, h1 1 and h2 1 are not desired. When we input h1 1 and h2 1 to the 2nd reversible layer, especially when we input h2 1 to F2, the representation continuity6 is broken, because h2 1 ̸= h1. We introduce two modifications to address this issue: (1) We set λ → 0, so h1 1 ≈ h1. (2) Then we switch the order of h1 1 and h2 1 before feeding to the next reversible layer, i.e. making h1 1 ≈ βh0 and h2 1 ≈ h1. In this way, h2 1 preserves the starting point. We don’t require h1 1 to preserve any starting point, because it is entered to G2 which is not a pre-trained layer. With the same above-mentioned design for the 2nd reversible layer, we obtain h1 2 ≈ βh1 and h2 2 ≈ h2. By analogy, h1 n ≈ βhn−1 and h2 n ≈ hn, which means h2 n always preserves the starting point from the PLM. Feeding h2 n to the next reversible layer, Fn+1, doesn’t break the representation continuity. After all layers, we input h′ N = (h1 N + h2 N )/2 to a task-specific head that is a brand new layer, same for the following methods.7 3.2 MEFT 2: Adapter as F, PLM layer as G Opposite to MEFT 1, we design F as an adapter and G as the PLM layer with an adapter for MEFT2 (see Figure 4c). In this case, we need to make sure that the input to G preserves the starting point. Let’s also start with the first layer, h1 1 = λh1 0 + F1(h2 0) = λh0 + F1(h0) ≈ λh0, h2 1 = βh2 0 + G1(h1 1) =βh0 + G1(h1 1) ≈ βh0 + G1(λh0), where the approximation holds because of our initialization of the adapters. To preserve the starting point from the PLM, we setλ → 1, β → 0 and switch the order of h1 1 and h2 1 before feeding to the next reversible layer. When setting λ → 1, we make sure the representation continuity is preserved for G1, resulting in h2 1 ≈ βh0 + h1. When β → 0 and the order of h1 1 and h2 1 is switched, h1 1 ≈ h1 and h2 1 ≈ h0. In this way, h1 1 preserves the initialization point, and we won’t break the representation continuity when feeding it to G2 in the next reversible layer. With the same setting for each layer, h1 n ≈ hn and h2 n ≈ hn−1, so h1 n always preserves the starting point. 3.3 MEFT 3: Attention block as F, MLP block as G As shown in Figure 4d, we can also design F as the pre-trained attention block with an adapter and G as the pre-trained MLP block with an adapter. Also starting with the first layer, we obtain h1 1 = λh1 0 + F1(h2 0) =λh0 + F1(h0), h2 1 = βh2 0 + G1(h1 1) =βh0 + G1(h1 1). λ → 0 is required, so h1 1 approximates the original output from the pre-trained attention block, and can be fed to G1 to preserve the starting point. β → 0 is also required, so h2 1 ≈ h1, and can be fed to F2 in the next reversible layer. By default, we set λ = β → 0. For MEFT3, one doesn’t need to switch the order of h1 1 and h2 1 before feeding to the next reversible layer. For each layer, h1 n is close to the original output from the attention block of the corresponding PLM layer, and h2 n ≈ hn. Compared to the vanilla RevNet [41] where λ = β = 1, we meticulously assign different values to λ and β to preserve the starting point from a PLM, and switch the order of the outputs before feeding to the next layer (if necessary) to preserve the representation continuity. We summarize the settings for all three MEFT methods in Table 1. 4 Experiments 4.1 Experimental setup Datasets and evaluation. We evaluate MEFTs on eight sequence representation tasks and five sequence-to-sequence tasks. All sequence representation tasks are from the GLUE benckmark [25]. 6The presentation continuity and the starting point hypothesis emphasize two aspects. The presentation continuity, for example, shows that one can’t feedh0 to the third pre-trained layer, focusing on the input. The starting point hypothesis shows that the output from a modified pre-trained layer should be close to the output from the original pre-trained layer, focusing on the output. However, they are also very related, since the output from the current layer is the input to the next layer. 7Read Appendix §C for a step-by-step tutorial if you still feel confused. 60.050.1 0.3 0.5 1 λ (MEFT1) or β (MEFT2) or λ = β (MEFT3) 60.0 62.5 65.0 67.5 70.0 72.5 75.0 77.5Average Score (%) Full FT MEFT1 MEFT2 MEFT3 Figure 5: MEFTs with vari- ous scaling factors on BERTbase over RTE, MRPC, STS-B and CoLA. Dashed and solid lines de- note MEFTs with vanilla and re- versible gradients, respectively. 242016128 81 82 83 84 85Average Score (%) Full FT LoRA freeze: false freeze: true 242016128 1 2 3 4 5 6Memory (GB) Number of Reversible Layers Figure 6: The trade-off between the performance and activation memory with MEFT1 on RoBERTalarge over RTE, MRPC, STS- B and CoLA. The line annotated by ‘freeze: true’ means the shallower PLM layers are frozen without any adaptation, while the line annotated by ‘freeze: false’ means the top MEFT layers with vanilla gradient, as shown in Figure 7. The sequence-to-sequence tasks are question-answering benchmarks, including OpenBookQA [44], PIQA [45], ARC (easy and challenge) [46] and SciQ [47]. We show the statistics of these datasets in Table 8 in Appendix. For the GLUE benchmark, we report accuracy on MNLI, QQP, QNLI, SST-2, MRPC and RTE, Pearson correlation coefficient on STS-B (if not specially mentioning) and Matthews correlation coefficient [48] on CoLA. We report accuracy on all question-answering tasks. In addition, we report all results on the development sets as our baselines. Models. We use the encoder-only models (BERTbase [1], RoBERTalarge [2] and BARTlarge encoder [26]) as the underlying models for all GLUE tasks, and the decoder-only models (OPT 1.3B and OPT6.7B [9]) for question-answering tasks. (See Table 9 in Appendix for model details.) Baselines. The most important baseline is full fine-tuning ( Full FT) that updates all parameters. Houlsby Adapter ( AdapterH) [ 14], Pfeiffer Adapter ( AdapterP) [ 16], Prefix-Tuning [15] and LoRA [17] are chosen as PEFT baselines. In addition, two unified PEFT methods, MAM [18] and AutoPEFT [49], that combine multiple PEFT methods are also chosen as PEFT baselines. Lastly, two feature-based tuning methods, Y-Tuning [20] and LST[21], that aim to reduce training memory serve as memory-efficient baselines. We report the baseline results from the original papers if possible. Implementation. For computational efficiency, we set β = 1for MEFT1, λ = 1for MEFT2, and only tune the factors that are required → 0 (see Table 1). After obtaining the optimal value, i.e. 0.1, we use this value for all three MEFT methods, tasks and backbones. On the GLUE benchmark, we sweep learning rates in {3, 4, 5}·10−4, batch sizes in {16, 32} and the number of epochs in {10, 20} for the tasks with >10k training samples. For the low-resource tasks with <10k training samples, we sweep learning rates in {5, 6, 7, 8} ·10−4, batch sizes in {16, 32} and the number of epochs in {20, 40}. These grid search spaces are inspired by our baselines, especially by LoRA [17]. We use the default Adam [28] setting with a warmup ratio of 6%. If the model’s performance on the development set is not improved over 5 epochs, we stop the training. We run the same task of a method in the above-mentioned grid search space three times with different random seeds, choose the best result from each run, and report the mean and standard deviation of these best results. For all question-answering tasks, we sweep learning rates in {1, 3, 5, 7}·10−4, batch sizes in {8, 16, 32} and the number of epochs in {3, 5, 10}, and keep other settings the same, which is inspired by [50]. The sequence length for all tasks is set to 512, 128, 128 and 128 for BERTbase, RoBERTalarge, BARTlarge and OPT as our baselines, respectively. We run all experiments on the Transformers framework [34] on a single NVIDIA RTX A6000 GPU with 48GB memory. Overall, a single run of any task could be finished within 8 hours, and most tasks could be finished in an hour. Fine-tuning settings are summarized in Table 7. 4.2 Results and discussions Importance of MEFT’s initialization. In the beginning, we further test the starting point hypothesis on our MEFTs by adjusting the scaling factors, λ and β. As depicted by the dashed lines in Figure 5, the degradation in performance is evident when the scaling factors deviate from their desired value of 7Table 2: Comparion with different methods on GLUE. The first and second best results are in bold and underlined, respectively. All baseline results for BERTbase and RoBERTalarge are from [49] and [17], respectively. We report Spearman’s Correlation for STS-B and matched accuracy for MNLI on BERTbase. The hyper-parameters after each backbone are used for measuring the memory footprint. r = 8for all MEFTs. All models are trained in FP16 if not specified with FP32. #Param. Memory (GB) Method (%) Peak Act. RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI Avg. BERTbase (batch size = 32, sequence length = 512) Full FT 100 16.67 14.98 71.1 1.5 85.71.8 89.00.5 59.30.6 92.60.2 91.50.1 91.50.0 84.40.2 83.2 Prefix-Tuning 0.17 13.58 13.00 70.5 0.5 85.90.9 88.80.2 58.91.2 91.90.5 90.80.1 89.10.1 82.80.2 82.3 LoRA 0.27 13.45 13.02 65.9 1.5 84.51.0 88.70.1 57.60.8 92.10.4 90.60.2 89.40.0 83.00.1 81.5 MAM 6.97 14.21 13.41 69.1 1.8 87.20.7 89.00.5 47.924. 83.917. 90.90.2 90.80.1 83.30.2 80.3 AutoPEFT 1.40 - - 72.4 0.9 87.50.9 89.20.0 60.91.5 92.10.3 91.10.1 90.60.1 84.00.1 83.5 vanilla gradient MEFT1 0.27 13.64 13.21 74.2 1.4 86.70.2 89.00.0 62.10.2 92.90.2 91.60.1 89.90.1 83.80.4 83.8 MEFT2 0.27 13.73 13.31 74.7 0.3 86.60.5 89.40.1 61.80.7 93.00.1 91.60.1 90.20.1 84.50.1 84.0 MEFT3 0.27 13.64 13.21 76.10.8 87.40.3 88.90.1 62.30.5 93.20.2 91.50.1 90.10.1 84.20.2 84.2 reversible gradient MEFT1(FP32) 0.27 2.75 2.33 73.9 0.5 86.50.2 88.80.1 60.30.6 92.70.4 91.40.0 88.80.1 83.40.1 83.2 MEFT2(FP32) 0.27 3.53 3.11 74.0 0.6 86.30.4 88.60.1 60.71.5 92.80.2 91.50.1 88.90.1 83.10.1 83.2 MEFT3(FP32) 0.27 2.99 2.57 70.8 0.6 84.60.5 88.20.3 53.91.0 92.20.4 90.40.2 86.90.3 81.50.1 81.1 RoBERTalarge (batch size = 32, sequence length = 128) Full FT 100 11.47 6.05 86.6 90.9 92.4 68.0 96.4 94.7 92.2 90.2 88.9 AdapterH 0.23 6.05 4.66 72.9 3.0 87.71.7 91.50.5 66.32.0 96.30.5 94.70.2 91.50.1 90.30.3 86.4 AdapterH 1.69 6.18 4.71 83.4 1.1 88.72.9 91.01.7 66.54.4 96.20.3 94.70.2 92.10.1 89.90.5 87.8 AdapterP 0.23 6.16 4.77 80.1 2.9 89.71.2 91.90.4 67.82.5 96.60.2 94.80.3 91.70.2 90.50.3 87.9 AdapterP 0.85 6.21 4.78 83.8 2.9 90.20.7 92.10.7 68.31.0 96.10.3 94.80.2 91.90.1 90.20.3 88.4 LoRA 0.23 6.11 4.72 85.2 1.1 90.21.0 92.30.5 68.21.9 96.20.5 94.80.3 91.60.2 90.60.2 88.6 vanilla gradient MEFT1 0.23 6.19 4.81 89.5 0.8 91.50.2 92.30.1 69.90.7 96.80.1 94.90.1 91.50.1 90.30.2 89.6 MEFT2 0.23 6.20 4.82 88.6 0.6 91.30.4 92.20.1 68.80.7 96.80.1 94.80.1 91.40.1 90.60.0 89.3 reversible gradient MEFT1 0.23 3.11 1.73 87.6 0.3 90.50.6 91.60.1 63.31.7 95.90.1 94.30.2 90.10.1 89.20.7 87.8 MEFT1(FP32) 0.23 3.63 2.25 90.00.5 91.20.2 92.40.1 66.10.7 96.70.3 94.80.1 90.20.5 90.10.1 88.9 MEFT2(FP32) 0.23 3.75 2.37 88.2 0.5 90.50.4 92.10.0 64.40.6 95.90.2 94.30.1 89.40.1 88.40.5 87.9 BARTlarge (batch size = 100, sequence length = 128) Full FT [20] 100 12.75 9.62 77.6 89.2 - 59.3 95.8 94.3 89.5 90.8 85.2 Y-Tuning [20] 7.7 - - 62.8 79.2 - 44.4 94.4 88.2 85.5 82.3 76.7 LST(FP32) [21] 2.6 7.05 6.14 69.7 87.3 - 55.5 94.7 91.9 89.5 86.1 82.1 reversible gradient MEFT1 0.20 2.54 1.75 72.2 1.3 88.11.3 - 51.0 1.8 95.10.2 92.40.1 87.50.0 87.00.2 81.9 MEFT1(FP32) 0.20 2.54 1.75 74.3 0.7 88.40.5 - 57.4 2.2 95.40.1 93.90.1 89.30.1 88.30.1 83.9 0 (as indicated in Table 1). However, when they are small enough (0.05 or 0.1), the results are even better than full fine-tuning. For most MEFT methods (MEFT1 and MEFT3), the optimal value for the scaling factors is 0.1. So we use this value for all MEFT methods in the following experiments. MEFTs with vanilla gradient are strong PEFT methods. Though MEFTs have reversible archi- tectures, we can still treat them as irreversible models and cache the intermediate activations during fine-tuning. In this way, they are simply PEFT methods. In Table 2, all MEFT methods, utilizing the vanilla gradient, consistently outperform both full fine-tuning and other baseline approaches by a significant margin. For example, MEFT3 outperforms Full FT by 1% and the best PEFT baseline (AutoPEFT) by 0.7% on BERTbase. MEFT1 outperforms Full FT by 0.7% on RoBERTalarge. Performance gap of MEFTs between vanilla and reversible gradients. In Figure 5, the results of MEFTs with reversible gradient (solid line) are often lower than the ones with vanilla gradient (dashed line). Recapping the discussion in §2.3, smaller scaling factors (< 1) and residual connections in F and G can cause a larger reconstruction error because of numerical stability. When modifying a PLM, we can’t remove the residual connections from it and have to set the scaling factors→ 0 due to the starting point hypothesis, which we believe is the main reason for the performance drop. Our claim is further supported by MEFT3 which has the most evident drop among all MEFTs. Compared to MEFT1 and MEFT2 that only have a residual connection in either F or G, both F and G of MEFT3 have residual connections. In addition, we have to set both λ and β close to 0 for MEFT3, which also causes a bigger reconstruction error than only setting one scaling factor (see Figure 3b middle). Since MEFT3 with reversible gradient performs the worst among all MEFTs, we only run it on BERTbase 8due to limited resources. Expectedly, MEFT1 trained in FP32 outperforms it trained in FP16 on both RoBERTalarge and BARTlarge (see Table 2), because FP16 causes more instability. Reversible MEFTs on deep model. Because of the starting point hypothesis, the residual connection from PLMs remains and the scaling factors are set closely to 0. With an increasing number of layers, the training instability is expected to become more severe (see Figure 3b left). As shown in Figure 6, when all RoBERTa layers are reversible (the number of reversible layers as 24), the score drops dramatically. To address this issue, we propose three settings in Figure 7: (1) Cache the activations for top layers (vanilla gradient) and apply reversible shallow layers (reversible gradient). (2) Freeze some shallow PLM layers, i.e. treating the shallow layers as a feature extractor. (3) Combine the above two settings. Notably, we have to put the reversible layers under the vanilla layers due to numerical stability. If we reverse the order, the reconstruction error is transferred to the vanilla layers. reversiblereversible freezefreeze vanillavanilla freeze freeze reversible reversible reversible reversible vanilla vanilla Figure 7: Three set- tings for deep models. We only explore the first two settings on RoBERTa and will discuss the third setting in the following, since RoBERTalarge doesn’t contain many layers. In Figure 6, when we apply the first setting (freeze: false) to RoBERTalarge, the average score becomes better when the number of reversible layers decreases, outperforms full fine-tuning when it’s≤ 16. However, the activation memory also increases with an increasing number of vanilla layers, since the vanilla layers require caching the activations. By default, we set the number of reversible layers as 16 for RoBERTalarge in Table 2. For the second setting (freeze: true), the results are always worse than full fine-tuning. However, its activation memory stays the same since all trainable layers are reversible. MEFTs are parameter and memory-efficient with a strong performance. Let’s go back to Table 2. Though there is a gap in MEFTs between vanilla and reversible gradients, reversible MEFTs still achieve strong results compared to previous baselines. On BERTbase, reversible MEFT1 and MEFT2 obtain the same average score as Full FT, slightly worse than the best PEFT method, AutoPEFT (83.2 vs. 83.5). However, reversible MEFTs only requires about 21% and 24% activation memory of Full FT and PEFTs. On RoBERTalarge, reversible MEFT1 (FP32) achieves the same score as Full FT and outperforms all PEFT methods, while only requiring 37% and 48% activation memory of Full FT and PEFTs. Due to limited computing resources, we only conduct experiments on the best MEFT method, MEFT1, on BARTlarge when compared to other memory-efficient methods. In addition, we don’t use our own grid search space on BARTlarge. Instead, we apply the same grid search space as LST, setting the learning rate in {3 · 10−4, 1 · 10−3, 3 · 10−3}, the batch size as 100 and the number of epochs as 20. In this way, we want to validate the robustness of MEFT. Similarly, MEFT 1 outperforms the memory-efficient baselines by a large margin while only requiring 29% LST’s activation memory. In addition, LST requires knowledge distillation to initialize the added layers and is not stable [21].8 MEFT trained in FP32 vs. in FP16, and the time-memory tradeoff. Though reversible MEFTs trained in FP32 outperform the ones trained in FP16, there are still some notable discussions about them: (1) The memory footprint required by reversible MEFTs trained in FP32 and FP16 is the same. In Table 2, MEFT1 and MEFT1 (FP32) have the same activation memory on BARTlarge, because the recomputed activations in back-propagation are always in FP32 due to the mixed precision training [51]. I.e. PyTorch [52] only allows FP32 in back-propagation; (2) FP16 still benefits the training of large PLMs. In Table 2, the peak and activation memory difference is about the backbone size in FP32 for PEFTs and MEFTs. If one could reduce the backbone size by loading in FP16, we can further reduce the peak memory; 2 (3) Training in FP16 is faster than the training in FP32 (about 1:1.2) due to the forward pass. In addition, since reversible MEFTs recompute the activations, they require more training time, about twice the training time for MEFTs with the vanilla gradient. Results on larger and deeper models. Here we explore a more realistic setting (the third setting in Figure 7) on larger and deeper models, OPT 1.3B and OPT6.7B, in Table 3. On OPT 1.3B with 24 layers, we set the number of frozen, reversible and vanilla layers as 8. On OPT6.7B with 32 layers, we use 8 reversible and vanilla layers, same as OPT1.3B. For a fair comparison, we freeze the first 8 PLM layers and modify the rest 16 layers with LoRA. MEFT1 is comparable to LoRA, while only requiring LoRA’s 65% activation memory. Though slightly worse than Full FT (-0.3%), MEFT1’s 8The comparison of memory footprint to Y-Tuning is in Table 10. 9Table 3: Results on question-answering tasks. r = 64for both MEFT1 and LoRA. All methods are trained in FP16. Due to limited computing resources, we only conduct one random run with these methods. A batch size of 32 and a sequence length of 128 are used to measure the memory footprint and training time. The training time is for one epoch on the OpenBookQA task. Check Appendix §D for the implementation detail. #Param. Memory (GB) Time Model Method (%) Peak Activation (s) OpenBookQA PIQA ARC-E ARC-C SciQ Avg. Full FT [50] 100 28.31 8.23 128.0 31.4 75.2 61.3 27.7 92.5 57.6 OPT1.3B LoRA 0.64 11.42 6.27 36.6 29.9 74.9 60.1 28.7 93.3 57.4 MEFT1 0.64 9.20 4.05 45.2 34.0 73.1 57.1 28.8 93.1 57.3 OPT6.7B ZeroShot - - - - 27.6 76.2 65.6 30.6 90.1 58.0 MEFT1 0.25 33.67 8.01 200.4 37.0 77.4 65.7 34.1 94.4 61.7 activation memory is only half of the one for Full FT. When using the same activation memory as Full FT by running on OPT6.7B, MEFT1 outperforms Full FT by a large margin. Table 4: Results on image classification. Method Acc@1 Peak Memory (GB) Full FT [27] 97.67 - AdaptFormer [27] 96.89 36 MEFT1 96.74 9 Transfer to image classification task. Though we only focused on NLP tasks, MEFT could be transferred to other tasks, even other architectures. We leave the transfer of MEFT to other architectures for future work, and here apply MEFT to ViT [53] for an image classification task, i.e. SVHN [27]. We follow the main training recipe from AdaptFormer [54], except for changing the optimizer from SGD to AdamW, setting the maximum gradient norm as 0.3. For MEFT1’s hyper-parameters, we set r = 64and λ = 0.3 (smaller λ is not stable for training). Similar to the NLP’s results, MEFT1 achieves comparable accuracy as AdaptFormer while saving a large amount of memory footprint in Table 4. For more results about comparing MEFT to gradient checkpointing, comparing MEFT to quantization methods, and combining MEFT with other memory-efficient methods, please go to Appendix §E. In addition, due to the page limit, we put the detailed related works in Appendix §A, and discuss the limitation of our work in Appendix §B. 5 Conclusion In this paper, we propose three memory-efficient fine-tuning methods (MEFTs), that fine-tune PLM in a parameter-efficient and memory-efficient way without the requirement of additional pre-training and match the performance of full fine-tuning. MEFTs modify the PLM architecture with adapters and make it reversible, by following the starting point hypothesis that is essential for PEFTs. So MEFTs don’t require caching the intermediate activations during training and significantly reduce the memory footprint occupied by activations. When applying MEFTs to various models, BERT, RoBERTa and BART, on the GLUE benchmark, MEFTs achieve a similar score as full fine-tuning and other strong baselines, while saving up to 84% activation memory. A similar story is also observed when applying MEFT to larger and deeper models, OPT, on five question-answering tasks. MEFT achieves a comparable score as full fine-tuning and only consumes its 50% activation memory. However, because of the recomputation of activations, MEFTs require slightly more training time than other PEFT methods and offer a slightly lower score when trained in FP16 instead of FP32. In the future, we are interested in applying MEFT to other areas, like computer vision and automatic speech recognition, and to other bigger backbones for more sequence-to-sequence tasks. Acknowledgements We thank all reviewers for their great feedback. We also thank our colleague Yan Meng for her helpful review of our draft. This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project number VI.C.192.080. 10References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171– 4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423. [2] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907. 11692. [3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020. URL http://jmlr. org/papers/v21/20-074.html. [4] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages 15979– 15988. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10. 1109/CVPR52688.2022.01553. [5] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: a simple framework for masked image modeling. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 9643–9653. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00943. URL https://doi.org/10.1109/CVPR52688.2022.00943. [6] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html. [7] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neu- ral Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13–23, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html. [8] Hao Tan and Mohit Bansal. LXMERT: learning cross-modality encoder representations from transformers. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages 5099–5110. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1514. URL https://doi.org/10.18653/v1/D19-1514. [9] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models.CoRR, abs/2205.01068, 2022. doi: 10.48550/arXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068. [10] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel 11Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/arXiv.2211.05100. URL https://doi.org/10.48550/arXiv.2211. 05100. [11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971. [12] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. CoRR, abs/2205.05131, 2022. doi: 10.48550/arXiv.2205.05131. URL https: //doi.org/10.48550/arXiv.2205.05131. [13] Baohao Liao, Yan Meng, and Christof Monz. Parameter-efficient fine-tuning without introducing new latency. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors,Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 4242–4260. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.233. URL https://doi. org/10.18653/v1/2023.acl-long.233. [14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html. [15] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/ v1/2021.acl-long.353. [16] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , pages 487–503. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.eacl-main.39. URL https://doi.org/10.18653/v1/2021.eacl-main. 39. [17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. [18] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. To- wards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open- Review.net, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. [19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 122021, pages 3045–3059. Association for Computational Linguistics, 2021. doi: 10.18653/v1/ 2021.emnlp-main.243. URL https://doi.org/10.18653/v1/2021.emnlp-main.243. [20] Yitao Liu, Chenxin An, and Xipeng Qiu. Y-tuning: An efficient tuning paradigm for large-scale pre-trained models via label representation learning. CoRR, abs/2202.09817, 2022. URL https://arxiv.org/abs/2202.09817. [21] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. LST: ladder side-tuning for parameter and memory efficient transfer learning. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/54801e196796134a2b0ae5e8adef502f-Abstract-Conference. html. [22] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531. [23] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://arxiv.org/abs/1910.01108. [24] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. CoRR, abs/2303.16199, 2023. doi: 10.48550/arXiv.2303.16199. URL https://doi.org/10.48550/arXiv.2303.16199. [25] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= rJ4km2R5t7. [26] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871– 7880. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.703. URL https://doi.org/10.18653/v1/2020.acl-main.703. [27] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. InNIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford. edu/housenumbers/nips2011_housenumbers.pdf. [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. [29] Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=rBCvMG-JsPd. [30] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Joaquin Quiñonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alché- Buc, editors, Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer, 2005. doi: 10.1007/ 11736790\\_9. URL https://doi.org/10.1007/11736790_9. [31] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second pascal recognising textual entailment challenge. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 01 2006. [32] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Satoshi Sekine, Kentaro Inui, Ido Dagan, 13Bill Dolan, Danilo Giampiccolo, and Bernardo Magnini, editors, Proceedings of the ACL- PASCAL@ACL 2007 Workshop on Textual Entailment and Paraphrasing, Prague, Czech Re- public, June 28-29, 2007, pages 1–9. Association for Computational Linguistics, 2007. URL https://aclanthology.org/W07-1401/. [33] Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. The fifth PASCAL recognizing textual entailment challenge. In Proceedings of the Second Text Analysis Conference, TAC 2009, Gaithersburg, Maryland, USA, November 16-17, 2009. NIST, 2009. URL https://tac.nist.gov/publications/2009/additional.papers/RTE5_ overview.proceedings.pdf. [34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6. [35] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/ huggingface/peft, 2022. [36] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7319–7328. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. acl-long.568. URL https://doi.org/10.18653/v1/2021.acl-long.568. [37] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Trans. Assoc. Comput. Linguistics, 7:625–641, 2019. doi: 10.1162/tacl\\_a\\_00290. URL https://doi.org/10.1162/tacl_a_00290. [38] Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. Semeval- 2017 task 1: Semantic textual similarity - multilingual and cross-lingual focused evaluation. CoRR, abs/1708.00055, 2017. URL http://arxiv.org/abs/1708.00055. [39] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para- phrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005. Asian Federation of Natural Language Processing, 2005. URL https://aclanthology.org/I05-5002/. [40] Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feicht- enhofer, and Jitendra Malik. Reversible vision transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18- 24, 2022, pages 10820–10830. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01056. URL https://doi.org/10.1109/CVPR52688.2022.01056. [41] Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible resid- ual network: Backpropagation without storing activations. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 2214–2224, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ f9be311e65d81a9ad8150a60844bb94c-Abstract.html. [42] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id= rkgNKkHtvB. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von 14Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. [44] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2381–2391. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-1260. URL https://doi.org/10.18653/v1/d18-1260. [45] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239. [46] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457. [47] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017 , pages 94–106. Association for Computational Linguistics, 2017. doi: 10.18653/v1/w17-4413. URL https://doi.org/10.18653/v1/w17-4413. [48] Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451, 1975. [49] Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Korhonen. Autopeft: Automatic configuration search for parameter-efficient fine-tuning. CoRR, abs/2301.12132, 2023. doi: 10.48550/arXiv. 2301.12132. URL https://doi.org/10.48550/arXiv.2301.12132. [50] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model. CoRR, abs/2302.04870, 2023. doi: 10.48550/arXiv.2302.04870. URL https://doi.org/10. 48550/arXiv.2302.04870. [51] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. CoRR, abs/1710.03740, 2017. URL http://arxiv.org/abs/1710. 03740. [52] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. [54] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 69e2f49ab0837b71b0e0cb7c555990f8-Abstract-Conference.html. [55] Baohao Liao, David Thulke, Sanjika Hewavitharana, Hermann Ney, and Christof Monz. Mask more and mask later: Efficient pre-training of masked language models by disentangling the [MASK] token. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 1478–1492. Association for Computational Linguistics, 2022. 15doi: 10.18653/v1/2022.findings-emnlp.106. URL https://doi.org/10.18653/v1/2022. findings-emnlp.106. [56] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to se- quence pre-training for language generation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 5926–5936. PMLR, 2019. URL http://proceedings.mlr.press/v97/ song19d.html. [57] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsu- pervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8440–8451. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.747. URL https://doi.org/10.18653/v1/2020.acl-main.747. [58] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7057–7067, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html. [59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [60] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. [61] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. [62] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550/arXiv.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556. [63] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311. [64] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, 16Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239. [65] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learners. InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. [66] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through mul- titask finetuning. CoRR, abs/2211.01786, 2022. doi: 10.48550/arXiv.2211.01786. URL https://doi.org/10.48550/arXiv.2211.01786. [67] Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=r1Ue8Hcxg. [68] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 6253–6264. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.433. URL https://doi.org/10.18653/ v1/2022.acl-long.433. [69] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/abs/1604.06174. [70] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. net/forum?id=rJl-b3RcF7. [71] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3259–3269. PMLR, 2020. URL http: //proceedings.mlr.press/v119/frankle20a.html. [72] Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Zaharia. LIT: learned interme- diate representation training for model compression. In Kamalika Chaudhuri and Rus- lan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Pro- ceedings of Machine Learning Research , pages 3509–3518. PMLR, 2019. URL http: //proceedings.mlr.press/v97/koratana19a.html. [73] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. In Irina Calciu and Geoff Kuenning, editors, 2021 USENIX Annual Technical Confer- ence, USENIX ATC 2021, July 14-16, 2021, pages 551–564. USENIX Association, 2021. URL https://www.usenix.org/conference/atc21/presentation/ren-jie. [74] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post- training quantization for generative pre-trained transformers. CoRR, abs/2210.17323, 2022. doi: 10.48550/arXiv.2210.17323. URL https://doi.org/10.48550/arXiv.2210.17323. 17[75] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 7947–7969. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.627. URL https://doi.org/10. 18653/v1/2021.emnlp-main.627. [76] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block- wise quantization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview. net/forum?id=shpkpVXzo3h. [77] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. CoRR, abs/2305.14314, 2023. doi: 10.48550/arXiv.2305.14314. URL https://doi.org/10.48550/arXiv.2305.14314. [78] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 3505–3506. ACM, 2020. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703. 18A Related works Pre-trained language models . PLMs, which are trained on extensive datasets for a common task such as predicting masked words [ 1, 2, 3, 26, 55, 56, 57, 58] or the next word [ 59, 60] in a sentence, play a vital role in facilitating knowledge transfer to downstream tasks. They have demonstrated remarkable achievements across various applications, consistently delivering state- of-the-art outcomes. Furthermore, scaling up PLMs has proven to yield predictable enhancements in performance for these downstream tasks [61, 62]. Consequently, the size of released PLMs has progressively grown, reaching an unprecedented scale of 100 billion parameters [9, 10, 11, 60, 63, 64]. Such large-scale PLMs unveil extraordinary capabilities, enabling zero-shot or in-context learning [59, 60] for a broad spectrum of tasks. Nevertheless, transfer learning remains a prevalent approach for effectively deploying these models in new task scenarios [29, 65, 66], which post unparalleled requirements on the computing resources. Parameter-efficient fine-tuning. With the advent of large-scale PLMs, a new method that aims to reduce storage requirements, PEFT, has been proposed [14, 15, 19]. PEFT adds and trains a small number of parameters while matching the performance of full fine-tuning. There are various ways to add new parameters. For example, Houlsby et al. [14] and Pfeiffer et al. [16] insert small bottleneck modules (adapters) to the PLM. LoRA [17] injects rank decomposition matrices into the pre-trained weights. HiWi [13] inserts the pre-trained parameters to a low-rank adapter. (IA)3 [29] scales the pre-trained weight with a trainable vector. Prompt-based methods [ 15, 19] append a sequence of trainable vectors to the word embeddings or attention components. Recently, some unified methods, which combine multiple PEFT methods in a heuristic way [ 18] or with the technique of neural architecture search [49, 67, 68], have also been proposed. Though PEFTs save the storage by a large margin compared to full fine-tuning, they still require a similar memory footprint during training as full fine-tuning [20, 21] because of the activation memory. Memory-efficient training. Memory-efficient training aims to reduce the memory footprint during the training process. Reversible neural networks [ 40, 41, 42] reduce the activation memory by recomputing the activations with the outputs during back-propagation. Gradient checkpointing [69] trade computation for memory by dropping some intermediate activations and recovering them from an extra forward pass. The activation memory is O(1) and O( √ N) for reversible neural networks and gradient checkpointing, respectively. MEFT is the first method that is proposed to modify a PLM to its reversible variant. When applying MEFT on a deeper model, one can use gradient checkpointing to further reduce the activation memory for the layers with vanilla gradient. Network compressions, like pruning [70, 71] and knowledge distillation [22, 23, 72], save the memory footprint for both training and inference. They compress a PLM to a smaller model by either deleting unimportant parameters or distilling knowledge from the PLM to the smaller model. Treating a PLM as a feature extractor and avoiding its gradient calculation is also an effective way to reduce the activation memory [20, 21]. However, these methods normally require extra pre-training before fine-tuning, or achieve a lower performance compared to full fine-tuning when using the same PLM. B Limitations We acknowledge the main limitation of this work is that we only evaluate our proposed methods on a limited amount of tasks and don’t conduct experiments on the encoder-decoder models. The main reason for the limited amount of tasks is that our computing resources are constrained. In addition, the major criterion for our selection of the underlying models is that we could find many strong baselines on them without reproduction. BERT and RoBERTa fulfill this criterion very well on the GLUE benchmark. Regarding the encoder-decoder model, recently there is a clear trend of applying a decoder-only model on sequence-to-sequence tasks. Therefore, we apply OPT in this paper and plan to include LLAMA [11] for the instruction-finetuning data in the future. Another limitation of MEFT is its lower score when trained in FP16 and on a deeper model. We have discussed this problem in §4.2. In sum, more reconstruction error is introduced by FP16 due to its numerical range and by a deeper model because of the error accumulation. Fortunately, the results are still comparable to the PEFT baselines when trained in FP16. Even trained in FP32, the activation memory footprints don’t increase compared to FP16. One only needs to spend more training time in FP32 when using the same batch size as in FP16 (about 20% more training time). However, since 19MEFTs reduce the memory footprint, a larger batch size during training is possible, which can save some training time. For deeper models, we offer a practical and effective setting in Figure 7. Last but not least, when fine-tuning larger models, like OPT1.3B and OPT6.7B [9], the peak memory footprint is occupied by the model parameters rather than the activation (see Table 3). One needs to combine other techniques with MEFT to reduce the peak memory footprint, like loading the model in FP16 or even in int8 rather than in FP32, combining MEFT with ZeRO [73] as in Table 6. C Step-by-step design for MEFT 1 For the reader’s easy understanding, in this section, we explain MEFT 1 step-by-step. First, let’s re-emphasize the guiding principles for our design: (1) For each reversible layer, we must have two inputs and two outputs as in Figure 3a. (2) We need to follow the starting point hypothesis. I.e. whenever we modify a PLM layer, we need to ensure the modified layer has almost the same output as the original PLM layer if we input the same input of the original PLM layer to the modified layer at the beginning of training. If the outputs are not similar, they become even more dissimilar after multiple layers, tearing down the PLM’s initialization. As shown in Figure 8a, for the first PLM layer, h0 is the input and h1 is the output. In Figure 8b, the inputs to the first reversible layer is h1 0 = h2 0 = h0. Recapping the architecture of F1 in Figure 4c (up), we simply insert an adapter in parallel to the two consecutive feed-forward layers, and initialize the adapter as Wdown, Wup ∼ N(0, 0.022), which results in h1 ≈ F1(h2 0) since h2 0 = h0. If we set λ → 0, h1 1 = λh1 0 + F1(h2 0) ≈ h1. In this way, h1 1 plays the role of preserving the starting point. Now let’s consider h2 1. Due to our initialization of the adapter, the output from G1 (G1 is simply an adapter as in Figure 4c (down)) is close to 0. So h2 1 = βh2 0 + G1(h1 1) ≈ βh0 + 0 = βh0. After switching the order of h1 1 and h2 1, h1 1 ≈ βh0 and h2 1 ≈ h1. For the second reversible layer, if we don’t switch the order of h1 1 and h2 1, it looks like Figure 8c. The input to F2 is βh0, which breaks down the representation continuity of a PLM since the input to the pre-trained F2 should be close to h1. If we switch their order as in Figure 8d, we preserve the representation continuity. And it results in h1 2 = λβh0 + F2(h1) ≈ h2 due to λ → 0 and h2 ≈ F2(h1). Similar to the first reversible layer, h2 2 ≈ βh1. After switching, h1 2 ≈ βh1 and h2 2 ≈ h2. By analogy, for the nth reversible layer, h1 n ≈ βhn−1 and h2 n ≈ hn. After the final layer, we simply take the mean of two outputs as h′ N = (h1 N + h2 N )/2, and input h′ N to a task-specific head, like a classification layer. The design procedure is similar for MEFT 2 and MEFT3. In sum, order switching is mainly for preserving the representation continuity, and setting the scaling factors close to 0 is mainly for preserving the starting point. D Implementation details of the question-answering tasks Compared to GLUE tasks where all tasks are classification tasks and the classification heads are randomly initialized, the question-answering tasks are sequence-to-sequence tasks and need the pre-trained output layer that shares the same parameters as the word embedding layer. The output Feed-Forward Feed-Forward ++Layer Norm Attention Block ℎ!\"# ℎ! (a) + + + ℎ!\"ℎ!# ℎ!#ℎ!\" ℎ## ℎ#\" + + +𝜆 𝜆 switch 𝛽F ! F ! G ! ℎ!#= ℎ!\"= ℎ! (b) + + + ℎ!\t𝛽ℎ\" ℎ#! ℎ## + + +𝜆 𝜆 𝛽F ! F ! G ! ℎ!\t𝛽ℎ\" (c) + + + ℎ!\t𝛽ℎ\" ℎ#! ℎ## + + +𝜆 𝜆 𝛽F ! F ! G ! ℎ!\t𝛽ℎ\" switch (d) Figure 8: (a) The nth PLM layer; (b) The first MEFT1 layer; (c) The second MEFT1 layer without order switching; (d) The second MEFT1 layer. 20layer requires the continuity of representation. I.e. at the beginning of training, the input to the output layer, h′ N , should be close to hN . Therefore, we need to do a modification to h′ N instead of using h′ N = (h1 N + h2 N )/2. Here we introduce a new scaling factor γ and require γ → 0. For MEFT 1, since h2 N ≈ hN (see Table 1), we set h′ N = γh1 N + h2 N ≈ h2 N ≈ hN . Similarly, h′ N = h1 N + γh2 N ≈ h1 N ≈ hN for MEFT2, and h′ N = γh1 N + h2 N ≈ h2 N ≈ hN for MEFT3. Without any tuning, we set γ = 0.1 as other tuned scaling factors by default. 128 256 512 40 60 80 100 120 140 160Samples / Second Full FT + gradient checkpointing LoRA + gradient checkpointing MEFT1 32 64 128 33 34 35 36 128 256 512 Sequence Length 0.5 1.0 1.5 2.0 2.5Activation Memory (GB) 32 64 128 Batch Size 2 4 6 8 10 Figure 9: Throughput and activation memory for different sequence length and batch sizes on BERTbase. By default, the sequence length is 512 and the batch size is 32. For your reference, LoRA’s throughput is 52.7 samples/second without gradient checkpointing for the default setting. Overall, MEFT shares the same level of throughput as LoRA with gradient checkpointing, while it is the lower bound of the activation memory for different settings. E More results E.1 Compared to gradient checkpointing Previously, we only theoretically stated that the activation memory for reversible network and gradient checkpointing is O(1) and O( √ N), respectively. In addition, we didn’t compare the training time of MEFT with PEFT in detail. Here we offer some empirical results for your better understanding. In Figure 9, we compare activation memory and throughput among MEFT , LoRA with gradient checkpointing and Full FT with gradient checkpointing. The throughput for all three methods is at the same level, maximum 12% difference between LoRA and MEFT when the sequence length is 128 and the batch size is 32. With an increasing sequence length, the gap becomes narrower to 7.5%. Notably, the throughput for LoRA without gradient checkpointing is 52.7 samples/second. With gradient checkpointing, it is 36.1 samples/second, 69% of the original throughput. For MEFT with the same setting, it is 33.5 samples/second, 64% of LoRA’s throughput without gradient checkpointing. In sum, MEFT’s throughput is at the same level as LoRA’s with gradient checkpointing, and about 64% of LoRA’s without gradient checkpointing. In addition, MEFT’s activation memory is always the lower bound among these three methods. The gap between LoRA with gradient checkpointing and MEFT becomes larger with an increasing sequence length and batch size. E.2 Compared to quantization methods Quantization is an orthogonal method to MEFT, which reduces the memory footprint of training or inference by reducing the parameter size to fewer bits and using low-bit-precision matrix mul- tiplication. There are mainly three different quantization methods: (1) Post-training quantization 21Table 5: Compared to QLoRA. r = 8 for all methods. Experimental setting stays the same as the default setting in Figure 9. Method Activation Memory (GB) Samples/Second LoRA + gradient checkpointing 2.62 36.1 QLoRA + gradient checkpointing 2.97 8.7 MEFT1 2.33 33.5 Table 6: Combine MEFT with ZeRO. Method Peak Memory (GB) Activation Memory (GB) MEFT1 28.2 8.2 MEFT1 + ZeRO 6.4 6.4 [74, 75] that quantizes a trained model after pre-training or fine-tuning; (2) Lower-bit optimizer [76] that stores the optimizer state with lower precision and de-quantizes it only for the optimization, similarly to FP16 or BF16 mixed precision training but with lower-bit; (3) Lower-bit frozen LLM with LoRA, i.e. QLoRA [77], that applies 4-bit quantization to compress the LLM. During fine-tuning, QLoRA backpropagates gradients through the frozen 4-bit quantized LLM into the low-rank adapters. Notably, the computation data type for QLoRA is BF16. It de-quantizes weights to the computation data type to perform the forward and backward passes. To some extent, all these three methods are orthogonal to our method and can be combined with MEFT: (1) Post-training quantization is mainly for reference and it can be applied to any trained models; (2) 8-bit Adam can also be applied to any models trained based on a gradient; (3) QLoRA is a combination of (1) and (2). For QLoRA, we conducted some experiments on BERT base with the default setting as Figure 9. As shown in Table 5, METF 1 saves the most activation memory while having a similar throughput as LoRA with gradient checkpointing. The reason for the larger activation memory of QLoRA than LoRA is that it has an additional de-quantization step, which also causes its smallest throughput. E.3 Combine MEFT with ZeRO ZeRO [73] saves memory by partitioning the model’s parameters and optimizer state among GPUs or between GPU and CPU. This method is orthogonal to MEFT, since MEFT saves memory from activations. We conduct some experiments on OPT1.3B by combining our method with DeepSpeed [78] ZeRO stage 3 that offloading model’s parameters and the optimizer state to CPUs. As shown in Table 6, ZeRO significantly reduces the memory footprint from the model’s parameters, therefore reducing MEFT’s peak memory from 28.2GB to 6.4GB. Table 7: Fine-tuning settings. Check §4.2 for the fine-tuning setting on BART. Hyper-parameter GLUE Question-Answering RTE, MRPC, STS-B, CoLA SST-2, QNLI, QQP, MNLI Learning Rate {5, 6, 7, 8} ·10−4 {3, 4, 5} ·10−4 {1, 3, 5, 7} ·10−4 Batch Size {16, 32} { 16, 32} { 8, 16, 32} Max Epochs {20, 40} { 10, 20} { 3, 5, 10} Weight Decay 0.1 0.1 0.1 Max Gradient Norm 1 1 1 Warmup Ratio 0.06 0.06 0.06 Learning Rate Decay Linear Linear Linear 22Table 8: Statics of datasets Task RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI-m MNLI-mm #Training 2.5k 3.7k 5.8k 8.6k 67.4k 104.7k 363.8k 392.7k #Development 0.3k 0.4k 1.5k 1k 0.9k 5.5k 40.4k 9.8k 9.8k Task OpenBookQA PIQA ARC-E ARC-C SciQ #Training 5.0k 16.1k 2.3k 1.1k 11.7k #Development 0.5k 3.1k 2.4k 1.2k 1k Table 9: Statics of models Model #Parameter #Layer d model Size in FP32 (GB) BERTbase 110M 12 768 0.4 BARTlarge encoder 205M 12 1024 0.8 RoBERTalarge 355M 24 1024 1.4 OPT1.3B 1.3B 24 2048 5.2 OPT6.7B 6.7B 32 4096 25.6 1 def backward_pass (self , y1 , y2 , dy1 , dy2 ): 2 with torch . enable_grad (): 3 y1. requires_grad = True 4 # The intermediate activations of G are stored 5 g_y1 = self .G(y1) 6 # Obtain the gradient of y1 7 g_y1 . backward (dy2 , retain_graph = True ) 8 9 with torch . no_grad (): 10 x2 = (y2 - g_y1 ) / self . x2_factor 11 # Save memory , same for below 12 del g_y1 , y2 13 dy1 += y1. grad 14 # Save memory 15 y1. grad = None 16 17 with torch . enable_grad (): 18 x2. requires_grad = True 19 # The intermediate activations of F are stored 20 f_x2 = self .F(x2) 21 # Obtain the gradient of x2 22 f_x2 . backward (dy1 , retain_graph = False ) 23 24 with torch . no_grad (): 25 x1 = (y1 - f_x2 ) / self . x1_factor 26 del f_x2 , y1 27 dy2 *= self . x2_factor 28 # dy2 =dx2 , save memory by using the same variable 29 dy2 += x2. grad 30 x2. grad = None 31 # dy1 = dx1 32 dy1 *= self . x1_factor 33 x2 = x2. detach () 34 return x1 , x2 , dy1 , dy2 Listing 1: Backward pass for each Layer. The peak memory happens at Line 10 or Line 25, depending on whether the subnetwork G is larger than F or the opposite. In the code, we use x1, x2, y1, y2, x1_factor, x2_factor to represent h1 n−1, h2 n−1, h1 n, h2 n, λ and β, respectively. 23Table 10: Compared to Y-Tuning on RoBERTalarge. We exclude the memory of Y-Tuning for BART in Table 2, because it was not reported. Instead, the memory usage of Y-Tuning for RoBERTalarge was reported. Notably, the STS-B task is excluded from the calculation of the average score, because it was not evaluated in Liu et al. [20]. Model #Parameter Peak Memory (GB) Average Score Full FT 100% 11.47 88.4 LoRA 0.23% 6.11 88.1 Y-Tuning 4.57% 2.08 82.1 MEFT1 0.23% 3.63 88.4 0 0.05 0.1 0.5 1 0 10 20 30 40 50 60 70 80 0 0.1 0.5 0.9c Average Score (%) default init default init with random PLM α = 1 α = 0, trainable α α = 1 / c Figure 10: The initialization effect for PEFT, Left: LoRA, Right: (IA) 3. Instead of initializing Wup = c like Figure 2b, here we initialize it as Wup ∼ N(c, 0.022), which should be more suitable for training due to its asymmetry. For convenient comparison, the results of Wup = c (in grey) are also included. Overall, the results between Wup = c and Wup ∼ N(c, 0.022) are comparable. However, when c = 0for LoRA, the result of Gaussian initialization is slightly worse than the constant initialization. This further supports our starting point hypothesis, since the Gaussian initialization can’t guarantee the output from the adapter is strictly equal to zero at the beginning of fine-tuning. 24",
      "meta_data": {
        "arxiv_id": "2306.00477v4",
        "authors": [
          "Baohao Liao",
          "Shaomu Tan",
          "Christof Monz"
        ],
        "published_date": "2023-06-01T09:26:17Z",
        "pdf_url": "https://arxiv.org/pdf/2306.00477v4.pdf",
        "github_url": "https://github.com/baohaoliao/mefts"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the memory inefficiency of existing Parameter-Efficient Fine-Tuning (PEFT) methods, which still require caching most intermediate activations, similar to full fine-tuning. The main contributions include identifying that preserving the Pre-trained Language Model's (PLM) starting point is crucial for PEFT success. Based on this, the authors propose Memory-Efficient Fine-Tuning (MEFT), a novel approach that inserts adapters into a PLM to make it reversible without additional pre-training. MEFT significantly reduces activation memory by up to 84% compared to full fine-tuning while requiring a negligible amount of trainable parameters. It achieves comparable performance to full fine-tuning on GLUE and question-answering tasks, with similar findings for image classification.",
        "methodology": "The core methodology revolves around making PLMs reversible by integrating adapters while strictly adhering to the 'Starting Point Hypothesis'. This hypothesis states that new parameters must be initialized to preserve the PLM's output at the beginning of training. Reversible models, by design, recompute intermediate activations during back-propagation from the final output, reducing activation memory from O(N) to O(1) for model depth. Three MEFT architectures are proposed: MEFT 1 designs the PLM layer with an adapter as F and an adapter as G (setting λ→0 and switching output order); MEFT 2 uses an adapter as F and the PLM layer with an adapter as G (setting λ→1, β→0, and switching order); MEFT 3 uses the attention block with an adapter as F and the MLP block with an adapter as G (setting both λ→0 and β→0). Adapters are initialized with weights from N(0, σ^2) to ensure their initial output is near zero. For deeper models, a hybrid approach is suggested, combining vanilla gradient layers (caching) for top layers with reversible shallow layers.",
        "experimental_setup": "The effectiveness of MEFT was evaluated on eight sequence representation tasks from the GLUE benchmark (MNLI, QQP, QNLI, SST-2, MRPC, RTE, STS-B, CoLA) and five sequence-to-sequence question-answering tasks (OpenBookQA, PIQA, ARC Easy, ARC Challenge, SciQ). An image classification task on SVHN was also used. Various PLM backbones were utilized: BERTbase, RoBERTalarge, and BARTlarge encoder for GLUE tasks; OPT 1.3B and OPT6.7B for question-answering; and ViT for image classification. Baselines included full fine-tuning, several PEFT methods (Houlsby Adapter, Pfeiffer Adapter, Prefix-Tuning, LoRA, MAM, AutoPEFT), and memory-efficient baselines (Y-Tuning, LST). Performance was measured using accuracy, Pearson correlation, and Matthews correlation for GLUE, accuracy for Q&A, and Acc@1 for image classification. Hyperparameters for scaling factors were tuned (optimal 0.1 for those tending to 0), and learning rates, batch sizes, and epochs were swept across specific ranges. Experiments were run on a single NVIDIA RTX A6000 GPU with 48GB memory, with results reported as mean and standard deviation over multiple runs.",
        "limitations": "The study's limitations include evaluation on a restricted set of tasks, specifically lacking experiments on encoder-decoder models, although decoder-only models were used for sequence-to-sequence tasks. MEFT exhibits lower performance when trained in FP16 compared to FP32, particularly in deeper models, due to accumulated reconstruction error from numerical instability. The recomputation of activations also leads to increased training time, approximately twice that of other PEFT methods or vanilla gradient MEFTs. For very large models (e.g., OPT1.3B, OPT6.7B), the peak memory footprint is dominated by model parameters rather than activations, meaning MEFT alone is insufficient for substantial peak memory reduction, necessitating combination with other techniques like quantization (FP16/int8 loading) or ZeRO.",
        "future_research_directions": "Future work will explore applying MEFT to other domains beyond NLP, such as computer vision and automatic speech recognition. Further research will also investigate its applicability to different model architectures and larger backbones for more sequence-to-sequence tasks, explicitly mentioning LLAMA for instruction fine-tuning.",
        "experimental_code": "import torch\nimport torch.nn as nn\n\nfrom transformers.activations import ACT2FN\n\n# Adapter architecture\nclass Adapter(nn.Module):\n    \"\"\"\n    The adapter architecture is borrowed from https://arxiv.org/abs/1902.00751\n    \"\"\"\n    def __init__(self, config, layernorm=False):\n        super().__init__()\n        self.dense1 = nn.Linear(config.hidden_size, config.adapter_bottleneck_dim)\n        self.dense2 = nn.Linear(config.adapter_bottleneck_dim, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        if isinstance(config.hidden_act, str):\n            self.act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.act_fn = config.hidden_act\n        if layernorm:\n            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        else:\n            self.LayerNorm = None\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        if self.LayerNorm is not None:\n            hidden_states = self.LayerNorm(hidden_states)\n        hidden_states = self.dense1(hidden_states)\n        hidden_states = self.act_fn(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.dense2(hidden_states)\n        return hidden_states\nfrom typing import List, Optional, Tuple, Union\nimport sys\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom torch.autograd import Function\n\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.bert.configuration_bert import BertConfig\nfrom transformers.models.bert.modeling_bert import (\n    BertSelfOutput,\n    BertAttention,\n    BertOutput,\n    BertLayer,\n    load_tf_weights_in_bert,\n    BertEmbeddings,\n    BertPooler,\n    BERT_INPUTS_DOCSTRING,\n    _CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n    _SEQ_CLASS_EXPECTED_OUTPUT,\n    _SEQ_CLASS_EXPECTED_LOSS,\n)\nfrom .modules import Adapter\n\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n_CONFIG_FOR_DOC = \"BertConfig\"\n\n\nclass RevBertSelfOutput(BertSelfOutput):\n    def __init__(self, config):\n        super().__init__(config)\n        if config.f_arch == \"attn\":\n            self.attn_adapter = Adapter(config)\n        else:\n            self.attn_adapter = None\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        if self.attn_adapter is not None: # For MEFT3\n            hidden_states = hidden_states + self.attn_adapter(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass RevBertAttention(BertAttention):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__(config, position_embedding_type=position_embedding_type)\n        self.output = RevBertSelfOutput(config)\n\n\nclass RevBertOutput(BertOutput):\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, adapter_change=None) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        # For MEFT1 and MEFT2: inserting an adapter in parallel to two consecutive feed-forward layers\n        if adapter_change is not None:\n            hidden_states = hidden_states + adapter_change\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass RevBertLayer(BertLayer):\n    def __init__(self, config):\n        super().__init__(config)\n        self.output = RevBertOutput(config)\n        self.ffn_adapter = Adapter(config)\n        if config.f_arch == \"attn\":  # For MEFT3: insert an adapter to the attention block\n            self.rev_adapter = None\n            self.attention = RevBertAttention(config)\n        else:  # For MEFT1 and MEFT2\n            self.rev_adapter = Adapter(config, layernorm=config.layernorm_in_adapter)\n\n        self.x1_factor = config.x1_factor  # by default, lambda = 0.1\n        self.x2_factor = config.x2_factor  # by default, beta = 0.1\n        self.f_arch = config.f_arch  # What is the choice for the F architecture\n        if config.f_arch == \"layer\":  # MEFT1\n            self.F = self.forward_layer\n            self.G = self.forward_adapter\n        elif config.f_arch == \"adapter\":  # MEFT2\n            self.F = self.forward_adapter\n            self.G = self.forward_layer\n        elif config.f_arch == \"attn\":  # MEFT3\n            self.F = self.forward_attention\n            self.G = self.forward_mlp\n        self.seeds = {}\n\n    def set_seed(self, key):\n        \"\"\"\n        Record the seed during the forward pass, so the activation can be recomputed.\n        \"\"\"\n        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:\n            # GPU\n            device_idx = torch.cuda.current_device()\n            seed = torch.cuda.default_generators[device_idx].seed()\n        else:\n            # CPU\n            seed = int(torch.seed() % sys.maxsize)\n\n        self.seeds[key] = seed\n        torch.manual_seed(seed)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        x1, x2 = torch.chunk(hidden_states, 2, dim=-1)\n        if self.training:\n            self.set_seed(\"F\")\n        f_outputs = self.F(\n            x2,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions\n        )\n        if isinstance(f_outputs, tuple):\n            f_x2 = f_outputs[0]\n            outputs = f_outputs[1:]\n        else:\n            f_x2 = f_outputs\n\n        y1 = self.x1_factor * x1 + f_x2\n\n        if self.training:\n            self.set_seed(\"G\")\n        g_outputs = self.G(\n            y1,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions\n        )\n        if isinstance(g_outputs, tuple):\n            g_y1 = g_outputs[0]\n            outputs = g_outputs[1:]\n        else:\n            g_y1 = g_outputs\n\n        y2 = self.x2_factor * x2 + g_y1\n\n        if self.f_arch == \"attn\":\n            y = torch.cat([y1, y2], dim=-1)  # Don't switch for MEFT3\n        else:\n            y = torch.cat([y2, y1], dim=-1)  # Switch for MEFT1 and MEFT2\n        outputs = (y,) + outputs\n\n        return outputs\n\n    def backward_pass(\n        self,\n        y,\n        dy,\n        attention_mask,\n        head_mask,\n        encoder_hidden_states,\n        encoder_attention_mask,\n        past_key_value\n    ):\n        assert self.training, (\n            \"If you want to train `ReversibleModel` and its variations, make sure to use `model.train()` to put the\"\n            \" model into training mode.\"\n        )\n        if self.f_arch == \"attn\":\n            y1, y2 = torch.chunk(y, 2, dim=-1)  # Don't Switch for MEFT3\n            dy1, dy2 = torch.chunk(dy, 2, dim=-1)\n        else:\n            y2, y1 = torch.chunk(y, 2, dim=-1)  # Swich for MEFT1 and MEFT2\n            dy2, dy1 = torch.chunk(dy, 2, dim=-1)\n        with torch.enable_grad():\n            y1.requires_grad = True\n            torch.manual_seed(self.seeds[\"G\"])\n            g_outputs = self.G(\n                y1,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                past_key_value=past_key_value,\n                output_attentions=False\n            )\n            if isinstance(g_outputs, tuple):\n                g_y1 = g_outputs[0]\n            else:\n                g_y1 = g_outputs\n\n            g_y1.backward(dy2, retain_graph=True)\n\n        with torch.no_grad():\n            x2 = (y2 - g_y1) / self.x2_factor\n            # save memory\n            del g_y1, y2\n            dy1 += y1.grad\n            y1.grad = None\n\n        with torch.enable_grad():\n            x2.requires_grad = True\n            torch.manual_seed(self.seeds[\"F\"])\n            f_outputs = self.F(\n                x2,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                past_key_value=past_key_value,\n                output_attentions=False\n            )\n            if isinstance(f_outputs, tuple):\n                f_x2 = f_outputs[0]\n            else:\n                f_x2 = f_outputs\n            f_x2.backward(dy1, retain_graph=False)\n\n        with torch.no_grad():\n            x1 = (y1 - f_x2) / self.x1_factor\n            del f_x2, y1\n            dy2 *= self.x2_factor\n            dy2 += x2.grad\n            x2.grad = None\n            dy1 *= self.x1_factor\n            x2 = x2.detach()\n\n        return torch.cat([x1, x2], dim=-1), torch.cat([dy1, dy2], dim=-1)\n\n    def forward_adapter(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        return self.rev_adapter(hidden_states)\n\n    def forward_layer(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n\n        # if decoder, the last output is tuple of self-attn cache\n        if self.is_decoder:\n            outputs = self_attention_outputs[1:-1]\n            present_key_value = self_attention_outputs[-1]\n        else:\n            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        cross_attn_present_key_value = None\n        if self.is_decoder and encoder_hidden_states is not None:\n            if not hasattr(self, \"crossattention\"):\n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                    \" by setting `config.add_cross_attention=True`\"\n                )\n\n            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                cross_attn_past_key_value,\n                output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n\n            # add cross-attn cache to positions 3,4 of present_key_value tuple\n            cross_attn_present_key_value = cross_attention_outputs[-1]\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        # if decoder, return the attn key/values as the last output\n        if self.is_decoder:\n            outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def forward_attention(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=None,\n        )\n        return self_attention_outputs\n\n    def forward_mlp(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        hidden_states = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, hidden_states\n        )\n        return hidden_states\n\n    def feed_forward_chunk(self, attention_output):\n        if self.ffn_adapter is not None:\n            adapter_change = self.ffn_adapter(attention_output)\n        else:\n            adapter_change = None\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, adapter_change=adapter_change)\n        return layer_output\n\n\nclass RevBackProp(Function):\n    @staticmethod\n    def forward(\n        ctx,\n        hidden_states,\n        layers,\n        attention_mask,\n        layer_head_mask,\n        encoder_hidden_states,\n        encoder_attention_mask,\n        past_key_value,\n        output_attentions,\n    ):\n        for i, layer_module in enumerate(layers):\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                layer_head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                past_key_value,\n                output_attentions,\n            )\n            hidden_states = layer_outputs[0]\n\n        ctx.save_for_backward(hidden_states.detach())\n        ctx.layers = layers\n        ctx.attention_mask = attention_mask\n        return hidden_states\n\n    def backward(ctx, dy):\n        y, = ctx.saved_tensors\n        layers = ctx.layers\n        attention_mask = ctx.attention_mask\n        for i, layer_module in enumerate(layers[::-1]):\n            y, dy = layer_module.backward_pass(\n                y,\n                dy,\n                attention_mask=attention_mask,\n                head_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                past_key_value=None\n            )\n        return dy, None, None, None, None, None, None, None\n\n\nclass RevBertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([RevBertLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n        self.num_rev_layers = config.num_rev_layers\n        assert self.num_rev_layers <= config.num_hidden_layers\n\n    def vanilla_backward(\n        hidden_states,\n        layers,\n        attention_mask\n    ):\n        for i, layer_module in enumerate(layers):\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask=attention_mask,\n                head_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                past_key_value=None,\n                output_attentions=False,\n            )\n            hidden_states = layer_outputs[0]\n        return hidden_states\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        # Set h_0^1 = h_1^1 = h_0\n        hidden_states = torch.cat([hidden_states, hidden_states], dim=-1)\n\n        # Layers with reversible gradient\n        layers_for_reverse_backward = self.layer[:self.num_rev_layers]\n        # Layers with vanilla gradient\n        layers_for_vanilla_backward = self.layer[self.num_rev_layers:]\n        if len(layers_for_reverse_backward) == 0:  # In this case, MEFTs are simply PEFT methods\n            executing_fn = RevBertEncoder.vanilla_backward\n            hidden_states = executing_fn(\n                hidden_states,\n                layers_for_vanilla_backward,\n                attention_mask\n            )\n        elif len(layers_for_reverse_backward) == self.config.num_hidden_layers:  # All layers are reversible\n            executing_fn = RevBackProp.apply\n            hidden_states = executing_fn(\n                hidden_states,\n                layers_for_reverse_backward,\n                attention_mask,\n                None,\n                None,\n                None,\n                None,\n                False\n            )\n        else:  # The first setting in Figure 7\n            lower_executing_fn = RevBackProp.apply\n            hidden_states = lower_executing_fn(\n                hidden_states,\n                layers_for_reverse_backward,\n                attention_mask,\n                None,\n                None,\n                None,\n                None,\n                False\n            )\n            higher_executing_fn = RevBertEncoder.vanilla_backward\n            hidden_states = higher_executing_fn(\n                hidden_states,\n                layers_for_vanilla_backward,\n                attention_mask\n            )\n\n        x1, x2 = torch.chunk(hidden_states, 2, dim=-1)\n        hidden_states = (x1 + x2) / 2.\n\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=None,\n            hidden_states=None,\n            attentions=None,\n            cross_attentions=None,\n        )\nfrom typing import Dict, Union, Any\nimport torch\nimport torch.nn as nn\nfrom packaging import version\nfrom transformers.trainer import Trainer\nfrom transformers.utils import is_sagemaker_mp_enabled, is_apex_available\n\nif is_sagemaker_mp_enabled():\n    import smdistributed.modelparallel.torch as smp\n    from smdistributed.modelparallel import __version__ as SMP_VERSION\n\n    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse(\"1.10\")\n\n    from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\nelse:\n    IS_SAGEMAKER_MP_POST_1_10 = False\n\nif is_apex_available():\n    from apex import amp\n\nclass CustomTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to train.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.do_grad_scaling:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            loss = self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        # Don't update the gradient of the layernorm parameters\n        if self.args.start_layer == -1:\n            model.bert.embeddings.LayerNorm.bias.grad = None\n            model.bert.embeddings.LayerNorm.weight.grad = None\n        else:\n            model.bert.encoder.layer[self.args.start_layer].output.LayerNorm.bias.grad = None\n            model.bert.encoder.layer[self.args.start_layer].output.LayerNorm.weight.grad = None\n\n        return loss.detach()",
        "experimental_info": "The methodology utilizes adapters to make PLMs reversible, adhering to the 'Starting Point Hypothesis', aiming to reduce activation memory. Three Memory Efficient Fine-Tuning (MEFT) architectures are proposed and controlled by the `f_arch` configuration parameter:\n1.  **MEFT 1 (`f_arch=\"layer\"`)**: The PLM layer acts as function `F` (`self.forward_layer`), and an adapter acts as function `G` (`self.forward_adapter`). The output order is switched in the forward pass (`y = torch.cat([y2, y1], dim=-1)`).\n2.  **MEFT 2 (`f_arch=\"adapter\"`)**: An adapter acts as function `F` (`self.forward_adapter`), and the PLM layer (which may also contain an adapter in its output) acts as function `G` (`self.forward_layer`). The output order is switched in the forward pass.\n3.  **MEFT 3 (`f_arch=\"attn\"`)**: The attention block with an integrated adapter acts as function `F` (`self.forward_attention`), and the MLP block with an integrated adapter acts as function `G` (`self.forward_mlp`). The output order is *not* switched in the forward pass (`y = torch.cat([y1, y2], dim=-1)`).\n\n**Reversibility Parameters**:\n*   `x1_factor` (λ) and `x2_factor` (β): Scaling factors for the reversible blocks, configurable (defaults to 1 in `run_glue.py`'s `ModelArguments`).\n*   `adapter_bottleneck_dim`: The bottleneck dimension for the adapter layers; a value of 0 implies no adapter is used.\n*   `layernorm_in_adapter`: A boolean flag indicating whether a LayerNorm is included within the adapter for G.\n\n**Hybrid Reversibility Approach**:\n*   `num_rev_layers`: Specifies the number of bottom layers that will be configured as reversible. If `num_rev_layers` is 0, vanilla back-propagation is used for all layers. If `num_rev_layers` equals the total number of hidden layers, all layers are reversible. Otherwise, a hybrid approach is employed where the specified number of bottom layers are reversible, and the remaining top layers use vanilla back-propagation (caching).\n\n**Starting Point Hypothesis Implementation**:\n*   **Adapter Initialization**: Adapters (specifically their `nn.Linear` layers) are initialized with weights drawn from a normal distribution with `mean=0.0` and `std=config.initializer_range`, ensuring their initial output is near zero as required by the hypothesis.\n*   **Parameter Freezing**: If `adapter_bottleneck_dim > 0`, all base model parameters are initially frozen. Only adapter parameters, the classifier, and specific LayerNorms are unfrozen to be trainable.\n*   **Gradient Handling**: To strictly adhere to the hypothesis and preserve the PLM's output at the beginning of training, the `CustomTrainer` explicitly sets the gradients of certain LayerNorm parameters to `None` after the backward pass. This prevents their update during training:\n    *   If `freeze_irreversible_layers` is false (`start_layer == -1`), `model.bert.embeddings.LayerNorm`'s bias and weight gradients are set to `None`.\n    *   If `freeze_irreversible_layers` is true, the `LayerNorm` of the layer *before* the first reversible layer (`model.bert.encoder.layer[self.args.start_layer].output.LayerNorm`) has its gradients set to `None`."
      }
    },
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces \"Spectral Adapter,\" a novel Parameter-Efficient Fine-Tuning (PEFT) method that enhances existing PEFT techniques by incorporating spectral information from pretrained weight matrices. It proposes two mechanisms: additive tuning (Spectral AdapterA) and orthogonal rotation (Spectral AdapterR) of the top singular vectors, obtained via Singular Value Decomposition (SVD). The work provides theoretical analysis showing improved rank capacity for Spectral AdapterA over LoRA and demonstrates superior parameter efficiency, tuning performance, and benefits for multi-adapter fusion.",
        "methodology": "The core methodology involves performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) and then fine-tuning only the top-r columns of the singular vector matrices (U and V). Spectral AdapterA additively tunes these top columns using trainable matrices (AU, AV) initialized to zero. Spectral AdapterR orthogonally rotates these top columns using trainable orthogonal matrices (RU, RV) initialized as identity, with orthogonality maintained via Cayley parameterization. The approach focuses on the top spectral space, justified by theoretical insights into adapter rank capacity and weight subspace alignment.",
        "experimental_setup": "The method was evaluated through extensive experiments on both Large Language Models (LLMs) and Diffusion Models. LLM experiments included fine-tuning Llama3 8B on Orca Math/GSM8K, DeBERTaV3-base on GLUE benchmarks, and Mistral 7B on GSM8K. Diffusion model experiments used Chilloutmix for multi-object fine-tuning (custom animals, toys, multi-character generation) and concept tuning (vase, chair, table). Baselines for comparison included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg. Evaluation metrics included training loss, validation scores (accuracy for LLMs), and CLIP-based alignment scores for diffusion models. All experiments were conducted on NVIDIA RTX A6000 GPUs, with hyperparameters largely following official baseline implementations.",
        "limitations": "A primary limitation lies in the fixed choice of tuning only the top spectral space, despite theoretical verification under simple settings. Further investigation into tuning different columns of singular vector matrices is needed to fully understand the role of spectral information. Additionally, the time consumption of the Singular Value Decomposition (SVD) procedure increases with larger models, posing a challenge.",
        "future_research_directions": "Future work could explore fine-tuning the spectral representation of specific components within large models, such as only the attention layer. Dynamically combining spectral adaptation with other PEFT methods, like AdaLoRA, is also a promising avenue. Further research is encouraged to investigate tuning different columns of singular vector matrices beyond just the top ones. Developing faster Singular Value Decomposition methods would also greatly benefit the scalability of this approach.",
        "experimental_code": "import math\n\nimport torch\nimport torch.nn as nn\nfrom diffusers.models.attention_processor import AttnProcessor\nfrom diffusers.utils.import_utils import is_xformers_available\nimport torch.nn.functional as F\nimport numpy as np\n\nif is_xformers_available():\n    import xformers\n    \n\ndef remove_edlora_unet_attention_forward(unet):\n    def change_forward(unet):  # omit proceesor in new diffusers\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and name == 'attn2':\n                layer.set_processor(AttnProcessor())\n            else:\n                change_forward(layer)\n    change_forward(unet)\n\n\nclass EDLoRA_Control_AttnProcessor:\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n    def __init__(self, cross_attention_idx, place_in_unet, controller, attention_op=None):\n        self.cross_attention_idx = cross_attention_idx\n        self.place_in_unet = place_in_unet\n        self.controller = controller\n        self.attention_op = attention_op\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            is_cross = False\n            encoder_hidden_states = hidden_states\n        else:\n            is_cross = True\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available() and not is_cross:\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            attention_probs = self.controller(attention_probs, is_cross, self.place_in_unet)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass EDLoRA_AttnProcessor:\n    def __init__(self, cross_attention_idx, attention_op=None):\n        self.attention_op = attention_op\n        self.cross_attention_idx = cross_attention_idx\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available():\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\ndef revise_edlora_unet_attention_forward(unet):\n    def change_forward(unet, count):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:\n                layer.set_processor(EDLoRA_AttnProcessor(count))\n                count += 1\n            else:\n                count = change_forward(layer, count)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0)\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx)\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx)\n    print(f'Number of attention layer registered {cross_attention_idx}')\n\n\ndef revise_edlora_unet_attention_controller_forward(unet, controller):\n    class DummyController:\n        def __call__(self, *args):\n            return args[0]\n\n        def __init__(self):\n            self.num_att_layers = 0\n\n    if controller is None:\n        controller = DummyController()\n\n    def change_forward(unet, count, place_in_unet):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:  # only register controller for cross-attention\n                layer.set_processor(EDLoRA_Control_AttnProcessor(count, place_in_unet, controller))\n                count += 1\n            else:\n                count = change_forward(layer, count, place_in_unet)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0, 'down')\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx, 'mid')\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx, 'up')\n    print(f'Number of attention layer registered {cross_attention_idx}')\n    controller.num_att_layers = cross_attention_idx\n\nclass SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        rank = 8\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r\n\n    def cayley(self, data: torch.Tensor) -> torch.Tensor:\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)\n        else:\n            raise Exception('')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv :\n            raise Exception('')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)\n",
        "experimental_info": "The method performs Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T) and fine-tunes only the top-`r` columns of the singular vector matrices (U and V) and the corresponding singular values (S). The specific implementation uses `SpectralLinearLayer_OFT`.\n\nKey experimental settings:\n- **Rank (`r`)**: The number of top singular values and vectors that are fine-tuned. This is specified by `revised_r` in the `SpectralLinearLayer_OFT` constructor and stored as `self.rank`. In the given code, `rank` is explicitly set to `8` within the `SpectralLinearLayer_OFT` constructor, overriding any passed `rank` parameter.\n- **Initialization**: \n    - `self.spectral_A` and `self.spectral_B` (trainable matrices for `U` and `V` modifications) are initialized to `torch.zeros(r, r)`. When passed through the `cayley` function, these initially result in identity orthogonal matrices, aligning with the "
      }
    },
    {
      "title": "Adapters Strike Back",
      "abstract": "Adapters provide an efficient and lightweight mechanism for adapting trained\ntransformer models to a variety of different tasks. However, they have often\nbeen found to be outperformed by other adaptation mechanisms, including\nlow-rank adaptation. In this paper, we provide an in-depth study of adapters,\ntheir internal structure, as well as various implementation choices. We uncover\npitfalls for using adapters and suggest a concrete, improved adapter\narchitecture, called Adapter+, that not only outperforms previous adapter\nimplementations but surpasses a number of other, more complex adaptation\nmechanisms in several challenging settings. Despite this, our suggested adapter\nis highly robust and, unlike previous work, requires little to no manual\nintervention when addressing a novel scenario. Adapter+ reaches\nstate-of-the-art average accuracy on the VTAB benchmark, even without a\nper-task hyperparameter optimization.",
      "full_text": "Adapters Strike Back Jan-Martin O. Steitz1 Stefan Roth1,2 1Department of Computer Science, TU Darmstadt 2 hessian.AI Abstract Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of dif- ferent tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as vari- ous implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architec- ture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual inter- vention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.† 1. Introduction Transfer learning from an off-the-shelf model, pre-trained on a large dataset like ImageNet [55] to a downstream task by fully fine-tuning the model’s parameters is a common paradigm. A typical CNN architecture, like a ResNet [24], has several tens of millions of parameters. However, since the introduction of transformers [59] into the realm of com- puter vision [4, 5, 13, 51, 52, 63], model sizes have grown exponentially from around a hundred million parameters for a vision transformer (ViT) [ 13] to more than a billion parameters [10, 46]. This leads to huge storage requirements when fine-tuning on multiple downstream tasks because a complete set of the model’s parameters needs to be saved per task. Additionally, large models require correspondingly large datasets [e.g., 56] to be trained to their full potential, yet tend to overfit easily if the target dataset in transfer learn- ing is too small. One solution is linear probing [12], where only the linear classifier is trained, but this usually yields inferior results compared to full fine-tuning. As a consequence, there is a growing interest in parameter- efficient tuning methods. The main idea is to freeze the †Code is available at https://github.com/visinf/adapter_plus. 0 0.1 0.2 0.3 0.4 0.5 0.6 72 74 76 78 Fine-tuning accuracy Linear probing # Parameters (M) Accuracy (%) Adapter+ (ours) SPT -Adapter [21]SSF [39] Adapter+ opt. (ours)SPT -Adapter⟳ SSF⟳ FacT -TK [32] Consolidator [20]VPT [31] FacT -TK⟳ LoRA [29] VPT⟳ Figure 1. Parameter-accuracy characteristics of adaptation methods on the VTAB [65] test sets. We report original results and re-evaluations ( ⟳) after a complete training schedule with suitable data normalization. Our Adapter+ has clearly the best parameter-accuracy trade-off. The vertical, dashed line shows the possible minimal number of tunable parameters when only the classifiers are trained, i.e., using linear probing (61% accuracy). parameters of the pre-trained model and add a compara- tively small amount of parameters to the model, which are then tuned together with the classifier to adapt the model to the downstream task at hand. Representative methods with different underlying concepts include VPT [31], which prepends the sequence of image tokens in the attention with trainable tokens to learn a prompt tuning, LoRA [29], where the attention weights are updated with learnable low-rank decomposition matrices, and Adapters [28], which are small bottleneck modules that are added to every transformer layer of the network. Adapters were first proposed for CNNs by Rebuffi et al. [53] and various formulations [22, 28, 49] exist for the now common ViT architecture. Recent work on parameter-efficient transfer learning [e.g., 20, 21, 31, 32, 39, 67] presents adapters as a baseline method for the adaptation to downstream tasks in computer vision. However, we identified various common issues in their imple- mentations, which we find to have a negative influence on the To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, W A, USA, 2024. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. arXiv:2406.06820v1  [cs.CV]  10 Jun 2024LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 80 81 82 83 84 82.4 82.5 80.3 82.6 82.4 82.2 84.0 Accuracy (%) Natural LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 85 86 84.3 84.6 85.7 84.7 86.3 85.3 86.5 Accuracy (%) Specialized LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 58 60 62 60.1 62.1 58.0 62.3 60.9 60.5 63.3 Accuracy (%) Structured Figure 2. Average accuracy for VTAB subgroups on thetest sets. For methods marked with ⟳, we report results of our re-evaluation after a complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank r∈[1..32]. adaptation performance. For further details, refer to the sup- plemental material. Additionally, while adapters have been well studied in natural language processing (NLP), there is no study that broadly examines the different adapter config- urations for vision transformers. As a result, adapters have seemed to underperform in comparison to recent parameter- efficient adaptation methods, e.g., reported accuracies of adapters on VTAB of 73.9% in [67] and 60.8% in [31]. In this work, we therefore revisit the idea of adapters and investigate how they can perform at their best in con- nection with ViTs. Our contribution hereby is threefold: (1) We show the first in-depth and systematic study on the effects of the adapter position in the transformer and of the adapter’s inner structure with ViTs, as well as evaluate differ- ent variants of parameter initialization. (2) We further pro- pose a learnable, channel-wise scaling as extension to plain adapters, which proves to be beneficial for computer vision tasks. (3) Finally, we present Adapter+, an adapter configu- ration with an excellent parameter-accuracy trade-off com- pared to other work, as shown in Fig. 1. Adapter+ reaches a state-of-the-art average accuracy of 77.6% on VTAB [65] without any hyperparameter optimization per task and 3.7 percentage points (pp) over previous adapter baselines. We also reach a state-of-the-art accuracy of 90.7% on FGVC [31] with the lowest number of parameters compared to other methods. Finally, Adapter+ shows the best robustness in terms of accuracy across the VTAB subgroups, see Fig. 2. 2. Related work One possibility to adapt a pre-trained network to a novel task, apart from full fine-tuning, is to only selectively tune some of the parameters, e.g., only training the classifier [12]. Cai et al. [3] proposed to tune only the biases of an otherwise frozen network to adapt it to a downstream task. BitFit [ 64] then showed the efficacy of this method for NLP transformers. Modular adaptation. The concept of adding small, train- able modules with only a few parameters to an otherwise frozen network was first proposed for adapting CNNs by Rebuffi et al. [53] and called adapters. Other approaches replaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts [19], learned binary masks to prune a pre-trained network per target task [ 41], or created a student network by aug- menting the original network with adapter-like modules and skip connections, which then mimicked a teacher network by disabling parts of its pre-trained and added modules [43]. Following the rise of transformers in NLP [ 11, 50, 59], Houlsby et al. [28] proposed adapter modules in the form of bottlenecks for transformer layers. Pfeiffer et al. [49] conducted an architecture search on NLP tasks to find a more parameter-efficient configuration of adapter modules that only acts on the transformer’s feed-forward network (FFN), thus saving roughly half of the parameters over [28]. Prompt tuning. Inspired by changing the output of a net- work for NLP with hand-crafted textual prompts, which modifies the attention over the original input tokens, Lester et al. [37] proposed prompt tuning: A set of learnable to- kens is added to the input sequence and trained with back- propagation to prompt a frozen language model to perform downstream tasks. Li and Liang [38] extended on prompt tuning by adding learnable tokens at every transformer layer of the model, which they termed prefix tuning. Jia et al. [31] applied prompt tuning to vision transformers, then called visual prompt tuning (VPT), by preprending the sequence of image patch embeddings with such trainable tokens (VPT- Shallow). They also showed a variant resembling prefix tuning with stronger adaptation capabilities that adds tokens at every layer of the network (VPT-Deep). Low-rank approaches. Also focusing on the attention part of the transformer layers, Hu et al. [29] proposed low-rank adaptation (LoRA) where the attention weights are updated with low-rank decomposition matrices. The matrices can be merged with the attention weights for inference. The structure of LoRA is very similar to an adapter, which can be seen as a superset of LoRA acting on the transformer’s FFN. He et al. [22] proposed a formalism to unify LoRA, adapters, and prefix tuning [ 38]. It allowed them to combine the beneficial aspects of all three methods into a scaled parallel adapter (Scaled PA) for NLP tasks. AdaptFormer [6] then applied the concept of Scaled PA to vision transformers. 2Other related work. Newer approaches for vision trans- formers proposed different techniques to further enhance the parameter-accuracy trade-off in adaptation. NOAH [67] per- forms an architecture search for a combination of adapters, LoRA, and VPT for each task. SSF [39] scales and shifts the features in the network after every operation, i.e., attention, FFN, layer normalization, with task-specific, trainable mod- ules. Jie and Deng [32] aggregate the weights of a ViT into a single 3D tensor. Task-specific weight updates of this tensor are learned as a matrix decomposed into parameter-efficient factors, hence they termed their method factor-tuning (FacT). SPT [21] measures the importance of the weights of a pre- trained network for a downstream task. Based on a desired parameter budget, the most important parameters are chosen for tuning and adapters or LoRA are used for weight matrices that contain enough parameters of importance. Consolidator [20] adapts weights in multiple orderings of channel-wise groups. The updates for all groups are merged for efficient storage and inference. Despite these new developments, we show that the sim- ple concept of adapters exhibits an even better parameter- accuracy trade-off in combination with vision transformers – if done right and with the addition of a channel-wise scaling. 3. Adapters for vision transformers 3.1. Vision transformer basics In this work, we concentrate on the parameter-efficient adaptation of vision transformers (ViT) [ 13]. The ViT is closely modeled after the transformer model for natural lan- guage processing (NLP) proposed by Vaswani et al. [59]. A learned linear projection embeds non-overlapping and flat- tened patches of the input image into a sequence of n tokens x ∈ Rn×d, where d is called the hidden dimension of the transformer. A positional encoding is added to the embed- dings and the sequence is prepended with a trainable [CLS] token. The sequence length and the dimension of the tokens stay fixed throughout the architecture. The sequence is sent through consecutive transformer layers that each consist of a multi-head self-attention and a feed-forward network (FFN). For the self-attention, the tokens are projected to queries, keys, and values (Q, K, and V ) and the output of each of the M attention heads is calculated as Attention(x) =Softmax \u0012Q(x)K(x)T √ d′ \u0013 V (x), (1) with d′ = d/M being the inner dimension of the head. The FFN consists of a multilayer perceptron with two linear layers (with weights Wi and biases bi) and a GELU [ 26] non-linearity as activation in between: FFN(x) =GELU(xW1 + b1)W2 + b2. (2) Both attention and FFN are employed with a preceding layer normalization (LN) [1] and a skip connection and, therefore, transform an input sequence x sequentially as x 7→ Attention(LN(x)) +x (3a) x 7→ FFN(LN(x)) +x. (3b) To keep the notation concise, we will omit the LNs of atten- tion and FFN in the following; each attention and FFN is assumed to be always preceded by an LN. 3.2. Adapters and their inner structure Adapters [28] are small modules that are added to the trans- former layers. They allow to tailor a network to a new task or domain, where instead of tuning the parameters of the whole network, only the adapter parameters and the classi- fier are trained. Adapters take the form of bottlenecks with an inner dimension of r ≪ d. We call r the rank of the adapter. In detail, a down-projection to dimension r with weights Wdown ∈ Rd×r and biases bdown ∈ Rr is followed by a non-linear activation function σ(·), typically a GELU [26] as used throughout the ViT, and an up-projection with weights Wup ∈ Rr×d and biases bup ∈ Rd back to the hid- den dimension d of the transformer layer. This yields a base adapter module Adapterbase(x) =σ(xWdown + bdown)Wup + bup . (4) The base adapter module can be further enhanced with a normalization layer, e.g., a layer normalization (LN) [ 1]. Additionally, the output of the bottleneck can be scaled by s as Adapter(x) =s · Adapterbase \u0000 LN(x) \u0001 . (5) For layer-wise scaling, the factor s is taken to be a scalar, i.e. s ∈ R, and can be either fixed as a hyperparameter or learned during training. Layer-wise scaling was proposed by He et al. [22] and Hu et al. [29] but deemed not effective compared to a fixed scaling for tasks in NLP. Here, we additionally propose to use a channel-wise, learned scaling where s ∈ Rd. We investigate its capabilities in Sec. 4.3. In most cases, the adapter is used with a skip connection, hence the complete feature transformation becomes x 7→ Adapter(x) +x. (6) The complete inner structure of an adapter including its skip connection is visualized in Fig. 3a. 3.3. Adapter positions Although the architecture of bottleneck adapters for trans- formers is rather simple, there are various ways to plug them into the transformer layer. Previous work has not yet in- vestigated what the optimum position is for the use with a ViT [13]. Here, we evaluate four possible adapter positions, shown in Figs. 3b to 3e. We postulate that it is easier for an adapter to learn to modify features previously transformed 3FF down FF up Act LN Scaling (a) Inner structure FFN Adapter (b) Pre FFN Adapter (c) Post FFN Adapter (d) Parallel FFN Adapter (e) Intermediate Figure 3. Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layer normalization (LN) and scaling, (b)–(d) different possible adapter positions to connect the adapter to the FFN section of the transformer layer. Modules with trainable parameters are shown in red and frozen modules in blue. by a frozen module in the network rather than to anticipate what changes to the features are needed in adapting for a frozen module that follows the adapter. Putting it differently, we argue that the adapter should follow a frozen module. Pre-Adapter. The first adapter position we analyze ap- plies the adapter to the output x of the attention section of the transformer layer before it is passed into the FFN, but with the skip connection of the attention already added (Fig. 3b). The feature transformation of the FFN section with the adapter attached, therefore, becomes x 7→ FFN \u0000 Adapter(x) +x \u0001 + \u0000 Adapter(x) +x \u0001 . (7) Note that the two occurrences of Adapter(x) in Eq. (7) refer to the same instantiation. In this configuration, the adapter has the full information from the feature transformation hap- pening in the attention but needs to estimate the transforma- tion that will be happening in the FFN that follows. As a result, especially the last FFN before the linear classifier will be hard to adapt. To the best of our knowledge, this adapter position has not been considered in the literature. Post-Adapter. In this case, the adapter is positioned at the very end of the transformer layer on the output of the FFN with its skip connection added as x 7→ Adapter \u0000 FFN(x) +x \u0001 + \u0000 FFN(x) +x \u0001 , (8) where the FFNs refer to the same intantiation (Fig. 3c). That way, the adapter has access to the feature transformation happening in the FFN and the unmodified features via the skip connection. This position has been proposed by Pfeiffer et al. [49] as the result of an architecture search, but only for adapting transformers for NLP tasks and not for a ViT. Parallel-Adapter. Next, we consider a parallel setting as proposed by [22], where the adapter is located parallel to the FFN and both share a skip connection (Fig. 3d): x 7→ FFN(x) +Adapter(x) +x. (9) Therefore, both adapter and FFN work on the output of the attention section of the transformer layer and the adapter needs to learn the necessary residual transformation to the one produced by the frozen FFN. Intermediate-Adapter. Finally, we consider the original adapter position as proposed by Houlsby et al. [28]. The adapter is plugged behind the FFN but before the skip con- nection of the FFN is added (Fig. 3e). The adapter addition- ally possesses its own skip connection: x 7→ Adapter \u0000 FFN(x) \u0001 + FFN(x) +x. (10) Note that the two occurrences of FFN(x) in Eq. (10) refer to the same instantiation. The adapter sees the transformed features coming from the FFN but cannot access the features added later on by the skip connection of the FFN. 3.4. Initialization of adapter parameters Since training a deep learning model is a non-convex opti- mization problem, the initialization of parameters is impor- tant. In this work, we evaluate three different variants of parameter initializations for adapters proposed in the litera- ture. All of them have the goal to initialize the adapters in a way that minimizes the initial influence of the adapters at the start of their training. This is a sensible goal since adapters extend an already pre-trained frozen network. Houlsby initialization. Houlsby et al. [28] propose to draw the weights of the projection matrices from a zero-centered Gaussian distribution with a standard deviation of σ = 0.01, truncated at 2σ, and use zero for their biases. BERT initialization. For the BERT model [11], the initial- ization works similar to [28] but the Gaussian distribution has a standard deviation of σ = 0.02 and is not truncated. This form of initialization is used by Pfeiffer et al. [49]. LoRA initialization. LoRA [29] initializes the weights and biases of the down-projection with a uniform Kaiming He ini- tialization [23]; the weights and biases of the up-projection 4are initialized to zero. Therefore, the output of the adapter at the beginning of training equals zero and the adapter initially does not contribute. 3.5. Data normalization in pre-processing Data normalization is common practice during image pre- processing. It is typically done by shifting and scaling of each input pixel xij for each channel c as ˆxijc = (xijc − µc)/σc . (11) Most widely used are the mean µ = (0.485, 0.456, 0.406)T and standard deviation σ = (0.229, 0.224, 0.225)T of the ImageNet dataset [55], commonly referred to as ImageNet normalization. Another option is using 0.5 for every element of µ and σ, which is commonly referred to as Inception normalization because it is used for the Inception family of CNN architectures, starting with Inception-v3 [58]. The Im- ageNet normalization aims to center the input data around 0 with a standard deviation of 1. The Inception normalization, on the other hand, transforms the input values such they are strictly in range [−1, 1]. Because we try to adapt to a target domain on a very low parameter budget, it is important to use the data normaliza- tion the network saw during its pre-training. Otherwise, the parameter-efficient transfer method of choice needs to first compensate for the shift in input data statistics and loses parts of its capacity to adapt to the target domain. 4. Experiments 4.1. Datasets In order to carry out a detailed study of the utility of adapters in the context of ViT models, we experiment with two stan- dard benchmarks for task adaptation. VTAB. The Visual Task Adaptation Benchmark (VTAB) [65] consists of 19 tasks, which are further grouped into three categories: Natural, Specialized, and Structured. The Natural group contains natural images captured using stan- dard photographic equipment. The Specialized group is built from datasets of images captured with specialized equip- ment, from remote sensing and medical domains. Lastly, the Structured group is for evaluating the understanding of the scene structure. Here, the majority of the datasets are compiled from synthetic images with scenes that are easy to assess for humans but have a large domain gap to natural image datasets. Each task of VTAB consists of 800 train- ing and 200 validation images. The test sets have the same number of images as the test sets in the original datasets. FGVC. Following Jia et al. [31], we compile five datasets for fine-grained visual classification (FGVC): CUB-200- 2011 [61], NABirds [ 27], Oxford Flowers [ 45], Stanford Dogs [34], and Stanford Cars [17]. Because VTAB bench- marks task adaptation in a low-data regime in terms of the Table 1. Adapter position. We report the average accuracy in % (± std. dev.) on the VTABval sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 82.4 ± 0.4 86.2 ± 0.8 57.5 ± 0.5 75.3 ± 0.3 Intermediate 83.0 ± 0.4 85.0 ± 0.8 57.2 ± 0.5 75.1 ± 0.3 Parallel 83.0 ± 0.3 86.2 ± 0.6 57.7 ± 0.6 75.6 ± 0.3 Post 83.0 ± 0.3 85.7 ± 0.4 59.1 ± 0.3 76.0 ± 0.2 number of available training images, we use FGVC to eval- uate adaptation methods in settings where training data is abundant. Where validation sets are not available in FGVC, we follow Jia et al. [31] to create the validation splits. For further details regarding the dataset properties of VTAB and FGVC, see supplemental material. 4.2. Experimental settings For all our experiments, we use a ViT-B/16 network [13] that was pre-trained on ImageNet-21k [55]. We follow its pre- training settings, in particular, regarding input data normal- ization. We train all models with an AdamW [40] optimizer with a learning rate of 10−3, a weight decay of 10−4, and a batch size of 64, following [ 67]. For full fine-tuning, we use a learning rate of 10−4, which we found leads to better results. We use a cosine learning rate schedule with a linear warm-up over the first 10 epochs and train for 100 epochs in total. We use stochastic depth with linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen network and with a drop rate of 0.1 for the adapters during training. Apart from data normalization (cf . Sec. 3.4), we resize input images to 224×224 px for VTAB and use a randomly resize crop to 224×224 px and horizontal flipping for FGVC. For the ablations and to determine hyperparam- eters, we evaluate on the validation splits. We include the validation sets in the training data for producing final results. 4.3. Exploring adapter configurations Adapter position. We first evaluate the four possible posi- tions to connect an adapter to the FFN section of the trans- former layer, as described in Sec. 3.3. In our ablation, we use Adapterbase (cf . Eq. (4)) with rank r=8 and use the Houlsby initialization. In this experiment, the adapters neither have a layer normalization nor use scaling. The results on the VTAB validation set for all four adapter positions are presented in Tab. 1. The Post-Adapter yields the best result with 76.0% average accuracy over all VTAB subgroups. It confirms our hypothesis that the adapter should follow the frozen FFN module because it can then post-hoc modify the features flowing through the network. The par- allel configuration comes in second with 75.6% average accuracy, receiving the same input as the FFN but having to 5Table 2. Inner adapter structure. We evaluate the different com- ponents of the adapter structure, e.g., normalization layer (Norm), layer-wise and channel-wise learnable scaling on the VTAB val sets. The difference to Adapterbase (first row) is shown in ∆base. Bias Norm Scaling Initialization Accuracy (%) ∆base ✓ Houlsby 76.0 ± 0.2 0.0 Houlsby 75.6 ± 0.4 −0.4 ✓ LoRA 75.5 ± 0.3 −0.5 ✓ BERT 75.8 ± 0.3 −0.2 ✓ ✓ Houlsby 75.9 ± 0.3 −0.1 ✓ ✓ layer Houlsby 75.9 ± 0.3 −0.1 ✓ layer Houlsby 76.2 ± 0.3 +0.2 ✓ ✓ channel Houlsby 75.8 ± 0.3 −0.2 ✓ channel Houlsby 76.5 ± 0.2 +0.5 learn a residual modification to the FFN instead of a subse- quent one. Pre-Adapter and Intermediate-Adapter are subpar compared to the other positions. They either do not have access to the feature transformation happening afterwards in the FFN or to the features of the skip connection containing the output of the attention. Inner structure. Next, we investigate the impact of the in- ner structure of adapters including their initialization. Tab. 2 shows our findings with average accuracies calculated over the three VTAB subgroups. Removing the biases from the linear layers leads to a decrease in accuracy of 0.4 percent- age points (pp). We find that the Houlsby initialization of the adapter parameters is best while BERT and LoRA initializa- tions reduce the accuracy by 0.2 pp and 0.5 pp. Adding layer normalization (LN) to the adapter is slightly detrimental for all settings, both with scaling and without, while addition- ally adding 2d parameters per layer. We find that a learned scaling is in general beneficial for image-classification tasks. Adding layer-wise scaling leads to a gain of 0.2 pp. The inclusion of a learned, channel-wise scaling, as proposed here, gives the strongest improvement of 0.5 pp, reaching an accuracy of 76.5% on the VTAB validation set while only adding half of the parameters compared to LN. What makes a great adapter? From our systematic explo- ration of possible adapter configurations, we conclude that adapter modules in the Post-Adapter position with a learn- able, channel-wise scaling and Houlsby initialization work best for computer vision tasks. We call our proposed adapter configuration Adapter+. The addition of layer normaliza- tion, as suggested by Pfeiffer et al. [49], is not necessary and even leads to detrimental effects in our setting. Configurations from previous work. Different configu- rations of adapters have been established in previous work. We compare their configurations to our systematic approach with rank r=8 on the VTAB validation sets. Using our own implementations already leads to better results than reported in literature but enables us to compare on equal footing. Houlsby et al. [28] use an Intermediate-Adapter with their Table 3. Comparison of Adapter+ with adapter configurations from previous work. We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param (M) Natural Specialized Structured Average Houlsby [28], r=8 0.39 82.9 ± 0.2 85.5 ± 0.3 58.9 ± 0.8 75.8 ± 0.3 Houlsby [28], r=4 0.24 82.9 ± 0.4 84.9 ± 0.3 58.3 ± 0.6 75.4 ± 0.3 Pfeiffer [49] 0.21 82.9 ± 0.3 86.1 ± 0.9 58.4 ± 0.7 75.8 ± 0.4 AdaptFormer [6] 0.19 83.0 ± 0.4 85.0 ± 0.2 57.4 ± 0.5 75.2 ± 0.2 Adapter+ 0.20 83.0 ± 0.2 86.8 ± 0.6 59.7 ± 0.4 76.5 ± 0.2 proposed initialization both at the FFN section as well at the attention part of the transformer layer. Additionally, they adapt the LN parameters of the backbone. We, therefore, compare their setting additionally with r = 4to compare on roughly the same parameter budget. Pfeiffer et al. [49] suggest a Post-Adapter like us but with a BERT initialization and they employ a layer normalization inside the adapter. AdaptFormer [6] has the same configuration as a scaled parallel adapter (Scaled PA) [22], which was proposed for NLP tasks, the only difference being the layer-wise scalings. Scaled PA uses a fixed scaling of s = 4 for the adapters whereas AdaptFormer suggests to use s = 0.1 for vision tasks. Optimizing s for VTAB may lead to better results. Our results are presented in Tab. 3. We see a clear advantage of our Adapter+ configuration, gaining at least 0.7 pp over all previous adapter realizations considered despite having the second lowest number of trainable parameters. 4.4. Main results VTAB. We evaluate Adapter+ on the VTAB test sets and compare to other methods in Tab. 4. We provide results for full fine-tuning and tuning only the linear classifier while freezing the rest of the backbone [12] as a baseline of classi- cal fine-tuning methods. As competing parameter-efficient tuning methods, we include LoRA [29], VPT [31], NOAH [67], SSF [39], FacT [32], Consolidator [20], and SPT [21]. Wherever possible, we re-evaluate the other methods with a suitable data normalization for the pre-trained backbone and after the full training schedule to enable a fair compar- ison. For LoRA, we use our own implementation because the original work does not cover VTAB. For VPT, we adopt the number of tokens per task from their hyperparameter optimization but find that we do not need to tune learning rate and weight decay per task. Additionally, deviating from the original implementation, we optimize with AdamW [40] instead of SGD [54] and change to an appropriate data nor- malization. We present the original results from [ 31] on VTAB together with our re-evaluation. Our improved imple- mentation of VPT increases the average accuracy by 4.4 pp from 72.0% to 76.4%. SSF, FacT, and SPT released code to evaluate on VTAB. For FacT and SPT, we change the data normalization to match the backbone; SSF already uses the correct one. We re-run the provided code and present the 6Table 4. Detailed results on the VTAB test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. †: Early-stopping based on the test set, •: unsuitable data normalization, E: per-task hyperparameter optimization. 1Average across the average accuracies of the VTAB groups, following previous work. 2No complete code release for Consolidator, hence training and evaluation details are unknown. Natural Specialized Structured # Param (M) Cifar100 [35] Caltech101 [15] DTD [8] Flower102 [45] Pets [47] SVHN [44] Sun397 [62] Average Camelyon [60] EuroSAT [25] Resisc45 [7] Retinopathy [14] Average Clevr-Count [33] Clevr-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori [42] sNORB-Azi. [36] sNORB-Ele. [36] Average Global Average1 Full 85.8 73.2 92.6 70.4 97.9 86.2 90.6 39.6 78.6 87.1 96.6 87.5 74.0 86.3 66.6 61.0 49.8 79.7 82.6 51.9 33.5 37.0 57.8 74.2 Linear 0.04 78.1 88.1 69.0 99.1 90.0 36.0 56.9 73.9 79.8 90.7 73.7 73.7 79.5 32.4 30.5 35.9 61.9 11.2 26.2 14.3 24.5 29.6 61.0 LoRA [29] 0.29 83.0 91.7 71.6 99.2 90.9 83.8 56.7 82.4 86.2 95.7 83.5 71.9 84.3 77.7 62.3 49.0 80.2 82.2 51.7 31.0 47.0 60.1 75.6 VPT-Deep E• [31] 0.60 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.5 81.8 96.1 83.4 68.4 82.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 55.0 72.0 VPT-Deep E⟳ 0.60 83.0 93.0 71.2 99.0 91.3 84.1 56.0 82.5 84.9 96.6 82.5 74.5 84.6 77.5 58.7 49.7 79.6 86.2 56.1 37.9 50.7 62.1 76.4 NOAH E†•◦ [67] 0.43 69.6 92.7 70.2 99.1 90.4 86.1 53.7 80.2 84.4 95.4 83.9 75.8 84.9 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 61.3 75.5 SSF E† [39] 0.24 69.0 92.6 75.1 99.4 91.8 90.2 52.9 81.6 87.4 95.9 87.4 75.5 86.6 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 59.0 75.7 SSF E⟳ 0.24 61.9 92.3 73.4 99.4 92.0 90.8 52.0 80.3 86.5 95.8 87.5 72.8 85.7 77.4 57.6 53.4 77.0 78.2 54.3 30.3 36.1 58.0 74.6 FacT-TK8 E†• [32] 0.05 70.3 88.7 69.8 99.0 90.4 84.2 53.5 79.4 82.8 95.6 82.8 75.7 84.2 81.1 68.0 48.0 80.5 74.6 44.0 29.2 41.1 58.3 74.0 FacT-TK8 E⟳ 0.05 74.9 92.7 73.7 99.1 91.3 85.5 57.7 82.1 86.8 94.9 84.1 70.9 84.2 81.9 64.1 49.2 77.2 83.8 53.1 28.2 44.7 60.3 75.5 FacT-TK≤32 E†• [32] 0.10 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.6 84.8 96.2 84.5 75.7 85.3 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.7 75.6 FacT-TK≤32 E⟳ 0.10 74.6 93.7 73.6 99.3 90.6 88.7 57.5 82.6 87.6 95.4 85.5 70.4 84.7 84.3 62.6 51.9 79.2 85.5 52.0 36.4 46.6 62.3 76.5 Consolidator 2 [20] 0.30 74.2 90.9 73.9 99.4 91.6 91.5 55.5 82.4 86.9 95.7 86.6 75.9 86.3 81.2 68.2 51.6 83.5 79.8 52.3 31.9 38.5 60.9 76.5 SPT-Adapter †• [21] 0.23 72.9 93.2 72.5 99.3 91.4 84.6 55.2 81.3 85.3 96.0 84.3 75.5 85.3 82.2 68.0 49.3 80.0 82.4 51.9 31.7 41.2 60.8 75.8 SPT-Adapter ⟳ 0.22 74.7 94.1 73.0 99.1 91.2 84.5 57.5 82.0 85.7 94.9 85.7 70.2 84.1 81.3 63.2 49.1 80.7 83.5 52.0 26.4 41.5 59.7 75.3 SPT-Adapter †• [21] 0.43 72.9 93.2 72.5 99.3 91.4 88.8 55.8 82.0 86.2 96.1 85.5 75.5 85.8 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 61.4 76.4 SPT-Adapter ⟳ 0.43 74.9 93.2 71.6 99.2 91.1 87.9 57.2 82.2 87.0 95.4 86.5 72.4 85.3 81.1 63.2 50.3 80.2 84.4 51.4 31.5 42.2 60.5 76.0 Adapter+, r=1 0.07 85.4 92.4 73.1 99.1 91.3 83.1 58.1 83.2 87.2 96.6 85.3 72.6 85.5 80.7 60.6 50.9 79.9 83.3 55.6 27.1 43.0 60.1 76.3 Adapter+, r=2 0.09 85.4 93.0 72.7 99.2 90.6 85.3 58.0 83.5 87.9 96.8 85.5 71.4 85.4 83.2 61.0 51.6 80.1 86.1 56.3 30.7 46.5 61.9 76.9 Adapter+, r=4 0.13 84.8 93.8 72.7 99.2 90.6 86.5 57.4 83.6 87.5 96.9 85.9 71.5 85.4 83.4 61.6 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r=8 0.20 84.6 94.2 72.3 99.3 90.7 87.6 56.7 83.6 87.7 97.0 86.7 72.3 85.9 83.2 60.9 53.8 80.3 88.1 55.6 35.7 47.7 63.1 77.6 Adapter+, r=16 0.35 83.7 94.2 71.5 99.3 90.6 88.2 55.8 83.3 87.5 97.0 87.4 72.9 86.2 82.9 60.9 53.7 80.8 88.4 55.2 37.3 46.9 63.3 77.6 Adapter+, r∈[1..4] E 0.11 85.4 93.8 72.7 99.1 90.6 86.5 58.1 83.7 87.5 96.8 85.9 71.4 85.4 83.4 61.0 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r∈[1..8] E 0.16 85.4 93.8 72.7 99.1 90.7 87.6 58.1 83.9 87.7 96.8 86.7 72.3 85.9 83.4 60.9 53.8 80.3 88.1 55.3 35.7 47.7 63.1 77.7 Adapter+, r∈[1..32] E 0.27 85.4 93.8 72.7 99.1 90.7 88.2 58.1 84.0 87.5 96.8 87.8 73.9 86.5 83.4 60.9 53.8 80.3 87.2 55.3 37.9 47.7 63.3 77.9 results after a full training schedule. For completeness, we also report the results from the original publications. How- ever, we found that the code releases of [21, 32, 39] use early stopping based on the best result on the test set. We argue that tuning hyperparameters such as the number of training epochs on the test set goes against established practices in machine learning; rather the validation set should be used for early stopping. Yet, due to the limited size of the training and validation sets in VTAB, it is not feasible to report test results without also training on the validation data. Hence, we chose to complete a full training schedule of 100 epochs instead of using early stopping. Training SSF for the full schedule leads to a decrease in average accuracy of 1.1 pp over the original publication and re-evaluating SPT leads to a decrease of up to 0.5 pp, even with a corrected data normalization. FacT on the other hand benefits from our re-revaluation, since the accuracy decrease from training a complete schedule is offset by improvements from applying the appropriate data normalization. There was no complete code release with configurations to train Consolidator on VTAB at the time of writing, hence we report results as-is. Adapter+ shows the best parameter-accuracy trade-off among all methods evaluated. This can also be clearly seen in Fig. 1. Additionally, Adapter+ sets a new state of the art with an average accuracy of up to 77.6% over all VTAB subgroups even without any per-task hyperparameter optimization. If we determine the optimal rankr per task on the validation set, we can further improve the accuracy to 77.9%. Optimizing the rank leads to a better parameter-accuracy trade-off than using a fixed rank across all tasks. In Fig. 2, we compare the average accuracy on the sub- groups of VTAB. Wherever possible, we present the results of re-evaluating methods after the last training epoch and matching the data normalization to the backbone. The aver- age accuracies of Adapter+ with r ∈ [1..32] are consistently higher than those of the competing methods. Note that the accuracies of other methods except SPT differ drastically across the different VTAB subgroups. Adapter+, on the other hand, shows a high degree of robustness to the domain shifts between groups. FGVC. Next, we present our results on the FGVC bench- mark in Tab. 5. From the contenders, only SSF [ 39] has released code and hyperparameter configurations for train- ing on FGVC at the time of writing. As we know from the code releases for VTAB, the reported numbers show the accuracy for early stopping based on the test set. There- fore, we expect a similar evaluation for FGVC. While we do not endorse early stopping based on the test set, we ad- 7Table 5. Detailed results on the FGVC test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. # Param (M) CUB200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] Average Full 86.0 88.0 81.5 99.2 85.6 90.6 89.0 Linear 0.18 88.9 81.8 99.5 92.6 52.8 83.1 VPT-Deep [31] 0.85 88.5 84.2 99.0 90.2 83.6 89.1 VPT-Deep ⟳ 0.85 90.1 83.3 99.6 90.3 85.0 89.7 SSF [39] 0.39 89.5 85.7 99.6 89.6 89.2 90.7 SSF ⟳ 0.39 88.9 85.0 99.6 88.9 88.9 90.3 SPT-Adapter [21] 0.40 89.1 83.3 99.2 91.1 86.2 89.8 SPT-LoRA [21] 0.52 88.6 83.4 99.5 91.4 87.3 90.1 Adapter+, r∈[1..32] 0.34 90.0 83.2 99.6 91.6 89.1 90.7 Adapter+ (best epoch) 0.34 90.4 85.0 99.7 92.6 89.1 91.4 ditionally provide numbers for that setting in Tab. 5 for the sake of comparability. Even when training for a complete schedule, Adapter+ shows the best average accuracy with 90.7% over all five datasets in FGVC,0.4 pp over the second best method under similar evaluation. When early stopping with the test set, Adapter+ reaches 91.4% average accuracy, 0.7 pp over the second best method and 2.4 pp better than full fine-tuning. This demonstrates that Adapter+ also yields state-of-the-art results for task adaptation when training data is abundant while having the best parameter efficiency. 4.5. Ablations Data normalization. We showcase the effect of using an unsuitable data normalization for the chosen ViT in Tab. 6. The gap between ImageNet and Inception normalization (see Sec. 3.5) is largest for VPT [31], with a 3.4 pp difference in average accuracy, which explains around two-thirds of the gain for our re-evaluation as shown in Fig. 1. We suspect that VPT has less of an ability to scale and shift the data because the learnable tokens only act on the attention mechanism. LoRA [29], FacT [32], and adapters all employ linear layers that can directly scale and shift the features of the frozen backbone and thus compensate better for improper data nor- malization. It is worth mentioning that our Adapter+ is the most robust to improper normalization out of the methods evaluated, with a gap of only 2.6 pp average accuracy. Training regularization. We investigate the importance of training regularization methods like stochastic depth [30] and dropout [16] for training adapters on a frozen ViT backbone and evaluate on the VTAB validation sets. We use linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen layers of the ViT model, and a drop rate Table 6. Effects of ImageNet vs. Inception data normalization. All methods are evaluated on the VTABval sets. In column ∆Average we report the increase in accuracy in pp across all VTAB subgroups. ImageNet norm Inception normNatural Specialized Structured Average Natural Specialized Structured Average ∆Average VPT 79.2 83.0 53.8 72.0 82.2 86.2 57.9 75.4 3.4 LoRA 78.4 84.1 53.2 71.9 82.0 85.8 56.4 74.7 2.8 FacT-TK 78.0 83.3 56.1 72.4 81.6 85.6 58.1 75.1 2.7 Adapter+ 80.5 85.0 56.0 73.9 83.0 86.8 59.7 76.5 2.6 Table 7. Influence of training regularization. We evaluate accu- racy in % with Adapterbase with rank r=8 on the VTAB val sets. Adapter Stochastic Depth Dropout None ViT Stochastic Depth 76.0 75.4 75.3 None 74.5 74.3 73.7 of 0.1 when using dropout or stochastic depth for the adapter modules. The results in Tab. 7 show a clear benefit for using stochastic regularization for the frozen layers as well as the adapters during training. Using dropout in the adapters is only slightly better than no regularization for adapters, with a gain of only 0.1 pp. With an increase in accuracy of 0.7 pp, stochastic depth is the preferred regularization method for adapters. However, our results show that the more important part is the stochastic depth regularization for the frozen modules of the ViT backbone. Disabling it in training leads to a loss of 1.5 pp accuracy compared to a training where stochastic depth is used throughout the model. 5. Conclusion Applied at the right position and with an optimal inner struc- ture, the simple concept of adapters produces state-of-the-art results for task adaptation. To understand how adapters can “strike back”, we conducted the first systematic and in-depth study on how to best construct adapters and integrate them with vision transformers. This allowed us to determine the optimal connection point for the adapter in the transformer layer. Further, we proposed to use a learnable, channel-wise scaling and showed its benefit for computer vision tasks. Our insights led us to the creation of Adapter+ that yields the highest accuracy and the best parameter-accuracy trade-off on VTAB (77.6%, 0.2M) without any per-task hyperparame- ter optimization and on FGVC (90.7%, 0.34M), showing its superiority over more complicated methods. Acknowledgements. This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. 8References [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016. [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Si- mon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801 [cs.AI], 2016. [3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for efficient on-device learn- ing. In NeurIPS*2020. [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End- to-end object detection with transformers. In ECCV, pages 213–229, 2020. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. InICCV, pages 9630–9640, 2021. [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapt- ing vision transformers for scalable visual recognition. In NeurIPS*2022. [7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):1865–1883, 2017. [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606–3613, 2014. [9] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. RandAugment: Practical automated data augmentation with a reduced search space. In NeurIPS*2020. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahen- dran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bast- ings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birod- kar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480–7512, 2023. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, pages 4171–4186, 2019. [12] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647–655, 2014. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Emma Dugas, Jorge Jared, and Will Cukierski. Diabetic retinopathy detection. Kaggle, 2015. [15] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learn- ing of object categories. IEEE T. Pattern Anal. Mach. Intell., 28(4):594–611, 2006. [16] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learn- ing. In ICML, pages 1050–1059, 2016. [17] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In AAAI, pages 4502–4508, 2017. [18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):1231–1237, 2013. [19] Yunhui Guo, Yandong Li, Liqiang Wang, and Tajana Rosing. Depthwise convolution is all you need for learning multiple visual domains. In AAAI, pages 8368–8375, 2019. [20] Tianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang Ding. Consolidator: Mergable adapter with group connections for visual adaptation. In ICLR, 2023. [21] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023. [22] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In ICCV, pages 1026–1034, 2015. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InCVPR, pages 770–778, 2016. [25] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217–2226, 2019. [26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:1606.08415 [cs.LG], 2023. [27] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Be- longie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595–604, 2015. [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 2790–2799, 2019. [29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 9LoRA: Low-rank adaptation of large language models. In ICLR, 2022. [30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, pages 646–661, 2016. [31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709–727, 2022. [32] Shibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, pages 1060–1068, 2023. [33] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988–1997, 2017. [34] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image catego- rization. In CVPR Workshop on Fine-grained Visual Classifi- cation, 2011. [35] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian Institute for Ad- vanced Research, 2009. [36] Yann LeCun, Fu Jie Huang, and Léon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, pages 97–104, 2004. [37] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582–4597, 2021. [39] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In NeurIPS*2022. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [41] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy- back: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 72–88, 2018. [42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset, 2017. [43] Pedro Morgado and Nuno Vasconcelos. NetTailor: Tuning the architecture, not just the weights. In CVPR, pages 3044–3054, 2019. [44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, pages 722–729, 2008. [46] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po- Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193 [cs.CV], 2023. [47] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InCVPR, pages 3498–3505, 2012. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS*2019, pages 8024–8035. [49] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In EACL, pages 487–503, 2021. [50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gener- ative pre-training. Technical report, OpenAI, 2018. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, ICML. [52] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. InICCV, pages 12159– 12168, 2021. [53] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS*2017, pages 506–516. [54] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Rev., 65(6):386–408, 1958. [55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vision, 115(13):211–252, 2015. [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION- 5B: An open large-scale dataset for training next generation image-text models. In NeurIPS*2022. [57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your ViT? Data, augmentation, and regularization in vision transformers. Trans. Mach. Learn. Res., 2022. [58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the In- 10ception architecture for computer vision. In CVPR, pages 2818–2826, 2016. [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS*2017, pages 5998–6008. [60] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. In MICCAI, pages 210–218, 2018. [61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical report, California Institute of Technology, 2011. [62] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485–3492, 2010. [63] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, José M. Álvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS*2021, pages 12077–12090. [64] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, 2022. [65] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv:1910.04867 [cs.CV], 2020. [66] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In ICLR, 2018. [67] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv:2206.04673 [cs.CV], 2022. 11Adapters Strike Back Supplementary Material In this appendix, we provide further details and results, which could not be included in the main paper due to space limitations. A. Why did adapters underperform for ViTs? First, we want to shed more light on why adapters do not rank well in the literature for parameter-efficient transfer learning for vision tasks. By comparison of numbers reported for adapters on VTAB in the publications referenced in Tab. 4 of the main paper, we found that they essentially stem from only two sources. The first source is VPT [31], where results for an adapter with a reduction factor of 256, amongst other configurations, are reported. For a ViT-B/16 with a hidden dimension of d=768, this is equal to an adapter with rank r=3. Despite citing Pfeiffer et al. [49], who suggest a Post-Adapter po- sition, the actual implementation in the code base * equals an Intermediate-Adapter that performs worse on VTAB (see Sec. 3.3 of the main paper). The initialization used for the adapter parameters most resembles a LoRA initialization but sets the adapter parameters to zero everywhere. Therefore, there is no randomization in the initialization of the adapter parameters, and different seeds only affect the initialization of the classifier. Additionally, the intermediate features in the adapter bottlenecks then become all zero, leading to iden- tical gradients in the up-projections at the start of training, which hinders optimization. As a result, the adapter baseline used by VPT only reaches 60.0% average accuracy on the VTAB test sets. This is a gap of 17.6 percentage points (pp) compared to our Adapter+ with rank r=8 (77.6% average accuracy). Even when considering the loss of around 2 pp to 3 pp caused by an unsuitable data normalization in the VPT implementation, this is still a very significant gap. The numbers for an adapter with rank r =3 from VPT are also reported in [39] as a baseline. The second source for adapter baseline results is the NOAH pre-print [67]. There, an adapter with rank r = 8 is used. Its implementation† performs the following feature transformation: x 7→ Adapter \u0000 FFN(x) \u0001 + x. (12) This is closest to the Intermediate-Adapter (cf . Eq. (10) of the main paper) but misses the skip connection bypassing the adapter and containing FFN(x). Thus, the adapter does not learn a residual function to an identity mapping but instead must learn a more complex mapping to transform its input. *https://github.com/KMnP/vpt †https://github.com/ZhangYuanhan-AI/NOAH Therefore, the adapter becomes harder to train [24], leading to an average accuracy of 73.9% on the VTAB test sets or 3.7 pp behind our Adapter+. For the NOAH adapter results, we see a proliferation to the publications of FacT [32] and SPT [21]. The adapter implementation from NOAH is also used in the code released for Consolidator ‡ [20] but their results are produced with rank r=16, giving a slightly better average accuracy of 74.3%, or 3.3 pp less than Adapter+. In summary, the examined baseline implementations dif- fer from the configurations proposed by Houlsby et al. [28] and Pfeiffer et al. [49] and introduce issues that lead to their underperformance. In our paper, we show that adapters are capable of reaching 77.6% average accuracy for rank r=8 and 77.9% for our optimized version of Adapter+, uplifting adapters from an easy-to-beat baseline to a state-of-the-art transfer method. B. Dataset properties In Tabs. 8 and 9, we show the statistics of each task in VTAB [65] and FGVC [31] with regard to the number of classes and the number of images in the train, validation, and test splits. The tables are largely “borrowed” from [31]. Table 8. Dataset details for VTAB. Group Task # Classes Splits Train Val Test Natural CIFAR-100 [35] 100 800 200 10 000 Caltech-101 [15] 102 6 084 DTD [8] 47 1 880 Oxford Flowers [45] 102 6 149 Pets [47] 37 3 669 SVHN [44] 10 26 032 Sun397 [62] 397 21 750 Specialized Patch Camelyon [60] 2 800 200 32 768 EuroSAT [25] 10 5 400 RESISC45 [7] 45 6 300 Diabetic Retinopathy [14] 5 42 670 Structured CLEVR-Count [33] 8 800 200 15 000 CLEVR-Distance [33] 6 15 000 DMLab [2] 6 22 735 KITTI-Distance [18] 4 711 dSprites-Location [42] 16 73 728 dSprites-Orientation [42] 16 73 728 smallNORB-Azimuth [36] 18 12 150 smallNORB-Elevation [36] 9 12 150 ‡https://github.com/THU-MIG/Consolidator iTable 9. Dataset details for FGVC.For datasets markedwith *, we follow [31] to randomly sample train and validation splits because validation sets are not available from the original datasets. Dataset # Classes Splits Train Val Test CUB-200-2011* [61] 200 5 394 600 5 794 NABirds* [27] 555 21 536 2 393 6 084 Oxford Flowers [45] 102 1 020 1 020 6 149 Stanford Dogs* [34] 120 10 800 1 200 8 580 Stanford Cars* [17] 196 7 329 815 8 041 C. More experimental settings For all experiments conducted with our implementation, we average the results over three seeds. This includes the (re-)evaluations of LoRA and VPT. We built our implemen- tation on PyTorch [48], PyTorch Lightning,§ and timm.¶ We run experiments with bfloat16 mixed precision on a NVIDIA RTX A6000 GPU. For our experiments in the main paper, we report results for a fixed adapter rank r as well as ranks optimized per task. For the per-task optimization of Adapter+, we use a hyper- parameter sweep over the set of ranks r∈{1, 2, 4, 8, 16, 32}. We evaluate on the validation sets of VTAB and FGVC and choose the per-task ranks from the specified range(s) to steer the number of average parameters. The ranks we used to produce the results on the VTAB and FGVC test sets (see Tabs. 4 and 5 in the main paper) are shown in detail in Tab. 10 and Tab. 11, respectively. D. Calculation of no. of trainable parameters Suppose we have a ViT with a hidden dimension d, N trans- former layers, and adapters with rank r. The total num- ber of learnable parameters for Adapter base modules ( cf . Eq. (4) of the main paper) attached to the FFN of every transformer layer then amounts to N(2dr + r + d). Includ- ing layer normalization in the adapter modules amounts to N2d additional parameters. The addition of learned, layer- wise scaling amounts to N extra parameters and choosing learned, channel-wise scaling instead adds Nd extra parame- ters. Adapter+ (see Sec. 4.3 of the main paper) thus amounts to N(2dr + 2d + r) total parameters. Additionally, for a task with c classes, we add a classifier with dc + c learnable parameters. E. Vision transformer pre-training As we add only very few parameters to an otherwise frozen backbone, the generalization capability of the feature repre- sentations produced by the backbone is important. For ViTs, there are a number of off-the-shelf models available with §https://lightning.ai/pytorch-lightning ¶https://github.com/huggingface/pytorch-image-models Table 10. Adapter rank r for each VTAB task for optimized versions of Adapter+ with different ranges of permitted ranks. Natural Specialized Structured # Param (M) CIFAR-100 [35] Caltech-101 [15] DTD [8] Flowers [45] Pets [47] SVHN [44] Sun397 [62] Camelyon [60] EuroSAT [25] RESISC45 [7] Retinopathy [14] CLEVR-Count [33] CLEVR-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori. [42] sNORB-Azi. [36] sNORB-Ele. [36] r∈[1..4] 0.11 1 4 2 1 4 4 1 4 2 4 2 4 2 4 4 4 4 4 4 r∈[1..8] 0.16 1 4 2 1 8 8 1 8 2 8 8 4 8 8 8 8 4 8 8 r∈[1..32] 0.27 1 4 2 1 8 16 1 16 2 32 32 4 8 8 8 32 4 32 8 Table 11. Adapter rank r for each FGVC dataset for optimized versions of Adapter+ with different ranges of permitted ranks. # Param (M) CUB-200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] r∈[1..32] 0.34 2 2 1 1 32 differences in their training procedures. Here, we examine three different pre-trainings as examples: (1) Original: The ViT-B/16 weights used in the main paper, pre-trained with su- pervision on ImageNet-21k [55] following the training proce- dure of the original ViT publication [13],|| (2) ImageNet-1k: the same ViT weights further fine-tuned on ImageNet-1k [55],** and (3) AugReg: weights from a pre-training with stronger data augmentation in the form of Mixup [66] and RandAugment [9] following [57].†† In Tab. 12, we summarize our results for Adapter+ with rank r=8 evaluated on the VTAB validation sets. We notice that additional fine-tuning on ImageNet-1k gives a slight edge (83.4% average accuracy over 83.0% for second best) in adaption for tasks that contain natural images. However, the fine-tuning is detrimental for the Specialized and Structured group. Not fine-tuning on ImageNet-1k is beneficial for the Structured group with a large increase of 3.7 pp. The Aug- Reg training setting improves the transfer to the Specialized group but is worse than the other settings for natural images. Overall, the original supervised training on ImageNet-21k generalizes best across all tasks in VTAB with an average accuracy of 76.5%, 0.3 pp better than AugReg training and 1.2 pp better than ImageNet-1k fine-tuning. ||https://storage.googleapis.com/vit_models/imagenet21k/ ViT-B_16.npz **https://storage.googleapis.com/vit_models/imagenet21k+ imagenet2012/ViT-B_16-224.npz ††https://storage.googleapis.com/vit_models/augreg/B_16- i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz iiTable 12. Influence of ViT pre-training. We use Adapter+ with rank r=8 for the evaluation and report the average accuracy in % for each subgroup and across all groups on the VTAB val sets. Pre-training Natural Specialized Structured Average ImageNet-1k 83.4 86.5 56.0 75.3 AugReg 81.6 87.2 59.7 76.2 Original 83.0 86.8 59.7 76.5 Table 13. Adapter position with DINO backbone. We report average accuracy in % ( ± std. dev.) on the VTAB val sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 76.8 ± 0.4 86.2 ± 0.6 53.6 ± 0.7 72.2 ± 0.3 Intermediate 76.8 ± 0.4 85.8 ± 0.8 52.6 ± 0.9 71.8 ± 0.4 Parallel 76.7 ± 0.3 86.8 ± 0.4 54.1 ± 0.7 72.5 ± 0.3 Post 76.9 ± 0.2 86.3 ± 0.5 55.3 ± 0.7 72.8 ± 0.3 Table 14. Comparison of Adapter+ with adapter configurations from previous work with DINO backbone.We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param Natural Specialized Structured Average Houlsby [28], r=8 0.39 77.4 ± 0.4 86.5 ± 0.7 52.9 ± 0.8 72.3 ± 0.4 Houlsby [28], r=4 0.24 77.2 ± 0.5 86.2 ± 0.5 53.2 ± 0.8 72.2 ± 0.3 Pfeiffer [49] 0.21 76.8 ± 0.4 86.2 ± 0.3 54.4 ± 1.0 72.5 ± 0.4 AdaptFormer [6] 0.19 76.5 ± 0.4 85.8 ± 0.4 53.0 ± 0.5 71.8 ± 0.3 Adapter+ 0.20 76.7 ± 0.3 86.4 ± 0.5 55.4 ± 0.8 72.8 ± 0.3 F. Generality of the conclusions Using DINO [5] as an example of a ViT trained with self- supervision, we show in Tab. 13 that the orders of best-to- worst adapter position is consistent with that of a supervised backbone in terms of average accuracy, albeit with a higher standard deviation. The ranking also stays the same for the comparison of Adapter+ with adapter configurations from previous work as presented in Tab. 14. This shows that our conclusions generalize beyond backbones with super- vised pre-training to backbones based on self-supervised pre-training. iii",
      "meta_data": {
        "arxiv_id": "2406.06820v1",
        "authors": [
          "Jan-Martin O. Steitz",
          "Stefan Roth"
        ],
        "published_date": "2024-06-10T22:07:57Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06820v1.pdf",
        "github_url": "https://github.com/visinf/adapter_plus"
      },
      "llm_extracted_info": {
        "main_contributions": "Addressed the underperformance of adapters in Vision Transformers (ViTs) by conducting the first systematic, in-depth study of adapter position, inner structure, and parameter initialization. Proposed a novel learnable, channel-wise scaling mechanism. Introduced Adapter+, an improved adapter architecture (Post-Adapter position, Houlsby initialization, channel-wise scaling) that achieves state-of-the-art average accuracy on VTAB (77.6% without per-task hyperparameter optimization, 77.9% with optimization) and FGVC (90.7% with the lowest number of parameters) benchmarks. Demonstrated Adapter+'s superior parameter-accuracy trade-off and high robustness, requiring minimal manual intervention.",
        "methodology": "The methodology focuses on parameter-efficient adaptation of Vision Transformers (ViT-B/16 with frozen parameters). Adapters are small bottleneck modules added to transformer layers, comprising a down-projection to rank 'r', GELU activation, and an up-projection. Key enhancements in Adapter+ include: selecting the Post-Adapter position (at the end of the transformer layer, after the FFN and its skip connection) after systematic evaluation of four positions; employing Houlsby initialization (zero-centered Gaussian, σ=0.01, truncated at 2σ, zero biases); and integrating a learnable, channel-wise scaling (s ∈ Rd) mechanism. Only the adapter parameters and the linear classifier are trained.",
        "experimental_setup": "Experiments utilized a ViT-B/16 network pre-trained on ImageNet-21k (with ablations on ImageNet-1k fine-tuned and AugReg pre-trained versions, and a DINO backbone for generality). Datasets include the Visual Task Adaptation Benchmark (VTAB), consisting of 19 tasks grouped into Natural, Specialized, and Structured categories (800 training, 200 validation images per task), and five Fine-Grained Visual Classification (FGVC) datasets (CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, Stanford Cars) for data-abundant settings. Models were trained with an AdamW optimizer, learning rates of 10^-3 (adapters) or 10^-4 (full fine-tuning), weight decay of 10^-4, and a batch size of 64. A cosine learning rate schedule with a linear warm-up (first 10 epochs) was used for a total of 100 epochs. Stochastic depth regularization (0-0.1 for frozen layers, 0.1 for adapters) was applied. Input images were resized to 224x224px (VTAB) or randomly cropped/flipped to 224x224px (FGVC), emphasizing data normalization consistent with pre-training. Validation sets were used for ablations and hyperparameter tuning, with validation data included in training for final results. Competing methods were re-evaluated under consistent conditions for fair comparison.",
        "limitations": "The paper highlights that while Adapter+ overcomes previous limitations of adapter implementations, its performance is still impacted by unsuitable data normalization, although it demonstrates higher robustness than other methods. Achieving optimal performance often requires determining the adapter rank 'r' on a per-task basis, suggesting that a single fixed rank may not be universally optimal. The overall performance is also dependent on the generalization capabilities and specific pre-training strategy of the underlying frozen backbone model.",
        "future_research_directions": "Future research could focus on developing adaptive mechanisms for automatically determining the optimal adapter rank 'r' per task, reducing the need for manual hyperparameter tuning. Further exploration of Adapter+'s applicability across a wider array of pre-trained vision transformer backbones and diverse pre-training strategies, beyond supervised and self-supervised models, is also a promising direction. Additionally, investigating the integration of Adapter+ into various computer vision tasks beyond classification, such as object detection or semantic segmentation, could be explored.",
        "experimental_code": "import timm\nfrom .vit_adapter import *\nfrom .vit_adapter import _create_vision_transformer_adapter\n\n__all__ = [\n    \"Adapter\",\n    \"LoRAAttention\",\n    \"AdapterBlock\",\n    \"AdapterResPostBlock\",\n    \"VisionTransformerAdapter\",\n]\n\ndef patch_timm():\n    timm.models.vision_transformer._create_vision_transformer = (\n        _create_vision_transformer_adapter\n    )\n\npatch_timm()\n\n# for Adapter+ set norm_layer to None and scaling to \"channel\"\nclass Adapter(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        bottleneck_dim=8,\n        drop_path=0.0,\n        dropout=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        scaling=1.0,\n        init=\"houlsby\",\n        bias=True,\n        pre_dropout=False,\n    ):\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Dropout(dropout) if dropout > 0 and pre_dropout else nn.Identity(),\n            nn.Linear(embed_dim, bottleneck_dim, bias=bias),\n            act_layer() if act_layer else nn.Identity(),\n            nn.Dropout(dropout) if dropout > 0 and not pre_dropout else nn.Identity(),\n            nn.Linear(bottleneck_dim, embed_dim, bias=bias),\n        )\n        self.norm_a = norm_layer(embed_dim) if norm_layer else nn.Identity()\n        self.drop_path_a = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.bottleneck_dim = bottleneck_dim\n        if scaling == \"learned\":\n            self.scaling = nn.Parameter(torch.ones(1))\n        elif scaling == \"channel\":\n            self.scaling = nn.Parameter(torch.ones(embed_dim))\n        else:\n            self.scaling = scaling\n\n        # init following (Houslby 2019)\n        if init == \"houlsby\":\n            std = 0.01\n            nn.init.trunc_normal_(\n                self.bottleneck[1].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[1].bias is not None:\n                nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.trunc_normal_(\n                self.bottleneck[4].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n        else:\n            raise ValueError(f\"Initialization {init} not implemented!\")\n\n    def forward(\n        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        x = self.norm_a(x)\n        x = self.drop_path_a(self.bottleneck(x))\n        x = x * self.scaling\n\n        y = x\n        if skip is not None:\n            y = y + skip\n\n        return y\n\n\nclass AdapterBlock(Block):\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\n\nclass AdapterResPostBlock(ResPostBlock):\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\ndef _create_vision_transformer_adapter(\n    variant: str, pretrained: bool = False, adapter=False, **kwargs\n) -> VisionTransformer:\n    out_indices = kwargs.pop(\"out_indices\", 3)\n    if \"flexi\" in variant:\n        _filter_fn = partial(\n            checkpoint_filter_fn, interpolation=\"bilinear\", antialias=False\n        )\n    else:\n        _filter_fn = checkpoint_filter_fn\n\n    strict = True\n    if \"siglip\" in variant and kwargs.get(\"global_pool\", None) != \"map\":\n        strict = False\n\n    if adapter:\n        block_fn = kwargs.pop(\"block_fn\", Block)\n        if block_fn == Block:\n            block_fn = AdapterBlock\n        elif block_fn == ResPostBlock:\n            block_fn = AdapterResPostBlock\n        else:\n            raise ValueError(f\"Adapters not implemented for {block_fn}!\")\n        return build_model_with_cfg(\n            VisionTransformerAdapter,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            block_fn=block_fn,\n            **kwargs,\n        )\n    else:\n        return build_model_with_cfg(\n            VisionTransformer,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            **kwargs,\n        )\n\n\nclass AdapterModel(pl.LightningModule):\n    def __init__(\n        self,\n        cfg,\n        img_size=224,\n        num_classes=1000,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.img_size = img_size\n        self.num_classes = num_classes\n        self.train_acc = torchmetrics.Accuracy(\n            task=\"multiclass\", num_classes=num_classes\n        )\n        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.label_smoothing = cfg.train.label_smoothing\n\n        self.vit = timm.create_model(\n            cfg.vit.model,\n            adapter=True,\n            pretrained=True,\n            num_classes=num_classes,\n            img_size=img_size,\n            drop_path_rate=cfg.vit.drop_path,\n            adapter_config=cfg.get(\"adapter\", None),\n            lora_config=cfg.get(\"lora\", None),\n            prompt_config=cfg.get(\"prompt\", None),\n        )\n\n        if cfg.get(\"adapter\", None) or cfg.get(\"lora\", None) or cfg.get(\"prompt\", None):\n            if not cfg.vit.finetune:\n                self.vit.requires_grad_(False)\n            self.vit.head.requires_grad_(True)\n            for m in self.vit.modules():\n                if isinstance(m, Adapter):\n                    m.requires_grad_(True)\n\n                if cfg.train.train_ln:\n                    if isinstance(m, nn.LayerNorm):\n                        m.requires_grad_(True)\n\n                if cfg.get(\"prompt\", None):\n                    if isinstance(m, AdapterBlock):\n                        m.prompt.requires_grad_(True)\n\n        if cfg.train.classifier_only:\n            self.vit.requires_grad_(False)\n            self.vit.head.requires_grad_(True)\n\n        self.save_hyperparameters()\n        self.trainable_params = {\n            \"trainable_params\": sum(\n                p.numel() for p in self.parameters() if p.requires_grad\n            )\n        }\n        self.save_hyperparameters(self.trainable_params)",
        "experimental_info": "The method uses a Vision Transformer (ViT-B/16) with frozen parameters for parameter-efficient adaptation. Adapters are integrated as small bottleneck modules, comprising a down-projection to rank 'r' (default `bottleneck_dim=8`), GELU activation, and an up-projection. The adapter is positioned at the 'Post-Adapter' location, meaning it is added at the end of the transformer layer, after the Feed-Forward Network (FFN) and its skip connection. Adapter parameters are initialized using Houlsby initialization: weights are drawn from a zero-centered Gaussian distribution with a standard deviation (σ) of 0.01, truncated at 2σ, and all biases are initialized to zero. A learnable, channel-wise scaling mechanism is applied to the adapter output. During training, only the adapter parameters and the linear classifier head are updated, while the base ViT model parameters (excluding LayerNorms if `cfg.train.train_ln` is false) remain frozen."
      }
    },
    {
      "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
      "abstract": "Multi-Task Learning (MTL) networks have emerged as a promising method for\ntransferring learned knowledge across different tasks. However, MTL must deal\nwith challenges such as: overfitting to low resource tasks, catastrophic\nforgetting, and negative task transfer, or learning interference. Often, in\nNatural Language Processing (NLP), a separate model per task is needed to\nobtain the best performance. However, many fine-tuning approaches are both\nparameter inefficient, i.e., potentially involving one new model per task, and\nhighly susceptible to losing knowledge acquired during pretraining. We propose\na novel Transformer architecture consisting of a new conditional attention\nmechanism as well as a set of task-conditioned modules that facilitate weight\nsharing. Through this construction (a hypernetwork adapter), we achieve more\nefficient parameter sharing and mitigate forgetting by keeping half of the\nweights of a pretrained model fixed. We also use a new multi-task data sampling\nstrategy to mitigate the negative effects of data imbalance across tasks. Using\nthis approach, we are able to surpass single task fine-tuning methods while\nbeing parameter and data efficient (using around 66% of the data for weight\nupdates). Compared to other BERT Large methods on GLUE, our 8-task model\nsurpasses other Adapter methods by 2.8% and our 24-task model outperforms by\n0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger\nvariant of our single multi-task model approach performs competitively across\n26 NLP tasks and yields state-of-the-art results on a number of test and\ndevelopment sets. Our code is publicly available at\nhttps://github.com/CAMTL/CA-MTL.",
      "full_text": "Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI -TASK LEARNING : IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA Jonathan Pilault1∗, Amine El hattami1∗, Christopher Pal1,2,3 1Polytechnique Montreal & Mila, 2Element AI, 3Canada CIFAR AI Chair {jonathan.pilault,amine.elhattami,christopher.pal}@polymtl.ca ABSTRACT Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overﬁtting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best perfor- mance. However, many ﬁne-tuning approaches are both parameter inefﬁcient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efﬁcient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model ﬁxed. We also use a new multi-task data sam- pling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task ﬁne-tuning methods while being parameter and data efﬁcient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task ﬁne-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL. 1 I NTRODUCTION The introduction of deep, contextualized Masked Language Models (MLM) 1 trained on massive amounts of unlabeled data has led to signiﬁcant advances across many different Natural Language Processing (NLP) tasks (Peters et al., 2018; Liu et al., 2019a). Much of these recent advances can be attributed to the now well-known BERT approach (Devlin et al., 2018). Substantial improvements over previous state-of-the-art results on the GLUE benchmark (Wang et al., 2018) have been obtained by multiple groups using BERT models with task speciﬁc ﬁne-tuning. The “BERT-variant + ﬁne-tuning” formula has continued to improve over time with newer work constantly pushing the state-of-the-art forward on the GLUE benchmark. The use of a single neural architecture for multiple NLP tasks has shown promise long before the current wave of BERT inspired methods (Collobert & Weston, 2008) and recent work has argued that autoregressive language models (ARLMs) trained on large-scale datasets – such as the GPT family of models (Radford et al., 2018), are in practice multi-task learners (Brown et al., 2020). However, even with MLMs and ARLMs trained for multi-tasking, single task ﬁne-tuning is usually also employed to achieve state-of-the-art performance on speciﬁc tasks of interest. Typically this ﬁne-tuning process may entail: creating a task-speciﬁc ﬁne-tuned model (Devlin et al., 2018), training specialized model components for task-speciﬁc predictions (Houlsby et al., 2019) or ﬁne-tuning a single multi-task architecture (Liu et al., 2019b). ∗Joint ﬁrst-authors 1For reader convenience, all acronyms in this paper are summarized in section A.1 of the Appendix. 1 arXiv:2009.09139v3  [cs.LG]  21 Apr 2022Published as a conference paper at ICLR 2021 Figure 1: CA-MTL base architecture with our uncertainty-based sampling algorithm. Each task has its own decoder. The input embedding layer and the lower Transformer layers are frozen. The up- per Transformer layer and Conditional Alignment module are modulated with the task embedding. Single-task ﬁne-tuning overall pretrained model parameters may have other issues. Recent analy- ses of such MLM have shed light on the linguistic knowledge that is captured in the hidden states and attention maps (Clark et al., 2019b; Tenney et al., 2019a; Merchant et al., 2020). Particularly, BERT has middle Transformer (Vaswani et al., 2017) lay- ers that are typically the most transferable to a downstream task (Liu et al., 2019a). The model proxies the steps of the traditional NLP pipeline in a localizable way (Tenney et al., 2019a) — with basic syntactic information appearing earlier in the network, while high-level semantic information ap- pearing in higher-level layers. Since pretraining is usually done on large-scale datasets, it may be use- ful, for a variety of downstream tasks, to conserve that knowledge. However, single task ﬁne-tuning causes catastrophic forgetting of the knowledge learned during MLM (Howard & Ruder, 2018). To preserve knowledge, freezing part of a pretrained network and using Adapters for new tasks have shown promising results (Houlsby et al., 2019). Inspired by the human ability to transfer learned knowledge from one task to another new task, Multi-Task Learning (MTL) in a general sense (Caruana, 1997; Rajpurkar et al., 2016b; Ruder, 2017) has been applied in many ﬁelds outside of NLP. Caruana (1993) showed that a model trained in a multi-task manner can take advantage of the inductive transfer between tasks, achieving a better generalization performance. MTL has the advantage of computational/storage efﬁciency (Zhang & Yang, 2017), but training models in a multi-task setting is a balancing act; particularly with datasets that have different: (a) dataset sizes, (b) task difﬁculty levels, and (c) different types of loss functions. In practice, learning multiple tasks at once is challenging since negative transfer (Wang et al., 2019a), task interference (Wu et al., 2020; Yu et al., 2020) and catastrophic forgetting (Serrà et al., 2018) can lead to worse data efﬁciency, training stability and generalization compared to single task ﬁne-tuning. Using Conditionally Adaptive Learning, we seek to improve pretraining knowledge retention and multi-task inductive knowledge transfer. Our contributions are the following: • A new task conditioned Transformer that adapts and modulates pretrained weights (Section 2.1). • A novel way to prioritize tasks with an uncertainty based multi-task data sampling method that helps balance the sampling of tasks to avoid catastrophic forgetting (Section 2.2). Our Conditionally Adaptive Multi-Task Learning (CA-MTL) approach is illustrated in Figure 1. To the best of our knowledge, our work is the ﬁrst to explore the use of a latent representation of tasks to modularize and adapt pretrained architectures. Further, we believe our work is also the ﬁrst to examine uncertainty sampling for large-scale multi-task learning in NLP. We show the efﬁcacy of CA-MTL by: (a) testing on 26 different tasks and (b) presenting state-of-the-art results on a number of test sets as well as superior performance against both single-task and MTL baselines. Moreover, we further demonstrate that our method has advantages over (c) other adapter networks, and (d) other MTL sampling methods. Finally, we provide ablations and separate analysis of the MT-Uncertainty Sampling technique in section 4.1 and of each component of the adapter in 4.2. 2 M ETHODOLOGY This section is organized according to the two main MTL problems that we will tackle: (1) How to modularize a pretrained network with latent task representations? (2) How to balance different tasks in MTL? We deﬁne each task as: Ti ≜ {pi(yi|xi,zi),Li,˜pi(xi)}, where zi is task i’s learnable shallow embedding, Li is the task loss, and ˜pi(xi) is the empirical distribution of the training data pair {xi,yi}, for i∈{1,...,T }and T the number of supervised tasks. The MTL objective is: min φ(z),θ1,...,θT T∑ i=1 Li(fφ(zi),θi(xi),yi) (1) 2Published as a conference paper at ICLR 2021 where f is the predictor function (includes encoder model and decoder heads), φ(z) are learnable generated weights conditioned on z, and θi are task-speciﬁc parameters for the output decoder heads. z is constructed using an embedding lookup table. 2.1 T ASK CONDITIONED TRANSFORMER Our task conditioned Transformer architecture is based on one simple concept. We either add conditional layers or modulate existing pretrained weights using a task representation by extending Feature Wise Linear Modulation (Perez et al., 2018) functions in several ways depending on the Transformer layer. We deﬁne our framework below. Deﬁnition 1 (Conditional Weight Transformations). Given a neural network weight matrixW, we compute transformations of the formφ(W|zi) = γi(zi)W + βi(zi), whereγi and βi are learned functions that transform the weights based on a learned vector embeddingzi, for taski. Deﬁnition 2 (Conditionally Adaptive Learning). In our setting, Conditionally Adaptive Learning is the process of learning a set ofφs for the conditionally adaptive modules presented below along with a set of task embedding vectorszi for T tasks, using a multi-task loss (see equation 1). In the subsections that follow: We introduce a new Transformer Attention Module using block- diagonal Conditional Attention that allows the original query-key based attention to account for task-speciﬁc biases (section 2.1.1). We propose a new Conditional Alignment method that aligns the data of diverse tasks and that performs better than its unconditioned and higher capacity predecessor (section 2.1.2). We adapt layer normalization statistics to speciﬁc tasks using a new Conditional Layer Normalization module (section 2.1.3). We add a Conditional Bottleneck that facilitates weight sharing and task-speciﬁc information ﬂow from lower layers (section 2.1.4). In our experiments we provide an ablation study of these components (Table 1) examining performance in terms of GLUE scores. 2.1.1 C ONDITIONAL ATTENTION Figure 2: Conditional Attention Module Given d, the input dimensions, the query Q, the key K, and the value V as deﬁned in Vaswani et al. (2017), we redeﬁne the attention operation: Attention(Q,K,V,zi)) = softmax [ M(zi) + QKT √ d ] V M(zi) = N⨁ n=1 A′ n(zi), A ′ n(zi) = Anγi(zi) + βi(zi) where ⨁is the direct sum operator (see section A.6), N is the number of block matrices An ∈R(L/N)×(L/N) along the diagonal of the attention matrix, Lis the input sequence, M(zi) = diag(A′ 1,...,A ′ N) is a block diagonal conditional matrix. Note that An is constructed using L/N trainable and randomly initialized L/N dimensional vectors. While the original attention matrix depends on the hidden states h, M(zi) is a learnable weight matrix that only depends on the task embedding zi ∈Rd. γi,βi : Rd ↦→RL2/N2 are Feature Wise Linear Modulation (Perez et al., 2018) functions. We also experimented with full-block Conditional Attention ∈RL×L. Not only did it have N2 more parameters compared to the block-diagonal variant, but it also performed signiﬁcantly worse on the GLUE development set (see FBA variant in Table 10). It is possible that GLUE tasks derive a certain beneﬁt from localized attention that is a consequence of M(zi). With M(zi), each element in a sequence can only attend to other elements in its subsequence of length L/N. In our experiments we used N = d/L. The full Conditional Attention mechanism used in our experiments is illustrated in Figure 2. 2.1.2 C ONDITIONAL ALIGNMENT Wu et al. (2020) showed that in MTL havingT separate alignment modules R1,...,R T increases BERTLARGE avg. scores on ﬁve GLUE tasks (CoLA, MRPC, QNLI, RTE, SST-2) by 2.35%. Inspired by this work, we found that adding a task conditioned alignment layer between the input embedding 3Published as a conference paper at ICLR 2021 layer and the ﬁrst BERT Transformer layer improved multi-task model performance. However, instead of having T separate alignment matrices Ri for each T task, one alignment matrix ˆR is generated as a function of the task embedding zi. As in Wu et al. (2020), we tested this module on the same ﬁve GLUE tasks and with BERTLARGE. Enabling task conditioned weight sharing across covariance alignment modules allows us to outperformsBERTLARGE by 3.61%. This is 1.26 % higher than having T separate alignment matrices. Inserting ˆRinto BERT, yields the following encoder function ˆf: ˆf = T∑ t=1 gθi(E(xi) ˆR(zi)B), ˆR(zi) = Rγi(zi) + βi(zi) (2) where xi ∈Rd is the layer input, gθi is the decoder head function for task iwith weights θi, Ethe frozen BERT embedding layer, Bthe BERT Transformer layers and Rthe linear weight matrix of a single task conditioned alignment matrix. γi,βi : Rd ↦→Rd are Feature Wise Linear Modulation functions. 2.1.3 C ONDITIONAL LAYER NORMALIZATION (CLN) We extend the Conditional Batch Normalization idea from de Vries et al. (2017) to Layer Normaliza- tion (Ba et al., 2016). For task Ti, i∈{1,...,T }: hi = 1 σ ⊙(ai −µ) ∗ˆγi(zi) + βi(zi), ˆγi(zi) = γ′γi(zi) + β′ (3) where hi is the CLN output vector, ai are the preceding layer activations associated with task i, µ and σare the mean and the variance of the summed inputs within each layer as deﬁned in Ba et al. (2016). Conditional Layer Normalization is initialized with BERT’s Layer Normalization afﬁne transformation weights and bias γ′and β′from the original formulation: h = 1 σ ⊙(a −µ) ∗γ′+ β′. During training, the weight and bias functions of γi(∗) and βi(∗) are always trained, while the original Layer Normalization weight may be kept ﬁxed. This module was added to account for task speciﬁc rescaling of individual training cases. Layer Normalization normalizes the inputs across features. The conditioning introduced in equation 2.1.3 allows us to modulate the normalization’s output based on a task’s latent representation. 2.1.4 C ONDITIONAL BOTTLENECK Figure 3: a) Conditional Bottleneck for CA-MTLBASE. b) Conditional Bottleneck for CA-MTLLARGE. We created a task conditioned two layer feed-forward bot- tleneck layer (CFF up/down in Figure 3). The conditional bottleneck layer follows the same transformation as in equation 2. The module in Figure 3a is added to the top most Transformer layers ofCA-MTLBASE and uses a CLN. For CA-MTLLARGE this module is the main building block of the skip connection added alongside all Transformer layers seen in Figure 3b. The connection at layer jtakes in the matrix sum of the Transformer layer output at j and the previous connection’s output atj−1. The Con- ditional bottleneck allows lower layer information to ﬂow upwards depending on the task. Our intuition for intro- ducing this component is related to recent studies (Tenney et al., 2019a) that showed that the “most important layers for a given task appear at speciﬁc positions”. As with the other modules described so far, each task adaptation is created from the weights of a single shared adapter that is modulated by the task embedding. 2.2 M ULTI -TASK UNCERTAINTY SAMPLING MT-Uncertainty Sampling is a task selection strategy that is inspired by Active Learning techniques. Our algorithm 1 is outlined in the Appendix, Section A.2. Similar to Active Learning, our algorithm ﬁrst evaluates the model uncertainty. MT-Uncertainty Sampling uses Shannon Entropy, an uncertainty measure, to choose training examples by ﬁrst doing forward pass through the model with b×T input samples. For an output classiﬁcation prediction with Ci possible classes and probabilities 4Published as a conference paper at ICLR 2021 (pi,1,...,p i,Ci), the Shannon Entropy Hi, for task Ti and i∈{1,...,T }, our uncertainty measure U(x) are given by: Hi = Hi(fφ(zi),θi(x)) = − Ci∑ c=1 pc log pc, U(xi) = Hi(fφ(zi),θi(x)) ˆH×H′ i (4) ˆH = max i∈{1,...,T} ¯Hi = max [ 1 b ∑ x∈xi Hi ] , H ′ i = − Ci∑ c=1 1 Ci log [ 1 Ci ] (5) where ¯Hi is the average Shannon Entropy across bsamples of task t, H′ i, the Shannon entropy of choosing classes with uniform distribution and ˆH, the maximum of each task’s average entropy over bsamples. H′ i is normalizing factor that accounts for differing number of prediction classes (without the normalizing factor H′ i, tasks with a binary classiﬁcation Ci = 1 were rarely chosen). Further, to limit high entropy outliers and to favor tasks with highest uncertainty, we normalize with ˆH. The measure in eq. 4 allows Algorithm 1 to choose bsamples from b×T candidates to train the model. 3 R ELATED WORK Multi-Tasking in NLP. To take advantage of the potential positive transfer of knowledge from one task to another, several works have proposed carefully choosing which tasks to train as an intermediate step in NLP before single task ﬁne-tuning (Bingel & Søgaard, 2017; Kerinec et al., 2018; Wang et al., 2019a; Standley et al., 2019; Pruksachatkun et al., 2020; Phang et al., 2018). The intermediate tasks are not required to perform well and are not typically evaluated jointly. In this work, all tasks are trained jointly and all tasks usedare evaluated from a single model. In Natural Language Understanding (NLU), it is still the case that to get the best task performance one often needs a separate model per task (Clark et al., 2019c; McCann et al., 2018). At scale, Multilingual NMT systems (Aharoni et al., 2019) have also found that MTL model performance degrades as the number of tasks increases. We notice a similar trend in NLU with our baseline MTL model. Recently, approaches in MTL have tackled the problem by designing task speciﬁc decoders on top of a shared model (Liu et al., 2019b) or distilling multiple single-task models into one (Clark et al., 2019c). Nonetheless, such MTL approaches still involves single task ﬁne-tuning. In this paper, we show that it is possible to achieve high performance in NLU without single task ﬁne-tuning. Adapters. Adapters are trainable modules that are attached in speciﬁc locations of a pretrained network. They provide another promising avenue to limit the number of parameters needed when confronted with a large number of tasks. This approach is useful with pretrained MLM models that have rich linguistic information (Tenney et al., 2019b; Clark et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a). Recently, Houlsby et al. (2019) added an adapter to a pretrained BERT model by ﬁne-tuning the layer norms and adding feed forward bottlenecks in every Transformer layer. However, such methods adapt each task individually during the ﬁne-tuning process. Unlike prior work, our method harnesses the vectorized representations of tasks to modularize a single pretrained model across all tasks. Stickland et al. (2019) and Tay et al. (2020) also mix both MTL and adapters with BERT and T5 encoder-decoder (Raffel et al., 2019) respectively by creating local task modules that are controlled by a global task agnostic module. The main drawback is that a new set of non-shared parameters must be added when a new task is introduced. CA-MTL shares all parameters and is able to re-modulate existing weights with a new task embedding vector. Active Learning, Task Selection and Sampling. Our sampling technique is similar to the ones found in several active learning algorithms (Chen et al., 2006) that are based on Shannon entropy estimations. Reichart et al. (2008) and Ikhwantri et al. (2018) examined Multi-Task Active Learning (MTAL), a technique that chooses one informative sample forT different learners (or models) for each T tasks. Instead we choose T tasks samples for one model. Moreover, the algorithm weights each sample by the corresponding task score, and the Shannon entropy is normalized to account for various losses (see equation 5). Also, our algorithm is used in a large scale MTL setup (≫2 tasks). Recently, Glover & Hokamp (2019) explored task selection in MTL using learning policies based on counterfactual estimations (Charles et al., 2013). However, such method considers only ﬁxed stochastic parameterized policies while our method adapts its selection criterion based on model uncertainty throughout the training process. 5Published as a conference paper at ICLR 2021 Hypernetworks. CA-MTL is a hypernetwork adapter. The method to generate task-conditioned adapter weights is inspired by von Oswald et al. (2020). Hypernetwork layers have also been ﬁnetuned along with pretrained models. For example, Ponti et al. (2021) uses stochastic variational inference Hoffman et al. (2013) to produce language and task latent codes that conditionally generates the weights of a BERT prediction head, a single hypernetwork linear layer shared across multiple languages and tasks. Unlike previous methods however, CA-MTL conditionally modulates pretrained weights and biases, attention matrices, hidden representations and normalization statistics with task embeddings. Further, CA-MTL can preserve the pretraining knowledge by freezing the underlying Transformer model. Finally, we show a synergy between our hypernetwork adapter and our active task sampling technique (see section 2.2) that allows CA-MTL to continue surpassing fully tuned models as we scale the number of tasks (see ﬁgure 7). 4 E XPERIMENTS AND RESULTS We show that our adapter of section 2 achieve parameter efﬁcient transfer for 26 NLP tasks. Our implementation of CA-MTL is based on HuggingFace (Wolf et al., 2019). Hyperparameters and our experimental set-up are outlined in A.5. To preserve the weights of the pretrained model, CA-MTL’s bottom half Transformer layers are frozen in all experiments (except in section 4.4). We also tested different layer freezing conﬁgurations and found that freezing half the layers worked best on average (see Section A.8). 4.1 M ULTI -TASK UNCERTAINTY SAMPLING 0 250005000075000100000125000150000175000200000Training iteration 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82Average score MT-UncertaintyCouterfactualTask sizeRandom Figure 4: MT-Uncertainty vs. other task sam- pling strategies: median dev set scores on 8 GLUE tasks and using BERTBASE. Data for the Counter- factual and Task Size policyπ|task|(eq. 6) is from Glover & Hokamp (2019). Our MT-Uncertainty sampling strategy, from section 2.2, is compared to 3 other task selection schemes: a) Counterfactual b) Task size c) Random. We used a BERTBASE (no adapters) on 200k iterations and with the same hyperparameters as in Glover & Hokamp (2019). For more information on Counterfactual task selection, we invite the reader to consult the full expla- nation in Glover & Hokamp (2019). For T tasks and the dataset Di for tasks i ∈{1,...,T }, we rewrite the deﬁnitions of Randomπrand and Task sizeπ|task| sampling: πrand = 1/T, π|task|= |Di| [ T∑ i=1 |Di| ]−1 (6) 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (a) Random 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (b) MT-Uncertainty MNLI-mm dev scoreCoLA dev score MNLI-mm train entropyCoLA train entropy Figure 5: CoLA/MNLI Dev set scores and Entropy for πrand (left) and MT-Uncertainty (right). In Figure 4, we see from the results that MT- Uncertainty converges faster by reaching the 80% average GLUE score line before other task sampling methods. Further, MT-Uncertainty maximum score on 200k iterations is at 82.2, which is 1.7% higher than Counterfactual sam- pling. The datasets in the GLUE benchmark offers a wide range of dataset sizes. This is useful to test how MT-Uncertainty manages a jointly trained low resource task (CoLA) and high resource task (MNLI). Figure 5 explains how catastrophic forgetting is curtailed by sam- pling tasks before performance drops. With πrand, all of CoLA’s tasks are sampled by it- eration 500, at which point the larger MNLI dataset overtakes the learning process and CoLA’s dev set performance starts to diminish. On the other hand, with MT-Uncertainty sampling, CoLA is sampled whenever Shannon entropy is higher than MNLI’s. The model ﬁrst assesses uncertain samples using Shannon Entropy then decides what data is necessary to train on. This process allows lower resource tasks to keep performance steady. 6Published as a conference paper at ICLR 2021 We provide evidence in Figure 8 of A.2 that MT-Uncertainty is able to manage task difﬁculty — by choosing the most difﬁcult tasks ﬁrst. 4.2 A BLATION AND MODULE ANALYSIS Table 1: Model ablation studya on the GLUE dev set. All models have the bottom half layers frozen. Model changes Avg Task σ % data GLUE GLUE used BERTBASE MTL (πrand) 80.61 14.41 100 + Conditional Attention 82.41 10.67 100 + Conditional Adapter 82.90 11.27 100 + CA and CLN 83.12 10.91 100 + MT-Uncertainty 84.03 10.02 66.3(CA-MTLBERT-BASE) aCA=Conditional Alignment, CLN=Conditional Layer Normal- ization, Task σ=scores standard deviation across tasks. In Table 1, we present the results of an ablation study to determine which elements of CA-MTLBERT-BASE had the largest positive gain on average GLUE scores. Starting from a MTL BERTBASE baseline trained us- ing random task sampling ( πrand). Apart for the Conditional Adapter, each module as well as MT- Uncertainty lift overall performance and reduce vari- ance across tasks. Please note that we also included accuracy/F1 scores for QQP, MRPC and Pearson/ Spearman correlation for STS-B to calculate score standard deviation Task σ. Intuitively, when negative task transfer occurs between two tasks, either (1) task interference is bidirectional and scores are both impacted, or (2) interference is unidirectional and only one score is impacted. We calculate Task σ to characterize changes in the dynamic range of performance across multiple tasks. We do this to asses the degree to which performance im- provements are distributed across all tasks or only subsets of tasks. As we can see from Table 1, Conditional Attention, Conditional Alignment, Conditional Layer Normalization, MT-Uncertainty play roles in reducing Task σand increasing performance across tasks. This provides partial evidence of CA-MTL’s ability to mitigating negative task transfer. Figure 6: Task performance vs. avg. covariance similarity scores (eq. 7) for MTL and CA-MTL. We show that Conditional Alignment can learn to capture covariate distribution differences with task embeddings co-learned from other adapter compo- nents of CA-MTL. In Figure 6, we arrive at similar conclusions as Wu et al. (2020), who proved that neg- ative task transfer is reduced when task covariances are aligned. The authors provided a “covariance simi- larity score” to gauge covariance alignment. For task iand j with mi and mj data samples respectively, and given d dimensional inputs to the ﬁrst Trans- former layer Xi ∈Rmi×d and Xj ∈Rmj×d, we rewrite the steps to calculate the covariance similarity score between task iand j: (a) Take the covariance matrix X⊤ i Xi, (b) Find its best rank- ri approxima- tion Ui,riDi,riU⊤ i,ri, where ri is chosen to contain 99% of the singular values. (c) Apply steps (a), (b) to Xj, and compute the covariance similarity score CovSimi,j: CovSimi,j := ∥(Ui,riD1/2 i,ri )⊤Uj,rj D1/2 j,rj ∥F ∥Ui,riD1/2 i,ri ∥F ·∥Uj,rj D1/2 j,rj ∥F . CovSimi = 1 T −1 ∑ j̸=i CovSimi,j (7) Since we are training models with T tasks, we take the average covariance similarity score CovSimi between task iand all other tasks. We measure CovSimi using equation 7 between 9 single-task models trained on individual GLUE tasks. For each task in Figure 6, we measure the similarity score on the MTL trained BERTBASE baseline, e.g., CoLA (MTL), or CA-MTLBERT-BASE model, e.g., MNLI (CA-MTL). Our score improvement measure is the % difference between a single task model and MTL or CA-MTL on the particular task. We ﬁnd that covariance similarity increases for 9 tasks and that performance increases for 7 out 9 tasks. These measurements conﬁrm that the Conditional Alignment is able to align task covariance, thereby helping alleviate task interference. 4.3 J OINTLY TRAINING ON 8 TASKS : GLUE In Table 2, we evaluate the performance of CA-MTL against single task ﬁne-tuned models, MTL as well as the other BERT-based adapters on GLUE. As in Houlsby et al. (2019), MNLIm and MNLImm are treated as separate tasks. Our results indicate that CA-MTL outperforms both the BASE adapter, 7Published as a conference paper at ICLR 2021 Table 2: Adapters with layer freezing vs. ST/MT on GLUE test set. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. * Individual scores not available. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Results from: 1Devlin et al. (2018) 2Stickland et al. (2019). 3Houlsby et al. (2019) . Method Type Total Trained # tasks GLUE params params/task g.e. ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Test Server Results BERTBASE 1 ST 9.0× 100% — 52.1 84.6/83.4 88.9 90.5 71.2 66.4 93.5 85.8 79.6 BERTBASE 2 MTL 1.0× 11.1% 2 51.2 84.0/83.4 86.7 89.3 70.8 76.6 93.4 83.6 79.9 PALs+Anneal Samp.2 MTL 1.13× 12.5% 4 51.2 84.3/83.5 88.7 90.0 71.5 76.0 92.6 85.8 80.4 CA-MTLBERT-BASE (ours) MTL 1.12× 5.6 % 5 53.1 85.9/85.8 88.6 90.5 69.2 76.4 93.2 85.3 80.9 Large Models — Test Server Results BERTLARGE 1 ST 9.0× 100% — 60.5 86.7/85.9 89.3 92.7 72.1 70.1 94.9 86.5 82.1 Adapters-2563 ST 1.3× 3.6% 3 59.5 84.9/85.1 89.5 90.7 71.8 71.5 94.0 86.9 80.0 CA-MTLBERT-LARGE (ours) MTL 1.12× 5.6% 3 59.5 85.9/85.4 89.3 92.6 71.4 79.0 94.7 87.7 82.8 PALS+Anneal Sampling (Stickland et al., 2019), and the LARGE adapter, Adapters-256 (Houlsby et al., 2019). Against single task (ST) models, CA-MTL is 1.3% higher than BERTBASE, with 5 out 9 tasks equal or greater performance, and 0.7% higher than BERTLARGE, with 3 out 9 tasks equal or greater performance. ST models, however, need 9 models or close to 9×more parameters for all 9 tasks. We noted that CA-MTLBERT-LARGE’s average score is driven by strong RTE scores. While RTE beneﬁts from MTL, this behavior may also be a side effect of layer freezing. In Table 10, we see that CA-MTL has gains over ST on more and more tasks as we gradually unfreeze layers. 4.4 T RANSFER TO NEW TASKS Table 3: Domain adaptation results on dev. sets for BASE models. 1Liu et al. (2019b), 2Jiang et al. (2020) % data used SciTail SNLI 0.1% 1% 10% 100% 0.1% 1% 10% 100% BERTBASE 1 51.2 82.2 90.5 94.3 52.5 78.1 86.7 91.0 MT-DNN1 81.9 88.3 91.1 95.7 81.9 88.3 91.1 95.7 MT-DNNSMART 2 82.3 88.6 91.3 96.1 82.7 86.0 88.7 91.6 CA-MTLBERT 83.2 88.7 91.4 95.6 82.8 86.2 88.0 91.5 In Table 3 we examine the ability of our method to quickly adapt to new tasks. We performed domain adaptation on SciTail (Khot et al., 2018) and SNLI (Bowman et al., 2015) datasets, using a CA-MTLBASE model trained on GLUE and a new linear decoder head. We tested several pretrained and randomly initialized task embeddings in a zero-shot setting. The com- plete set of experiments with all task embeddings can be found in the Appendix, Section A.4. We then selected the best task embedding for our results in Table 3. STS-B and MRPC MTL-trained task embeddings performed best on SciTail and SNLI respectively. CA-MTLBERT-BASE has faster adapta- tion than MT-DNNSMART (Jiang et al., 2020) as evidenced by higher performances in low-resource regimes (0.1% and 1% of the data). When trained on the complete dataset, CA-MTLBERT-BASE is on par with MT-DNNSMART. Unlike MT-DNNSMART however, we do not add context from a semantic similarity model – MT-DNNSMART is built off HNN (He et al., 2019). Nonetheless, with a larger model, CA-MTL surpasses MT-DNNSMART on the full SNLI and SciTail datasets in Table 6. 4.5 J OINTLY TRAINING ON 24 TASKS : GLUE/S UPER -GLUE, MRQA AND WNUT2017 Figure 7: Effects of adding more datasets on avg GLUE scores. Experiments conducted on 3 epochs. When 23 tasks are trained jointly, performance of CA-MTLBERT-BASE continues to improve. Effects of Scaling Task Count. In Figure 7 we continue to test if CA-MTL mitigates task in- terference by measuring GLUE average scores when progressively adding 9 GLUE tasks, 8 Super-GLUE tasks (Wang et al., 2019b), 6 MRQA tasks (Fisch et al., 2019). Tasks are described in Appendix section A.9. The results show that adding 23 tasks drops the performance of our baseline MTL BERTBASE (πrand). MTL BERT increases by 4.3% when adding MRQA but, with 23 tasks, the model performance drops by 1.8%. The opposite is true when CA-MTL modules are integrated into the model. CA-MTL continues to show gains with a large number of tasks and surpasses the baseline MTL model by close to 4% when trained on 23 tasks. 8Published as a conference paper at ICLR 2021 Table 4: 24-task CA-MTL vs. ST and vs. 24-task MTL with frozen layers on GLUE, SuperGLUE, MRQA and NER development sets. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Details in section A.5. Model Task Grouping Avg # tasks Total GLUE SuperGLUE MRQA NER e.g. ST Params BERT-LARGE models STReImp 84.5 68.9 79.7 54.1 76.8 — 24× MTLReImp 83.2 72.1 77.8 42.2 76.4 9/24 1× CA-MTL 86.6 74.1 79.5 49.0 79.1 17/24 1.12× RoBERTa-LARGE models STReImp 88.2 76.5 83.6 57.8 81.9 — 24× MTLReImp 86.0 78.6 80.7 49.3 80.7 7/24 1× CA-MTL 89.4 80.0 82.4 55.2 83.1 15/24 1.12× 24-task CA-MTL. We jointly trained large MTL baselines and CA-MTL models on GLUE/Super-GLUE/MRQA and Named En- tity Recognition (NER) WNUT2017 (Derczyn- ski et al., 2017). Since some dev. set scores are not provided and since RoBERTa results were reported with a median score over 5 random seeds, we ran our own single seed ST/MTL baselines (marked “ReImp”) for a fair compar- ison. The dev. set numbers reported in Liu et al. (2019c) are displayed with our baselines in Table 9. Results are presented in Table 4. We notice in Table 4 that even for large models, CA-MTL provides large gains in performance on average over both ST and MTL models. For the BERT based models, CA-MTL provides 2.3% gain over ST and higher scores on 17 out 24 tasks. For RoBERTa based models, CA-MTL provides 1.2% gain over ST and higher scores on 15 out 24 tasks. We remind the reader that this is achieved with a single model. Even when trained with 16 other tasks, it is interesting to note that the MTL baseline perform better than the ST baseline on Super GLUE where most tasks have a small number of samples. Also, we used NER to test if we could still outperform the ST baseline on a token-level task, signiﬁcantly different from other tasks. Unfortunately, while CA-MTL performs signiﬁcantly better than the MTL baseline model, CA-MTL had not yet overﬁt on this particular task and could have closed the gap with the ST baselines with more training cycles. Table 5: Our 24-task CA-MTL vs. other large models on GLUE. F1 is reported for QQP/MRPC, Spearman’s corr. for STS-B, Matthew’s corr. for CoLA and accuracy for other tasks. *Split not available. **Uses intermediate task ﬁne-tuning + ST. Model GLUE tasks AvgCoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B BERT-LARGE based models on Dev set. MT-DNN 63.5 87.1 /86.7 91.0 92.9 89.2 83.4 94.3 90.6 85.6 STILTS ** 62.1 86.1* 92.3 90.5 88.5 83.4 93.2 90.8 85.9 BAM! 61.8 87.0* – 92.5 – 82.8 93.6 89.7 – 24-task CA-MTL 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa-LARGE based models on Test set. RoBERTA** with 67.8 91.0/90.8 91.6 95.4 74.0 87.9 97.5 92.5 87.3Ensemble 24-task CA-MTL 62.2 89.0/88.4 92.0 94.7 72.3 86.2 96.3 89.8 85.7 Comparisons with other methods. In Table 5, CA-MTLBERT is com- pared to other Large BERT based methods that either use MTL + ST, such as MT-DNN (Liu et al., 2019b), intermediate tasks + ST, such as STILTS (Phang et al., 2018) or MTL model distillation + ST, such as BAM! (Clark et al., 2019c). Our method scores higher than MT-DNN on 5 of 9 tasks and by 1.0 % on avg. Against STILTS, CA-MTL realizes a 0.7 % avg. score gain, surpassing scores on 6 of 9 tasks. We also show that CA-MTLRoBERTa is within only 1.6 % of a RoBERTa ensemble of 5 to 7 models per task and that uses intermediate tasks. Using our 24-task CA-MTL large RoBERTa-based model, we report NER F1 scores on the WNUT2017 test set in Table 6a. We compare our result with RoBERTaLARGE and XLM-RLARGE (Nguyen et al., 2020) the current state-of-the-art (SOTA). Our model outperforms XLM-RLARGE by 1.6%, reaching a new state-of-the-art. Using domain adaptation as described in Section 4.4, we report results on the SciTail test set in Table 6b and SNLI test set in Table 6b. For SciTail, our model matches the current SOTA2 ALUM (Liu et al., 2020), a RoBERTa large based model that additionally uses the SMART (Jiang et al., 2020) ﬁne-tuning method. For SNLI, our model outperforms SemBert, the current SOTA3. Table 6: CA-MTL test performance vs. SOTA. (a) WNUT2017 F1 RoBERTaLARGE 56.9 XLM-RLARGE 57.1 CA-MTLRoBERTa (ours) 58.0 (b) SciTail % Acc MT-DNN 94.1 ALUMRoBERTa 96.3 ALUMRoBERTa-SMART 96.8 CA-MTLRoBERTa (ours) 96.8 (c) SNLI % Acc MT-DNN 91.6 MT-DNNSMART 91.7 SemBERT 91.9 CA-MTLRoBERTa (ours) 92.1 2https://leaderboard.allenai.org/scitail/submissions/public on 09/27/2020 3https://nlp.stanford.edu/projects/snli/ on 09/27/2020 9Published as a conference paper at ICLR 2021 5 C ONCLUSION We believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. In a large-scale 24-task NLP experiment, CA-MTL outperforms fully tuned single task models by 2.3% for BERT Large and by 1.2% for RoBERTa Large using 1.12 times the number of parameters, while single task ﬁne-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. When a BERT vanilla MTL model sees its performance drop as the number of tasks increases, CA-MTL scores continue to climb. Performance gains are not driven by a single task as it is often the case in MTL. Each CA-MTL module that adapts a Transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. This evidence shows that CA-MTL is able to mitigate task interference and promote more efﬁcient parameter sharing. We showed that MT-Uncertainty is able to avoid degrading performances of low resource tasks. Tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. Overall, CA-MTL offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. Extending such ideas will be an objective for future work. 10Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This research was supported by the Canada CIFAR AI Chairs Program, NSERC and PROMPT. Exper- iments in this article were conducted with Compute Canada and MILA computational infrastructure and we thank them for their support. We would like to thank Colin Raffel, Sandeep Subramanian, and Nicolas Gontier for their useful feedback and the anonymous reviewers for helpful comments, discussions and suggestions. REFERENCES Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3874–3884, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388. URL https://www.aclweb.org/anthology/N19-1388. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41–48, 2009. Joachim Bingel and Anders Søgaard. Identifying beneﬁcial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 164–169, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/E17-2026. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, pp. arXiv–2005, 2020. Rich Caruana. Multitask learning. Mach. Learn., 28(1):41–75, July 1997. ISSN 0885-6125. doi: 10.1023/A:1007379606734. URL https://doi.org/10.1023/A:1007379606734. Richard Caruana. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning, pp. 41–48. Morgan Kaufmann, 1993. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001. Denis Charles, Max Chickering, and Patrice Simard. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14:3207–3260, November 2013. Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer. An empirical study of the behavior of active learning for word sense disambiguation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 120–127, New York City, USA, June 2006. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/N06-1016. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. CoRR, abs/1711.02257, 2017. URL http://arxiv.org/abs/1711.02257. 11Published as a conference paper at ICLR 2021 Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. doi: 10.18653/ v1/N19-1300. URL https://www.aclweb.org/anthology/N19-1300. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August 2019b. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https: //www.aclweb.org/anthology/W19-4828. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V . Le. Bam! born-again multi-task networks for natural language understanding. CoRR, abs/1907.04829, 2019c. URL http://arxiv.org/abs/1907.04829. Edward Collins, Nikolai Rozanov, and Bingbing Zhang. Evolutionary data measures: Understanding the difﬁculty of text classiﬁcation tasks. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pp. 380–391, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/K18-1037. URL https://www.aclweb.org/ anthology/K18-1037. Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neural networks with multitask learning. In ICML, pp. 160–167, 2008. URL https://doi. org/10.1145/1390156.1390177. Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investi- gating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107– 124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.601. URL https://ojs.ub.uni-konstanz. de/sub/index.php/sub/article/view/601. Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems 30 , pp. 6594– 6604. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7237-modulating-early-visual-processing-by-language.pdf . Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 140–147, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4418. URL https://www.aclweb.org/ anthology/W17-4418. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/anthology/I05-5002. Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Güney, V olkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine.CoRR, abs/1704.05179, 2017. URL http://arxiv.org/abs/1704.05179. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 1–13, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5801. URL https: //www.aclweb.org/anthology/D19-5801. 12Published as a conference paper at ICLR 2021 John Glover and Chris Hokamp. Task selection policies for multitask learning. CoRR, 2019. URL http://arxiv.org/abs/1907.06214. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausi- ble alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main confer- ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se- mantic Evaluation (SemEval 2012), pp. 394–398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/S12-1052. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioriti- zation for multitask learning. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network model for commonsense reasoning. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pp. 13–21, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-6002. URL https://www.aclweb.org/ anthology/D19-6002. Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(4):1303–1347, 2013. URL http://jmlr.org/ papers/v14/hoffman13a.html. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. CoRR, abs/1902.00751, 2019. URL http://arxiv.org/abs/1902.00751. Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://www.aclweb.org/anthology/ P18-1031. Fariz Ikhwantri, Samuel Louvan, Kemal Kurniawan, Bagas Abisena, Valdi Rachman, Alfan Farizki Wicaksono, and Rahmad Mahendra. Multi-task active learning for neural semantic role labeling on low resource conversational corpus. InProceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pp. 43–50, 2018. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled regular- ized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2177–2190, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.197. URL https://www.aclweb.org/anthology/2020. acl-main.197. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601– 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/ v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. CoRR, abs/1907.10529, 2019. URL http://arxiv.org/abs/1907.10529. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CoRR, abs/1705.07115, 2017. URL http://arxiv.org/ abs/1705.07115. Emma Kerinec, Chloé Braud, and Anders Søgaard. When does deep multi-task learning work for loosely related document classiﬁcation tasks? In Proceedings of the 2018 EMNLP Workshop 13Published as a conference paper at ICLR 2021 BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 1–8, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5401. URL https://www.aclweb.org/anthology/W18-5401. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. InProceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL https://www.aclweb.org/anthology/N18-1023. Tushar Khot, A. Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question answering. In AAAI, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Hector J. Levesque. The winograd schema challenge. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning. AAAI, 2011. URL http://dblp.uni-trier.de/ db/conf/aaaiss/aaaiss2011-6.html#Levesque11. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855, 2019a. URL http://arxiv.org/abs/1903.08855. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019b. URL http://arxiv.org/ abs/1901.11504. Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019c. URL http://arxiv.org/abs/1907.11692. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to bert embeddings during ﬁne-tuning? arXiv preprint arXiv:2004.14448, 2020. Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. Bertweet: A pre-trained language model for english tweets. arXiv preprint arXiv:2005.10200, 2020. Hao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h - 1 heads is better than h heads. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6566–6577, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.587. URL https://www.aclweb.org/anthology/2020. acl-main.587. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. 14Published as a conference paper at ICLR 2021 Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. CoRR, abs/1808.08949, 2018. URL http: //arxiv.org/abs/1808.08949. Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. CoRR, abs/1811.01088, 2018. URL http://arxiv. org/abs/1811.01088. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 67–81, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1007. URL https://www.aclweb.org/ anthology/D18-1007. Edoardo M. Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela Parovic, Roi Reichart, and Anna Korhonen. Parameter space factorization for zero-shot learning across tasks and languages. Transactions of the Association for Computational Linguistics, 9:410–428, 2021. doi: 10.1162/tacl_a_00374. URL https://aclanthology.org/2021.tacl-1.25. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R Bowman. Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? arXiv preprint arXiv:2005.00628, 2020. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016a. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/ anthology/D16-1264. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016b. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. Multi-task active learning for linguistic annotations. In Proceedings of ACL-08: HLT, pp. 861–869, 2008. Sebastian Ruder. An overview of multi-task learning in deep neural networks.ArXiv, abs/1706.05098, 2017. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. CoRR, abs/1810.04650, 2018. URL http://arxiv.org/abs/1810.04650. Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, pp. 4555–4564, 2018. URL http:// proceedings.mlr.press/v80/serra18a.html. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. 15Published as a conference paper at ICLR 2021 Trevor Standley, Amir Roshan Zamir, Dawn Chen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? CoRR, abs/1905.07553, 2019. URL http://arxiv.org/abs/1905.07553. Asa Cooper Stickland, Iain Murray, someone, and someone. BERT and PALs: Projected attention layers for efﬁcient adaptation in multi-task learning. volume 97 of Proceedings of Machine Learning Research, pp. 5986–5995, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/stickland19a.html. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid: Efﬁcient multi-task transformers with grid-wise decomposable hyper projections. arXiv preprint arXiv:2007.05891, 2020. Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. CoRR, abs/1905.05950, 2019a. URL http://arxiv.org/abs/1905.05950. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316, 2019b. URL http://arxiv.org/abs/1905.06316. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 191–200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL https: //www.aclweb.org/anthology/W17-2623. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and João Sacramento. Continual learning with hypernetworks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgwNerKvB. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019a. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537, 2019b. URL http://arxiv.org/abs/ 1905.00537. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. CoRR, abs/1805.12471, 2018. URL http://arxiv.org/abs/1805.12471. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //www.aclweb.org/anthology/N18-1101. 16Published as a conference paper at ICLR 2021 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771. Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer in multi-task learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylzhkBtDB. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Com- putational Linguistics. doi: 10.18653/v1/D18-1259. URL https://www.aclweb.org/ anthology/D18-1259. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885, 2018. URL http://arxiv.org/abs/1810.12885. Yu Zhang and Qiang Yang. A survey on multi-task learning. CoRR, abs/1707.08114, 2017. URL http://arxiv.org/abs/1707.08114. 17Published as a conference paper at ICLR 2021 A A PPENDIX A.1 S UMMARY OF ACRONYMS Acronyms of datasets and descriptions can be found below in section A.9. Table 7: List of acronyms used in this paper. Acronym Description ARLM Autoregressive Language Models CA-MTL Conditional Adaptive Multi-Task Learning: our architecture CFF Conditional Feed-Forward: a feed-forward layer modulated by a conditioning vector CLN Conditional Layer Normalization in section 2.1.3 EDM Evolutionary Data Measures (Collins et al., 2018): a task difﬁculty estimate GLUE General Language Understanding Evaluation Wang et al. (2018): a benchmark with multiple datasets QA Question Answering MT Multi-Task MTAL Multi-Task Active Learning: ﬁnding the most informative instance for multiple learners (or models) MLM Masked Language Model: BERT Devlin et al. (2018) is an example of an MLM MTL Multi-Task Learning: \"learning tasks in parallel while using a shared representation\" (Caruana, 1997) MRQA Machine Reading for Question Answering Fisch et al. (2019): a benchmark with multiple datasets NER Named Entity Recognition NLP Natural Language Processing SOTA State of the art ST Single Task ﬁne-tuning: all weights are typically updated ST-A ST with Adapter modules: one adapter per task is trained and pretrained weights are optionally updated A.2 U NCERTAINTY SAMPLING : A LGORITHM AND ADDITIONAL RESULTS Algorithm 1: Multi-task Uncertainty Sampling Input: Training data Dt for task t∈[1,...,T ]; batch size b; Ct possible output classes for task t; f := fφ(zi),θi our model with weights φ,θi; Output: B′- multi-task batch of size b 1 B←∅ 2 for t←1 to T do 3 Generate xt := {xt,1,...,x t,b} i.i.d. ∼Dt 4 for i←1 to bdo 5 Ht,i ←−∑Ci c=1 pc(f(xt,i)) log pc(f(xt,i)) ⊿Entropy of each sample 6 7 end 8 Compute ¯Ht ←1 b ∑ x∈xi Ht,i ⊿Average entropy for task t 9 10 Compute H′ t ←−∑Ct c=1 1 Ct log [ 1 Ct ] ⊿Max entropy (uniform distribution) 11 12 B←B∪ xt and Dt ←Dt \\xt 13 if Dt = ∅then 14 Reload Dt 15 end 16 for i←1 to bdo 17 Compute: Ut,i ←Ht,i/H′ t ⊿Uncertainty normalized with max entropy 18 end 19 end 20 Compute ˆH← maxi∈{1,...,T}[ ¯Ht] ⊿Entropy of task with highest average entropy 21 Update Ut,i ←Ut,i/ˆH ⊿Normalize each sample’s uncertainty measure 22 B′←top_b({Ut,i|t∈[1,...,T ],i ∈[1,...,b ]}) ⊿b samples w/ highest uncertainty Return: With B′, solve eq. 1 with gradient descent; updated model f 18Published as a conference paper at ICLR 2021 An advantage of our MT-Uncertainty Sampling approach is its ability to manage task difﬁculty. This is highlighted in Figure 8. In this experiment, we estimated task difﬁculty using the Evolutionary Data Measures (EDM)4 proposed by Collins et al. (2018). The task difﬁculty estimate relies on multiple dataset statistics such as the data size, class diversity, class balance and class interference. Interestingly, estimated task difﬁculty correlates with the ﬁrst instance that the selection of a speciﬁc task occurs. Supposing that QNLI is an outlier, we notice that peaks in the data occur whenever tasks are ﬁrst selected by MT Uncertainty sampling. This process follows the following order: 1. MNLI 2. CoLA 3. RTE 4. QQP 5. MRPC 6.SST-2, which is the order from highest task difﬁculty to lowest task difﬁculty using EDM. As opposed to Curriculum Learning (Bengio et al., 2009), MT-Uncertainty dynamically prioritizes the most difﬁcult tasks. As also discovered in MTL vision work (Guo et al., 2018), this type of prioritization on more difﬁcult tasks may explain MT-Uncertainty’s improved performance over other task selection methods. In MTL, heuristics to balance tasks during training is typically done by weighting each task’s loss differently. We see here how MT-Uncertainty is able to prioritize task difﬁculty. 0 500 1000 1500 2000 2500 3000 Train iteration 0 4 8 12 16 20 24 28 32Number of samples Task Difﬁculty MNLI 4.2 QNLI 3.8 CoLA 3.7 RTE 3.6 MRPC 3.5 QQP. 3.5 SST-2 3.2 Figure 8: Task composition of MT-Uncertainty sampling and estimated task difﬁculty using EDM: number of training samples per task at each iteration for batch size of 32. The occurrence of ﬁrst peaks and estimated difﬁculty follow the same order: From highest to lowest: MNLI >CoLA >RTE >QQP = MRPC >SST-2. While the EDM difﬁculty measure is shown to correlate well with model performance, it lacks precision. As reported in Collins et al. (2018), the average score achieved on the Yahoo Answers dataset is 69.9% and its difﬁculty is 4.51. The average score achieved on Yelp Full is 56.8%, 13.1% less than Yahoo Answers and its difﬁculty is 4.42. The authors mention that “This indicates that the difﬁculty measure in its current incarnation may be more effective at assigning a class of difﬁculty to datasets, rather than a regression-like value”. A.3 O THER RELATED WORK Multi-Tasking in NLP and other ﬁelds.MTL weight sharing algorithms such as Mixture-of-Experts (MoE) have found success in NLP (Lepikhin et al., 2020). CA-MTL can complement MoE since the Transformers multi-headed attention can be seen as a form of MoE (Peng et al., 2020). In Vision, MTL can also improve with optimization (Sener & Koltun, 2018) or gradient-based approaches (Chen et al., 2017; Yu et al., 2020). Active Learning, Task Selection and Sampling. Ikhwantri et al. (2018) examined multi-task active learning for neural semantic role labeling in a low resource setting, using entity recognition as the sole auxiliary task. They used uncertainty sampling for active learning and found that 12% less data could be used compared to passive learning. Reichart et al. (2008) has examined different active learning techniques for the two task annotation scenario, focusing on named entity recognition and syntactic parse tree annotations. In contrast, here we examine the larger scale data regime, the modularization of a multi-task neural architecture, and the many task (≫2) setting among other differences. Other than MTAL (Reichart et al., 2008; Ikhwantri et al., 2018), Kendall et al. (2017) leveraged model uncertainty to balance MTL losses but not to select tasks as is proposed here. 4https://github.com/Wluper/edm 19Published as a conference paper at ICLR 2021 A.4 Z ERO -SHOT RESULTS ON SCITAIL AND SNLI Before testing models on domain adaptation in section 4.4, we ran zero-shot evaluations on the development set of SciTail and SNLI. Table 8 outlines8-task CA-MTLBERT-BASE’s zero-shot transfer abilities when pretrained on GLUE with our MTL approach. We expand the task embedding layer to accommodate an extra task and explore various embedding initialization. We found that reusing STS-B and MRPC task embeddings worked best for SciTail and SNLI respectively. Table 8: CA-MTL is ﬂexible and extensible to new tasks. However, CA-MTL is sensitive to the new task’s embedding. We tested multiple task embeddings that worked best on either SciTail or SNLI by checking performance in a zero shot setting or using 0% of the data. Initialization of new SciTail SNLI task embedding layer 0% of data 0% of data CoLA’s embeddings 43.0 34.0 MNLI’s embeddings 24.2 33.0 MRPC’s embeddings 34.5 45.5 STS-B’s embeddings 46.9 33.2 SST-2’s embeddings 25.8 34.2 QQP’s embeddings 31.7 37.3 QNLI’s embeddings 32.0 38.0 RTE’s embeddings 32.3 40.6 WNLI’s embeddings 29.0 30.4 Average 28.7 37.7 Random initialization 46.8 34.0 Xavier initialization 29.8 37.6 A.5 M ORE EXPERIMENTAL DETAILS We used a batch size of 32 and a seed of 12 in all experiments. We used Adam (Kingma & Ba, 2015) as the optimizer with a learning rate of 2e-5. We applied a learning rate decay with warm up over the ﬁrst 10% of the training steps. Unless otherwise speciﬁed, we used 5 epochs, a seed of 12 and a sequence length of 128. Additional details are outlined in section . Our data prepossessing and linear decoder heads are the same as in Devlin et al. (2018). We used the same dropout rate of 0.1 in all layers. To run our experiments, we used either four NVIDIA P100 GPU for base models or four NVIDIA V100 GPU for larger ones. We did not perform parameter search. We do not use ensemble of models or task-speciﬁc tricks (Devlin et al., 2018; Liu et al., 2019b; Clark et al., 2019c). All models are either 12 Transformer layers for BASE and 24 Transformer layers for LARGE. Apart from CA-MTL, models trained in multi-task learning (BERT or RoBERTa without adapters) used random task sampling. For Table 1 and Figure 7, all BERT-based model have half their layers frozen (untrained) for a fair comparison of ablation results. For the 24-task MTL and CA-MTL models in Tables 4 and 5, we increased the input sequence length to 256 and used 8 epochs. A.6 T HE DIRECT SUM OPERATOR In section 2.1.1, we used the direct sum operator ⊕. This operation allows us to create a block diagonal matrix. The direct sum of a matrix A∈Rn×m and B ∈Rp×q results in a matrix of size (m+ p) ×(n+ q), deﬁned as: A ⊕B = [ A 0 0 B ] =   a11 ··· a1n 0 ··· 0 ... ... ... ... ... ... am1 ··· amn 0 ··· 0 0 ··· 0 b11 ··· b1q ... ... ... ... ... ... 0 ··· 0 bp1 ··· bpq   20Published as a conference paper at ICLR 2021 A.7 B ASELINES AND OTHER EXPERIMENTAL RESULTS In this section, we present our baseline results for BERT, RoBERTa, CA-MTL as well as other models. Our single task results (ST) that we ran ourselves surpass other paper’s reported scores in Table 9. Liu et al. (2019c) reports random seed median scores for RoBERTa. However, our RoBERTa ST baseline matches or surpasses the original paper’s scores4 out 7 times on the development set when scores are comparable (QQP F1 and STS-B spearman are not reported). Table 9: F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. ST=Single Task, MTL=Multitask. *QNLI v1 (we report v2) **F1 score or Spearman’s correlation is not reported. ***Unknown random seeds. Results from: 1Stickland et al. (2019) 2Liu et al. (2019b) 3Phang et al. (2018) 4Liu et al. (2019c). Method Total Trained GLUE params params/task CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Dev set Results PALs+Anneal Samp.1 1.13× 12.5% – – – – – – – – 81.70 8-task CA-MTLBERT-BASE (ours) 1.12× 5.6% 60.9 82.7/83.1 88.9 90.7 90.3 79.1 91.9 88.8 84.03 BERT LARGE Models — Dev set Results ST BERT-LARGE2 9× 100% 60.5 86.7/85.9 89.3 92.7* 89.3 70.1 94.9 86.5 84.0 ST BERT-LARGE3 9× 100% 62.1 86.2/86.2 92.3 89.4 88.5 70.0 92.5 90.1 84.1 ST BERT-LARGE (ours) 9× 100% 63.6 86.5/86.0 91.4 91.0 88.5 70.2 94.7 88.2 84.5 24-task CA-MTLBERT-LARGE (ours) 1.12× 5.6% 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa LARGE Models — Dev set Results RoBERTa-LARGE4 9× 100% 68.0 90.2 90.9 94.7 ** 86.6 96.4 ** –(Median 5 runs)*** ST RoBERTa-LARGE (ours) 9× 100% 68.3 89.2/88.9 92.6 94.8 84.6 87.0 96.4 91.7 88.2 24-task CA-MTLRoBERTa-LARGE (ours) 1.12× 5.6% 69.7 89.4/89.3 93.9 94.9 88.8 91.0 96.2 91.0 89.4 A.8 S OME RESULTS ON LAYER FREEZING AND WITH FULL BLOCK ATTENTION . All experiments in this section were run for only 5 epochs, exclusively on the GLUE dataset for the large BERT-based 8-task CA-MTL model. Results in Table 10 reveal that as we freeze more layers, performance tends to decrease. However, since we wanted to preserve as much pretrained knowledge as possible, we chose to keep at least 50% of layers frozen. While this has slightly lowered our performance on 9 GLUE tasks, we believe that keeping as much of the original pretrained weights is beneﬁcial when increasing the total number of tasks in MTL to 24 or more tasks. However, we did not explore this hypothesis more. Table 10: 8-task CA-MTLBERT-LARGE (see section 4.3) for various layer freezing conﬁgurations. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. FBA = Full Block Attention Method % frozen # tasks GLUE layers g.e ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg LARGE Models — Dev set Results ST BERT-LARGE (ours) 0% — 63.6 86.5/86.0 91.4 91.0 88.5 70.2 93.1 88.2 84.3 CA-MTL 0% 7 60.2 86.2/86.0 92.0 91.5 88.7 76.3 93.3 89.5 84.9 CA-MTL 25% 6 63.7 86.1/85.8 89.1 91.2 88.6 79.7 92.9 88.5 85.1 CA-MTL 50% 3 63.2 85.5/85.5 91.8 90.9 88.3 81.4 93.0 90.1 85.5 CA-MTL FBA 50% 0 60.2 81.7/81.1 88.0 85.8 85.7 78.7 88.6 87.1 81.8 A.9 D ATASET DESCRIPTION The datasets that were used for the domain adaptation experiments were SciTail 5 and SNLI6. We jointly trained a CA-MTLRoBERTa-LARGE model on 9 GLUE tasks, 8 Super-GLUE7 tasks, 6 MRQA8 tasks, and on WNUT20179 (Derczynski et al., 2017). All GLUE tasks are binary classiﬁcation, except STS-B (regression) and MNLI (three classes). We used the same GLUE data preprocessing as in Devlin et al. (2018). 5https://allenai.org/data/scitail; Leaderboard can be found at: https://leaderboard.allenai.org/scitail/submissions/public 6https://nlp.stanford.edu/projects/snli/ 7https://super.gluebenchmark.com/tasks 8https://github.com/mrqa/MRQA-Shared-Task-2019 9https://github.com/leondz/emerging_entities_17 21Published as a conference paper at ICLR 2021 Table 11: GLUE (Wang et al., 2018) dataset description. References: 1Warstadt et al. (2018), 2Socher et al. (2013), 3Dolan & Brockett (2005), 4Cer et al. (2017), 5Williams et al. (2018), 6Wang et al. (2018), 7Levesque (2011) Acronym Corpus |Train| Task Domain CoLA1 Corpus of Linguistic Acceptability 8.5K acceptability miscellaneous SST-22 Stanford Sentiment Treebank 67K sentiment detection movie reviews MRPC3 Microsoft Research Paraphrase Corpus 3.7K paraphrase detection news STS-B4 Semantic Textual Similarity Benchmark 7K textual similarity miscellaneous QQP Quora Question Pairs 364K paraphrase detection online QA MNLI5 Multi-Genre NLI 393K inference miscellaneous RTE6 Recognition Textual Entailment 2.5K inference/entailment news, Wikipedia WNLI7 Winograd NLI 634 coreference ﬁction books Table 12: Super-GLUE (Wang et al., 2019b) dataset description. References:1Clark et al. (2019a),2de Marneffe et al. (2019), 3Gordon et al. (2012), 4Khashabi et al. (2018), 5Zhang et al. (2018), 6Wang et al. (2019b), 7Poliak et al. (2018), 8Levesque (2011) Acronym Corpus |Train| Task Domain BoolQ1 Boolean Questions 9.4K acceptability Google queries, Wikipedia CB2 CommitmentBank 250 sentiment detection miscellaneous COPA3 Choice of Plausible Alternatives 400 paraphrase detection blogs, encyclopedia MultiRC4 Multi-Sentence Reading Comprehension5.1K textual similarity miscellaneous ReCoRD5 Reading Comprehension 101K paraphrase detection news and Commonsense Reasoning RTE6 Recognition Textual Entailment 2.5K inference news, Wikipedia WiC7 Word-in-Context 6K word sense disambiguationWordNet, VerbNet WSC8 Winograd Schema Challenge 554 coreference resolution ﬁction books Table 13: MRQA (Fisch et al., 2019) dataset description. References: 1Rajpurkar et al. (2016a), 2Trischler et al. (2017), 3Joshi et al. (2017), 4Dunn et al. (2017), 5Yang et al. (2018), 6Kwiatkowski et al. (2019) Acronym Corpus |Train| Task Domain SQuAD1 Stanford QA Dataset 86.6K crowdsourced questions Wikipedia NewsQA2 NewsQA 74.2K crowdsourced questions news TriviaQA3 TriviaQA 61.7K trivia QA web snippets SearchQA4 SearchQA 117.4K Jeopardy QA web snippets HotpotQA5 HotpotQA 72.9K crowdsourced questions Wikipedia Natural Questions6 Natural Questions 104.7K search logs Wikipedia SuperGLUE has a more diverse task format than GLUE, which is mostly limited to sentence and sentence-pair classiﬁcation. We follow the same preprocessing procedure as in Wang et al. (2019b). All tasks are binary classiﬁcation tasks, except CB (three classes). Also, WiC and WSC are span based classiﬁcation tasks. We used the same modiﬁed MRQA dataset and preprocessing steps that were used in Joshi et al. (2019). All MRQA tasks are span prediction tasks which seeks to identify start and end tokens of an answer span in the input text. Table 14: SNLI (Bowman et al., 2015) and SciTail (Khot et al., 2018) datasets description. Acronym Corpus |Train| Task Domain SNLI1 Stanford Natural Language Inference 550.2k inference human-written English sentence pairs SciTail2 Science and Entailment 23.5K entailment Science question answering SNLI is a natural inference task where we predict three classes. Examples of three target labels are: Entailment, Contradiction, and Neutral (irrelevant). SciTail is a textual entailment dataset. The hypotheses in SciTail are created from multiple-choice science exams and the answer candidates (premise) are extracted from the web using information retrieval tools. SciTail is a binary true/false classiﬁcation tasks that seeks to predict whether the premise entails the hypothesis. The two datasets are used only for domain adaptation in this study (see section A.4 for the details of our approach). 22",
      "meta_data": {
        "arxiv_id": "2009.09139v3",
        "authors": [
          "Jonathan Pilault",
          "Amine Elhattami",
          "Christopher Pal"
        ],
        "published_date": "2020-09-19T02:04:34Z",
        "pdf_url": "https://arxiv.org/pdf/2009.09139v3.pdf",
        "github_url": "https://github.com/CAMTL/CA-MTL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Conditionally Adaptive Multi-Task Learning (CA-MTL), a novel Transformer-based Adapter designed to improve transfer learning in NLP by addressing challenges like overfitting to low-resource tasks, catastrophic forgetting, and negative task transfer in Multi-Task Learning (MTL). CA-MTL achieves more efficient parameter sharing and mitigates forgetting by fixing half of a pretrained model's weights and using a new multi-task data sampling strategy. It surpasses single-task fine-tuning and other Adapter methods in performance while being parameter and data efficient, achieving state-of-the-art results on several NLP benchmarks across up to 26 tasks.",
        "methodology": "CA-MTL modularizes a pretrained network using latent task representations and balances tasks with an uncertainty-based sampling method. Key components include: 1) Task Conditioned Transformer architecture that adapts and modulates pretrained weights via Conditional Weight Transformations. 2) Specific modules: Conditional Attention (block-diagonal to account for task-specific biases), Conditional Alignment (a task-conditioned alignment layer for covariate distribution differences), Conditional Layer Normalization (adapting layer normalization statistics to specific tasks), and Conditional Bottleneck (a two-layer feed-forward bottleneck facilitating weight sharing and task-specific information flow). 3) Multi-Task Uncertainty Sampling, an active learning-inspired task selection strategy that uses Shannon Entropy to prioritize uncertain training examples, normalizing for class distribution and highest average task entropy.",
        "experimental_setup": "The method was evaluated on 26 NLP tasks, including GLUE (9 tasks), SuperGLUE (8 tasks), MRQA (6 tasks), and WNUT2017 (Named Entity Recognition). Domain adaptation was tested on SciTail and SNLI datasets. Models used were BERTBASE, BERTLARGE, and RoBERTaLARGE, implemented using HuggingFace. Baselines included single-task fine-tuning (ST), standard MTL (with random or counterfactual sampling), and other adapter networks (PALs, Adapters-256, MT-DNN, STILTS, BAM!). Performance was measured using average GLUE scores, F1, Spearman's correlation, Matthew's correlation, and accuracy. Experiments involved freezing the bottom half of Transformer layers, using Adam optimizer (learning rate 2e-5, warm-up over first 10%), batch size 32, sequence length 128 (256 for 24-task models), and 5-8 epochs. Ablation studies analyzed individual module contributions and layer freezing configurations. Task difficulty was estimated using Evolutionary Data Measures (EDM). Experiments were conducted on NVIDIA P100/V100 GPUs.",
        "limitations": "While generally outperforming baselines, the chosen strategy of freezing half the Transformer layers, aimed at preserving pretrained knowledge for a larger number of tasks, slightly lowered performance on some GLUE tasks. For the token-level NER task (WNUT2017), CA-MTL, despite significantly outperforming the MTL baseline, had not yet overfit and could potentially close the gap with single-task baselines with more training cycles. For zero-shot transfer, CA-MTL was found to be sensitive to the initialization of the new task's embedding. The external Evolutionary Data Measures (EDM) for task difficulty estimation lacked precision for regression-like values.",
        "future_research_directions": "Future work will focus on extending the ideas of task-conditioned adaptive learning within a single model, further exploring dynamic adaptation, and modularizing knowledge embedded in large monolithic pretrained models.",
        "experimental_code": "from dataclasses import dataclass, field\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import BertPreTrainedModel\n\nfrom src.model.decoder import Decoder\nfrom src.model.encoders.bert import _BertEncoder\nfrom src.model.encoders.ca_mtl_base import CaMtlBaseEncoder\nfrom src.model.encoders.ca_mtl_large import CaMtlLargeEncoder\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass CaMtlArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from: CA-MTL-base, CA-MTL-large, bert-base-cased \"\n                    \"bert-base-uncased, bert-large-cased, bert-large-uncased\"\n        }\n    )\n    freeze_encoder_layers: str = field(\n        default=None,\n        metadata={\"help\": \"Freeze encoder layers. format: <start_layer>-<end_layer>\"},\n    )\n\n\nclass CaMtl(BertPreTrainedModel):\n    def __init__(\n        self,\n        config,\n        model_args,\n        data_args,\n    ):\n        super().__init__(config)\n\n        self.data_args = data_args\n        self.bert = self._create_encoder(model_args.model_name_or_path)\n        self.decoders = nn.ModuleList()\n        for task in data_args.tasks:\n            self.decoders.append(Decoder(config.hidden_size, task))\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        task_id=None,\n        span_locs=None,\n        sample_id=None,\n    ):\n\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            task_id=task_id,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n\n        loss_list = []\n        unique_task_ids = torch.unique(task_id)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        loss_grouped_per_task = (\n            torch.zeros_like(task_id[0]).repeat(len(self.data_args.tasks)).float()\n        )\n        batch_entropy_per_task = torch.zeros(input_ids.shape[0])\n        batch_entropy_mean_per_task = torch.zeros(input_ids.shape[0])\n        max_mean_batch_entropy = None\n        logits = None\n        for unique_task_id in unique_task_ids_list:\n            task_id_filter = task_id == unique_task_id\n            decoder_id = unique_task_id\n            logits, current_loss, batch_entropy = self.decoders[decoder_id].forward(\n                sequence_output[task_id_filter],\n                pooled_output[task_id_filter],\n                labels=None if labels is None else labels[task_id_filter],\n                attention_mask=attention_mask[task_id_filter],\n            )\n\n            batch_entropy_mean = batch_entropy.mean().item()\n            batch_entropy_per_task[task_id_filter] = batch_entropy\n            batch_entropy_mean_per_task[task_id_filter] = torch.full_like(\n                batch_entropy, batch_entropy_mean\n            )\n            if (\n                max_mean_batch_entropy is None\n                or batch_entropy_mean > max_mean_batch_entropy\n            ):\n                max_mean_batch_entropy = batch_entropy_mean\n\n            if labels is not None:\n                loss_grouped_per_task[unique_task_id] = current_loss\n                loss_list.append(current_loss)\n\n        outputs = (\n            (logits,)\n            + outputs[2:]\n            + (\n                batch_entropy_per_task,\n                batch_entropy_mean_per_task,\n                max_mean_batch_entropy,\n            )\n        )\n\n        if loss_list:\n            loss = torch.stack(loss_list)\n            outputs = (loss.mean(),) + outputs + (loss_grouped_per_task.view(1, -1),)\n\n        return outputs\n\n    def _create_encoder(self, model_name_or_path):\n        if model_name_or_path == \"CA-MTL-large\":\n            return CaMtlLargeEncoder(self.config, data_args=self.data_args)\n        elif model_name_or_path == \"CA-MTL-base\":\n            return CaMtlBaseEncoder(self.config, data_args=self.data_args)\n        else:\n            return _BertEncoder(self.config)\n\n    @staticmethod\n    def get_base_model(model_name_or_path):\n        if model_name_or_path == \"CA-MTL-large\":\n            return \"bert-large-cased\"\n        elif model_name_or_path == \"CA-MTL-base\":\n            return \"bert-base-cased\"\n        else:\n            return model_name_or_path\n\n    def freeze_encoder_layers(\n        self,\n        model_args,\n        unfrozen_modules=[\n            \"random_weight_matrix\",\n            \"film.gb_weights\",\n            \"ln_weight_modulation.gb_weights\",\n            \"adapter\",\n        ],\n    ):\n        if model_args.freeze_encoder_layers is not None:\n            start_layer, end_layer = model_args.freeze_encoder_layers.split(\"-\")\n\n            for name, param in self.bert.named_parameters():\n                requires_grad = True\n                match = re.match(self.bert.get_layer_regexp(), name)\n                if match:\n                    layer_number = int(match.groups()[0])\n                    requires_grad = not int(start_layer) <= layer_number <= int(\n                        end_layer\n                    ) or any([module in match.string for module in unfrozen_modules])\n                elif name.startswith(\"embedding\"):\n                    requires_grad = False\n                param.requires_grad = requires_grad\n\n        for name, param in self.bert.named_parameters():\n            logger.info(\n                \"%s - %s\", name, (\"Unfrozen\" if param.requires_grad else \"FROZEN\")\n            )\nimport torch\nimport numpy\nfrom scipy.stats import entropy\nfrom transformers import glue_tasks_num_labels\nfrom torch.nn import MSELoss, CrossEntropyLoss, Softmax, Dropout, Linear, Softmax\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_size, task_name):\n        super().__init__()\n        self.num_labels = glue_tasks_num_labels[task_name]\n        self.dropout = Dropout(0.1)\n        self.model = Linear(hidden_size, self.num_labels)\n\n    def forward(self, sequence_output, pooled_output, labels=None, **kwargs):\n        loss = None\n        pooled_output = self.dropout(pooled_output)\n        logits = self.model(pooled_output)\n\n        batch_entropy = self.calculate_entropy(logits)\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.float().view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.long().view(-1))\n\n        return logits, loss, batch_entropy\n\n    def calculate_entropy(self, logits):\n        probas = Softmax(dim=1)(logits.detach())\n        samples_entropy = entropy(probas.transpose(0, 1).cpu())\n        even_preds = numpy.array(\n            [[1 / self.num_labels for _ in range(self.num_labels)]]\n        )\n        max_entropy = entropy(even_preds.T)\n        epsilon = 1e-5\n        samples_entropy = samples_entropy / (max_entropy.item() + epsilon)\n        return torch.tensor(samples_entropy)\nimport math\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_bert import (\n    BertEmbeddings,\n    BertPooler,\n    BertPreTrainedModel,\n    BertAttention,\n    BertIntermediate,\n    BertLayer,\n    BertSelfOutput,\n)\n\nfrom src.model.encoders.conditional_modules import FiLM, CBDA, ConditionalBottleNeck, ConditionalLayerNorm\n\n\nclass MyBertSelfAttention9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.max_seq_length = config.max_seq_length\n        assert config.hidden_size % self.max_seq_length == 0, \\\n            \"Block decomposed attention will only work if this condition is met.\"\n        self.num_blocks = config.hidden_size//self.max_seq_length\n        self.cond_block_diag_attn = CBDA(\n            config.hidden_size, math.ceil(self.max_seq_length/self.num_blocks), self.num_blocks\n        )  # d x L/N\n\n        self.random_weight_matrix = nn.Parameter(\n            torch.zeros(\n                [config.max_seq_length, math.ceil(self.max_seq_length/self.num_blocks)]\n            ),\n            requires_grad=True,\n        )\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        if encoder_hidden_states is not None:\n            mixed_value_layer = self.value(encoder_hidden_states)\n            attention_mask = encoder_attention_mask\n        else:\n            mixed_value_layer = self.value(hidden_states)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        mixed_key_layer = self.key(hidden_states)\n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        attention_scores1 = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n\n        attention_scores2 = self.cond_block_diag_attn(\n            x_cond=task_embedding,\n            x_to_film=self.random_weight_matrix,\n        )\n\n        attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n        # b x seq len x hid dim\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # y = ax + b(task_emb)\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(\n            attention_scores\n        )  # b x num heads x seq length x head dim\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs)\n            if self.output_attentions\n            else (context_layer,)\n        )\n        return outputs\n\n\nclass MyBertSelfOutput9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertOutput9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAttention9(BertAttention):\n    def __init__(self, config, add_conditional_layernorm=True):\n        super().__init__(config)\n        self.self = MyBertSelfAttention9(config)\n        self.add_conditional_layernorm = add_conditional_layernorm\n        if add_conditional_layernorm:\n            self.output = MyBertSelfOutput9(config)\n        else:\n            self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            task_embedding=task_embedding,\n        )\n        if self.add_conditional_layernorm:\n            attention_output = self.output(self_outputs[0], hidden_states, task_embedding, task_id)\n        else:\n            attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass BertAdapter9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bottleneck = ConditionalBottleNeck(config)\n        self.condlayernorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, bert_layer_input, hidden_states, task_embedding, task_id):\n        hidden_states = self.bottleneck(task_embedding, hidden_states)\n        hidden_states = self.condlayernorm(hidden_states + bert_layer_input, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAdapterLayer9(nn.Module):\n    \"\"\"Adapter Layer trained from scratch (sub layer names are changed)\"\"\"\n    def __init__(self, config):\n        super(MyBertAdapterLayer9, self).__init__()\n        self.new_attention = MyBertAttention9(config)\n        self.new_intermediate = BertIntermediate(config)\n        self.new_output = MyBertOutput9(config)\n        self.adapter = BertAdapter9(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.new_attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        intermediate_output = self.new_intermediate(attention_output)\n        layer_output = self.new_output(\n            intermediate_output, attention_output, task_embedding=task_embedding, task_id=task_id\n        )\n        adapted_layer_output = self.adapter(\n            attention_output, layer_output, task_embedding=task_embedding, task_id=task_id\n        )\n        outputs = (adapted_layer_output,) + outputs\n        return outputs\n\n\nclass MyBertLayer9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MyBertAttention9(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = MyBertOutput9(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        if self.is_decoder and encoder_hidden_states is not None:\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:]\n            )  # add cross attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, task_embedding, task_id)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass BertLayer9(BertLayer):\n    \"\"\"Same as BertLayer but with different inputs\"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.attention = MyBertAttention9(config, add_conditional_layernorm=False)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass MyBertEncoder9(nn.Module):\n    def __init__(self, config, tasks):\n        super().__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n        num_bert_layers = config.num_hidden_layers//2\n        num_mybert_layers = config.num_hidden_layers//2-1\n        assert num_bert_layers+num_mybert_layers+1 == config.num_hidden_layers\n        self.layer = nn.ModuleList(\n            [BertLayer9(config) for _ in range(num_bert_layers)] +\n            [MyBertLayer9(config) for _ in range(num_mybert_layers)] +\n            [MyBertAdapterLayer9(config)]  # FiLM8\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_type=None,\n        task_embedding=None\n    ):\n        all_hidden_states = ()\n        all_attentions = ()\n        task_embedding = self.task_transformation(task_embedding)\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                head_mask[i],\n                encoder_hidden_states,\n                encoder_attention_mask,\n                task_embedding,\n                task_type\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass CaMtlBaseEncoder(BertPreTrainedModel):\n    r\"\"\"\n    # NOTE: Combination of: (might work best for base) and uses:\n        -- block diagonal attention\n        -- conditional layer norm for the top half layers base=6-10 and large=11-22, top layer excluded\n        -- conditional bias attention term to the original attention matrix\n        -- conditional adapter for the top layer only at layer=11 for base and layer=23 for large\n        -- conditional alignment after the embedding layer only\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertFiLMModel.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n\n    def __init__(self, config, data_args=None, **kwargs):\n        super().__init__(config)\n        tasks = data_args.tasks\n        self.task_id_2_task_idx = {i: i for i, t in enumerate(tasks)}\n        self.config = config\n        self.config.num_tasks = len(tasks)\n        config.max_seq_length = data_args.max_seq_length\n        self.task_type_embeddings = nn.Embedding(len(tasks), config.hidden_size)\n        self.conditional_alignment = FiLM(\n            config.hidden_size, config.hidden_size\n        )  # FiLM5\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = MyBertEncoder9(config, tasks)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass on the Model.\n\n        The model can behave as an encoder (with only self-attention) as well\n        as a decoder, in which case a layer of cross-attention is added between\n        the self-attention layers, following the architecture described in:\n        .. _`Attention is all you need`:\n            https://arxiv.org/abs/1706.03762\n\n        \"\"\"\n        task_type = self._create_task_type(kwargs[\"task_id\"])\n        task_embedding = self.task_type_embeddings(task_type)\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                causal_mask = causal_mask.to(\n                    torch.long\n                )  # not converting to long will cause errors with pytorch version < 1.3\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\n            if encoder_attention_mask.dim() == 3:\n                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n            elif encoder_attention_mask.dim() == 2:\n                encoder_extended_attention_mask = encoder_attention_mask[\n                    :, None, None, :\n                ]\n            else:\n                raise ValueError(\n                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n                        encoder_hidden_shape, encoder_attention_mask.shape\n                    )\n                )\n\n            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # fp16 compatibility\n            encoder_extended_attention_mask = (\n                1.0 - encoder_extended_attention_mask\n            ) * -10000.0\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n        embedding_output = self.conditional_alignment(\n            x_cond=task_embedding,\n            x_to_film=embedding_output,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            task_type=task_type,\n            task_embedding=task_embedding,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def _create_task_type(self, task_id):\n        task_type = task_id.clone()\n        unique_task_ids = torch.unique(task_type)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        for unique_task_id in unique_task_ids_list:\n            task_type[task_type == unique_task_id] = self.task_id_2_task_idx[\n                unique_task_id\n            ]\n        return task_type\n\n    @classmethod\n    def get_layer_regexp(cls):\n        return r\"encoder.layer.*\\\\.([0-9]+)\\\\..*\"\nimport math\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_bert import (\n    BertEmbeddings,\n    BertPooler,\n    BertPreTrainedModel,\n    BertAttention,\n    BertIntermediate,\n    BertLayer,\n    BertSelfOutput,\n)\n\nfrom src.model.encoders.conditional_modules import FiLM, CBDA, ConditionalLayerNorm, ConditionalBottleNeck\n\n\nclass MyBertSelfAttention10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.max_seq_length = config.max_seq_length\n        assert config.hidden_size % self.max_seq_length == 0, \\\n            \"Block decomposed attention will only work if this condition is met.\"\n        self.num_blocks = config.hidden_size // self.max_seq_length\n        self.cond_block_diag_attn = CBDA(\n            config.hidden_size, math.ceil(self.max_seq_length / self.num_blocks), self.num_blocks\n        )  # d x L/N\n\n        self.random_weight_matrix = nn.Parameter(\n            torch.zeros(\n                [config.max_seq_length, math.ceil(self.max_seq_length / self.num_blocks)]\n            ),\n            requires_grad=True,\n        )\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        if encoder_hidden_states is not None:\n            mixed_value_layer = self.value(encoder_hidden_states)\n            attention_mask = encoder_attention_mask\n        else:\n            mixed_value_layer = self.value(hidden_states)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        mixed_key_layer = self.key(hidden_states)\n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        attention_scores1 = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n\n        attention_scores2 = self.cond_block_diag_attn(\n            x_cond=task_embedding,\n            x_to_film=self.random_weight_matrix,\n        )\n\n        attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n        # b x seq len x hid dim\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # y = ax + b(task_emb)\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(\n            attention_scores\n        )  # b x num heads x seq length x head dim\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs)\n            if self.output_attentions\n            else (context_layer,)\n        )\n        return outputs\n\n\nclass MyBertSelfOutput10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertOutput10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAttention10(BertAttention):\n    def __init__(self, config, add_conditional_layernorm=True):\n        super().__init__(config)\n        self.self = MyBertSelfAttention10(config)\n        self.add_conditional_layernorm = add_conditional_layernorm\n        if add_conditional_layernorm:\n            self.output = MyBertSelfOutput10(config)\n        else:\n            self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            task_embedding=task_embedding,\n        )\n        if self.add_conditional_layernorm:\n            attention_output = self.output(self_outputs[0], hidden_states, task_embedding, task_id)\n        else:\n            attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass MyBertLayer10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MyBertAttention10(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = MyBertOutput10(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        if self.is_decoder and encoder_hidden_states is not None:\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:]\n            )  # add cross attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, task_embedding, task_id)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass BertLayer10(BertLayer):\n    \"\"\"Same as BertLayer but with different inputs\"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.attention = MyBertAttention10(config, add_conditional_layernorm=False)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass MyBertEncoder10(nn.Module):\n    def __init__(self, config, tasks):\n        super().__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n        num_bert_layers = config.num_hidden_layers//2 + config.num_hidden_layers % 2\n        num_mybert_layers = config.num_hidden_layers//2\n        assert num_bert_layers+num_mybert_layers == config.num_hidden_layers\n        self.layer = nn.ModuleList(\n            [BertLayer10(config) for _ in range(num_bert_layers)] +\n            [MyBertLayer10(config) for _ in range(num_mybert_layers)]\n        )\n        assert len(self.layer) == config.num_hidden_layers\n        #FiLM6\n        self.adapter_layer = nn.ModuleList(\n            [\n                ConditionalBottleNeck(config)\n                for _ in range(config.num_hidden_layers)\n            ]\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_type=None,\n        task_embedding=None\n    ):\n        all_hidden_states = ()\n        all_attentions = ()\n        task_embedding = self.task_transformation(task_embedding)\n        hidden_film = torch.zeros_like(hidden_states)\n        for i, (layer_module, adapter_module) in enumerate(zip(self.layer, self.adapter_layer)):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                head_mask[i],\n                encoder_hidden_states,\n                encoder_attention_mask,\n                task_embedding,\n                task_type\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n            # FiLM layer with a skip connection\n            hidden_film = adapter_module(\n                x_cond=task_embedding, hidden_states=hidden_states + hidden_film\n            )\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_film,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass CaMtlLargeEncoder(BertPreTrainedModel):\n    r\"\"\"\n    # NOTE: Combination of: (might work best for large) and uses:\n        -- block diagonal attention\n        -- conditional layer norm for the top half layers base=6-10 and large=11-22, top layer excluded\n        -- conditional bias attention term to the original attention matrix\n        -- down/projection bottleneck for all layers\n        -- conditional alignment after the embedding layer only\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertFiLMModel.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n\n    def __init__(self, config, data_args=None, **kwargs):\n        super().__init__(config)\n        tasks = data_args.tasks\n        self.task_id_2_task_idx = {i: i for i, t in enumerate(tasks)}\n        self.config = config\n        self.config.num_tasks = len(tasks)\n        config.max_seq_length = data_args.max_seq_length\n        self.task_type_embeddings = nn.Embedding(len(tasks), config.hidden_size)\n        self.conditional_alignment = FiLM(\n            config.hidden_size, config.hidden_size\n        )  # FiLM5\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = MyBertEncoder10(config, tasks)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass on the Model.\n\n        The model can behave as an encoder (with only self-attention) as well\n        as a decoder, in which case a layer of cross-attention is added between\n        the self-attention layers, following the architecture described in:\n        .. _`Attention is all you need`:\n            https://arxiv.org/abs/1706.03762\n\n        \"\"\"\n        task_type = self._create_task_type(kwargs[\"task_id\"])\n        task_embedding = self.task_type_embeddings(task_type)\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                causal_mask = causal_mask.to(\n                    torch.long\n                )  # not converting to long will cause errors with pytorch version < 1.3\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\n            if encoder_attention_mask.dim() == 3:\n                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n            elif encoder_attention_mask.dim() == 2:\n                encoder_extended_attention_mask = encoder_attention_mask[\n                    :, None, None, :\n                ]\n            else:\n                raise ValueError(\n                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n                        encoder_hidden_shape, encoder_attention_mask.shape\n                    )\n                )\n\n            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # fp16 compatibility\n            encoder_extended_attention_mask = (\n                1.0 - encoder_extended_attention_mask\n            ) * -10000.0\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n        embedding_output = self.conditional_alignment(\n            x_cond=task_embedding,\n            x_to_film=embedding_output,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            task_type=task_type,\n            task_embedding=task_embedding,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def _create_task_type(self, task_id):\n        task_type = task_id.clone()\n        unique_task_ids = torch.unique(task_type)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        for unique_task_id in unique_task_ids_list:\n            task_type[task_type == unique_task_id] = self.task_id_2_task_idx[\n                unique_task_id\n            ]\n        return task_type\n\n    @classmethod\n    def get_layer_regexp(cls):\n        return r\"encoder.layer.*\\\\.([0-9]+)\\\\..*\"\nimport torch\nimport numbers\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FiLM(nn.Module):\n    \"\"\" Feature-wise Linear Modulation (FiLM) layer\"\"\"\n    def __init__(self, input_size, output_size, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(FiLM, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        return out\n\n\nclass CBDA(nn.Module):\n    \"\"\" Conditional Block Diagonal Attention (CBDA) layer\"\"\"\n    def __init__(self, input_size, output_size, blocks=1, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(CBDA, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        self.blocks = blocks\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        out = [torch.block_diag(*list(out_b.chunk(self.blocks, 0))) for out_b in out]\n        out = torch.stack(out)\n        return out[:, :, :out.size(1)]\n\n\nclass ConditionalLayerNorm(nn.Module):\n    r\"\"\"Applies Conditional Layer Normalization over a mini-batch of inputs.\n\n    .. math::\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma(z) + \\beta(z)\n\n    The mean and standard-deviation are calculated separately over the last\n    certain number dimensions which have to be of the shape specified by\n    :attr:`normalized_shape`.\n    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n\n    .. note::\n        Unlike Batch Normalization and Instance Normalization, which applies\n        scalar scale and bias for each entire channel/plane with the\n        :attr:`affine`, Layer Normalization applies per-element scale and\n        bias with :attr:`elementwise_affine`.\n\n    This layer uses statistics computed from input data in both training and\n    evaluation modes. The affine transformation is molulated by a conditional tensor.\n    In our case, we use task embeddings z.\n\n    Args:\n        normalized_shape (int or list or torch.Size): input shape from an expected input\n            of size\n\n            .. math::\n                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n\n            If a single integer is used, it is treated as a singleton list, and this module will\n            normalize over the last dimension which is expected to be of that specific size.\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        elementwise_affine: a boolean value that when set to ``True``, this module\n            has learnable per-element affine parameters initialized to ones (for weights)\n            and zeros (for biases). Default: ``True``.\n\n    Shape:\n        - Input: :math:`(N, *)`\n        - Output: :math:`(N, *)` (same shape as input)\n\n    Examples::\n\n        >>> input_ = torch.randn(20, 5, 10, 10)\n        >>> condition = torch.randn(20, 10)\n        >>> # With Learnable Parameters\n        >>> m = ConditionalLayerNorm([10, 10])\n        >>> # Normalize over last dimension of size 10\n        >>> m = nn.LayerNorm(10)\n        >>> # Activating the module\n        >>> output = m(input_, condition)\n\n    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450\n    .. _`Conditional Layer Normalization`: https://arxiv.org/\n    \"\"\"\n    __constants__ = ['normalized_shape', 'condition_size', 'weight', 'bias', 'eps']\n\n    def __init__(self, normalized_shape, condition_size, eps=1e-5):\n        super(ConditionalLayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = tuple(normalized_shape)\n\n        self.condition_size = condition_size\n        self.eps = eps\n\n        self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.ln_weight_modulation = FiLM(condition_size, sum(normalized_shape))\n        self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input_, condition, task_id):\n        unique_task_ids = torch.unique(task_id)\n        cln_output = torch.zeros_like(input_)\n        for unique_task_id in unique_task_ids:\n            task_id_filter = task_id == unique_task_id\n            task_emb = condition[task_id_filter][0].unsqueeze(0)\n            weight = self.ln_weight_modulation(task_emb, self.weight).view(-1)\n            cln_output[task_id_filter] = F.layer_norm(input_[task_id_filter], self.normalized_shape, weight, self.bias, self.eps)\n        return cln_output\n\n    def extra_repr(self):\n        return '{normalized_shape}, {condition_size}, eps={eps}'.format(**self.__dict__)\n\n\nclass ConditionalBottleNeck(nn.Module):\n    \"\"\"Down projection and up projection with FiLM layers within Transformer layer.\"\"\"\n    def __init__(self, config):\n        super(ConditionalBottleNeck, self).__init__()\n        self.emb_transf = nn.Linear(config.hidden_size, config.hidden_size)\n        self.hidden_modulation = FiLM(config.hidden_size, config.hidden_size)\n        self.down_proj_layer = nn.Linear(config.hidden_size, config.hidden_size//3)\n        self.up_proj_layer = nn.Linear(config.hidden_size//3, config.hidden_size)\n\n    def forward(self, x_cond, hidden_states):\n        x_cond = self.emb_transf(x_cond)\n        hidden_states = self.hidden_modulation(x_cond=x_cond, x_to_film=hidden_states)\n        hidden_states = self.down_proj_layer(hidden_states)\n        hidden_states = self.up_proj_layer(hidden_states)\n        return hidden_states\nimport os\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional, Callable, Tuple\n\nimport torch\nimport numpy as np\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler\nfrom transformers import Trainer, TrainingArguments, EvalPrediction, glue_output_modes\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup\n\nfrom src.data.glue_utils import compute_glue_metrics\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass MultiTaskTrainingArguments(TrainingArguments):\n    use_mt_uncertainty: bool = field(\n        default=False,\n        metadata={\"help\": \"Use MT-Uncertainty sampling method\"},\n    )\n    uniform_mt_sampling: bool = field(\n        default=False,\n        metadata={\"help\": \"Sample each task an equal amount to times per epoch.\"},\n    )\n    percent_of_max_data_size: float = field(\n        default=1.0,\n        metadata={\n            \"help\": \"If uniform_mt_sampling=True, specify the samples per task per \"\n            \"epoch based on the maximum dataset length. If below 0.0 or above 1.0,\"\n            \"it will be set to the closest of 0.0 or 1.0.\"\n        },\n    )\n    warmup_proportion: float = field(\n        default=0.1,\n        metadata={\"help\": \"0.0 to args.lr for warmup_proportion * num_training_steps\"},\n    )\n\n\nclass MultiTaskTrainer(Trainer):\n    def __init__(\n        self,\n        tokenizer,\n        data_args,\n        eval_datasets=None,\n        test_datasets=None,\n        *args,\n        **kwargs,\n    ):\n        super(MultiTaskTrainer, self).__init__(*args, **kwargs)\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        self.eval_datasets = eval_datasets\n        self.test_datasets = test_datasets\n        self.eval_results = {}\n\n    def get_optimizers(\n        self, num_training_steps: int\n    ) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n        \"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well.\n        If you want to use something else, you can pass a tuple in the Trainer's init,\n        or override this method in a subclass.\n        \"\"\"\n        if self.optimizers is not None:\n            return self.optimizers\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": self.args.weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        num_warmup_steps = (\n            self.args.warmup_proportion * num_training_steps\n        )  # this is different from overridden function\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.args.learning_rate,\n            eps=self.args.adam_epsilon,\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,  # this is different from overridden function\n        )\n        return optimizer, scheduler\n\n    def get_train_dataloader(self) -> DataLoader:\n        if self.args.use_mt_uncertainty:\n            return self._create_custom_dataloader()\n        else:\n            return super().get_train_dataloader()\n\n    def _create_custom_dataloader(self):\n        class MtUcertaintyIterator:\n            \"\"\"Sample tasks using uncertainty measure.\"\"\"\n\n            def __init__(self, my_loader):\n                self.my_loader = my_loader\n                self.loader_iters = [iter(loader) for loader in self.my_loader.loaders]\n                self.loader_iter_sizes = [len(i) for i in self.loader_iters]\n                self.max_count = len(self.my_loader)\n                self.batch_count = 0\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self.batch_count == self.max_count:\n                    self.batch_count = 0\n                    raise StopIteration()\n\n                test_batch = {}\n                for idx, loader_iter in enumerate(self.loader_iters):\n                    try:\n                        batch = loader_iter.__next__()\n                    except StopIteration:\n                        new_loader_iter = iter(self.my_loader.loaders[idx])\n                        self.loader_iters[idx] = new_loader_iter\n                        batch = new_loader_iter.__next__()\n\n                    test_batch = self.batchify_data(batch, test_batch)\n\n                inputs = {}\n                for k, v in test_batch.items():\n                    if k not in [\"labels\"]:\n                        inputs[k] = v.detach().to(self.my_loader.args.device)\n\n                with torch.no_grad():\n                    model.select_batch_mode = True\n                    outputs = model(**inputs)\n                    model.select_batch_mode = False\n\n                (\n                    test_batch_entropy,\n                    test_batch_entropy_mean,\n                    max_mean_batch_entropy,\n                ) = outputs[-3:]\n\n                for _, v in inputs.items():\n                    del v  # free GPU mem\n                del inputs\n\n                test_batch_entropy_mean = (\n                    test_batch_entropy_mean / max_mean_batch_entropy\n                )\n                test_batch_entropy = test_batch_entropy * test_batch_entropy_mean\n\n                if \"sts-b\" in tasks and \"mrpc\" in tasks:\n                    stsb_idx = test_batch[\"task_id\"] == tasks.index(\"sts-b\")\n                    mrpc_idx = test_batch[\"task_id\"] == tasks.index(\"mrpc\")\n                    num_items = min(\n                        len(test_batch_entropy[stsb_idx]),\n                        len(test_batch_entropy[mrpc_idx]),\n                    )\n                    stsb_idx = stsb_idx.nonzero()[:num_items]\n                    mrpc_idx = mrpc_idx.nonzero()[:num_items]\n                    test_batch_entropy[stsb_idx] = test_batch_entropy[mrpc_idx]\n                    test_batch_entropy_mean[stsb_idx] = test_batch_entropy_mean[\n                        mrpc_idx\n                    ]\n\n                select_size = min(\n                    self.my_loader.args.train_batch_size,\n                    test_batch[\"input_ids\"].shape[0],\n                )  # Handled the last batch if it is lower than the batch size\n\n                top_entropy = torch.topk(test_batch_entropy, select_size)\n\n                for k, v in test_batch.items():\n                    test_batch[k] = torch.index_select(v, 0, top_entropy.indices)\n\n                self.batch_count += 1\n\n                return test_batch\n\n            @staticmethod\n            def batchify_data(data, curr_batch):\n                for k in data.keys():\n                    if k in curr_batch.keys():\n                        curr_batch[k] = torch.cat((curr_batch[k], data[k]), dim=0)\n                    else:\n                        curr_batch[k] = data[k]\n                return curr_batch\n\n        class CustomLoader:\n            def __init__(self, loaders, datasets, loader_args):\n                self.loaders = loaders\n                self.dataset = datasets\n                self.args = loader_args\n                self.current_epoch = 0\n\n            def __iter__(self):\n                iterator = MtUcertaintyIterator(self)\n\n                # for determinism across runs\n                # https://github.com/pytorch/examples/issues/501\n                for l in self.loaders:\n                    if isinstance(l.sampler, DistributedSampler):\n                        l.sampler.set_epoch(self.current_epoch)\n                self.current_epoch += 1\n                return iterator\n\n            def __len__(self):\n                loader_len = [len(loader) for loader in self.loaders]\n                if self.args.uniform_mt_sampling:\n                    return int(\n                        self.args.percent_of_max_data_size\n                        * max(loader_len)\n                        * len(self.loaders)\n                        / self.args.train_batch_size\n                    )\n                elif self.args.use_mt_uncertainty:\n                    return int(\n                        max(loader_len)\n                        * len(self.loaders)\n                        * self.args.percent_of_max_data_size\n                    )\n                else:\n                    return sum(loader_len)\n\n        model = self.model\n        tasks = self.data_args.tasks\n\n        data_loaders = []\n        for dataset in self.train_dataset.datasets:\n            train_sampler = (\n                RandomSampler(dataset)\n                if self.args.local_rank == -1\n                else DistributedSampler(dataset)\n            )\n\n            data_loader = DataLoader(\n                dataset,\n                batch_size=self.args.train_batch_size,\n                sampler=train_sampler,\n                collate_fn=self.data_collator.collate_batch,\n            )\n            data_loaders.append(data_loader)\n\n        return CustomLoader(data_loaders, self.train_dataset, self.args)\n",
        "experimental_info": "### Model Configuration (from `CaMtlArguments`):\n- `model_name_or_path`: \"Path to pretrained model or model identifier from: CA-MTL-base, CA-MTL-large, bert-base-cased bert-base-uncased, bert-large-cased, bert-large-uncased\"\n- `freeze_encoder_layers`: \"Freeze encoder layers. format: <start_layer>-<end_layer>\"\n- When `freeze_encoder_layers` is applied, the following modules are specifically unfrozen: `[\"random_weight_matrix\", \"film.gb_weights\", \"ln_weight_modulation.gb_weights\", \"adapter\"]`.\n\n### Data Configuration (from `MultiTaskDataArguments`):\n- `max_seq_length`: Maximum total input sequence length after tokenization, sequences longer are truncated, shorter are padded (default: `128`).\n\n### Training Configuration (from `MultiTaskTrainingArguments` and `MultiTaskTrainer`):\n- `use_mt_uncertainty`: Boolean to enable the Multi-Task Uncertainty sampling method (default: `False`).\n- `uniform_mt_sampling`: Boolean to sample each task an equal amount of times per epoch (default: `False`).\n- `percent_of_max_data_size`: Specifies samples per task per epoch based on the maximum dataset length if `uniform_mt_sampling=True`. If below 0.0 or above 1.0, it will be set to the closest of 0.0 or 1.0 (default: `1.0`).\n- `warmup_proportion`: The proportion of training steps used for linear warmup of the learning rate (default: `0.1`).\n\n### Multi-Task Uncertainty Sampling Mechanism (from `MultiTaskTrainer` and `Decoder`):\n- **Entropy Calculation**: Shannon Entropy is calculated for the logits of each sample after passing through a Softmax function in the `Decoder`.\n- **Entropy Normalization**: The calculated `samples_entropy` is normalized by `max_entropy`, which is the entropy of an even distribution across classes (`1/num_labels`). A small epsilon (`1e-5`) is added for numerical stability.\n- **Uncertainty Prioritization**: In the `_create_custom_dataloader` method:\n    - `batch_entropy`, `batch_entropy_mean`, and `max_mean_batch_entropy` are retrieved from the model's forward pass for a combined batch of samples from all tasks.\n    - `batch_entropy_mean` is normalized by `max_mean_batch_entropy`.\n    - The `batch_entropy` for each sample is then multiplied by this normalized `batch_entropy_mean` to prioritize samples.\n- **Sample Selection**: From the prioritized `batch_entropy` values, `torch.topk` is used to select `select_size` samples with the highest entropy. `select_size` is defined as `min(self.my_loader.args.train_batch_size, test_batch[\"input_ids\"].shape[0])` to handle batches smaller than the `train_batch_size`.\n- **Special Handling for STS-B and MRPC**: If both \"sts-b\" and \"mrpc\" tasks are present, the `test_batch_entropy` and `test_batch_entropy_mean` for a subset of \"sts-b\" items are overwritten with those from \"mrpc\" items, matching the number of items (`num_items = min(len(test_batch_entropy[stsb_idx]), len(test_batch_entropy[mrpc_idx]))`).\n- **Custom Loader Length**: The length of the `CustomLoader` is determined by:\n    - If `uniform_mt_sampling` is `True`: `int(percent_of_max_data_size * max(loader_len) * len(self.loaders) / train_batch_size)`.\n    - If `use_mt_uncertainty` is `True`: `int(max(loader_len) * len(self.loaders) * percent_of_max_data_size)`.\n    - Otherwise, it's the sum of all individual loader lengths."
      }
    },
    {
      "title": "Multi-Head Adapter Routing for Cross-Task Generalization",
      "abstract": "Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists\nin pre-training adapters on a multi-task training set before few-shot\nadaptation to test tasks. Polytropon [Ponti et al., 2023] ($\\texttt{Poly}$)\njointly learns an inventory of adapters and a routing function that selects a\n(variable-size) subset of adapters for each task during both pre-training and\nfew-shot adaptation. In this paper, we investigate the role that adapter\nrouting plays in its success and design new variants based on our findings.\nFirst, we build on the intuition that finer-grained routing provides more\nexpressivity. Hence, we propose $\\texttt{MHR}$ (Multi-Head Routing) which\ncombines subsets of adapter parameters and outperforms $\\texttt{Poly}$ under a\ncomparable parameter budget; by only fine-tuning the routing function and not\nthe adapters ($\\texttt{MHR}$-$z$) we achieve competitive performance with\nextreme parameter efficiency. Second, we find that\n$\\texttt{Poly}$/$\\texttt{MHR}$ performance is a result of better multi-task\noptimization, rather than modular inductive biases that facilitate adapter\nrecombination and local adaptation, as previously hypothesized. In fact, we\nfind that $\\texttt{MHR}$ exhibits high gradient alignment between training\ntasks. We find that routing is most beneficial during multi-task pre-training\nrather than during few-shot adaptation and propose $\\texttt{MHR}$-$\\mu$, which\ndiscards routing and fine-tunes the average of the pre-trained adapters on each\ndownstream tasks. This establishes $\\texttt{MHR}$-$\\mu$ as an effective method\nfor single-adapter fine-tuning. We also show that $\\texttt{MHR}$-$\\mu$ can be\nused as an effective zero-shot transfer method by training the average of the\npre-trained adapters for a few additional steps on the multi-task training set:\nthis yields gains up to 3% on absolute accuracy w.r.t. the baselines.",
      "full_text": "Multi-Head Adapter Routing for Cross-Task Generalization Lucas Caccia Edoardo Ponti Zhan Su Matheus Pereira Nicolas Le Roux Alessandro Sordoni Microsoft Research, McGill University, MILA, University of Edinburgh, Université de Montréal, University of Copenhagen lucas.page-caccia@mail.mcgill.ca,alsordon@microsoft.com Abstract Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ( Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose MHR (Multi-Head Routing), which combines blocks of parameters from different adapters and outperforms Poly under a comparable parameter budget; by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency. Second, we find that Poly/MHR performance is a result of better multi-task optimization, rather than modular inductive biases that facilitate adapter recombination and local adaptation, as previously hypothesized. In fact, we find that MHR exhibits high gradient alignment between training tasks. We find that routing is most beneficial during multi-task pre-training rather than during few-shot adaptation and propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks. This establishes MHR-µ as an effective method for single-adapter fine-tuning. We also show thatMHR-µ can be used as an effective zero-shot transfer method by training the average of the pre-trained adapters for a few additional steps on the multi-task training set: this yields gains up to 3% on absolute accuracy w.r.t. the baselines. Code is available at https://github.com/microsoft/mttl. 1 Introduction The ability to train effective models with a relatively small number of training data is of paramount importance due to the paucity of annotated examples for most tasks. One effective few-shot learning approach is to leverage large models pre-trained on a vast amount of unlabelled data and fine-tune them on the few examples available for each downstream task. To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022], SFT [Ansell et al., 2022], or (IA)3 [Liu et al., 2022]. These only fine-tune adapters while leaving the pre-trained model ‘frozen’. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2211.03831v3  [cs.AI]  13 Nov 2023Add & Norm Feed-Forward  Multi-Head  Attention Add & Norm Wpretrained     LoRA + A BT Head # 2 Head # 1 Tasks Active Task Modules Z = Multi-Head Routing  (MHR) Head # 2 Head # 1 GLUE-rte … ZMHR- Q W0 K V O W1 Figure 1: Left: A LoRA adapter with weight AB⊤ is trained on top of a frozen, pre-trained linear layer W. Our method MHR partitions the A, Bparameter indexes into h blocks (or heads). For each block, a separate routing function selects the active modules for the current task amongm copies with different parameter values, and combines them via averaging to form a task-specific head. The heads are then concatenated to form the LoRA adapter. Using multiple heads allows for more fine-grained mixing of task parameters with a negligible increase in overall parameter count. Right: During few-shot adaptation, one can fine-tune only the multi-head routing parameters (MHR-z), keeping the modules frozen, resulting in highly parameter-efficient adaptation. Nevertheless, it remains unclear how to best exploit a set of training tasks to better generalize to a set of unseen test tasks in a sample-efficient fashion, based on just a few examples. One straightforward solution is to perform multi-task pre-training, i.e. first train the large model on the union of the examples from the training tasks, then fine-tune the obtained model to the test task [Liu et al., 2022, Ye et al., 2021]. However, this solution does not take into account that test tasks may require solving different combinations of sub-problems compared to training tasks [Vu et al., 2020], thus failing to achieve compositional generalization [Rosenbaum et al., 2019, Ponti, 2021]. Moreover, specializing the model towards different tasks during training may result in negative transfer, due to their corresponding gradients being misaligned [Wang et al., 2021]. Several PEFT approaches have been proposed to enable better cross-task generalization by training adapters (or soft prompts) on each task independently [Pfeiffer et al., 2021, Vu et al., 2021, Asai et al., 2022, Chronopoulou et al., 2023]. Given a new test task, parameters from similar training tasks are aggregated, which enables transfer. While solely having task-specific parameters is an effective strategy to mitigate interference across training tasks, it also inhibits any positive transfer within the same task pool. Polytropon (Poly) was recently proposed by Ponti et al. [2023] to address these issues: the model assumes that task-specific adapters are learned combinations of a reusable inventory of basis adapters ormodules. In practice, each module is implemented as a LoRA [Hu et al., 2022] adapter, which modifies a large pre-trained model, such as T5 [Raffel et al., 2020]. During both multi-task pre-training and few-shot adaptation, Poly learns both the inventory of adapters and a (continuously relaxed) binary task–module routing matrix, which determines which module is active for each task. While Poly shows promising results, several questions remain unanswered: 1) Does the expressivity of the routing function matter? 2) Why do routing-based PEFT methods yield superior performance? 3) Is routing useful during both multi-task pre-training and few-shot adaptation? To answer the first question, we propose a new routing function, MHR, that mixes adapters at a more granular level. Differently from Poly, where routing decisions are made for each adapter as a whole, in MHR we linearly combine blocks of the adapter dimensions (i.e. heads), each with different combination coefficients. We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]. Based on our results, we report thatMHR outperforms Poly and single adapter baselines. Additionally, we show that, thanks to the increased expressivity of the routing function, it becomes possible to fine-tune only the parameters of the routing function (and not the adapters) during few-shot adaptation: the resulting method, MHR-z, yields competitive performance while requiring orders of magnitude fewer parameters. 2Regarding the second and third questions, we uncover that optimization during multitask pretraining plays a key role in explaining the downstream performance of routing-based PEFT approaches. Specifically, we find that MHR exhibits a higher cosine similarity between gradients from different tasks than Poly and single-adapter multi-task training. Hence, routing enables more knowledge transfer and less interference across tasks during multi-task pre-training. This finding led us to investigate whether routing is useful also during few-shot adaptation. It has been hypothesized [Ponti et al., 2023] that one of the reasons behind Poly’s performance resides in the inductive bias of the modular architecture, which allows test tasks to recombine and locally adapt the most relevant modules. To test this hypothesis, we propose MHR-µ, where the routing function is discarded and all available adapter parameters are averaged before few-shot adaptation. We find that MHR-µ can recover the performance of MHR, hinting that Poly/MHR gains are only a result of better multi-task optimization. Finally, we show that MHR-µ can also be used as an effective zero-shot transfer method by training the average of the pre-trained adapters for a few additional steps on the multi-task training set. This yields gains up to 3% on absolute accuracy w.r.t. to strong baselines such as T0-11B. 2 Background In cross-task generalization, we are given a set of tasks T = {T1, ..,T|T |}, with each task Ti dataset containing a set of samples Di = {(x1, y1), ...,(xn, yn)}. The set of all tasks is partitioned into training and test tasks, T = Ttrain ∪ Teval, and the objective is to leverage data inTtrain and transfer knowledge to facilitate learning of the test tasks Teval. For all the methods discussed, learning takes place in two phases, excluding the original unsupervised pre-training of the language model backbone on a separate corpus. The first phase consists of multi-task pre-training, in which either an adapter, such as LoRA or (IA)3, or the full backbone is trained on the set of training tasks Ttrain. The second phase consists in few-shot adaptation, where the learned adapters are fine-tuned independently on each test task in Teval. We follow the procedure from [Raffel et al., 2020] and formulate each task as a text-to-text problem, enabling standard maximum-likelihood training with teacher forcing [Bengio et al., 2015] and a cross-entropy loss. 2.1 Adapters: LoRA & (IA)3 LoRA [Hu et al., 2022] and (IA)3 [Liu et al., 2022] are two recently proposed adapter architectures that achieve competitive trade-offs between performance and parameter efficiency [Karimi Mahabadi et al., 2021, Liu et al., 2022]. For each linear transformation corresponding to the query (q), key (k), value (v) and output (o) of the self-attention layers, LoRA modifies the base model parameters as follows: hq,k,v,o = Wq,k,v,o 0 x + s · Aq,k,v,o(Bq,k,v,o)⊤x, (LoRA) where W0 are the (frozen) weights of the pre-trained model (e.g. T5 [Raffel et al., 2020]). A, B ∈ Rd×r are low-rank learnable parameters and s ≥ 1 is a tunable scalar hyperparameter. (IA)3, on the other hand, modifies key and value representations in self-attention element-wise, and also modifies the feed-forward MLP (f): hk,v = lk,v ⊙ (Wk,v 0 x); hf = (lf ⊙ γ(Wf 1 x))Wf 2 , ((IA)3) where lk,v,f ∈ Rd are learnable parameters , Wf 1,2 the frozen parameters of the feed-forward layer in the backbone, and γ a non-linearity. For clarity, we will drop the superscripts q, k, v, oin the rest of the paper. 2.2 Polytropon: Adapter Routing Typical adapter methods either fully share adapters across tasks or train individual adapters for each task. Poly addresses the multi-task problem by softly sharing adapter parameters across tasks. Each Poly layer contains 1) an inventory of adapter modules M = {ϕ1, . . . , ϕm} with |M| ≪ |T |; 2) a routing function r(·) that chooses which subset of the modules to combine for each task. Each module corresponds to a LoRA adapter, where ϕi are its associated parameters A(i), B(i) ∈ Rd×r. r(·) is implemented as a task–module routing matrix Z ∈ R|T |×|M|. zτ = Zτ,: ∈ R|M| is a routing vector of task Tτ , with cell Zτ,j being the probability logits of using module ϕj for task Tτ in the current layer. Differently from mixture-of-experts [Fedus et al., 2022], which perform 3Method Pre-Training Fine-Tuning Inference Full FT d × d d × d d × d LoRA d × 2r d × 2r d × 2r Poly d × 2r × |M|+ |T | × |M| d × 2r × |M|+ |M| d × 2r Poly-z d × 2r × |M|+ |T | × |M| |M| |M| MHR-µ d × 2r × |M|+ |T | × |M| d × 2r d × 2r MHR-z d × 2r × |M|+ |T | × |M| ×h |M| ×h |M| ×h MHR d × 2r × |M|+ |T | × |M| ×h d × 2r × |M|+ |M| ×h d × 2r Table 1: Number of parameters (per layer) used for each method. The calculation uses LoRA as the base adapter, modifying a linear transform in Rd×d. Note that the total number of parameters changed by Full FT is larger, given that the method also changes parameters for layers not modified by LoRA. token-level top-k routing, Z converges to a binary matrix, defining a soft partition over modules. This is achieved by using a Gumbel-sigmoid distribution [Jang et al., 2017] during training, with ˆZτ,j ∼ Gumbel(Zτ,j ). At each forward pass, Poly can be defined as : Aτ = X i αiA(i); Bτ = X i αiB(i) (Poly) where αi = ˆZτ,iP j ˆZτ,j , and A(i), B(i), Aτ , Bτ ∈ Rd×r. We normalize the mixing coefficients ˆZτ,i for each task to ensure that the number of active modules does not affect the norm ofAτ , Bτ . Overall, this approach enables different subsets of modules to be activated for the current layer and combined in a task-specific way. Following LoRA, the output of the Poly layer is added to the output of the original layer of the frozen backbone: h = W0x + sAτ (Bτ )⊤x. During multi-task pre-training, for each query, key, value, and output projection in self-attention layers, the parameters learned by Poly are the adapter parameters, {Ai, Bi}|M| i=1 , and the routing matrices Z. During fine-tuning, for each test task τ, Poly randomly initialize the routing vector zτ ∈ R1×|M| and fine-tunes both zτ and all the modules parameters M. 3 Multi-Head Adapter Routing ( MHR) In Poly, module combination remains coarse: only linear combinations of modules are possible, and thus the resulting aggregated adapter remains a linear function of the modules. We propose to augment the expressivity of the module combination while keeping the parameter count similar. MHR (Fig. 1) takes inspiration from multi-head attention [Vaswani et al., 2017]: it partitions the input dimensions into h different disjoint blocks, performs a separate Poly-style combination for each of them, and finally concatenates them. This corresponds to learning a different routing matrix Z for each block of input features, therefore enabling the model to select different adapters for different blocks of the input dimensions. This aggregation approach is piecewise linear (i.e., linear within disjoint intervals), which allows for more expressive combinations of modules. In each MHR layer, the routing function is a third-order tensor Z ∈ R|T |×|M|×h, where Z:,:,h ∈ R|T |×|M|is a 2D slice of the tensor Z. A slice represents the routing matrix for each of the h heads. Let us denote with W[k] ∈ R d h ×r the k-th partition along the rows of the matrix W ∈ Rd×r. The adapter parameters Aτ ∈ Rd×r for task τ, and for each adapter layer, are computed as (similarly for Bτ ): Aτ k = X j Aj[k] · ˆZτ,j,k P j ˆZτ,j,k with Aτ k ∈ R d h ×r, (MHR) Aτ = concat(Aτ 1, . . . ,Aτ h) where concat concatenates along the first dimension. Multi-task pre-training and fine-tuning are similar to Poly. Note that MHR results in only a negligible increase in the total amount of parameters, since most of the parameters are contained in the LoRA weights A, B (Tab. 1). 4Routing-Only Fine-Tuning (MHR-z) Prior work [Shao et al., 2023, inter alia] has shown that compositional generalization can be achieved by learning to (re-)combine in novel ways pre-existing modules. We investigate whether fine-tuning the module parameters is really needed for few-shot adaptation in the context of both Poly and MHR. Therefore, we name Poly-z and MHR-z the variants that, during few-shot adaptation, keep the parameters of the modules learned during multi-task pre-training fixed and just update the routing parameters Z. Crucially, this enables highly parameter- efficient adaptation: for LoRA adapters, A and B matrices constitute the overwhelming majority of parameters. Therefore, by freezing the A, B matrices and only updating Z, we can significantly reduce the parameter cost when transferring knowledge to a new task. Adapter Average Fine-Tuning (MHR-µ) To assess the importance of the routing parameters during few-shot adaptation, we propose an additional variant of MHR, MHR-µ, which shares the same multi- task pre-training procedure as MHR, but for each test task τ, fixes zτ = (1/|M|, . . . ,1/|M|) during few-shot adaptation. This is equivalent to discarding the routing parameters and averaging the module parameters into a single one before fine-tuning. Specifically, the adapter used during fine-tuning is initialized with (similarly for Bτ ): Aτ = 1 |M| X i A∗ i ; Aτ ∈ Rd×r (MHR-µ) where A∗ i are the parameters of the adapters after MHR multi-task pre-training. Note that, differently from MHR, MHR-µ fine-tunes the same amount of parameters as the single adapter baseline. Thus, any difference in performance between the single adapter baseline and MHR-µ comes from differences in the adapter initialization and must be due to the optimization process taking place in the multi-task pre-training, before few-shot adaptation. Routing Granularity In the original Poly, Ponti et al. [2023] showed that learning a routing matrix Z for each model layer gave better performance than sharing a single Z matrix across all layers. We therefore investigate whether this holds true also for its multi-head counterpart, MHR. In addition, we explore intermediate approaches between one Z per layer and a single one shared for the entire model. In particular, we consider sharing Z 1) for the adapter layers belonging to the same Transformer block; or 2) for every block of l layers, which enables us to easily trade off expressivity for parameter efficiency. As we will demonstrate in section 5.1, this is an efficient mechanism to navigate this Pareto front in regimes of very small budgets of parameters per task. 4 Experiments Our experimental evaluation aims to answer three research questions: 1) Does the expressivity of the routing function matter? 2) Why do routing-based PEFT methods yield superior performance? 3) Is routing useful during both multi-task pre-training and few-shot adaptation? We first present the baselines and datasets used in our evaluation and then discuss each question in turn.1 4.1 Baselines In addition to Poly, we compare MHR to the following baselines for task-level generalization. LoRA/(IA)3 trains a single adapter common to all pre-training tasks, which is then fine-tuned on each test task separately. This is arguably the most widespread approach for parameter-efficient cross-task generalization [Liu et al., 2022, Pfeiffer et al., 2023]. AdapterSoup Chronopoulou et al. [2023] trains a different adapter for each task. The method only averages the adapter weights of the training tasks most similar to a given test task, before proceeding with few-shot adaptation. To compute task relatedness, we measure the cosine similarity of sentence embeddings for each task averaged over their training dataset. Notably, unlike the methods proposed in this paper, there is no knowledge sharing (nor interference) during multi-task pre-training as task adapters are trained independently. 1We note that all experiments were run on a single NVIDIA A100 GPU. 5T0 Dataset Avg. Test Backbone T5-XL-LM (IA)3 62.40.4 AdapterSoup 62.11.0 LoRA 66.01.6 LoRA-big 65.40.9 Poly-z 66.40.3 Poly 68.01.0 MHR-z 68.30.8 MHR 69.11.0 Backbone T0-3B T-Few Liu et al. [2022] 66.2 0.5 AdapterSoup 66.10.6 LoRA 67.40.8 Poly-z 65.31.0 Poly 69.00.8 MHR-z 68.41.2 MHR 69.31.2 IA3 AdapterSoup LoRA LoRA-big Poly-z Poly MHR 5K 500K 5M 50M 62 63 64 65 66 67 68 69 Adaptation Parameters Accuracy SOTA results MHR-z Figure 2: Left: Results of few-shot adaptation on T0 dataset Sanh et al. [2022]. We report the mean of the best validation accuracy for each test task. Subscripts correspond to standard deviation. Right: Accuracy of PEFT methods on the T0 dataset when applied on top of T5-XL. The x-axis shows the parameter count during the fine-tuning process. 4.2 Datasets We test our methods on the T0 Sanh et al. [2022] evaluation suite, following the same setup as Liu et al. [2022], and SuperNI Wang et al. [2022a], a large-scale dataset with more than 1,600 training tasks. T0 Tasks We follow the pre-training and fine-tuning procedure discussed in Liu et al. [2022], using hyper-parameters and losses suggested in the public codebase for T-Few.2 All methods were tested with T5-XL Raffel et al. [2020] and T0-3B Sanh et al. [2022] as the backbone model. Crucially, T5 is simply pre-trained on (masked) language modelling, whereas T0 is further instruction tuned: in particular, the full model is fine-tuned on examples from multiple training tasks that have been augmented with task instructions. To ensure fairness for all methods, we report the median and standard deviation of the best validation accuracy for each test task across 3 seeds, when evaluated every 50 training epochs. We treat each data subset–template pair as a unique task, yielding a total of 313 tasks. SuperNI To limit computational costs, we report the result on 20 out of 119 test tasks. Tasks were chosen at random, with the requirement that at least 300 examples were available, and were equally split into 100 training, 100 validation and 100 test examples. For every method, we perform early stopping on the validation set. We report results with Rouge-L averaged across 3 seeds. All methods use T5-XL [Raffel et al., 2020] as the backbone and not T0, as T0 training tasks and SuperNI test tasks may overlap. 5 Results and Discussion 5.1 Does the expressivity of the routing function matter? MHR outperforms PEFT approaches We start our analysis by evaluating the effectiveness of our proposed technique when applied over a backbone that has not undergone prior training on 2https://github.com/r-three/t-few 6instruction-following data (T5-XL). As indicated in the T0 benchmark results in the top table of Fig. 2, it is clear that multi-head routing techniques have a distinct advantage, outperforming both single-head routing Poly by 1.1%, and surpassing standard LoRA approaches by an impressive 3.1%. We also study the impact of performing instruction tuning of the full backbone before adapter training. To this end, we also experiment with T0-3B as a backbone. In the bottom table of Fig. 2, we can observe that while the relative gap between MHR and baselines is smaller, multi-head routing still manages to yield favourable results. Hence, the gains of MHR compound with other multi-task methods such as instruction tuning. Finally, we turn our attention towards the SuperNI dataset (Tab. 2). Here, MHR continues to surpass analogous baselines. MHR-z facilitates extreme parameter efficiency Fig. 2 (right) reveals intriguing findings regarding MHR-z. When we restrict training to only the routing parameters Z in the original Poly, the results are unfortunately not up to par with its version where both routing and adapters are updated. However, when we apply the same constraint to MHR, the performance is significantly closer to the optimum achieved in this setting. In fact, MHR-z surpasses prior baselines while simultaneously necessitating fewer parameters for effective adaptation to new tasks. Moreover, by controlling the number of layers which share the same Z allocation (see sec. 3), MHR-z is able to trade-off performance for parameter efficiency, even surpassingPoly-z in settings with only 3K trainable parameters per test task (see also § A.2.1 for a more in-depth analysis). This trend is similarly observed in the SuperNI benchmark (Tab. 2), where updates restricted to the routing parameters yield performance on par with standard fine-tuning. We therefore conclude that the MHR-z represents a robust approach for achieving extreme parameter efficiency in adaptation. SuperNI Dataset Rouge-L LoRA 67.60.8 LoRA-big 67.20.7 Poly-z 64.60.3 Poly 67.80.8 MHR-z 68.00.2 MHR 68.50.3 Table 2: Results on SuperNI dataset. Sub- scripts are standard deviation. Additional routing heads is more beneficial than extra modules In the original Poly ap- proach, a tradeoff between capacity and param- eter efficiency can be achieved by adding ex- tra modules for each adapter layer. However, this results in a linear increase in the number of multi-task parameters, which can become im- practical. To explore a more effective tradeoff, we investigate the option of adding additional routing heads instead of extra modules. Fig 3 (right) presents the comparison between the two approaches. It demonstrates that increasing the number of routing heads leads to better perfor- mance compared to adding more modules. Importantly, the benefit of multi-head routing is twofold: it provides increased expressivity for the model, while also maintaining parameter efficiency. This finding highlights the advantage of multi-head routing as a more effective approach for balancing expressivity and parameter count in few-shot adaptation scenarios. T0 Dataset Avg. Test Backbone T0-11B T-Few Liu et al. [2022] 72.5 0.9 LoRA 72.31.0 Poly-z 70.00.6 Poly 74.90.6 MHR-z 72.90.8 MHR 74.70.6 Table 3: Few-shot results over 11B pa- rameter backbones. Routing-based methods also excel at the 11B scale We proceed to evaluate if Poly and MHR surpass established PEFT approaches when trained over a larger model back- bone. To accomplish this, we employ the 11B version of T0. As depicted in Tab. 3, routing-based methods once again outshine standard adapter training, surpassing our reproduction of the previous state-of-the-art in Liu et al. [2022] by over 2%. We observe that Poly and MHR show similar performance in standard fine-tuning, but MHR z-tuning remains more performant in routing-only fine- tuning. Indeed, MHR-z (221K params) outperforms Poly-z (3.5K params) by 2.9%, while still remaining more param- eter efficient than Liu et al. [2022] (1.1M params). 5.2 Why do routing-based PEFT methods yield superior performance? While our proposed methods have demonstrated promising results across a broad spectrum of datasets and varying adaptation parameter budgets, the question of why routing-based PEFT exhibits superior 7102 103 104 105 Training Steps 0 2 4 6 8 10 12 14Cumulative Cosine Similarity Cumulative Gradient Cosine Similarity  MHR Poly LoRA 20 40 60 80 100 120 140 Multi-T ask pre-training trainable params (M) 67.8 68.0 68.2 68.4 68.6 68.8 69.0T0 T est Accuracy 8 modules 32 modules 64 modules 8 heads 32 heads 64 heads 128 heads T est Performance w.r.t Trainable Parameter Budget MHR Poly Figure 3: Left: Gradient alignment between tasks during multi-task pretraining. Right: Increasing the number of heads offer better scaling properties than increasing the number of modules. performance remains unanswered. In this section, we aim to uncover the key components that drive MHR’s superior performance. Learning the Routing Function is essential Given that Poly and MHR have access to more parameters than standard adapters during multi-task pretraining, we investigate whether this, and not the routing mechanism, is responsible for their superior performance. To do so, we compare them to a baseline approach. Instead of learning the routing function, we randomly assign a binary module allocation to each data point in a minibatch, disregarding task information. This random routing approach, akin to Wang et al. [2022b], allows us to directly assess the influence of additional parameters during multi-task training. At test time, the learned modules are averaged into a single one before fine-tuning; we therefore refer to this baseline as Random-µ. On the T0 benchmark with the T5-XL backbone, Random-µ performs similarly to a standard LoRA adapter (66.0%), while Poly and MHR outperform it by 2% and 3.1% respectively. Therefore, we conclude that learning a routing function is crucial, and merely increasing capacity during training does not directly lead to improvements. MHR fosters transfer and mitigates interference across pretraining tasks Recognizing the pivotal role of the multi-task pretraining step in bolstering Poly’s performance, we explore the extent of transfer and interference across training tasks. By monitoring the average gradient alignment for each task pair (in terms of cosine similarity) throughout the training process, we are able to gauge the level of positive transfer. As Fig. 3 (left) shows, MHR displays a greater degree of gradient cosine similarity across tasks compared to other PEFT alternatives, including Poly. This finding suggests that the enhanced flexibility offered by multi-head routing may serve to mitigate interference across tasks to a larger extent than standard routing while simultaneously promoting positive transfer. 5.3 Is routing important for task generalization? T0 Dataset Test Acc. LoRA 66.01.6 AdapterSoup 62.11.0 Poly 68.00.8 Poly-µ 67.80.6 MHR 69.11.1 MHR-µ 69.10.9 SuperNI Rouge-L LoRA 67.60.8 Poly 67.80.8 Poly-µ 68.30.5 MHR 68.50.6 MHR-µ 68.50.8 Table 4: Evaluating the impact of modular adaptation at test time. We assessed the importance of routing during pre- training. We now proceed to verify whether it is important to learn routing during few-shot adapta- tion, too. Poly-µ and MHR-µ consistently outper- form LoRA, and match the performance ofPoly / MHR (Tab. 4). This demonstrates that, for few-shot adap- tation, the average of the pre-trained modules pro- vides a better initialization than learning an adapter shared across all the tasks during pre-training. The consistently superior performance of Poly-µ with respect to Random-µ and AdapterSoup stresses the importance of routing during multi-task pre-training (but not during adaptation), which provides an effec- tive adapter initialization for few-shot learning. This 8Method Zero-Shot Test with k-shot Extra Training k = 0 k = 1000 k = 5000 k = 10000 Backbone T5-XL-LM 43.2 LoRA 56.5 56.0 56.1 55.7 Poly-µ 46.0 53.0 56.8 56.3 MHR-µ 48.0 58.0 57.1 56.3 Backbone T0-11B [Sanh et al., 2022] 61.0 LoRA 61.2 61.6 61.5 61.5 Poly-µ 62.1 63.6 63.9 64.4 MHR-µ 63.5 64.5 64.5 64.4 Table 5: Zero-shot performance for MHR and the baselines, reported as the average over the 11 evaluation datasets from Sanh et al. [2022]. To obtain these zero-shot results, we average the learnt Poly/MHR adapters before performing k additional fine-tuning steps on the multi-task pretraining data. This effectively enables zero-shot transfer to downstream tasks using the same amount of parameters/flops as the baseline LoRA. MHR outperform baseline LoRA by up to 3% absolute accuracy points on T0-11B. finding could potentially inspire future work for improving meta-learning and weight-averaging approaches [Izmailov et al., 2018]. MHR-µ excels at zero-shot learning For many downstream tasks of interest, additional labelled data may not be available. In such settings, it is unclear how to leverage MHR-µ and Poly-µ methods. To address this, we fine-tune the average of the multi-task trained adapters on the multi-task pre- training data (instead of using the downstream few-shot data), for an additional k steps. The results are presented in Table 5. We find that without any additional fine-tuning ( k = 0), averaging the adapters does not yield good results. This is due to a potential mismatch between adapters learned via task-specific routing, and the uniform routing strategy. We can observe that when fine-tuning the average of the adapters on the multi-task pre-training data for an additional k steps, MHR-µ show strong performance when evaluated in a zero-shot manner. For a fair comparison, we also additionally fine-tune LoRA for the same number of additional steps. Our best model achieves a zero-shot performance of 64.5 on top of T0-11B, achieving an absolute gain of 3.5% accuracy points. 6 Related Work Multi-task learning is effective for low-resource tasks [Wei et al., 2022, Aribandi et al., 2022, Sanh et al., 2022], as knowledge can be borrowed from similar tasks by sharing the model parameters. Multi- task learning has also been applied across languages and modalities [Ponti et al., 2019, Bugliarello et al., 2022]. In the context of NLP, several families of methods enable learning new tasks from a limited set of labelled examples. Few-shot in-context learning [ICL; Brown et al., 2020], where examples of a new task are concatenated into an input prompt, enables models to generalize tounseen tasks without any gradient-based training. Such approaches are however sensitive to the prompt format and example ordering [Zhao et al., 2021]. More importantly, ICL methods incur a significant compute overhead, as for every prediction, the full set of examples must be processed by the model [Liu et al., 2022]. To remedy this, many parameter-efficient fine-tuning (PEFT) methods have been proposed as an alternative to ICL, where a small number of new parameters are added over the frozen pretrained network. To name a few, LoRA [Hu et al., 2022] injects learnable low-rank matrices into each Transformer layer. Alternatively, the learnable matrix can be sparse, selecting nonzero shifts via the Lottery-Ticket hypothesis [Ansell et al., 2021] or via their approximate Fisher information [Sung et al., 2021]. Finally, prefix-tuning methods [Li and Liang, 2021] prepend learnable embeddings to the input or intermediate representations to specialize the model towards a downstream task. Modular networks partition their parameters into several expert modules, each of them specialized to handle specific sub-tasks [Jacobs et al., 1991, Kirsch et al., 2018]. Modular networks are an appealing solution to the problem of adapting to unseen tasks [Corona et al., 2021], as the model 9can leverage its existing modules and recombine them in a novel way, thus achieving systematic generalization [Bahdanau et al., 2019]. They have also been tested in learning scenarios with data presented sequentially [Ostapenko et al., 2021], and with changing environments Goyal et al. [2021]. In NLP, mixture-of-experts (MoE) models [Shazeer et al., 2017, Fedus et al., 2022], where a learned gating mechanism routes token representations to appropriate experts (Feed-Forward layers), have shown success in scaling the number of parameters while retaining time efficiency. This results in higher performance when compared to their dense counterparts using a similar compute budget. 7 Conclusions In this paper, we tackle the challenge of generalizing to new tasks based on a few examples after multi- task pre-training. Specifically, we focus on Polytropon [Ponti et al., 2023], a model where each task is associated with a subset of adapters by a routing function. We investigate how varying the level of control afforded by the routing function impacts performance on two comprehensive benchmarks for multi-task learning, T0 and Super-Natural Instructions. First, a newly proposed variant of the routing function, where multiple heads are responsible for different blocks of input dimensions, improves consistently over all other baselines, including LoRA and (IA)3 adapters. Second, we identify the cause of the success of routing in its ability to prevent interference between tasks, as it yields a better alignment between their gradients. Third, we find that simple averaging of all multi-task pre-trained adapters before few-shot adaptation to new tasks provides comparable performance, thus offering state-of-the-art performance for single-adapter few-shot learning. Multi-head routing demonstrates the importance of fine-grained adapter selection for sample-efficient generalization and holds promise to improve other modular methods, such as Mixtures of Experts [MoEs; Fedus et al., 2022] in future research. References A. Ansell, E. M. Ponti, J. Pfeiffer, S. Ruder, G. Glavaš, I. Vuli ´c, and A. Korhonen. MAD-G: Multilingual adapter generation for efficient cross-lingual transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762–4781, Nov. 2021. doi: 10.18653/v1/ 2021.findings-emnlp.410. URL https://aclanthology.org/2021.findings-emnlp.410. A. Ansell, E. M. Ponti, A. Korhonen, and I. Vuli ´c. Composable sparse fine-tuning for cross- lingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022. URL https://arxiv.org/pdf/2110.07560.pdf. V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta, H. Zhuang, V . Q. Tran, D. Bahri, J. Ni, J. Gupta, K. Hui, S. Ruder, and D. Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=Vzh1BFUCiIX. A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6655–6672, 2022. D. Bahdanau, S. Murty, M. Noukhovitch, T. H. Nguyen, H. de Vries, and A. Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HkezXnA9YX. S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. E. Bugliarello, F. Liu, J. Pfeiffer, S. Reddy, D. Elliott, E. M. Ponti, and I. Vuli´c. IGLUE: A benchmark for transfer learning across modalities, tasks, and languages. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2370–2392. 10PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/bugliarello22a. html. A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models. arXiv preprint arXiv:2302.07027, 2023. R. Corona, D. Fried, C. Devin, D. Klein, and T. Darrell. Modular networks for compositional instruction following. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1033–1040, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 81. URL https://aclanthology.org/2021.naacl-main.81. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022. URL http://jmlr.org/papers/v23/21-0998.html. A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y . Bengio, and B. Schölkopf. Recurrent independent mechanisms. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=mLcmdlEUxy-. E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. CoRR, abs/1803.05407, 2018. URL http: //arxiv.org/abs/1803.05407. R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79–87, 1991. URL https://watermark.silverchair.com/neco. 1991.3.1.79.pdf. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-softmax. In International Conference on Learning Representations , 2017. URL https://openreview.net/pdf?id= rkE3y85ee. R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for Transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 565–576, Aug. 2021. URL https://aclanthology.org/ 2021.acl-long.47. L. Kirsch, J. Kunze, and D. Barber. Modular networks: Learning to decompose neural computation. Advances in Neural Information Processing Systems, 31, 2018. URL https://proceedings. neurips.cc/paper/2018/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf. X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Pro- ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pages 4582–4597, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. Raffel. Few-shot parameter- efficient fine-tuning is better and cheaper than in-context learning, 2022. URL https://arxiv. org/abs/2205.05638. O. Ostapenko, P. Rodriguez, M. Caccia, and L. Charlin. Continual learning via local module composition. Advances in Neural Information Processing Sys- tems, 34, 2021. URL https://proceedings.neurips.cc/paper/2021/file/ fe5e7cb609bdbe6d62449d61849c38b0-Paper.pdf. 11J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 487–503, Apr. 2021. URL https://aclanthology.org/2021.eacl-main.39. J. Pfeiffer, S. Ruder, I. Vuli ´c, and E. M. Ponti. Modular deep learning. arXiv preprint arXiv:2302.11529, 2023. URL https://arxiv.org/pdf/2302.11529.pdf. E. Ponti. Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning . PhD thesis, University of Cambridge, 2021. URL https://aspace.repository.cam.ac.uk/ bitstream/handle/1810/319303/thesis_electronic.pdf. E. M. Ponti, H. O’Horan, Y . Berzak, I. Vuli ´c, R. Reichart, T. Poibeau, E. Shutova, and A. Ko- rhonen. Modeling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics , 45(3):559–601, 2019. URL https://watermark.silverchair.com/coli_a_00357.pdf. E. M. Ponti, A. Sordoni, Y . Bengio, and S. Reddy. Combining parameter-efficient modules for task- level generalisation. In Proceedings of the 17th Conference of the European Chapter of the Associ- ation for Computational Linguistics, pages 687–702, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.49. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, P. J. Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. C. Rosenbaum, I. Cases, M. Riemer, and T. Klinger. Routing networks and the challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774, 2019. URL https: //arxiv.org/pdf/1904.12774.pdf. V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V . Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Févry, J. A. Fries, R. Teehan, S. Biderman, L. Gao, T. Bers, T. Wolf, and A. M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, 2022. URL https://arxiv.org/pdf/2110.08207.pdf. N. Shao, Z. Cai, H. xu, C. Liao, Y . Zheng, and Z. Yang. Compositional task representations for large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6axIMJA7ME3. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/pdf?id=B1ckMDqlg. Y .-L. Sung, V . Nair, and C. Raffel. Training neural networks with fixed sparse masks. In A. Beygelz- imer, Y . Dauphin, P. Liang, and J. W. Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=Uwh-v1HSw-x. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706. 03762. T. Vu, T. Wang, T. Munkhdalai, A. Sordoni, A. Trischler, A. Mattarella-Micke, S. Maji, and M. Iyyer. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7882–7926, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.635. URL https://aclanthology.org/2020.emnlp-main.635. T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. 12Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, E. Pathak, G. Karamanolakis, H. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, M. Patel, K. K. Pal, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. K. Sampat, S. Doshi, S. Mishra, S. Reddy, S. Patro, T. Dixit, X. Shen, C. Baral, Y . Choi, N. A. Smith, H. Hajishirzi, and D. Khashabi. Benchmarking generalization via in-context instructions on 1,600+ language tasks, 2022a. URL https://arxiv.org/abs/2204.07705. Y . Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, and J. Gao. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprint arXiv:2205.12410, 2022b. Z. Wang, Y . Tsvetkov, O. Firat, and Y . Cao. Gradient vaccine: Investigating and improving multi- task optimization in massively multilingual models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=F1vEjWK-lH_. J. Wei, M. Bosma, V . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. Q. Ye, B. Y . Lin, and X. Ren. CrossFit: A few-shot learning challenge for cross-task generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163–7189, Nov. 2021. doi: 10.18653/v1/2021.emnlp-main.572. URL https: //aclanthology.org/2021.emnlp-main.572. Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of language models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697–12706. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr. press/v139/zhao21c.html. 13A Appendix A.1 Additional Results More detailed numbers on the T0 Sanh et al. [2022] and SuperNI Wang et al. [2022a] datasets using different backbones, and different adapter layouts over the base model are found in Table 6. Multi-Task params is the number of additional parameters that must be conserved after Model Multi-Task Params Adaptation Params Avg. Test T0 Dataset Backbone T5-XL-LM Multi-Task Full Finetuning + LoRA 2.8B 2.2M 68.9 x.x (IA)3 540K 540K 62.4 0.4 AdapterSoup 84M 2.2M 62.1 1.0 LoRA 2.2M 2.2M 66.0 1.6 LoRA-big 35M 35M 65.4 0.9 Poly-z 17M 3.5K 66.4 0.3 Poly 17M 2.2M 68.0 1.0 MHR-z (64 h) 17M 220K 68.30.8 MHR (64 h) 17M 2.2M 69.11.0 Backbone T0-3B T-Few Liu et al. [2022] 540K 540K 66.2 0.5 AdapterSoup 84M 2.2M 66.1 0.6 LoRA 2.2M 2.2M 67.4 0.8 LoRA-big 35M 35M 68.0 0.8 Poly-z 17M 3.5K 65.3 1.0 Poly 17M 2.2M 69.0 0.8 MHRz (64 h) 17M 220K 68.41.2 MHR (8 h) 17M 2.2M 69.31.2 Backbone T0-3B light version : (k, v, ff layers only) l-LoRA (rank 1) 934K 934K 66.2 0.9 l-LoRA (rank 16) 15M 15M 67.6 1.1 AdapterSoup (l-LoRA) 35M 934K 64.9 1.0 l-Poly-z 7.5M 2.1K 62.9 1.2 l-Poly 7.5M 934K 68.0 0.5 l-MHRz (32 h) 7.5M 74K 66.81.1 l-MHR (8 h) 7.5M 934K 68.50.7 SuperNI Dataset Rouge-L Backbone T5-XL-LM light version : (k, v, ff layers only) l-LoRA 934K 934K 67.6 0.8 l-LoRA-big 18M 18M 67.2 0.7 l-Poly-z 7.5M 2.1K 64.6 0.3 l-Poly 7.5M 934K 67.8 0.8 l-MHRz (64 h) 7.5M 147K 68.00.2 l-MHR (8 h) 7.5M 934K 68.50.3 Table 6: (top) Results on T0 dataset Sanh et al. [2022], we report the mean of the best validation accuracy for each test task, when evaluated every 50 train epochs. T-Few is our reproduction of the results in Liu et al. [2022]. LoRA-big means a LoRA adapter with a larger rank. (bottom) Results on SuperNatural Instructions dataset. multi-task pretraining to enable transfer to a downstream task. Adaptation Params refer to the number of parameters required to learn a new downstream task. For e.g.Poly and MHR, the multi-task parameters includes the learned modules, but not the routing over the training tasks, as these are not required for transfer on a new task. Moreover, variants which average the learned modules prior to fine-tuning (MHR-µ and Poly-µ) will have both multi-task and adaptation parameters equal to that of a single shared adapter, since after multi-task pretraining one can average the modules. 14A.2 Navigating the parameter efficiency / performance trade-off of tuning only the routing Here we provide additional results on how different routing based methods can be more expressive when only learning a new routing function (over frozen modules) to adapt to a new task. Head # 2 Head # 1 Tasks Active Task Modules Tasks Active Task Modules Feed-Forward  Add & Norm Multi-Head  Attention Add & Norm W0 W1 V O Q K Tasks Modules Polytropon Multi-Head Routing (MHR) Sharing Z allocation in MHR  Figure 4: Different ways to control the expressivity of routing based methods. Left : In Polytropon, one can only add additional modules, resulting in a linear parameter increase. Right : In MHR, additional heads only introduce routing matrices Z, resulting in a negligible parameter increase. In Fig. 4 (left), we see that in order to build more expressive routing functions Z, in Poly one can only do so by increasing the number of skills at each layer. However, this has a significant impact on the number of multi-task parameters which much be kept in order to perform few-shot transfer. MHR on the other hand, can increase routing capacity in a much more parameter efficient way. A.2.1 On the granularity of routing tensor in MHR Here we provide additional results when modifying the granularity of Z for MHR. We see that one can easily trade-off more parameters for better performance. 103 104 105 T est-task param count 65.5 66.0 66.5 67.0 67.5 68.0 68.5T0 T est Accuracy Performance when tuning only Z w.r.t T est Parameter Budget MHR Poly Figure 5: Routing-Only Fine-Tuning (MHR-z) 15B Broader Impact In our work, we focus on advancing parameter-efficient fine-tuning methods for cross-task general- ization. While our research primarily addresses technical challenges and performance improvements, when applying such methods, it is crucial to consider the potential negative societal impacts. Specif- ically, we believe that prior to applying our proposed adaptation method, critically examining the potential biases and ethical implications of the underlying large language model, and the data itself must be properly addressed. This includes issues related to fairness, privacy, and the spread of misinformation. 16",
      "meta_data": {
        "arxiv_id": "2211.03831v3",
        "authors": [
          "Lucas Caccia",
          "Edoardo Ponti",
          "Zhan Su",
          "Matheus Pereira",
          "Nicolas Le Roux",
          "Alessandro Sordoni"
        ],
        "published_date": "2022-11-07T19:35:55Z",
        "pdf_url": "https://arxiv.org/pdf/2211.03831v3.pdf",
        "github_url": "https://github.com/microsoft/mttl"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the role of adapter routing in parameter-efficient fine-tuning (PEFT) for cross-task generalization, building upon Polytropon (Poly). It introduces Multi-Head Routing (MHR), which enhances expressivity by combining blocks of parameters from different adapters, outperforming Poly with a comparable parameter budget. A variant, MHR-z, is proposed for extreme parameter efficiency by fine-tuning only the routing function during few-shot adaptation. The research reveals that the superior performance of routing-based PEFT methods like Poly/MHR stems from better multi-task optimization, characterized by higher gradient alignment between tasks, rather than modular inductive biases. Furthermore, it introduces MHR-µ, which discards routing during few-shot adaptation and fine- tunes the average of pre-trained adapters, achieving competitive performance and establishing an effective single-adapter fine-tuning method. MHR-µ is also shown to be effective for zero-shot transfer with additional fine-tuning on the multi-task pre-training set, yielding significant accuracy gains.",
        "methodology": "The core methodology extends Polytropon (Poly), which uses an inventory of LoRA adapters and a routing function to select task-specific subsets of modules. The proposed MHR (Multi-Head Routing) partitions adapter dimensions into 'h' disjoint blocks (heads) and applies a separate Poly-style combination for each block, learning a distinct routing matrix for each head. These head-specific combinations are then concatenated to form the task-specific adapter, enabling more fine-grained, piecewise linear mixing of parameters. MHR-z is a variant that freezes adapter parameters and only updates the routing parameters during few-shot adaptation for parameter efficiency. MHR-µ initializes the adapter for fine-tuning by averaging all pre-trained module parameters, effectively discarding the routing function during adaptation to assess its necessity. For zero-shot transfer, MHR-µ involves training the averaged pre-trained adapters for additional steps on the multi-task pre-training data. The paper also analyzes gradient alignment between tasks using cosine similarity during multi-task pre-training to understand the underlying optimization benefits.",
        "experimental_setup": "The methods were evaluated using T5-XL, T0-3B, and T0-11B as backbone models. Experiments were conducted on two primary datasets: the T0 evaluation suite, comprising 313 unique tasks (data subset–template pairs), and a subset of 20 randomly selected test tasks from Super-Natural Instructions (SuperNI), each with 100 training, 100 validation, and 100 test examples. Performance on T0 was measured by the mean of the best validation accuracy across 3 seeds, while SuperNI used Rouge-L averaged over 3 seeds. Baselines included standard PEFT methods like LoRA and (IA)3, AdapterSoup, the original Polytropon (Poly), and a random-routing baseline (Random-µ) for comparative analysis. All experiments were performed on a single NVIDIA A100 GPU.",
        "limitations": "One limitation identified is that without additional fine-tuning steps (k=0), MHR-µ for zero-shot learning does not yield good results, primarily due to a potential mismatch between adapters learned via task-specific routing and the uniform routing strategy used for averaging. The broader impact section also highlights a general necessity to critically examine potential biases and ethical implications of the underlying large language models and training data when applying such adaptation methods.",
        "future_research_directions": "The findings suggest potential avenues for future work in improving meta-learning and weight-averaging approaches, inspired by the effectiveness of MHR-µ. Additionally, multi-head routing's demonstration of fine-grained adapter selection for sample-efficient generalization holds promise for enhancing other modular methods, such as Mixtures of Experts (MoEs).",
        "experimental_code": "dataclasses.dataclass\nclass SkilledLoRAConfig(LoRAConfig):\n    n_skills: int = 1\n    n_splits: int = 1\n@mttl.models.modifiers.base.Modifier.register(\"skilled_lora\", config_cls=SkilledLoRAConfig)\nclass SkilledLoRA(LoRA):\n    def __init__(\n        self,\n        config: SkilledLoRAConfig,\n        layer: nn.Module,\n        **kwargs,\n    ):\n        self.n_splits = config.n_splits\n        self.n_skills = config.n_skills\n        super().__init__(config, layer)\n    def create_for_layer(self, layer):\n        self.lora_a = nn.Parameter(\n            torch.empty(\n                self.n_skills,\n                self.n_splits,\n                layer.in_features // self.n_splits,\n                self.rank,\n            ).to(device=self.weight.device)\n        )\n        self.lora_b = nn.Parameter(\n            torch.empty(\n                self.n_skills,\n                self.rank,\n                self.n_splits,\n                layer.out_features // self.n_splits,\n            ).to(device=self.weight.device)\n        )\n    @classmethod\n    def parallel_linear_weighted_forward(\n        cls,\n        input: torch.Tensor,\n        skilled_loras: List[\"SkilledLoRAView\"],\n        weights: torch.Tensor,\n        dim_names: List[str],\n        merge_after: bool = False,\n    ):\n        if len(set([lora.layer for lora in skilled_loras])) > 1:\n            raise ValueError(\"Cannot parallelize loras applied to different layers.\")\n        if len(dim_names) != weights.ndim:\n            raise ValueError(\"Not all dimensions are present in the weights tensor.\")\n        device = input.device\n        n_skills = skilled_loras[0].lora_a.shape[0]\n        assert np.all(skl.n_skills == n_skills for skl in skilled_loras)\n        if num_skilled_loras == 1:\n            skilled_loras_a = skilled_loras[0].lora_a.unsqueeze(0)\n            skilled_loras_b = skilled_loras[0].lora_b.unsqueeze(0)\n        else:\n            skilled_loras_a = torch.stack(\n                [lora.lora_a for lora in skilled_loras], dim=0\n            )\n            skilled_loras_b = torch.stack(\n                [lora.lora_b for lora in skilled_loras], dim=0\n            )\n        if skilled_loras_a.device != device:\n            skilled_loras_a = skilled_loras_a.to(device=device)\n            skilled_loras_b = skilled_loras_b.to(device=device)\n        expected_dims = [\"batch\", \"sequence\", \"splits\", \"experts\"]\n        for i, dim in enumerate(expected_dims):\n            if dim not in dim_names:\n                weights = weights.unsqueeze(i)\n        weights = weights.to(dtype=skilled_loras[0].lora_a.dtype)\n        layer_out = skilled_loras[0].layer(input)\n        input_lora = input.to(skilled_loras[0].lora_a.dtype)\n        input_lora = skilled_loras[0].dropout_layer(input_lora)\n        if input_lora.ndim == 2:\n            input_lora = input_lora.unsqueeze(1)\n        if merge_after:\n            partial_out = torch.einsum(\"bld,beqdr->bleqr\", input_lora, skilled_loras_a)\n            adapter_out = torch.einsum(\n                \"bleqr,berqd,blqe->blqd\", partial_out, skilled_loras_b, weights\n            )\n            adapter_out = adapter_out.flatten(2, 3)\n        else:\n            A = torch.einsum(\"blqe,beqdr->blqdr\", (weights, skilled_loras_a))\n            B = torch.einsum(\"blqe,berqd->blrqd\", (weights, skilled_loras_b))\n            batch_size, sequence_length, rank, n_splits, d_split = B.shape\n            A, B = A.flatten(2, 3), B.flatten(3, 4)\n            partial_out = torch.einsum(\"bld,bldr->blr\", (input_lora, A))\n            adapter_out = torch.einsum(\"blr,blrd->bld\", (partial_out, B))\n        adapter_out.mul_(scaling)\n        if layer_out.ndim == 2:\n            adapter_out = adapter_out.squeeze(1)\n        return layer_out + adapter_out.to(dtype=input.dtype)\n\n\n@dataclasses.dataclass\nclass PolySelectorConfig(mttl.models.containers.selectors.base.SelectorConfig):\n    n_splits: int = 1\n    task_names: List[str] = None\n    allow_unknown_tasks: bool = False\n\n\n@mttl.models.containers.selectors.base.Selector.register(\"poly_router\", config_cls=PolySelectorConfig)\nclass PolySelector(mttl.models.containers.selectors.base.Selector):\n    avg_selector_warned: bool = False\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.n_tasks = len(self.config.task_names) if self.config.task_names else 0\n        shape = (\n            self.n_tasks + (1 if self.config.allow_unknown_tasks else 0),\n            self.config.n_splits,\n        )\n        self.module_logits = nn.Parameter(torch.empty(*shape).uniform_(-1e-3, 1e-3))\n        if self.n_tasks == 0:\n            logger.warning(\n                \"No task names found in the config. Using a single task for PolySelector.\"\n            )\n\n    def _get_weights(self, task_names: List[str] = None) -> torch.Tensor:\n        if self.n_tasks == 0:\n            task_ids = [0]\n        else:\n            if task_names is not None:\n                task_ids = self._convert_task_names_to_ids(task_names)\n            else:\n                routing_info: RoutingInfo = self.routing_infos\n                if hasattr(routing_info, \"task_ids_from_name\"):\n                    task_ids = routing_info.task_ids_from_name\n                else:\n                    task_ids = self._convert_task_names_to_ids(routing_info.task_names)\n                    self.routing_infos.task_ids_from_name = task_ids\n            if task_ids.max() < self.n_tasks:\n                if PolySelector.avg_selector_warned:\n                    logger.warning(\n                        f\"Task ids were found. Reverting to default task-based routing\"\n                    )\n                PolySelector.avg_selector_warned = False\n            else:\n                if not PolySelector.avg_selector_warned:\n                    not_found_tasks = set(\n                        [\n                            t\n                            for t in self.routing_infos.task_names\n                            if t not in self.config.task_names\n                        ]\n                    )\n                    if self.config.allow_unknown_tasks:\n                        logger.warning(\n                            f\"Tasks {not_found_tasks} not in training tasks. Defaulting to average selector.\"\n                        )\n                        PolySelector.avg_selector_warned = True\n                    else:\n                        raise ValueError(\n                            f\"Tasks {not_found_tasks} not in training tasks.\"\n                        )\n                assert not self.training, \"Unknown tasks during training\"\n        module_logits = torch.sigmoid(self.module_logits[task_ids])\n        module_logits = module_logits.view(\n            module_logits.size(0), self.config.n_splits, self.n_experts\n        )\n        module_weights = module_logits / (module_logits.sum(dim=-1, keepdim=True) + EPS)\n        return module_weights\n\n    @mttl.models.containers.selectors.base.forward_with_cache\n    def forward(self, input, **kwargs) -> Union[\n        BatchExpertsSplitsAndWeightsSelectorOutput,\n        ExpertsSplitsAndWeightsSelectorOutput,\n    ]:\n        weights = self._get_weights()\n        if self.n_tasks == 0:\n            return ExpertsSplitsAndWeightsSelectorOutput(\n                ALL_EXPERTS, weights.squeeze(0)\n            )\n        return BatchExpertsSplitsAndWeightsSelectorOutput(ALL_EXPERTS, weights)\n\n\n@mttl.models.containers.base.ExpertContainer.register(\"skilled_lora\", mttl.models.modifiers.lora.SkilledLoRAConfig)\nclass SkilledLoRAExpertContainer(LoRAExpertContainer):\n    __supports_configs__ = [SkilledLoRAConfig]\n\n    def __init__(\n        self,\n        config,\n        layer,\n        selector=None,\n        **kwargs,\n    ):\n        super().__init__(config, layer, selector)\n\n        if not isinstance(self.layer, nn.Linear):\n            raise ValueError(\n                \"Expert containers for layers other than nn.Linear have not been implemented, current layer is {}\".format(\n                    self.layer.__class__.__name__\n                )\n            )\n        self.dummy_config = SkilledLoRAConfig(\n            lora_alpha=config.lora_alpha,\n            lora_dropout=config.lora_dropout,\n            lora_init_b_random=config.lora_init_b_random,\n            lora_rank=config.lora_rank,\n            n_splits=config.n_splits if isinstance(config, SkilledLoRAConfig) else 1,\n            n_skills=0,\n        )\n        self.experts = SkilledLoRA(self.dummy_config, layer)\n\n    def on_add_expert(\n        self, expert: Expert, is_default=False, device: str = None\n    ) -> None:\n        from mttl.models.containers.utils import filter_expert_weights\n\n        self._check_config(expert.expert_config)\n        lora_type = get_modifier_name(expert.expert_config)\n        LoRA_cls = {\"lora\": LoRA, \"skilled_lora\": SkilledLoRA}[lora_type]\n        modifier_module = LoRA_cls(\n            expert.expert_config, self.layer, layer_name=self.__layer_name__\n        )\n        if expert.expert_weights:\n            expert_weights = filter_expert_weights(\n                self.__layer_name__, expert.expert_weights\n            )\n            modifier_module.load_lora_weights(expert_weights)\n        self.experts.add_skill(modifier_module)\n\n    def route(self, input, selection, **kwargs):\n        if isinstance(selection, BatchExpertsSelectorOutput):\n            batch_size = len(selection.experts)\n            indices = torch.LongTensor(\n                self._convert_expert_names_to_indices(\n                    selection.experts,\n                    use_default_expert=self.default_expert_name is not None,\n                )\n            )\n            weights = (\n                torch.zeros(\n                    (batch_size, self.experts.n_skills),\n                )\n                .scatter_add(\n                    1, indices.unsqueeze(1), torch.ones((len(selection.experts), 1))\n                )\n                .to(device=self.experts.lora_a.device, dtype=torch.float32)\n            )\n            module_output = SkilledLoRA.parallel_linear_weighted_forward(\n                input,\n                [self.experts],\n                weights,\n                dim_names=[\"batch\", \"experts\"],\n                merge_after=self.selector.config.lora_merge_after,\n            )\n            return module_output\n        elif (\n            isinstance(selection, BatchSequenceExpertsAndWeightsSelectorOutput)\n            or isinstance(selection, BatchExpertsAndWeightsSelectorOutput)\n            or isinstance(selection, ExpertsAndWeightsSelectorOutput)\n        ):\n            if selection.experts is not ALL_EXPERTS:\n                if not isinstance(selection.experts, torch.Tensor):\n                    selection.experts = torch.LongTensor(\n                        self._convert_expert_names_to_indices(\n                            selection.experts,\n                            use_default_expert=self.default_expert_name is not None,\n                        )\n                    ).to(selection.weights.device)\n                weights = torch.zeros(\n                    (selection.weights.shape[:-1] + (self.experts.n_skills,)),\n                    device=selection.weights.device,\n                    dtype=selection.weights.dtype,\n                ).scatter_add(\n                    selection.weights.ndim - 1, selection.experts, selection.weights\n                )\n            else:\n                weights = selection.weights\n                assert weights.shape[-1] == self.experts.n_skills\n            module_output = SkilledLoRA.parallel_linear_weighted_forward(\n                input,\n                [self.experts],\n                weights,\n                dim_names=selection.dim_names,\n                merge_after=self.selector.config.lora_merge_after,\n            )\n            return module_output\n        else:\n            raise ValueError(\"Unknown selection type.\")\n\n\n    def container_forward(self, input, **kwargs):\n        selection = self.selector(input, container=self, **kwargs)\n        return self.route(input, selection, **kwargs)",
        "experimental_info": "MHR (Multi-Head Routing) is a method that extends Polytropon (Poly) by partitioning adapter dimensions into 'h' disjoint blocks (heads) and applying a separate Poly-style combination for each block, learning a distinct routing matrix for each head. These head-specific combinations are then concatenated to form the task-specific adapter.\n\n**Core MHR Configuration:**\n- The number of disjoint blocks or heads (`h`) is configured via `n_splits` in both `SkilledLoRAConfig` and `PolySelectorConfig`:\n    - `SkilledLoRAConfig.n_splits: int` (defines how adapter dimensions are partitioned)\n    - `PolySelectorConfig.n_splits: int` (defines the routing matrix dimensions)\n- The learning of a distinct routing matrix for each head is handled by `PolySelector` where `module_logits` has a shape of `(n_tasks, n_splits)`.\n\n**MHR-z Variant (Freezing Adapter Parameters):**\n- This variant freezes adapter parameters and updates only the routing parameters during few-shot adaptation.\n- This behavior is configured by setting `trainable_param_names` in `TrainingArgs` (from `mttl/arguments.py`) to target only selector and module logits parameters:\n    - `args.trainable_param_names = \".*module_logits.*|.*selector.*\"` (as seen in `finetune_polylib_selector` in `projects/modular_llm/finetune_experts.py`).\n\n**MHR-µ Variant (Averaging Pre-trained Modules):**\n- This variant initializes the adapter for fine-tuning by averaging all pre-trained module parameters, effectively discarding the routing function during adaptation.\n- The initialization involves averaging module parameters, which can be achieved by `WeightedLinearMerge` (from `mttl/models/library/library_transforms.py`) as used in `create_mean_expert` (from `projects/modular_llm/finetune_experts.py`).\n- Discarding the routing function during adaptation is achieved in functions like `pretrain_poly` (from `projects/modular_llm/finetune_experts.py`) with:\n    - `module.model.switch_selector_to_average()`\n    - `module.model.resize_module_logits(1)`\n- For zero-shot transfer, MHR-µ involves training the averaged pre-trained adapters for additional steps on the multi-task pre-training data.\n\n**Training and Optimization Settings (from `TrainingArgs` in `mttl/arguments.py`):**\n- `router_weight_decay: float = None`\n- `router_learning_rate: float = None`\n- `module_logits_relaxed_bernoulli: bool = True`\n- `module_logits_straight_through: bool = False`\n- `module_logits_learning_rate: float = 0.1`\n- `adapters_learning_rate: float = None`\n- `adapters_weight_decay: float = None`\n- `module_logits_dropout: float = 0.0`\n- `module_logits_l2_norm: float = False`\n- Gradient alignment analysis during multi-task pre-training is performed using:\n    - `monitor_grad_alignment_on: str = None`"
      }
    },
    {
      "title": "Sparse High Rank Adapters",
      "abstract": "Low Rank Adaptation (LoRA) has gained massive attention in the recent\ngenerative AI research. One of the main advantages of LoRA is its ability to be\nfused with pretrained models, adding no overhead during inference. However,\nfrom a mobile deployment standpoint, we can either avoid inference overhead in\nthe fused mode but lose the ability to switch adapters rapidly, or suffer\nsignificant (up to 30% higher) inference latency while enabling rapid switching\nin the unfused mode. LoRA also exhibits concept-loss when multiple adapters are\nused concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA),\na new paradigm which incurs no inference overhead, enables rapid switching, and\nsignificantly reduces concept-loss. Specifically, SHiRA can be trained by\ndirectly tuning only 1-2% of the base model weights while leaving others\nunchanged. This results in a highly sparse adapter which can be switched\ndirectly in the fused mode. We further provide theoretical and empirical\ninsights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing\nconcept loss. Our extensive experiments on LVMs and LLMs demonstrate that\nfinetuning only a small fraction of the parameters in the base model\nsignificantly outperforms LoRA while enabling both rapid switching and\nmulti-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA\nimplementation based on Parameter-Efficient Finetuning (PEFT) Library which\ntrains at nearly the same speed as LoRA while consuming up to 16% lower peak\nGPU memory, thus making SHiRA easy to adopt for practical use cases. To\ndemonstrate rapid switching benefits during inference, we show that loading\nSHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU.",
      "full_text": "Sparse High Rank Adapters Kartikeya Bhardwaj∗§ Nilesh Prasad Pandey∗† Sweta Priyadarshi† Viswanath Ganapathy† Shreya Kadambi Rafael Esteves Shubhankar Borse Paul Whatmough § Risheek Garrepalli Mart Van Baalen Harris Teague § Markus Nagel§ Qualcomm AI Research‡ §{kbhardwa,pwhatmou,hteague,markusn}@qti.qualcomm.com Abstract Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5×-16× faster than LoRA fusion on a CPU.¶ 1 Introduction Low Rank Adaptation (LoRA) [13] is an established technique to tune the behavior of large generative models such as Large Language Models (LLMs) [30, 29] and Stable Diffusion [24, 22]. As the name suggests, LoRA requires very few parameters since it trains low rank projection weights that consume very low memory during the finetuning process while producing excellent results. Moreover, these low rank weights can be fused analytically into the base model, thereby incurring no additional overhead during inference. Despite its success, there are still several limitations of low rank adaptation methods. First, if LoRA parameters are fused into the corresponding pretrained base model weights, they modify the entire weight tensor. Therefore, deploying LoRA on large models such as LLaMA-1/2 (7B+ parameters) or Stable Diffusion (1.5B+ parameters) on mobile devices would require changing a large number of weights during inference. Consequently, for mobile scenarios, if an application requiresrapid adapter switching, existing low rank methods would incur a significant memory and latency cost. This is a major deployment challenge because, unlike large GPUs, local memory of small AI accelerators is limited and cannot store all weights at the same time. These challenges can be partially addressed by ∗Equal contribution. †Work done while employed at Qualcomm AI Research. ‡Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. ¶Code: https://github.com/Qualcomm-AI-research/SHiRA. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.13175v2  [cs.LG]  27 Jan 2025LoRA Car  dog in space  Elephant  House, mountain  Thunder Bird  knight                                      BLUEFIRE SHiRA-SNIP                                      PAINTINGS                                       MULTI-ADAPTER Figure 1: Sparse High Rank Adapters (SHiRA): Changing about 1-2% weights of the pretrained generative model is often sufficient to achieve high performance. Due to its extreme sparsity, SHiRA enables rapid switching and also reduced concept loss during multi-adapter fusion. In contrast, LoRA modifies majority of parameters when fused, thus prohibiting rapid switching on mobile devices, and also experiences concept loss during multi-adapter fusion. For LoRA, elephant for single “paintings” adapter case has artifacts (extra/broken tusks); bird and knight for multi-adapter case lose “paintings” concept and keep only the “blue fire” effects. SHiRA does not experience these issues. running LoRA in unfused mode; however, unfused inference can incur as high as30% additional latency compared to the base model [1] (see section 2.1 for details). This increased inference time in unfused mode and time for adapter switching significantly hampers user experience; hence, this is an important problem which has been a focus of recent research by various industries [9]. Second, LoRA has a well-known limitation called concept loss when using multiple concurrent adapters, e.g., combining multiple style transfer adapters, etc. Specifically, it has been well documented [34, 26, 8] that a simple additive merging of multiple LoRA adapters leads to concept loss of one or more adapters. Finally, recent literature also contributes important theoretical and empirical knowledge towards the value of high rank adapters. For instance, Kalajdzievski [16] shows that the high rank adapters can greatly outperform low rank adapters when used with correct scaling factors. This calls for further investigation into whether other high rank adapters would significantly outperform LoRA. In view of the above, we address the following key problems in this paper: (i) How can we perform rapid switching for fused adapters? ( ii) Is there a simpler solution for multi-adapter fusion to reduce concept loss? (iii) Can we build high rank adapters that have high expressive power without significantly increasing the training or inference costs? To this end, we propose Sparse High Rank Adapters (SHiRA), a single solution to all three problems above. SHiRA is a highly sparse but a high rank adapter which relies on training only a very small subset of parameters from the original pretrained network. One of the crucial insights we demonstrate is that even finetuning merely 1-2% parameters of the pretrained generative model is sufficient to achieve high performance on many adapter tasks (see Fig. 1). However, unlike LoRA layers that modify all parameters in the weight tensors in the fused mode, SHiRA still keeps a very low percentage of parameters that need to be switched, thus enabling rapid switching at inference time. Moreover, since the pretrained weights are huge, SHiRA being a very sparse adapter greatly aids multi-adapter fusion by significantly reducing concept loss. Finally, we theoretically and emprically analyze the high rank vs. sparsity properties of SHiRA and why that helps with adapter performance. Overall, we make the following key contributions: • We propose SHiRA, a new high rank adapter paradigm to demonstrate that changing as few as 1-2% parameters of the original network is sufficient for adaptation. Our crucial insight is that even the most basic masking criteria (to identify the top 1-2% parameters) enable SHiRA to significantly outperform LoRA on diverse vision and language tasks. • SHiRA enables on-device rapid adapter switching and provides a natural multi-adapter fusion technique due to high sparsity, thus, significantly reducing concept loss. We also theoretically analyze SHiRA through the lens of high rank adaptation vs. sparsity. • We conduct extensive experiments on LLMs (LLaMA-7B, LLaMAv2-7B) and LVMs (Stable Diffusion, SDXL) where we demonstrate that SHiRA significantly outperforms LoRA on both single- and multi-adapter tasks. On LLMs, we show that SHiRA achieves up to 2.7% better accuracy than LoRA on commonsense reasoning. SHiRA also complements advanced variants of LoRA such as DoRA [20] and can be easily applied on top of them. 2• Finally, on the training side, we provide a PEFT-based latency- and memory-efficient implementation for SHiRA which trains nearly as fast as standard LoRA while consuming 16% lower peak GPU memory. Beyond PEFT, we provide a simple way to turn any trainer into SHiRA finetuning. For inference, we demonstrate that SHiRA weights can be loaded on a CPU up to 5×-16× faster than equivalent LoRA fusing, thereby enabling rapid switching. The rest of this paper is organized as follows: section 2 presents the background and related work. We propose SHiRA in section 3 while describing its theoretical properties in section 4. We then conduct extensive experiments for SHiRA in section 5. Finally, we discuss the key findings in section 6 and conclude the paper in section 7. 2 Background and Related Work 2.1 Background: Edge Deployment Challenges for LoRA There are three existing deployment options for LoRA: (i) fuse the adapter offline and then deploy on-device: this changes a large fraction of the weight tensors compared to base model which prohibits rapid switching since it will increase DRAM traffic considerably; (ii) keep the adapter unfused and run the inference in unfused mode: this can help with rapid switching but would incur significant addi- tional (up to 30% higher) latency as shown in [1] since we would have LoRA branches in the forward pass during inference; (iii) use the Huggingface/Diffusers pipeline [1] (built for server-grade GPUs) for mobile inference. This pipeline consists of load→fuse→inference→unfuse→unload to switch adapters. Here, unfused LoRA-A and LoRA-B weights (see Fig. 2(a)) are first loaded into the memory and then fused into the base model by computing Wnew = W + AB; this new weight is used for inference. To switch the adapter, we can unfuse the adapter as W = Wnew − AB and then unload existing LoRA weights to load the new ones. We provide further evidence in Appendix A to demonstrate that such a pipeline is not feasible for edge devices. This is primarily because edge devices are memory-limited and not all weights of large generative models can be stored in the local memory at the same time. Hence, loading and fusing needs to happen layerwise on a mobile device that obviously results in massive inference latency costs. 2.2 Related Work LoRA, its variants, and sparse adapters. Many LoRA variants exist in literature: DoRA [ 20], LoRA+ [11], VeRA [17], LoRA-FA [35], RS-LoRA [16], among many others. The crucial difference between this literature and our work is that we develop a high rank adapter without increasing training and inference costs. Also, for such methods, the final fused adapter still updates all elements in the pretrained weight tensor, thus prohibiting rapid switching. Moreover, for completeness, we will also show that SHiRA is orthogonal to and can be applied on top of some of the latest, more advanced LoRA variants such as DoRA [20] while preserving the benefits of rapid switching. A few other LoRA variants have also explored a combination of sparsity and low rank adaptation. Ex- amples include RoSA [21], SoRA [6], Sparse-Adapters [12], etc. Among these, Sparse-Adapters [12] explores the use of popular pruning techniques (e.g., SNIP [19]) to prune out adapters to improve their efficiency. SoRA [6] proposes an adaptive rank version of LoRA by gating elements of down and up projection layers and pruning out the zero entries at inference. Finally, RoSA [21] combines a sparse adapter with a low rank one to achieve some high rank benefits. However, since they combine their method with LoRA, the fused adapter weight still overwrites the entire pretrained weight tensor. Partial Finetuning. Our work is most closely related to partial finetuning techniques that were mostly proposed in the pre-LoRA era [36, 28, 3, 33, 10]. These methods use a mix of fixed sparse masks [28] or learned masks [36, 10] to finetune a pretrained network. Note that, these techniques have been mostly explored for relatively small language models, and not for recent LLMs and diffusion models. Since the LoRA models exploded in popularity, it has been unclear if other sparse finetuning techniques would achieve comparable results to LoRA on generic adapter tasks, particularly in the vision domain. One significant limitation of partial finetuning, as opposed to LoRA-based methods, is its high GPU memory consumption, making it impractical to be used for large generative models. Consequently, the reduced memory consumption for finetuning was a key factor to LoRA’s success and its widespread adoption. To this end, we provide a memory- and latency-efficient PEFT-based implementation for SHiRA which trains as efficiently as LoRA, thus requiring significantly lower memory consumption compared to prior partial finetuning techniques. Further, we explore the effectiveness of sparse finetuning on both large language and vision models and provide a detailed analysis on rapid switching and multi-adapter fusion of the high rank adapters. 3Backward Pass Forward Pass Trainable weightsFrozen weights Weights 1 1 111 11 1 Masked GradientsMaskOriginal Gradients Linear Layer Input Features Output Features Non-Zero GradientsZero Gradients Pretrained Weights, W b.Sparse High Rank Adaptation(SHiRA)a.Low Rank Adaptation(LoRA) LoRA-A LoRA-B + Rank r h xFused weight at inference would modify all elements of pretrained weight W⊙= Figure 2: (a) LoRA when fused into the pretrained model modifies all weights and prevents rapid adapter switching. (b) SHiRA does not require additional weights during training but finetunes very few pretrained weights. Our approach relies on a sparse mask for gradient-masking during training. We show that finetuning as low as1-2% parameters is sufficient to achieve high accuracy. A notable concurrent work is SpIEL [4] which scales partial finetuning to modern LLMs and also has a PEFT implementation that results in comparable speed and memory as LoRA. The main differences between SpIEL and SHiRA are as follows: (i) SpIEL works with dynamic masks while SHiRA uses a static mask. (ii) Dynamic mask in SpIEL requires users to install custom sparse linear layer kernels for the GPUs. In contrast, SHiRA does not require installing any custom kernels and directly works with native Pytorch. Hence, SHiRA’s biggest advantage is its ease of training/inference deployment. (iii) We also analyze multi-adapter fusion properties, e.g., impact of sparsity on orthogonality between adapters, which were not discussed in SpIEL. (iv) Finally, SHiRA demonstrates its effectiveness on both vision and language tasks, whereas SpIEL only discusses the language tasks. Multi-Adapter Fusion. Existing Multi-adapter fusion methods focus on preventing concept loss [8, 34, 26]. However, these methods usually either just use the base LoRA as it is (and then perform some non-trivial postprocessing on them) [34, 26], or some create some minor variants [8]. In contrast, we introduce a new adapter for the concept loss problem where multiple concepts naturally do not interfere with each other. In that respect, our work is orthogonal to the prior multi-adapter fusion work since our adapter can be further postprocessed using such techniques. 3 Proposed Approach 3.1 S parse High Rank Adapters (SHiRA) SHiRA exploits highly sparse trainable parameters in the pretrained model. In its simplest form, our adapter can be trained by masking gradients such that only a fraction of original weights get updated. Specifically, we do not add any new weights to the forward pass like LoRA (see Fig. 2(a)) but rather make a small percentage of existing weights trainable (see Fig. 2(b) top). To this end, we first create an extremely sparse (∼98-99% zeros) mask M ∈Rn×m = {0, 1}n×m, where n, mare dimensions of the pretrained weight matrix. M is then used to mask the gradients during backpropagation using a Hadamard product (see Fig. 2(b) bottom). Thus, very few parameters get updated during training and our adapter consists of just those sparse weights. Concrete gradient masking-based and another latency-/memory-efficient PEFT implementations for SHiRA are discussed in section 3.3. We consider the following masks M (only 1-2% trainable parameters, see also Appendix B): 1. SHiRA-Struct: In this structured mask, certain rows or columns of the weight as well as its diagonal are set to be trainable. All other rows/columns are not trainable. The diagonal makes the mask high rank whereas the structured trainable rows/columns – set to 1 to enable gradient flow to corresponding parameters – lead to a rank 1 adapter. Thus, SHiRA-Struct is a combination of a high rank but very sparse adapter and a rank 1 adapter. 2. SHiRA-Rand: This mask is obtained by randomly setting 1-2% parameters as trainable. 3. SHiRA-WM: Here we pick top-K parameters to train based on their weight magnitudes (WM), the absolute value of the weight for each layer. 4SHiRAAdapter 1SHiRAAdapter 2Fused Multi-Adapter+ α2 =+ α1 Sparse WeightsIndices +Storing [Sparse Weights + Indices] consumes much less memory than pretrained weightsWeights trained for SHiRA b.Multi-adapter fusiona.Rapid adapter switching Base Model Weights that changed during adaptationFrozen weights Non-Zero Weights for SHiRAAdapter 2Zero WeightsNon-Zero Weights for SHiRAAdapter 1Base Model Weights Figure 3: (a) Rapid adapter switching: The sparse finetuned weights can be stored as weights and their indices. At inference time, these weights can be loaded on the base model weights. Since only 1-2% weights need to be overwritten, the adapter can be efficiently switched with different weights at inference, eliminating the need for a separate fusion stage. (b) Multi-adapter fusion: Concept-loss can be reduced if multiple adapters do not significantly interfere with each other. 4. SHiRA-Grad: This is a gradient-based mask. We first collect gradients on a small calibra- tion set and then pick top 1-2% weights that receive the highest gradient magnitudes. 5. SHiRA-SNIP: The SNIP metric from the pruning literature [19] combines weight magnitude and gradient strategies, i.e., SNIP equals magnitude of the gradient times the weight. 3.2 Rapid Adapter Switching, Multi-Adapter Fusion, and High Rank Since very few base weights change during the SHiRA training, we can simply extract them out and store them as sparse weights and their indices (see Fig. 3(a)). Hence, SHiRA is comparable to LoRA in model size but overwrites only a fraction of the pretrained weights at inference time. In contrast, LoRA fuses into base weights as Wnew = W + AB and changes the entire weight. Note that, we do not actually need to fuse SHiRA but rather just need to overwrite the modified value at the correct index in the pretrained weight tensor. This enables rapid switching on resource-constrained devices. To verify that SHiRA indeed provides rapid switching benefits compared to LoRA, we provide an optimized implementation based on scatter_op to overwrite base model weights instead of fusing them like LoRA. We demonstrate that on a CPU, weight loading for SHiRA adapters can be up to 5×-16× faster than equivalent LoRA fusing for inference (see Appendix C and Fig 7). Next, we discuss multi-adapter fusion in SHiRA. Given two adapters A1 and A2 with sparse masks M1 and M2, we ask the following questions: (i) What is the impact of sparsity on relative interference between adapters in the multi-adapter setting? (ii) Is it possible to create masks that result in nearly orthogonal SHiRA weights so they do not significantly interfere with each other at inference time? Getting adapters that do not interfere with each other is essential to avoid concept-loss. To this end, we define specific metrics in section 4.2 to analyze orthogonality properties between adapter weights for various SHiRA strategies. We theoretically show that at least one of the SHiRA methods, i.e., SHiRA-Struct can in fact create near-orthogonal adapters. We further experimentally demonstrate in section 5.2.2 that SHiRA-Struct indeed outperforms other methods for multi-adapter fusion. Finally, since we do not have any low rank weights in the forward pass, our proposed adapters can be high rank albeit highly sparse. We theoretically analyze the rank vs. sparsity properties in section 4. 3.3 Memory- and Latency-Efficient SHiRA Training We have created two implementations for SHiRA: (i) a backward hook-based gradient masking to turn any trainer into SHiRA finetuning (see Appendix D), and (ii) a PEFT-based implementation. As discussed in Appendix E, the PEFT-based SHiRA implementation consumes16.63% lower peak GPU memory and trains almost at a similar speed as LoRA. On the contrary, DoRA exhibits a 40.99% and 28.9% increase in memory and training time respectively compared to LoRA. 4 Theoretical Insights for SHiRA 4.1 Rank vs. Sparsity Below we discuss parameter and learning complexity, parallels between LoRA and SHiRA, as well as its optimization properties from the lens of rank and sparsity. Lemma 4.1. The parameter complexity and learning complexity of SHiRA is equal to the number of non-zero elements in the adapter. Appendix F.1 provides the proof. This lemma suggests that despite high rank property of SHiRA, it would not require significantly larger datasets to converge. 5Lemma 4.2. If we specify a sparsity factor, the LoRA is r rank approximation of SHiRA with approximation error bounded by σ2 r+1, the (r + 1)th singular value of the SHiRA adapter. The above lemma is proved in section F.2. As a consequence of this lemma, anyr rank LoRA adapter of size (m, n) can be seen as an approximation of a SHiRA adapter with mr + rn non-zero elements. Lemma 4.3. Scaling factor for SHiRA is independent of the rank of the adapter and can be set to 1. Please see the proof in Appendix F.3. Lemma 4.3 states that we do not need scaling factors to stabilize the training and, therefore, we do not need additional hyperparameters like α or independent learning rates for separate A and B matrices like in LoRA[13] or LoRA+ [11]. Of note, the scaling factor α can still be used at inference time to vary the intensity of the adapter. 4.2 Adapter Weight Orthogonality in Multi-Adapter Fusion In this section, we provide theoretical and empirical insights by studying properties of SHiRA and LoRA adapter designs for multi-adapter fusion. Lemma 4.4. Consider two adapters, ∆W1 and ∆W2. If one of the adapters, ∆W1 or ∆W2 lies in the null space of the other, then the adapters will not interfere multiplicatively. Proof is given in Appendix F.4. The above lemma implies that two adapters can be efficiently fused without interference if they are orthogonal. In order to analyze the orthogonality between any two adapter weights, we define the following metrics: Definition 1. Adapter Weight Orthogonality Magnitude (AWOM)is defined as the l2 norm of the product AT 1 A2 for two sparse adapter weights A1, A2 ∈ Rn×m. AWOM enables us to understand how far the product AT 1 A2 is from a zero matrix O ∈ Rm×m (Oi,j = {0}∀i, j). Definition 2. Adapter Weight Orthogonality Ratio (AWOR)is defined as the sparsity ratio of the product AT 1 A2. Specifically, AWOR = h 1 − \u0010 ||AT 1 A2||0 m2 \u0011i , where m2 is #elements in AT 1 A2. Together, AWOM and AWOR can provide us an idea of relative orthogonality between adapter weights A1 and A2. Next, we analyze how at least one of the SHiRA strategies (i.e., SHiRA-Struct) can result in near-orthogonal adapters. Recall that, SHiRA-Struct adapters train certain rows/columns and the diagonal elements while keeping all other parameters frozen. Hence, the final trained adapter (after subtracting the pretrained weight) contains a structured pattern of rows/columns and diagonal elements, everything else being zero. Now, without loss of generality, consider two SHiRA-Struct adapters for a layer with square m × m weights: A1 = I + S1 and A2 = I + S2, where S1 and S2 are row-wise patterns of trained weights for two different tasks, and I is an identity matrix. Also, S1 and S2 are non-overlapping, e.g., both have same number of non-zero rows but are offset from each other such that they do not have any common trained rows. Then, the following result holds: Lemma 4.5. Non-overlapping SHiRA-Struct adapters are nearly orthogonal: AWOR for non- overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. Since all SHiRA masks are highly sparse,AT 1 A2 has a lot of zeros, thus making the adapters nearly orthogonal. Proof is provided in Appendix F.5. We demonstrate the orthogonality properties of various adapters and report the simulation results in Fig. 4. For our experiment, we compute AWOM and AWOR for a variety of adapter designs - Figure 4: Comparison of average AWOM (left) and AWOR (right) for 50 randomly initialized adapters. We compare different adapters, namely - Dense, Sparse LoRA, SHiRA-WM and SHiRA-Struct. dense, sparse-LoRA [12] (sparse LoRA A and B weights), SHiRA-WM and SHiRA- Struct based adapters. As shown in Fig. 4, both dense and sparse LoRA have low AWOR for adapters with larger dimen- sions, e.g., 4096 × 4096 which is typical in LLMs. This signifies that these adapter weights are non-orthogonal. On the con- trary, SHiRA-WM achieves much higher AWOR than the LoRA variants. More inter- estingly, SHiRA-Struct is nearly orthogo- nal. Note that, due to high sparsity, AWOM also tends to be much lower for SHiRA adapters than the dense counterparts. Com- bined with the fact that AWOR of SHiRA 6adapters is 63-96% higher sparsity than LoRA, this may suggest that AT 1 A2 would be closer to zero for SHiRA adapters, thus potentially bringing them closer to orthogonality and less interference. Finally, although we have shown interesting properties for SHiRA-Struct, it is still a rank 1 + diagonal adapter. Hence, we need to tradeoff single adapter performance (which strongly depends on adapter’s expressive power) against the multi-adapter fusion capabilities. For instance, next we will see that while SHiRA-Struct is good for vision, SHiRA-SNIP performs well across both LVMs and LLMs. Remark 1. The orthogonality property shown here can lead to disentangled representation for adapter outputs before they merge into the base model. However, this property does not hold for other SHiRA masks that do not have a regular sparsity pattern like SHiRA-Struct even if other SHiRA strategies are still more orthogonal than LoRA weights (e.g., see SHiRA-WM AWOR in Fig. 4(right)). Interestingly, for unstructured sparse masks like SHiRA-WM, SHiRA-Grad, SHiRA-SNIP, etc., both overlapping and non-overlapping adapters have similar orthogonality properties. We discuss this in more detail in section 5.3.2. Finally, this analysis only focuses on orthogonality of adapter weights and not on orthogonality of subspaces. We leave the subspace analysis of SHiRA for future work. 5 Experiments 5.1 Training Setup and Datasets For the vision tasks, we use the RealisticVision-v3 model checkpoint for Stable Diffusion-v1.5, and finetune it using different adapters on two style transfer datasets collected using public domain images. The first dataset is called Bluefire which provides a “blue fire” effect to images. The second dataset is a painting dataset which gives a “paintings” effect (see Appendix section G for more details). For both these datasets, we conduct single- and multi-adapter experiments. To quantify the image quality, we use the Human Preference Score-V2 (HPSv2) [32]. On the language domain, we experiment with LLaMA 7B [29], LLaMA2-7B [30] and evaluate it on various commonsense reasoning benchmarks such as HellaSwag, PIQA, SIQA, BoolQ, Arc-easy, Arc-challenge, OpenBookQA and Winogrande. Similar to our vision investigations, we conduct single- and multi-adapter experiments on LLMs as well. Specifically, for language finetuning, we follow the setup adopted by [14, 20] for training and evaluating LoRA [13], DoRA [20], and SHiRA based finetuned models on downstream tasks. Finally, we also explore generalizability of SHiRA to other popular LoRA models and applications such as SDXL [22] and DreamBooth [25]. Detailed training setups are provided in the Appendix H. 5.2 Vision Results 5.2.1 Impact of Various SHiRA Masks We first evaluate the image quality for SHiRA and LoRA on Paintings and Blue- fire datasets for both single and multi-adapter usecases. Fig. 1 demonstrates com- parison between SHiRA-SNIP and LoRA. As evident, by merely changing 2% pre- trained weights, SHiRA generates high quality images for both finetuning tasks. Style Method %Params HPSv2 score( ↑) α = 1 α = 0.5 Paintings LoRA 3.84 24.7 ± 1.8 31 .3 ± 1.5 SHiRA-Struct 1.99 31.2 ±1.7 33 .0 ±1.8 SHiRA-Grad 2.05 30.3 ± 1.8 32 .3 ± 1.8 SHiRA-SNIP 2.05 29.8 ± 1.8 31 .6 ± 1.8 Bluefire LoRA 3.84 32.6 ± 1.9 33 .6 ± 1.6 SHiRA-Struct 1.99 34.2 ±1.6 34 .1 ±1.5 SHiRA-Grad 2.05 34.2 ±1.5 33.7 ± 1.7 SHiRA-SNIP 2.05 33.7 ± 1.7 33 .7 ± 1.6 Table 1: HPSv2 score of various adapters on Paintings and Bluefire. SHiRA-Struct outperforms all other methods. Next, we compare various types of SHiRA masks in Fig. 5. Clearly, all SHiRA schemes produce impressive images for different prompts and sig- nificantly outperform LoRA. We fur- ther quantify the image quality using HPSv2 for each of the masks. The results are presented in Table 1. As evident, all variants of SHiRA con- sistently achieve superior or similar HPSv2 scores than LoRA, especially for larger α (see details on scaling factor α in Appendix I). More results are provided in Appendices J and K: see Table 10 and Fig. 10, 11, 12. 5.2.2 SHiRA Adapters aid Multi-Adapter Fusion As explained in section 4.2, high sparsity of SHiRA reduces their AWOM and increases the AWOR metrics by increasing the number of zeros in AT 1 A2 product even for unstructured schemes such as SHiRA-WM, SHiRA-Grad, and SHiRA-SNIP. We hypothesized that this may lead to improved multi-adapter fusion performance. This was also pointed out by [26, 8, 31]: naively merging multiple LoRA adapters leads to poor performance and concept loss. 7LoRA thunder bird  Cat  Ship, sunset, sea  House, Prairie  fox  night flower SHiRA-Struct SHiRA-Grad                                     BLUEFIRE SHiRA-SNIP                                     PAINTINGS                                      MULTI-ADAPTER Figure 5: Comparison between different SHiRA masking methods for single- and multi-adapter image generation. For multi-adapter fusion, SHiRA-Struct outperforms all other adapters by generating exceptional images with high frequency details and good concept fusion (e.g., see fox and flower). We now validate the effectiveness of various SHiRA schemes on multi-adapter fusion. The right two columns in Fig. 1 and Fig. 5 show our results. SHiRA is clearly better at capturing both concepts than LoRA. For example, both bird and knight images in Fig. 1 generated with LoRA lose most of the paintings concept. Similarly, for the fox image in Fig. 5, LoRA does not show significant bluefire concept. In contrast, SHiRA-Struct and SHiRA-SNIP consistently perform well on many different prompts and produce exceptional images for multi-adapter fusion. Please refer to Appendix K.1 (Fig. 10, 11, 12, and 13) for additional results. For certain classes that were not included in the training set for both adapters (e.g., see Koala in Fig. 10, 12, and 13 in Appendix), we observe that LoRA produces significant artifacts whereas SHiRA generates high quality images. 5.3 Language Results 5.3.1 Single Adapter SHiRA Finetuning Similar to vision results, we demonstrate the effectiveness of SHiRA on language tasks. For our experiments, each adapter (i.e., weight-magnitude, gradient-magnitude, and SNIP based SHiRA) is trained on the combined 170K sample commonsense reasoning dataset released by [14, 20]. Similar to [20], we train our SHiRA adapters for 3 epochs and compare it against the LoRA baselines. As shown in Table 2, various SHiRA adapters outperform LoRA by 1.9-2.7% on an average on LLaMA-7B. Importantly, SHiRA only modifies 1% base parameter weights as compared to 66.72% (4.5B weights) changed by LoRA in the fused mode, thus enabling rapid switching on edge devices. Interestingly, we found that SHiRA-Struct does not perform well on language tasks likely because it is a rank 1 + diagonal adapter and may not have sufficient expressive power. Moreover, when compared to newer techniques like DoRA [20], our proposed work takes an orthogo- nal approach by finetuning very few parameters of the pretrained weights. This strategy allows for an efficient integration of our adapter with methods like DoRA to improve the expressiveness of the adapters. As we show in Table 2, our proposed adapter benefits from DoRA based finetuning and achieves almost comparable performance (within 0.3%) to DoRA on an average, with an added benefit of changing only 1% parameters at inference time. In contrast, DoRA would lead to 66.72% (4.5B weights ≈ 9GB memory in FP16 format) parameter change in the fused mode. Therefore, SHiRA is orthogonal to other existing low rank methods and can be efficiently integrated with them. 8Model %Params %C BoolQ( ↑) PIQA( ↑) Arc-e( ↑) Arc-c( ↑) WG( ↑) OBQA( ↑) HS( ↑) SIQA( ↑) Avg.( ↑) LoRA 0.83 66.72 68.9 80.7 77.8 61.3 78.8 74.8 78.1 77.4 74.7 (+0%) SHiRA-Grad 1.0 1.0 68.4 80.9 80.2 64.7 80.4 78.2 80.3 79.4 76.6 (+1.9%) SHiRA-WM 1.0 1.0 69.6 81.6 81.5 66.5 79.8 79.4 79.6 77.8 77.0 (+2.3%) SHiRA-SNIP 1.0 1.0 68.3 80.6 81.5 67.9 80.0 79.6 82.1 79.1 77.4(+2.7%) DoRA 0.84 66.72 68.5 82.9 81.4 65.8 80.8 81.0 84.8 79.6 78.1 (+0%) SHiRA-WM-DoRA 6.25 ∗ 1.0 70.9 81.9 81.7 64.9 80.8 79.2 84.5 78.6 77.8(-0.3%) Table 2: Evaluation of LLaMA-7B on Commonsense Reasoning. WG and HS denote WinoGrande and HellaSwag, respectively. %C represents parameters changed in the fused mode. (↑): the higher the better. Green denotes improvement. ∗Trained by masking a high-rank DoRA with a WM mask of top 1% weights, thus changing only 1% of the model during both training and inference. Model %Params %C BoolQ( ↑) PIQA( ↑) Arc-e( ↑) Arc-c( ↑) WG( ↑) OBQA( ↑) HS( ↑) SIQA( ↑) Avg.( ↑) LoRA 0.83 66.72 69.90 79.9 79.8 64.7 82.6 81.0 83.6 79.5 77.61 (+0%) DoRA 0.84 66.72 71.8 83.7 83.7 68.2 82.6 82.4 89.1 76.0 79.68(+2.07%) SHiRA-SNIP 1.0 1.0 70.42 81.71 83.25 68.6 80.51 81.0 89.78 79.01 79.28(+1.67%) Table 3: Results for LLaMA2-7B on Commonsense Reasoning. Finally, we experiment with LLaMA2-7B [30] and demonstrate that SHiRA-SNIP – which achieved the best results on LLaMA-7B – yields significant accuracy gains compared to LoRA and nearly the same accuracy as DoRA (within 0.4%, see Table 3). 5.3.2 Multi-Adapter Fusion on LLMs We now extend our LLM experiments to the multi-adapter fusion setting. To this end, we create anew setup where we independently train multiple adapters on training sets of individual commonsense reasoning benchmarks, i.e., one adapter each for BoolQ, PIQA, and Arc-Easy. In contrast, each adapter in section 5.3.1 was trained on a combined dataset containing 170K samples from all eight commonsense benchmarks as proposed in [ 14, 20]. In the present section, the goal is to evaluate how much accuracy drop various adapters experience when we perform multi-adapter fusion. Due to its simplicity towards constructing a mask, we will use SHiRA-WM in the rest of this paper. Further, we explore two settings - overlapping and non-overlapping SHiRA-WM adapters. The overlapping mask consists of top 1% parameters being trained for all tasks. On the other hand, the non-overlapping setting trains the top 1% weights for the first task, next top 1% for the second task, and so on. We compare the performance of both LoRA and SHiRA across the multi-adapter fusion of these three tasks. As shown in Table 4, both overlapping and non-overlapping multi-SHiRA outperform multi-LoRA on all three commonsense benchmarks. This is inline with our theoretical analysis in section 4.2 where we suggest that even unstructured sparse SHiRA adapters such as SHiRA-WM would have more orthogonal behavior than LoRA due to high sparsity (see higher AWOR of SHiRA-WM in Fig. 4(right)). In comparison, independently trained LoRA adapters would have no such property and suffer greatly during multi-adapter fusion. As a result, we see that both SHiRA models outperform LoRA by more than 6.5% accuracy on average. Further analysis of the properties of these trained adapters is discussed in Appendix K.3 (see Table 13 and Fig. 9). Of note, this experiment also demonstrates the value of creating a good mask for single adapter performance: Non-overlapping masks achieve lower single adapter accuracy than the corresponding overlapping masks since they train less important parameters. Hence, creating an optimal mask for SHiRA should be of significant interest to future research. 5.4 Content/Style Personalization: Generalizing SHiRA to SDXL and DreamBooth Finally, we extend SHiRA to focus on DreamBooth [25] using a much bigger vision model called SDXL [22]. We follow a similar setup as adopted by [2]. Specifically, one content (vase) and two style (wooden sculpture and canvas) datasets with five images each were collected from the DreamBooth dataset [25] and public domains, respectively. These datasets were used to train various content and style adapters. For our experiments, we use SDXL [ 23] as our base model and train both LoRA and SHiRA adapters with comparable trainable parameters on individual single-concept datasets. During training, prompts containing special identifier tokens like \"<CONTENT>\" or \"<STYLE>\" (e.g., <SBU> as content token for vase and <SZN> as style token for wooden sculpture and canvas) are used 9Single Adapter Multi-Adapter Model BoolQ( ↑) PIQA( ↑) Arc_e( ↑) Avg( ↑) BoolQ( ↑) PIQA( ↑) Arc_e( ↑) Avg( ↑) %Drop ( ↓) LoRA 80.52 79.05 75.67 78.41 77.22 71.27 57.45 67.33 (+0%) 11.08 SHiRA-WM-Overlap 78.07 79.71 77.57 78.45 77.43 76.88 67.76 74.02(+6.69%) 4.43 SHiRA-WM-Non-Overlap 76.94 79.71 75.97 77.54 74.22 78.4 69.15 73.92 (+6.59%) 3.62 Table 4: Multi-adapter fusion evaluation of independently trained SHiRA and LoRA adapters on BoolQ, PIQA, and Arc-Easy. %Drop is calculated as drop in average accuracy for multi-adapter fusion compared to the single adapter average accuracy for each adapter. LoRA  SHiRA  LoRA  SHiRA  LoRA  SHiRA Figure 6: LoRA- vs. SHiRA-based DreamBooth on SDXL. Prompts for content/style personalization - left pair: \"A picture of a dog in<STYLE:WOODEN-SCULPTURE> style in a bucket\",center pair: \"A pic- ture of a <CONTENT:VASE> with flowers\", and right pair: \"A picture of a sunset in <STYLE:CANVAS> style\". Here, \"<CONTENT>\" and \"<STYLE>\" are special identifier tokens for content/style. to finetune the SDXL network for content or style personalization, respectively. During inference, similar prompts are used to generate images from LoRA- or SHiRA-based DreamBooth. Fig 6 shows DreamBooth generated images for LoRA and SHiRA. Clearly, our proposed adapter produces high quality personalized images of target concept in different scenarios. This highlights the broad applicability of our adapter while still preserving the benefits of rapid adapter switching. 6 Discussion To summarize our main contributions, we highlight that SHiRA – when used with even the most basic pruning metrics (such as weight- or gradient-magnitude, SNIP, structured masks, etc.) – significantly outperforms LoRA on a variety of large-scale tasks in both large vision and large language domains. For LVM style transfer applications, we found that SHiRA-Struct is the most effective masking technique due to its special orthogonality properties that aid multi-adapter fusion. However, SHiRA- SNIP and SHiRA-Grad are not too far behind and achieve competitive performance as SHiRA-Struct. On the LLM commonsense reasoning side, SHiRA-SNIP is the best strategy out of the masking techniques we have considered in this work. Specifically, SHiRA-Struct did not achieve good results on the more complex commonsense reasoning tasks since it is a combination of a rank-1 + a highly sparse diagonal adapter. SHiRA-Grad on LLMs is about 0.8% worse accuracy than SHiRA-SNIP (76.6% vs. 77.4% average accuracy on commonsense reasoning for LLaMA-1). Therefore, in conclusion, for the applications/fields and the masking techniques considered in this paper, SHiRA- SNIP works well across both language and vision domains. Hence, we recommend that SHiRA-SNIP is one of the strongest candidates that we have considered for sparse finetuning. 7 Conclusion In this paper, we have proposed SHiRA, a new high rank adapter paradigm to demonstrate that even finetuning merely 1-2% parameters of the pretrained generative models is sufficient to achieve high performance on many adapter tasks. We have demonstrated SHiRA’s ability to rapidly switch adapters and to avoid concept loss with support from both theory and experiments. Furthermore, we have shown how specially designed sparse masks can lead to near-orthogonal adapter weights which allows for natural multi-adapter fusion. We have conducted extensive single- and multi-adapter experiments on several vision and language tasks to demonstrate the superiority of SHiRA over LoRA. Our latency- and memory-efficient PEFT-based implementation for training SHiRA runs at nearly the same speed as LoRA while consuming about 16% lower peak GPU memory. Finally, for inference, we have provided a scatter_op based method that can load our SHiRA 5×-16× faster than equivalent LoRA fusion on a CPU, thus demonstrating our rapid switching benefits. 10Acknowledgments We thank anonymous reviewers for insightful comments and constructive feedback which significantly improved the quality of our work. References [1] Goodbye cold boot - how we made LoRA Inference 300% faster. https://huggingface. co/blog/lora-adapters-dynamic-loading . Accessed: 2024-05-15. [2] Sdxl lora for dreambooth. https://github.com/huggingface/diffusers/blob/main/ examples/dreambooth/README_sdxl.md. Accessed: 2024-05-15. [3] Alan Ansell, Edoardo Maria Ponti, Anna Korhonen, and Ivan Vuli ´c. Composable sparse fine-tuning for cross-lingual transfer. arXiv preprint arXiv:2110.07560, 2021. [4] Alan Ansell, Ivan Vuli´c, Hannah Sterz, Anna Korhonen, and Edoardo M Ponti. Scaling sparse fine-tuning to large language models. arXiv preprint arXiv:2401.16405, 2024. [5] Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong.Mathematics for machine learning. Cambridge University Press, 2020. [6] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint arXiv:2311.11696, 2023. [7] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936. [8] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [9] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. [10] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. [11] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354, 2024. [12] Shwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao. Sparseadapter: An easy approach for improving the parameter-efficiency of adapters. arXiv preprint arXiv:2210.04284, 2022. [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [14] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. [15] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019. [16] Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora. arXiv preprint arXiv:2312.03732, 2023. [17] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023. [18] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611, 2020. [19] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. 11[20] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. [21] Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh. Rosa: Accurate parameter-efficient fine- tuning via robust adaptation. arXiv preprint arXiv:2401.04679, 2024. [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [23] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500–22510, 2023. [26] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023. [27] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. [28] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:24193–24205, 2021. [29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [31] Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. Orthogonal subspace learning for language model continual learning. arXiv preprint arXiv:2310.14152, 2023. [32] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [33] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. arXiv preprint arXiv:2109.05687, 2021. [34] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. arXiv preprint arXiv:2311.03099, 2023. [35] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303, 2023. [36] Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. Masking as an efficient alternative to finetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. 12A Edge Deployment Challenges for LoRA (Contd.) To understand the overhead of each of the stages to the standard huggingface LoRA in- ference pipeline (i.e., load, fuse, unfuse, unload), we experiment with the pipeline Stage Server-GPU (s) Desktop-CPU (s) load 0.883 ± 0.085 0 .786 ± 0.056 fuse 0.306 ± 0.044 3 .003 ± 0.023 unfuse 0.206 ± 0.041 2 .916 ± 0.014 unload 0.007 ± 0.001 0 .007 ± 0.001 Table 5: Latency (in s) to load, fuse, unfuse, unload [1] adapters on SDXL on Server-GPU and Desktop- CPU. On a mobile device, fusing/un- fusing would happen for each layer iteratively since we cannot store all weights at the same time on local on- chip memory (unlike a large GPU), re- sulting in much higher overhead. provided in [ 1] and iteratively add adapters to SDXL model [22]. As evident from Table 5, on a server-grade GPU, load time dominates whereas fuse/unfuse/unload times are relatively negligible. However, if we try to run the exact same pipeline on an everyday device like a desktop-grade CPU, we see that the fuse and unfuse times start dominat- ing and can hinder rapid adapter switching. Note that, on an even more constrained device like a mobile phone, AI accelerators do not have sufficient memory to store weights from all layers at the same time in the local memory. Hence, on such devices, we would need to load base model weights for each layer into the local memory, and then fuse corre- sponding LoRA weights before we can run inference for that layer. This obviously leads to a massive inference latency overhead. As a result, existing deployment options are not feasible for rapid switching on mobile devices. B More Details on SHiRA Masks Selecting important salient weights pertinent to a task can be done in many ways, and one popular approach is to use masks. In this section we discuss various strategies to construct sparse mask based on different heuristics to select weights for efficient finetuning of large generative models. B.1 Structured Sparse Mask (SHiRA-Struct) This is a simple structured mask. We begin with making every f rows or columns in a weight matrix trainable, where we call f as the frequency parameter and we choose it based on how much sparsity we need in the adapter. That is, the mask M consists of every f rows or columns containing ones and everything else as zeros. This actually makes it a rank 1 mask because all rows and columns would be linearly dependent. Therefore, to make it high rank, we also add a diagonal parameter which makes the resulting mask M high rank. B.2 Unstructured Sparse Random Mask (SHiRA-Random) Unstructured sparse random masks involve masking individual weights without any specific pattern or structure. The masked weights are randomly scattered throughout the weight tensor, resulting in a sparse weight tensor. However, as the weights are selected without considering their salience to the task, randomly selected unstructured masks may often be sub-optimal for finetuning. One common way of constructing random sparse marks is using Bernoulli sampling: f(k; p) = \u001ap if k = 1, 1 − p if k = 0. (1) where, p is the probability of sampling 1 from the distribution. B.3 Weight Magnitude-Based Sparse Mask (SHiRA-WM) Many earlier works [ 18, 27] have shown the importance of weight magnitude based masks for identifying important weights in the network. Motivated by this literature, we design a weight magnitude based proxy to adapt the behavior of the pretrained network. Specifically, we create a mask by choosing the top-K weight magnitudes at specific layers where SHiRA is employed. We finetune only these top-K weights and keep the rest of them frozen to their pretrained values. Typically,K is a very small percentage of parameters so that the overall number of parameters to be tuned stays comparable to LoRA and its variants. 13B.4 Gradient Based Sparse Mask (SHiRA-Grad) Despite the efficacy of employing weight magnitude based scheme, this approach lacks an inherent awareness of the specific task for which the model is being finetuned. To address this challenge, we design a similar gradient magnitude based proxy to identity important top-K weights for the task and only adapt them during the finetuning process. B.5 SNIP Based Sparse Mask (SHiRA-SNIP) SNIP [19] combines both weight and gradient based schemes and is computed as the magnitude of the product of the weight and its corresponding gradient. This formulation effectively captures the interplay between the weight magnitude, which reflects its overall contribution to the model’s output, and its gradient information, which encodes the weight’s task-specific relevance during finetuning. SNIP for a weight parameter is defined as: SNIP ≜ |⟨Θi, ∇θiL⟩| (2) where ⟨.⟩ represents inner product, Θi is the weight parameter, ∇θiL is the gradient of weight parameter with respect to the task loss L for the ith parameter in the network. C Fuse and Scatter Op implementation In this section, we compare fusing times of LoRA with our efficient scatter_op (torch.Tensor.scatter_) based implementation for SHiRA. For our experiments, we perform benchmarking on a Desktop-grade CPU and compute the average times for various tensor dimensions (e.g., tensor dimension = 4096 implies a weight of size 4096 × 4096, which is typical in modern LLMs). As shown in Fig. 7, our scatter_op-based SHiRA inference pipeline is up to 13×-16× faster than fusing LoRA weights, specially for larger dimensions. 13x13x 13x16x Figure 7: Comparison between average times for LoRA-fuse and SHiRA-scatter_op implementa- tion for 50 randomly initialized weights of various dimensions on a CPU (e.g., dimension = 4096 means that the weight has shape 4096 × 4096). For fusing, we compute time taken to merge LoRA adapters into the base weights (W + AB). Similarly, for the scatter_op, we report time taken to overwrite base weights with SHiRA weights using the scatter op (torch.Tensor.scatter_) based implementation in Pytorch. Next, we present end-to-end switching times for prevalent LVMs and LLMs: SDXL and LLaMA2-7B. Notably, even for a smaller model like SDXL (2.6B params compared to 7B params in LLaMA2-7B), SHiRA achieves a 4.68x faster switching time (0.77s vs. 3.6s), while for LLaMA2-7B, with larger tensor dimensions, SHiRA attains a 5.71x speedup (4.93s vs. 28.15s) on a consumer grade CPU (see Table 6). Note that, fusing LoRA adapters for LLaMA2-7B on a CPU is 28.15s (nearly half a minute). Indeed, waiting half a minute for the adapter to switch/fuse is quite substantial and hampers user experience significantly. In contrast, SHiRA can get the adapter ready for inference within 4.93s, 14Model LoRA SHiRA Speed-up SDXL 3.64 ±0.10 0.77±0.09 4 .68× LLaMA2-7B 28.15 ±1.62 4.93±0.23 5 .71× Table 6: End-to-End switching time on CPU for SDXL and LLaMA2-7B: We achieve a very high (4.7×-5.7×) speed up in switching time compared to LoRA. thus significantly improving the user experience. Note that, once the adapters are fused, inference time on the hardware is equal for both LoRA and SHiRA. Moreover, as discussed in [1], for unfused LoRA case (which can enable rapid switching), the inference latency can be up to 30% higher which is not the case with SHiRA. D Turn any Trainer into SHiRA: Gradient Hook based Implementation In this section, we provide a method to convert any floating point training into SHiRA based finetuning. Specifically, SHiRA can be implemented directly using a functionality called post_accumulate_gradient_hooks available in Pytorch 2.1.0. This gradient_hook can be used to mask gradients after the gradient accumulation step is completed. Moreover, this enables us to apply SHiRA on any publicly available trainer (e.g., Transformers.Trainer, SFT_Trainer, etc.). Therefore, implementing SHiRA on any task is trivial and can be done even without PEFT library, thus making SHiRA very easy to implement. With this gradient hook based implementation, we were able to train all our adapters (including for models such as LLaMA-7B, LLaMA2-7B and SD-1.5) on a single NVIDIA A100 GPU at nearly the same speed as PEFT based LoRA implementation. SHiRA runs at 2.17 it/sec as compared to LoRA which is at 2.42 it/sec for LLaMA-7B finetuning. E Latency- and Memory-Efficient PEFT based Implementation for SHiRA As discussed in Appendix C, scatter_op can be utilized to manage sparse weight updates during inference. Given that SHiRA only finetunes a small subset of the pretrained model weights, we adopt a similar scatter_op-based approach for training. This allows us to retain only the sparse training parameters in the optimizer, thereby significantly reducing the peak GPU memory utilization during training. As shown in Table 7, SHiRA not only trains at almost similar speed as LoRA, but also consumes ∼ 16% lower peak GPU memory. Compared to other variants like DoRA, SHiRA training consumes significantly (∼ 40%) lower peak GPU memory and also trains much faster (SHiRA is about 36% faster than DoRA). All memory requirement data was collected using psutil utility used within the Transformers.Trainer training loop for LLaMA2-7B. Finally, note that, partial finetuning techniques proposed in the pre-LoRA era [36, 28, 3, 33, 10] do not have such memory-efficient implementations, which makes them impractical for large generative models. Therefore, SHiRA significantly outperforms prior partial finetuning techniques in training memory costs and is highly practical for modern LVM and LLM adaptations tasks. Adapter Peak GPU memory (GB) #Training steps/s LoRA-PEFT 35.10 0 .69 DoRA-PEFT 49.49 (+40.99%) 0.49 (-28.98%) SHiRA-PEFT 29.26 (-16.63%) 0.67 (-2.89%) Table 7: Peak GPU memory consumption (in GBs) and #Training steps per second during training for PEFT-based implementation of various adapters for LLaMA2-7B. Relative changes compared to LoRA are highlighted: Green indicates improved performance (lower memory consumption, faster training speed), while Red indicates degraded performance (higher memory consumption, slower training speed). SHiRA trains at nearly the same speed as LoRA but consumes up to 16% lower peak GPU memory. 15F Proofs of Lemma F.1 Lemma 4.1 Lemma 4.1. The parameter complexity and learning complexity of SHiRA is equal to the number of non-zero elements in the adapter. Proof. The parameter complexity and learning complexity depends on the parameters to be learned. The number parameters of the adapter is equal to || ∆W ||0. F.2 Lemma 4.2 Lemma 4.2. If we specify a sparsity factor, the LoRA is r rank approximation of SHiRA with approximation error bounded by σ2 r+1, the (r + 1)th singular value of the SHiRA adapter. Proof. Let ∆W be the given SHiRA adapter of size (m, n) and sparsity factor ρ. Consider the SVD decomposition of ∆W. Next, we construct an r rank matrix approximation using the r largest singular values of the adapter. This reconstructedr rank matrix can be seen as a LoRA adapter. Based on Eckart-Young theorem ([7]) and theorem 4.95 in [5], the approximation error is equal to (r + 1)-th singular value of the SHiRA adapter (σ2 r+1). If the ∆W is an r rank matrix then the approximation error is zero. F.3 Lemma 4.3 Lemma 4.3. Scaling factor for SHiRA is independent of the rank of the adapter and can be set to 1. Proof. The LoRA update equation for any given adapter is as follows: Yout = (W + αrBA)Xin + b. (3) Note αr = α r is the scaling factor, where α is a hyperparameter and r is the rank. Three possible initialization for A and B are as follows: • if A and B are initialized to zero, no learning occurs since this corresponds to saddle point [11]. • A and B are initialized to N(0, σ2 a) and 0 respectively. Here, σ2 a = Θ(n−1), to ensure that AT xi remains bounded with width n of the adapter. • A and B are initialized to 0 and N(0, 1) respectively. Here, it is important to note that the variance of B does not depend of the width of the adapter. However, to avoid gradient collapse for higher ranks, [ 16] recommends to set αr as α√r . Further, optimal convergence the update of A and B matrix updates have different learning rates [11]. For the SHiRA adapter, the update equation is given below: Yout = (W + S)Xin + b. (4) where, S is the sparse matrix with a designed sparsity ratio. All non-zero locations in S are implicitly initialized to the base matrix weights. This initialization ensures that the updates remain bounded during the finetuning stage using stochastic gradient descent. It is also important to note that the scaling is independent of the rank for SHiRA. F.4 Lemma 4.4 Lemma 4.4. Consider two adapters, ∆W1 and ∆W2. If one of the adapters, ∆W1 or ∆W2 lies in the null space of the other, then the adapters will not interfere multiplicatively. 16Proof. The proof leverages two facts: (i) ∆W1 T ∆W2 = O given that one adapter lies in the null space of other. Here, O is a zero matrix ( Oi,j = {0}∀i, j). ( ii) Power series expansion of the non-linear activation function: The power series expansion has terms involving the matrix product of adapters. Since each adapter is in the null space of the other, all terms involving product of adapters are equal to zero. Therefore the adapters do not interfere multiplicatively. This lemma can be extended to a scenario with more than two parallel additive adapters. If all possible pairs of adapters lie in the null space of each others all cross-terms between adapters are zero. F.5 Lemma 4.5 Lemma 4.5. Non-overlapping SHiRA-Struct adapters are nearly orthogonal. That is, AWOR for non-overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. Since all SHiRA masks are highly sparse, this means that the product AT 1 A2 has a lot of zeros, thus making the adapters nearly orthogonal. Proof. Continuing from the adapter definitions used in the main text for this lemma, let us compute AT 1 A2 and then analyze its AWOR: AT 1 A2 = (I + S1)T (I + S2) = I + IS2 + ST 1 I + ST 1 S2 = I + S2 + ST 1 (5) Here, ST 1 S2 is zero by design because S1 and S2 do not have common non-zero rows. Moreover, since both S1 and S2 are highly sparse, AT 1 A2 has a sparsity equal to the sum of sparsity of I, S1 and S2. Note that, I + S2 = A2. Thus, AWOR for non-overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. G Dataset and Evaluation Metric Descriptions G.1 Datasets G.1.1 Language Datasets Dataset #Train #Val Test PiQA 16K 2K 3K BoolQ 9.4K 2.4K 2.4K SIQA 33.4K 1.9K 1.9K OBQA 4.9K 0.5K 0.5K Winogrande 9.2K 1.3K 1.8K HellaSwag 39.9K 10K 10K Arc_easy 2.25K 570 2.36K Arc_challenge 1.12K 299 1.12K Table 8: Commonsense Benchmarks For language finetuning tasks, we use the commonsense rea- soning datasets, which comprise 8 sub-tasks, each with a pre- defined training and testing set as shown in Table 8. We follow the setting of [ 14] for SHiRA Single Adapter training. The common sense reasoning training dataset is a combination of the training datasets provided by [15], while we evaluate each evaluation dataset separately as in Table 2. For multi-adapter LLM experiments, we train each adapter from one particu- lar task, and then perform multi-adapter evaluation on all the tasks. G.1.2 Vision Datasets For style transfer adaptation tasks as described in sections 5.2.1 and 5.2.2, we use two datasets, Bluefire and Paintings. Images present in both of these datasets are collected from public-domain (CC-0 license). The Bluefire dataset consists of a total of 54 images consisting of 6 different concepts - Cars, Dragons, Birds, Foxes, Men and Castles. For all these concepts, images with \"blue-fire\" effect are collected and used for style transfer finetuning. The validation of the Bluefire dataset consists of 30 images. 9 of the 30 images contain one of the 6 concepts in the training set, and the rest 21 are new. A few examples of unseen concepts in the validation set: football, monster, sword, chess rook, lion, koala etc. Similarly, the painting datasets contain a total of 90 images of \"painting\" style images of 9 different concepts - fire, birds, elephants, ships, horses, flowers, women, men and tigers. The validation set of the Paintings dataset consists of 21 images, out of which 9 contain concepts from the training set. The remaining 12 are new concepts not included in the training set. A few examples of unseen concepts in the validation set: lion, tiger, dog, cat, koala, panda, and other landscapes. 17Alpha=0.0  Alpha=0.25  Alpha=0.50                                                 blazing fiery car, lightning Alpha=0.75  Alpha=1.0  Alpha=1.25 Figure 8: Effect of α scaling on image quality. α = 0.0 is the base model output without any adapter effects. We can see that as theα increases, the SHiRA adapter effect increases similar to how it works for LoRA inference. G.2 Evaluation Metrics HPSv2 metric evaluation For all style transfer finetuning experiments with Bluefire and Paintings dataset, we report HPS metric to quantify the quality of the generated images. For Bluefire validation, 30 images per validation prompt are generated for different seeds, hence generating 900 images for HPS analysis. We follow a similar paradigm for Paintings and generate 630 images with 21 prompts. H Training Details In this section, we list hyperparameters used for our experiments for Language and Vision finetuning tasks in Table 9. Method Adapter Target Modules Optimizer LR LR-Scheduler Rank LoRA LVM q-proj,k-proj,v-proj,up-proj,down-proj AdamW 1e − 4 Cosine 64 SHiRA LVM 1e − 4 Cosine NA LoRA LLM 2e − 4 Linear 32 DoRA LLM 2e − 4 Linear 32 SHiRA LLM 5e − 4 Linear NA Table 9: Training hyperparameters used for finetuning experiments. All finetuning and evaluation experiments for language and vision tasks are done using a single NVIDIA A100 GPU. I Effect of Scaling Factor α during Inference As described in section 3.1, in order to adapt the pretrained model to a new task, we only finetune very few weight parameters relevant to the task. For our adapter, we can easily extract out these modified weights as S = Wnew − W, where Wnew is the weight obtained after SHiRA training, and W is the prertained weight. Since only 1-2% weights change during SHiRA training, S is highly sparse and thus constitutes our sparse adapter. Hence, the new finetuned weights of the base model can be viewed as Wnew = W + S. Similar to LoRA, the strength of SHiRA adapter at inference time can be modified using a scaling factor α. For any defined α scaling, the new weights of the model can be expressed as Wnew = W + αS. Fig. 8 shows the effect of varying α on the output image. As evident, choosing an α <1 reduces the \"blue fire\" in the generated image and whereas α >1 amplifies the style transfer effect. For α = 0.0, the adapter is disabled and the model’s output is the same as that for the base model. J More Detailed Comparison among Various Masks We provide HPSv2 scores for all SHiRA masking schemes in Table 10. 18Adapter Style Adapter Method %Params HPSv2 score( ↑) α = 1 α = 0.75 α = 0.5 Paintings LoRA 3.84 24.7 ± 1.8 28 .4 ± 1.4 31 .3 ± 1.5 SHiRA-Struct 1.99 31.2 ± 1.7 32 .1 ± 1.8 33 .0 ± 1.8 SHiRA-Rand 2.05 30.7 ± 1.9 31 .7 ± 1.8 32 .7 ± 1.9 SHiRA-WM 2.05 29.7 ± 1.9 30 .6 ± 1.7 32 .1 ± 1.8 SHiRA-Grad 2.05 30.3 ± 1.8 31 .3 ± 1.7 32 .3 ± 1.8 SHiRA-SNIP 2.05 29.8 ± 1.8 30 .8 ± 1.8 31 .6 ± 1.8 Bluefire LoRA 3.84 32.6 ± 1.9 34 .1 ± 1.5 33 .6 ± 1.6 SHiRA-Struct 1.99 34.2 ± 1.6 34 .7 ± 1.5 34 .1 ± 1.5 SHiRA-Rand 2.05 33.4 ± 1.9 34 .1 ± 1.5 33 .7 ± 1.7 SHiRA-WM 2.05 31.9 ± 2.1 33 .3 ± 1.6 33 .1 ± 1.7 SHiRA-Grad 2.05 34.2 ± 1.5 34.4 ± 1.5 33 .7 ± 1.7 SHiRA-SNIP 2.05 33.7 ± 1.7 34 .3 ± 1.4 33 .7 ± 1.6 Table 10: Comparison between LoRA and various SHiRA schemes with respect to HPSv2 metric. For vision problems, SHiRA-Struct outperforms all other methods. Adapter cifar10 cifar100 food101 dtd LoRA 97.94 87 .97 84 .27 69 .41 SHiRA 98.05 88.15 84.43 69.73 Table 11: LoRA vs SHiRA for Image Classification using ViT-Base model. SHiRA consistently outperforms LoRA on these transfer learning tasks. K More Results K.1 Additional Sample Images for Vision Style Transfer Applications We show many more sample images for various adaptation usecases in Fig. 10, 11, 12, and 13. K.2 Image Classification and GLUE We further conduct more experiments on image classification and GLUE tasks using SHiRA-WM. For image classification, we finetune Vision Transformer (ViT) using LoRA and SHiRA for four common transfer learning datasets, namely, CIFAR-10, CIFAR-100, Food101, and Describable Textures Dataset (DTD) (see Table 11). Both methods have comparable parameters around 300K. As shown in Table 11, we outperform LoRA on all image classification tasks. For GLUE, we use the code released by SoRA [6] which relies on dynamically adjusting the ranks of the adapters. In Table 12, we report accuracy on four common GLUE tasks: QNLI, COLA, SST2, and MRPC. Accuracy numbers for LoRA and SoRA are directly taken from the SoRA paper since we are using the official code to run SHiRA experiments. As evident, with nearly 2x smaller adapter, SHiRA outperforms LoRA by 1.1% accuracy on average. Further, SHiRA achieves a similar accuracy as SoRA while being 30% smaller in adapter size. Indeed, SoRA cannot enable rapid switching like SHiRA. Therefore, we again demonstrate that a simple approach like SHiRA-WM outperforms LoRA and its advanced variants with a similar or significantly better accuracy while providing additional deployment benefits. K.3 Analysis of Trained Adapters Are adapter tasks sufficiently different? Table 13 shows the L2 analysis for the adapters trained in Table 4. We compute the L2 distance between each adapter and the original pretrained weights (all adapters train top 1% weights in the overlap setting) as well as the L2 distance between each adapter. Clearly, each adapter is closer to the pretrained weights compared to the other adapters. This demonstrates that the tasks are sufficiently different. Why does SHiRA-WM-Overlap perform well? Next, as shown in Fig. 9, for unstructured SHiRA masks, both overlapping and non-overlapping adapters have identical AWOR and AWOM values. This suggests that their orthogonality characteristics are quite similar due to the high sparsity. We hypothesize that this is the main reason for the good performance of SHiRA-WM-overlap and explains the results in Table 4. 19Adapter#Params COLA QNLI MPRC SST2 Average LoRA 1.33M 69.73 93 .76 89 .71 95 .57 87 .19(+0%) SoRA 910K 71.48 94.28 91.98 95 .64 88.34(+1.15%) SHiRA 636K 70.62 93 .90 92.15 96.50 88.29(+1.10%) Table 12: GLUE benchmarking for the DeBERTa-V3-base. As evident, with nearly 2x smaller adapter, SHiRA outperforms LoRA by 1.1% accuracy on average. Further, SHiRA achieves a similar accuracy as SoRA while being 30% smaller in adapter size. Hence, SHiRA generalizes to other language tasks as well. Base Arc_e BoolQ PIQA Base 0 37.0 67 .0 75 .0 Arc_e 0 75 .0 81 .5 BoolQ 0 98 .5 PIQA 0 Table 13: L2 distances between pretrained base weights and SHiRA adapters vs. distances between adapters: Adapters are closer to the base model weights than to each other. 128 256 512 1024 2048 4096 8192 Dimension of the Adapter Weight 102 103 104 105 106 Adapter Weight Orthogonality Magnitude SHiRA-WM-overlap SHiRA-WM-nonoverlap Dense 128 256 512 1024 2048 4096 8192 Dimension of the Adapter Weight 0.0 0.2 0.4 0.6 0.8 1.0Adapter Weight Orthogonality Ratio Figure 9: Adapter Weight Orthogonality Magnitude (AWOM: L2 magnitude) and Adapter Weight Orthogonality Ratio (AWOR: Sparsity Ratio) of the productAT 1 A2 between two adapters for unstruc- tured SHiRA-WM overlap and non-overlapping cases (99% sparse). We vary the adapter dimensions (e.g., 4096 refers to a pretrained weight of dimensions 4096 × 4096) and measure AWOM and AWOR for each weight size (averaged over 50 seeds). For unstructured SHiRA masks, overlapping and non-overlapping adapters achieve coinciding AWOR and AWOM, thus suggesting that their orthogonality properties are very similar due to high sparsity. This explains our multi-adapter LLM results in Table 4. L Societal Impact Our work enables on-device deployment of adapters which can have a clear positive impact on society as it allows for privacy-preserving generative AI use. With our work, users would be able to rapidly generate images in specific styles directly on-device. On the other hand, while efficient finetuning techniques have many advantages, they bring the potential risk of digital forgery. This is mainly due to finetuning the generative models on a much smaller subset of data, leading to potential overfitting. As our proposed method is also a parameter-efficient finetuning technique, it suffers from similar potential risk as the other PEFT algorithms. M Limitations and Future Work In this work, we show that our proposed sparse high rank adapter, SHiRA, with merely finetuning 1- 2% parameters of the pretrained generative models is sufficient to achieve high performance on many adapter tasks. However, in order to adopt our method for mobile deployment, hardware-software 20co-design techniques, such as lookup-table (LUT) based approaches, may be necessary to optimize the implementation for edge devices. Moreover, as discussed in the main text, building optimal sparse masks (i.e., which parameters to train for a given task) warrants further investigation. LoRA Man  Lion, Forest  Ship, sunset, sea  House, Prairie  Koala Bear  Horse, Knight SHiRA-Struct SHiRA-Grad                                     BLUEFIRE SHiRA-SNIP                                     PAINTINGS                                      MULTI-ADAPTER Figure 10: More image samples for single and multi-adapter fusion. We observe that LoRA exhibits artifacts for koala and concept loss for knight in Multi-Adapter fusion while SHiRA produces significantly better images. 21LoRA Man  Astronaut in Galaxy  House on Mountain  Bird  Thunder bird  Fox SHiRA-Struct SHiRA-Rand SHiRA-WM SHiRA-Grad                                         BLUEFIRE SHiRA-SNIP                                         PAINTINGS                                          MULTI-ADAPTER Figure 11: More image samples for single and multi-adapter fusion. We observe that LoRA images exhibit concept loss for bird in Multi-Adapter fusion. 22LoRA car  lion  bird  Ship,sunset  koala bear  Tiger SHiRA-Struct SHiRA-Rand SHiRA-WM SHiRA-Grad                                         BLUEFIRE SHiRA-SNIP                                         PAINTINGS                                          MULTI-ADAPTER Figure 12: More image samples for single and multi-adapter fusion. Koala is not included in the training set of either of the Bluefire and Paintings Adapter styles. We observe that for this class, LoRA has significant artifacts whereas SHiRA produces exceptional images. 23LoRA man in mythical forest  Koala Bear  Bird  Car  Fox  House on prairie,storms,fire SHiRA-Struct SHiRA-Rand SHiRA-WM SHiRA-Grad SHiRA-SNIP Figure 13: More results for multi-adapter fusion. Koala is not included in the training set of either of the Bluefire and Paintings Adapter styles. We observe that for this class, LoRA has significant artifacts whereas SHiRA produces exceptional images. 24NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The manuscript discusses and reports detailed results accurately reflecting the claims and the scope of the work. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, discussed in section M. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] 25Justification: Discussed in section Appendix F. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All experimentation details for training and inference are included in the main and supplementary materials. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 26Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We plan to open source the code and datasets pending legal approval. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experimentation details required for understanding the results are included in the main and supplementary materials. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Yes, mean and standard deviation of the performance metrics are reported across various seed values. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). 27• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details of compute used for training and inference are included. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform to NeurIPS code of ethics. • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes, discussed in section Appendix L. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 28• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable since our models do not have high risk of misuse. • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We follow the license terms for every model and dataset we use. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 2913. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, details of the datasets are provided in the Appendix G.1.2. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not Applicable Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not Applicable Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 30",
      "meta_data": {
        "arxiv_id": "2406.13175v2",
        "authors": [
          "Kartikeya Bhardwaj",
          "Nilesh Prasad Pandey",
          "Sweta Priyadarshi",
          "Viswanath Ganapathy",
          "Shreya Kadambi",
          "Rafael Esteves",
          "Shubhankar Borse",
          "Paul Whatmough",
          "Risheek Garrepalli",
          "Mart Van Baalen",
          "Harris Teague",
          "Markus Nagel"
        ],
        "published_date": "2024-06-19T03:13:11Z",
        "pdf_url": "https://arxiv.org/pdf/2406.13175v2.pdf",
        "github_url": "https://github.com/Qualcomm-AI-research/SHiRA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Sparse High Rank Adapters (SHiRA), a new parameter-efficient finetuning (PEFT) paradigm that addresses limitations of Low Rank Adaptation (LoRA), specifically related to mobile deployment, rapid adapter switching, and multi-adapter fusion. SHiRA enables training by modifying only 1-2% of base model weights, resulting in a highly sparse adapter that can be rapidly switched in a fused mode without inference overhead. Key contributions include: (i) proposing SHiRA to enable rapid switching for fused adapters and reduce concept loss in multi-adapter fusion; (ii) demonstrating that finetuning a small fraction of parameters (1-2%) significantly outperforms LoRA on diverse vision and language tasks; (iii) providing theoretical and empirical insights into how high sparsity aids multi-adapter fusion by reducing concept loss; (iv) offering a PEFT-based, memory- and latency-efficient implementation that trains nearly as fast as LoRA while consuming 16% lower peak GPU memory, and enables 5x-16x faster inference loading on CPU than LoRA fusion.",
        "methodology": "SHiRA operates by making a small percentage (1-2%) of existing pretrained model weights trainable, rather than adding new weights. This is achieved through gradient masking during backpropagation, where a highly sparse mask (M = {0, 1}) is applied to gradients using a Hadamard product. The paper explores various mask construction strategies: SHiRA-Struct (structured rows/columns/diagonal), SHiRA-Rand (random selection), SHiRA-WM (top-K weights by magnitude), SHiRA-Grad (top 1-2% weights by gradient magnitude), and SHiRA-SNIP (combining weight and gradient magnitudes). For rapid switching, SHiRA stores sparse weights and their indices, directly overwriting base model weights using a `torch.Tensor.scatter_` operation. For multi-adapter fusion, SHiRA leverages high sparsity to reduce interference between adapters, aiming for near-orthogonality, especially with SHiRA-Struct. Theoretical insights include analysis of parameter/learning complexity, LoRA as an approximation of SHiRA, and adapter weight orthogonality (AWOM and AWOR metrics). Implementations include a backward hook-based gradient masking for general trainers and a PEFT-based one for efficiency.",
        "experimental_setup": "The research conducted extensive experiments across Large Vision Models (LVMs) and Large Language Models (LLMs). For vision tasks, the RealisticVision-v3 model checkpoint for Stable Diffusion-v1.5 and SDXL were finetuned on two style transfer datasets: Bluefire (54 images for 'blue fire' effect) and Paintings (90 images for 'paintings' effect). DreamBooth was also used with SDXL for content/style personalization. Image quality was quantified using Human Preference Score-V2 (HPSv2). For language tasks, LLaMA-7B and LLaMA2-7B were evaluated on commonsense reasoning benchmarks (HellaSwag, PIQA, SIQA, BoolQ, Arc-easy, Arc-challenge, OpenBookQA, Winogrande), using a combined 170K sample dataset for single adapters, and individual task datasets for multi-adapter fusion. Accuracy was the metric. Further experiments included Image Classification (ViT on CIFAR-10, CIFAR-100, Food101, DTD) and GLUE tasks (DeBERTa-V3-base on QNLI, COLA, SST2, MRPC). All finetuning and evaluation experiments were performed on a single NVIDIA A100 GPU. Inference speed benchmarks were conducted on a Desktop-grade CPU, comparing LoRA fusion vs. SHiRA `scatter_op` for various tensor dimensions, and end-to-end switching times for SDXL and LLaMA2-7B.",
        "limitations": "The paper identifies several limitations: (i) For optimal mobile deployment, hardware-software co-design techniques (e.g., lookup-table based approaches) may be necessary to fully optimize SHiRA's implementation on edge devices. (ii) Building optimal sparse masks, specifically determining which parameters to train for a given task, requires further investigation. (iii) SHiRA-Struct, while effective for vision tasks, did not perform well on more complex language tasks like commonsense reasoning, likely due to its inherent rank 1 + diagonal adapter structure potentially lacking sufficient expressive power. (iv) Similar to other parameter-efficient finetuning (PEFT) techniques, SHiRA carries the potential risk of digital forgery due to finetuning on smaller subsets of data, which could lead to overfitting. (v) A deeper subspace analysis of SHiRA's properties is left for future work.",
        "future_research_directions": "Future research directions include: (i) Exploring hardware-software co-design techniques, such as lookup-table (LUT) based approaches, to further optimize SHiRA's implementation for resource-constrained edge devices and achieve more efficient mobile deployment. (ii) Investigating advanced strategies for building optimal sparse masks, focusing on identifying the most pertinent parameters to train for specific tasks to maximize performance and efficiency. (iii) Conducting a comprehensive subspace analysis of SHiRA to better understand its theoretical properties and how it interacts with the base model, which was explicitly left as future work."
      }
    },
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
      "full_text": "DoRA: Weight-Decomposed Low-Rank Adaptation Shih-Yang Liu1 2 Chien-Yi Wang1 Hongxu Yin 1 Pavlo Molchanov1 Yu-Chiang Frank Wang1 Kwang-Ting Cheng 2 Min-Hung Chen 1 Abstract Among the widely used parameter-efficient fine- tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decom- position analysis to investigate the inherent dif- ferences between FT and LoRA. Aiming to re- semble the learning capacity of FT from the findings, we propose Weight-Decomposed Low- Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, mag- nitude and direction, for fine-tuning, specifically employing LoRA for directional updates to ef- ficiently minimize the number of trainable pa- rameters. By employing DoRA, we enhance both the learning capacity and training stabil- ity of LoRA while avoiding any additional in- ference overhead. DoRA consistently outper- forms LoRA on fine-tuning LLaMA, LLaV A, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruc- tion tuning, and image/video-text understanding. Code is available at https://github.com/ NVlabs/DoRA. 1. Introduction Models that are pre-trained with extensive general domain datasets have demonstrated remarkable generalization abil- ities, significantly benefiting a wide array of applications, from natural language processing (NLP) tasks (Qin et al., 2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022; Liu et al., 2023a). To tailor these general models for spe- cific downstream tasks, full fine-tuning (FT) is commonly 1NVIDIA 2HKUST. Correspondence to: Shih-Yang Liu <shi- hyangl@nvidia.com, sliuau@connect.ust.hk>, Min-Hung Chen <minhungc@nvidia.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Magnitude B  A  Pretrained Weight Merged Weight Pretrained Weight Pretrained Weight Adapt Frozen Trainable Magnitude Direction Direction Decompose (Initialize) Merge Figure 1.An overview of our proposed DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that || · ||c denotes the vector-wise norm of a matrix across each column vector. employed, involving the retraining of all model parameters. Nevertheless, as the size of models and datasets expand in scale, the expense associated with fine-tuning the entire model becomes prohibitively large. To address this issue, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019) have been introduced to fine- tune the pre-trained models with only a minimal number of parameters. Among these, LoRA (Hu et al., 2022), which does not change the model architecture, has become notably popular for its simplicity and efficacy. Nevertheless, there is still a capacity gap between LoRA and FT, which is often attributed to the limited number of trainable parameters without further exploration of other underlying causes (Hu et al., 2022; Kopiczko et al., 2024). Drawing on Weight Normalization (Salimans & Kingma, 2016), which achieves faster convergence via improving the conditioning of the gradient with weight reparameterization, we introduce a novel weight decomposition analysis that ini- tially reparameterizes model weights into magnitude and di- rectional components, subsequently examining the changes 1 arXiv:2402.09353v6  [cs.CL]  9 Jul 2024DoRA: Weight-Decomposed Low-Rank Adaptation in magnitude and direction introduced by LoRA and FT. Our analysis reveals that LoRA and FT exhibit markedly distinct patterns of updates, leading us to surmise that these variations mirror the learning capability of each method. Inspired by our findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA), which begins by decompos- ing the pre-trained weight into its magnitude and directional components, then fine-tunes both. Given the substantial size of the directional component in terms of parameters, we exploit LoRA for the directional adaptation to enable efficient fine-tuning, as illustrated in Figure.1. Moreover, by showing a learning behavior similar to FT both empir- ically and mathematically, suggesting a learning capacity closely resembling FT, we have validated DoRA across a wide variety of tasks, from NLP to Vision-Language, and over various backbones, including LLM and LVLM. The experimental results show that DoRA consistently outper- forms LoRA without sacrificing inference efficiency, such as commonsense reasoning (+3.7/+1.0 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +4.4 on LLaMA3-8B), visual in- struction tuning (+0.6 on LLaV A-7B), and image/video-text understanding (+0.9/+1.9 on VL-BART). The summary of our contributions is as follows: • We introduce DoRA, a novel PEFT method that incor- porates weight decomposition, achieving a learning capacity closely resembling FT without any additional inference latency over LoRA. • We introduce a novel weight decomposition analysis to uncover the fundamental differences in the learning patterns of FT and different PEFT methods. • DoRA consistently surpasses LoRA on various tasks, from NLP to Vision-Language benchmarks and across various backbones, including LLM and LVLM. 2. Related Works Parameter-Efficient Fine-Tuning (PEFT)methods are de- signed to reduce the high expense of fine-tuning large-scale models. They achieve this by training a relatively small subset of parameters, compared to the total number of pa- rameters, for adapting to downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as Adapter-based methods, which involve introducing additional trainable modules into the original frozen backbone, such as (Houlsby et al., 2019; He et al., 2021; Karimi Mahabadi et al., 2021; mahabadi et al., 2021). For example, (Houlsby et al., 2019) proposes adding linear modules in sequence to the existing layer, whereas (He et al., 2021) advocates for integrating these modules in parallel with the original layer to enhance performance. The second category is Prompt-based methods. These methods add extra soft tokens (prompts) to the initial input and fo- cus solely on fine-tuning these trainable vectors, as seen in works like (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023). However, these approaches typically face challenges due to their sensitivity to initialization, af- fecting their overall effectiveness. These first two categories, whether altering the model’s input or architecture, result in increased inference latency compared to the baseline model. LoRA (Hu et al., 2022) and its variants are among the third category of PEFT, notable for not adding any extra inference burden. These methods apply low-rank matrices to approximate weight changes during fine-tuning and can merge with pre-trained weights prior to inference. For ex- ample, (Zhang et al., 2023) employs SVD decomposition and prunes less significant singular values for more efficient updates. (Hyeon-Woo et al., 2022) focuses on low-rank Hadamard product for federated learning. (Qiu et al., 2023; Liu et al., 2023b) exploit orthogonal factorization in fine- tuning diffusion models. (Renduchintala et al., 2023) uses weight tying to further reduce the trainable parameters. (Yeh et al., 2023) introduces a unified LoRA family framework for Stable diffusion. (Ponti et al., 2022) chooses different combinations of LoRAs from the inventory with a routing function for different tasks. (Kopiczko et al., 2024) imple- ments learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Our research also falls within this third category, and we validate the efficacy of our proposed method alongside LoRA and its variants through comprehensive experimentation. 3. Pattern Analysis of LoRA and FT 3.1. Low-Rank Adaptation (LoRA) Building upon the hypothesis that updates made during the fine-tuning exhibit a low “intrinsic rank”, LoRA (Hu et al., 2022) proposes using the product of two low-rank matri- ces to update the pre-trained weights incrementally. For a pre-trained weight matrix W0 ∈ Rd×k, LoRA models the weight update ∆W ∈ Rd×k utilizing a low-rank decompo- sition, expressed as BA, where B ∈ Rd×r and A ∈ Rr×k represent two low-rank matrices, withr ≪ min(d, k). Con- sequently, the fine-tuned weight W′ can be represented as: W′ = W0 + ∆W = W0 + BA (1) where W0 remains static during the fine-tuning process, and the underlined parameters are being trained. The ma- trix A is initialized with uniform Kaiming distribution (He et al., 2015), while B is initially set to zero, resulting in ∆W = BA being zero at the start of training. Notably, this decomposition of ∆W can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Ad- ditionally, based on Eq. (1), we can merge the learned ∆W with the pre-trained weight W0 and obtain W′ in advance 2DoRA: Weight-Decomposed Low-Rank Adaptation of deployment, and given that both W′ and W0 both fall within the dimensionality of Rd×k, LoRA and its related variants do not introduce any extra latency during the infer- ence compared to the original model. 3.2. Weight Decomposition Analysis The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank r of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Con- sequently, many previous studies have attributed the dis- crepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans & Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two sep- arate components, magnitude and direction, to reveal the inherent differences in LoRA and FT learning patterns. Analysis Method: This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of W ∈ Rd×k can be formulated as: W = m V ||V ||c = ||W||c W ||W||c (2) where m ∈ R1×k is the magnitude vector, V ∈ Rd×k is the directional matrix, with ||·|| c being the vector-wise norm of a matrix across each column. This decomposition ensures that each column of V/||V ||c remains a unit vector, and the corresponding scalar in m defines the magnitude of each vector. For our weight decomposition analysis, we select the VL- BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre- trained weight W0, the full fine-tuned weight WFT, and the merged LoRA weight WLoRA of query/value weight matrix using Eq. (2). The magnitude and directional variations between W0 and WFT can be defined as follows: ∆Mt FT = Pk n=1 |mn,t FT − mn 0 | k (3) ∆Dt FT = Pk n=1(1 − cos(V n,t FT , Wn 0 )) k (4) Here, ∆Mt FT and and ∆Dt FT represent the magnitude dif- ference and directional difference between W0 and WFT at t training step respectively, with cos(·, ·) being the cosine similarity function. Mn,t FT and Mn 0 are the nth scalars in their respective magnitude vectors, while V n,t FT and Wn 0 are the nth columns in V t FT and W0. The magnitude and direc- tional differences between WLoRA and W0 are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition anal- ysis on each of these checkpoints to determine the ∆M and ∆D throughout different layers. Analysis Results: Figure 2 (a) and (b) illustrate the alter- ations in the query weight matrix of FT and LoRA, with each point representing a ( ∆Dt, ∆Mt) pair from query weight matrices across different layers and training steps. Similarly, Figure 7 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively neg- ative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direc- tion updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and direc- tional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resem- bling that of FT, and can improve the learning capacity over LoRA. 4. Method 4.1. Weight-Decomposed Low-Rank Adaptation Drawing from the insights of our weight decomposition analysis, we introduce Weight- Decomposed L ow-Rank Adaptation (DoRA). DoRA initially decomposes the pre- trained weight into its magnitude and directional compo- nents and finetunes both of them. Because the directional component is large in terms of parameter numbers, we fur- ther decompose it with LoRA for efficient finetuning. Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tun- able simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag- 3DoRA: Weight-Decomposed Low-Rank Adaptation 0.014 0.016 0.018 0.020 0.022 D (a) 0.05 0.06 0.07 0.08 0.09 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.10 0.11 0.12 0.13 0.14 0.15 0.16 D (b) 0.25 0.30 0.35 0.40 0.45 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 D (c) 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 2.Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer. nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinc- tion between DoRA and weight normalization (Salimans & Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, mak- ing the method sensitive to different initializations. Con- versely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight W0 as outlined in Eq. (2), where m = ||W0||c and V = W0 after initialization. We then keep V frozen and m a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as: W′ = m V + ∆V ||V + ∆V ||c = m W0 + BA ||W0 + BA||c (5) where ∆V is the incremental directional update learned by multiplying two low-rank matrices B and A, and the underlined parameters denote the trainable parameters. The matrices B ∈ Rd×r and A ∈ Rr×k are initialized in line with LoRA’s strategy to ensure that W′ equals W0 before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency. We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and W0 in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for (∆D, ∆M) of both DoRA and FT, we reveal that in contrast to LoRA’s pattern, DoRA, and FT are characterized by a distinct neg- ative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substan- tial knowledge suitable for various downstream tasks. There- fore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally com- pute the correlation between ∆D and ∆M for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT’s signifies its superior learning capacity over LoRA. 4.2. Gradient Analysis of DoRA In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the opti- mization of ∆V . Subsequently, we analyze from the gradi- ent’s perspective to explicate the learning pattern of DoRA, which tends to have a negative slope. From Eq. (5), we can obtain the gradient of Loss L with respect to m and V ′ = V + ∆V as: ∇V ′L = m ||V ′||c \u0012 I − V ′V ′T ||V ′||2c \u0013 ∇W′L (6) ∇mL = ∇W′L ·V ′ ||V ′||c (7) Eq. (6) reveals that the weight gradient ∇W′L is scaled by m/||V ′||c and is projected away from the current weight matrix. These two effects contribute to aligning the gra- dient’s covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans & Kingma, 2016). Additionally, given that V ′ = V + ∆V , the gradient ∇V ′L is equivalent to ∇∆V L. Therefore, the optimization benefits derived from this decomposition are fully transferred to ∆V , enhancing the learning stability of LoRA. We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent dis- cussion, we represent vectors using lower-case letters in- stead of the previous matrix form notation. Consider w′′ = w′ + ∆w as the parameter update for a weight vector, where ∆w ∝ ∇w′L. In two hypothetical update 4DoRA: Weight-Decomposed Low-Rank Adaptation scenarios, S1 and S2, S1 involves a smaller directional update (∆DS1), while S2 involves a larger one ( ∆DS2). Assuming ||∆wS1|| = ||∆wS2||, and at time 0, we have ∆v = 0 and v′ = v. From ∆DS1 < ∆DS2, it follows that |cos(∆wS1, w′)| > |cos(∆wS2, w′)|. Since ∆w ∝ ∇w′L, it implies |cos(∇S1 w′ L, w′)| > |cos(∇S2 w′ L, w′)|. From Sec 4.1, with v initialized as v0 and w′ = w0 at time 0, we get |cos(∇w′L, w′)| = |cos(∇w′L, v′)| = |cos(∇w′L, v)|. Using the cosine similarity equation with ∆v = 0: cos(∇w′L, v′) = cos(∇w′L, v) = ∇w′L ·v ||∇w′L||||v|| (8) denote m∗ as the magnitude scalar of vector w′ then Eq. (7) w.r.t m∗ can be rewritten to: ∇m∗L = ∇w′L ·v′ ||v′|| = ||∇w′L|| ·cos(∇w′L, v) (9) Given that ||∆wS1|| = ||∆wS2|| for S1 and S2, and ||∇S1 w′ L|| = ||∇S2 w′ L||. Therefore, with: ||∇S1 w′ L|| · |cos(∇S1 w′ L, v)| > ||∇S2 w′ L|| · |cos(∇S2 w′ L, v)| (10) it can be inferred that |∇S1 m∗L| > |∇S2 m∗L| which indicate that S1 has larger magnitude updates over S2 while having smaller directional alteration than that of S2. Our conclu- sion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT. 4.3. Reduction of Training Overhead In Eq. (1), the gradients of W′ and ∆W are the same. How- ever, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low- rank updates differs from that ofW′, as illustrated in Eq. (6). This divergence necessitates extra memory during backprop- agation. To address this, we suggest treating||V + ∆V ||c in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while ||V + ∆V ||c dynamically reflects the updates of ∆V , it won’t receive any gradient during backpropagation. With this modification, the gra- dient w.r.t m remains unchanged, and ∇V ′L is redefined as: ∇V ′L = m C ∇W′L where C = ||V ′||c (11) This approach reduces the gradient graph memory consump- tion drastically without a noticeable difference in accuracy. We conduct an ablation study to evaluate the impact of the proposed modification on fine-tuning LLaMA-7B and VL- BART. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains un- changed for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. For a comprehensive comparison of training memory usage and accuracy differences, please see Table 7 in the appendix. Consequently, all subsequent experiments with DoRA incorporate this adjustment. 5. Experiments We conduct a variety of experiments to showcase the effi- cacy of DoRA on various tasks including language, image, and video domains. Firstly, we evaluate DoRA against sev- eral Parameter-Efficient Fine-Tuning (PEFT) methods by fine-tuning LLaMA-7B/13B, LLaMA2-7B, and LLaMA3- 8B on commonsense reasoning tasks. Subsequently, we extend from single modality to multimodality. We compare DoRA with LoRA across multi-task image/video-text under- standing tasks using VL-BART and visual instruction tuning with LLaV A-1.5-7B. Following this, we explore the com- patibility of DoRA with LoRA and VeRA (Kopiczko et al., 2024) for instruction-tuning on LLaMA-7B and LLaMA2- 7B. Furthermore, we perform a series of ablation studies to illustrate that DoRA surpasses LoRA in performance, irre- spective of the number of fine-tuning training samples and rank variations. Lastly, We analyze the tuning granularity of DoRA, and show that DoRA can achieve better accuracy than LoRA with fewer trainable parameters by selectively updating only the directional components of certain mod- ules. 5.1. Commonsense Reasoning We evaluate DoRA against LoRA and several baseline meth- ods which include Prompt learning (Prefix) (Li & Liang, 2021), Series adapter (Series) (Houlsby et al., 2019), and Parallel adapter (Parallel) (He et al., 2021) on LLaMA- 7B/13B (Touvron et al., 2023) for commonsense reasoning tasks. We also include ChatGPT’s accuracy obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Ope- nAI, 2023; Wei et al., 2022). The commonsense reasoning tasks comprise 8 sub-tasks, each with a predefined training and testing set. We follow the setting of (Hu et al., 2023) and amalgamate the training datasets from all 8 tasks to create the final training dataset and conduct evaluations on the individual testing dataset for each task. To ensure a fair comparison, we initially fine-tuned models with DoRA following the LoRA config- uration, maintaining the same rank while adjusting only the learning rate. The marginal increase of 0.01% in the number of trainable parameters for DoRA over LoRA, as detailed in Table 1, arises from the inclusion of learnable magnitude components (parameter of size 1 × k). Then, we further halve the rank used in DoRA compared to LoRA 5DoRA: Weight-Decomposed Low-Rank Adaptation Table 1.Accuracy comparison of LLaMA 7B/13B, LLaMA2 7B, and LLaMA3 8B with various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods on LLaMA 7B/13B are taken from (Hu et al., 2023). Results of LoRA on LLaMA2 7B and LLaMA3 8B are obtained using the hyperparameters described in (Hu et al., 2023). DoRA†: the adjusted version of DoRA with the rank halved. Model PEFT Method # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B Prefix 0.11 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series 0.99 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel 3.54 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2 LoRA 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 DoRA† (Ours) 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA (Ours) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 LLaMA-13B Prefix 0.03 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 Series 0.80 71.8 83 79.2 88.1 82.4 82.5 67.3 81.8 79.5 Parallel 2.89 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4 LoRA 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5 DoRA† (Ours) 0.35 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8 DoRA (Ours) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LLaMA2-7B LoRA 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6 DoRA† (Ours) 0.43 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5 DoRA (Ours) 0.84 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 LLaMA3-8B LoRA 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA† (Ours) 0.35 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA (Ours) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 and denote this adjusted configuration as DoRA†. See Table 8 for details on the hyperparameters used. Table 1 demonstrates that DoRA consistently surpasses all baseline methods across both LLaMA-7B/13B, LLaMA2- 7B and LLaMA3-8B. Notably, in the LLaMA-7B model, where LoRA exceeds the performance of other baselines, DoRA further enhances accuracy by 3.7%, outstripping ChatGPT’s accuracy levels. Conversely, for LLaMA-13B, where LoRA’s effectiveness is inferior to the Parallel adapter, DoRA achieves superior accuracy over LoRA by 1% and comparable accuracy to the Parallel adapter, with only a quarter of the trainable parameters required by the Parallel adapter and without adding any extra inference overhead as the Parallel adapter. Additionally, DoRA consistently surpasses LoRA on both LLaMA2-7B and LLaMA3-8B by 2.1% and 4.4%, respectively. Furthermore, DoRA† exceeds LoRA’s performance on LLaMA-7B by 2.8%, on LLaMA- 13B by 1%, on LLaMA2-7B by 2.9%, and on LLaMA3-8B by 4.2%, despite having only half as many trainable parame- ters as LoRA. This outcome suggests that the integration of DoRA enhances the learning capability of LoRA, thereby reducing the need for a higher rank to surpass LoRA in terms of accuracy. Additionally, in previous sections, we hypothesize that a negative correlation between the magnitude update and di- rectional update is more optimal than a positive correlation. This is because pre-trained weights already contain sub- stantial knowledge suitable for downstream tasks, and a larger magnitude or direction alteration alone is sufficient Figure 3.Magnitude (a) and direction (b) difference of LoRA/DoRA and the pre-trained weight of the query matrices across different layers. for downstream adaptation. To further validate our hypoth- esis, we used LLaMA2-7B fine-tuned with DoRA/LoRA on commonsense reasoning datasets as a case study. We visualized the magnitude (∆M) and directional difference (∆D) between the DoRA/LoRA weights and the pre-trained model weights across different modules and layers. In Fig- ure 3 (a) and (b), we observe that the DoRA fine-tuned weights show less deviation from the pre-trained weights in both magnitude and direction, while the differences for the LoRA fine-tuned weights are significantly larger. Coupled with the experimental results that DoRA significantly out- performs LoRA, we can conclude that our earlier hypothesis is valid: a robust foundation model does not require signif- icant alterations for effective downstream adaptation and having the ability to perform more fine-grained magnitude and directional update explains the superiority of DoRA over LoRA. We leave the visualization of the value and key 6DoRA: Weight-Decomposed Low-Rank Adaptation weight matrices in the appendix. 5.2. Image/Video-Text Understanding Table 2.The multi-task evaluation results on VQA, GQA, NVLR2 and COCO Caption with the VL-BART backbone. Method #Params (%) VQAv2 GQA NVLR2 COCO Cap Avg. FT 100 66.9 56.7 73.7 112.0 77.3 LoRA 5.93 65.2 53.6 71.9 115.3 76.5 DoRA (Ours) 5.96 65.8 54.7 73.1 115.9 77.4 Table 3.The multi-task evaluation results on TVQA, How2QA, TVC, and YC2C with the VL-BART backbone. Method #Params (%) TVQA How2QA TVC YC2C Avg. FT 100 76.3 73.9 45.7 154 87.5 LoRA 5.17 75.5 72.9 44.6 140.9 83.5 DoRA (Ours) 5.19 76.3 74.1 45.8 145.4 85.4 Having shown that DoRA can consistently achieve bet- ter accuracy on fine-tuning LLM, we would like to see if DoRA can remain competitive on multi-modality fine- tuning tasks. We compare DoRA with LoRA and full fine- tuning on VL-BART which comprises a vision encoder (CLIP-ResNet101 (Radford et al., 2021)) and an encoder- decoder language model ( BARTBase (Lewis et al., 2020)) across four different image-text tasks: VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for visual ques- tion answering, NLVR2 (Suhr et al., 2019) for visual reason- ing, and MSCOCO (Chen et al., 2015) for image captioning, and four different video-text tasks from the V ALUE (Li et al., 2021) Benchmark: TVQA (Lei et al., 2018) and How2QA (Li et al., 2020) for video question answering, TVC (Lei et al., 2020) and YC2C (Zhou et al., 2018) for video captioning. We follow the same framework as (Sung et al., 2022) and fine-tuned VL-BART within a multi-task framework for both image/video-text tasks. We adopt the same setup as that of LoRA outlined in (Sung et al., 2022) when ap- plying DoRA. See Table 9 for the complete hyperparam- eters. The result of LoRA and FT for both image/video- text tasks is directly quoted from (Sung et al., 2022). We can see that DoRA uniformly surpasses LoRA in accuracy while maintaining a similar count of trainable parameters in both Table 2 and Table 3. In particular, DoRA exceeds LoRA’s performance by nearly 1% in image-text under- standing tasks, reaching the accuracy level of FT. Moreover, DoRA achieves roughly 2% higher accuracy than LoRA in video-text understanding tasks. 5.3. Visual Instruction Tuning We further scale up the model size and compare DoRA to LoRA and FT on the visual instruction tuning tasks with Table 4.Visual instruction tuning evaluation results for LLaV A- 1.5-7B on a wide range of seven vision-language tasks. We directly use checkpoints from (Liu et al., 2023a) to reproduce their results. Method # Params(%) Avg. FT 100 66.5 LoRA 4.61 66.9 DoRA (Ours) 4.63 67.6 LLaV A-1.5-7B (Liu et al., 2023a) which is composed of a language model, Vicuna-1.5-7B (Peng et al., 2023), and a vision encoder, CLIP ViT-L/336px (Radford et al., 2021). The training datasets contain several datasets from VQA (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Schwenk et al., 2022), OCR (Mishra et al., 2019; Sidorov et al., 2020), region-level VQA (Kazemzadeh et al., 2014; Krishna et al., 2017; Mao et al., 2016), visual conversation (Liu et al., 2023a), and language conversation data. We follow the setting of (Liu et al., 2023a) to filter the training data and construct the tunning prompt format. For a fair comparison, DoRA follows the same configuration as the LoRA configuration provided by (Liu et al., 2023a). The fine-tuned models are then evaluated on seven vision- language benchmarks: VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018) SQA (Lu et al., 2022), VQAT (Singh et al., 2019), POPE (Li et al., 2023), and MMBench (Liu et al., 2023c). From Table 4, we can observe that the average accuracy of LoRA already surpasses FT, which could imply that FT might be experiencing issues with overfitting. Given that DoRA is designed to enhance LoRA’s performance to more closely resemble that of FT, in scenarios where FT is infe- rior to LoRA, DoRA’s improvement over LoRA might not be as pronounced as observed in other experiments where FT usually outperforms LoRA. Nonetheless, DoRA still demonstrates superior performance over both LoRA and FT, with an average improvement of 0.7% over LoRA and 1.1% over FT. See Table 10 for the hyperparameters setting and Table 12 for the score of each evaluation benchmark. 5.4. Compatibility of DoRA with other LoRA variants Recall from Equation.(1) that ∆W can be adapted by differ- ent LoRA variants. With DoRA, the concept of incremental directional update ∆V introduced in Equation.(5) can like- wise be replaced with alternative LoRA variants. In this section, we select VeRA (Kopiczko et al., 2024) as a case study to explore DoRA’s compatibility with other LoRA variants. VeRA suggests freezing a unique pair of random low-rank matrices to be shared across all layers, employ- ing only minimal layer-specific trainable scaling vectors to capture each layer’s incremental updates. This approach allows VeRA to reduce trainable parameters significantly 7DoRA: Weight-Decomposed Low-Rank Adaptation Table 5.Average scores on MT-Bench assigned by GPT-4 to the answers generated by fine-tuned LLaMA-7B/LLaMA2-7B. Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA (Ours) 2.33 5.5 VeRA 0.02 4.3 DV oRA (Ours) 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA (Ours) 2.33 6.0 VeRA 0.02 5.5 DV oRA (Ours) 0.04 6.0 by 10x compared to LoRA, with only a minimal impact on accuracy. We apply VeRA for the directional update in DoRA and name such combination DV oRA. We assess the effectiveness of both DV oRA and DoRA compared to VeRA and LoRA across LLaMA-7B and LLaMA2-7B, focusing on instruction tuning with the 10K subset of cleaned Alpaca dataset (Taori et al., 2023). We utilize the official imple- mentation of VeRA to obtain the results of VeRA and LoRA and fine-tune the model with DV oRA and DoRA using the identical training settings as VeRA and LoRA (see Table 11 in the appendix for more details). The performance of the fine-tuned models is then evaluated on the MT-Bench bench- mark (Zheng et al., 2023) by generating model responses to a pre-defined set of 80 multi-turn questions. These re- sponses are then evaluated by GPT-4, which reviews each answer and assigns a numerical score out of 10. Table 5 presents the average scores for DV oRA, DoRA, VeRA, and LoRA, demonstrating that our proposed method exhibits consistent improvements over VeRA and LoRA for both LLaMA-7B and LLaMA2-7B. This effectively show- cases the compatibility of DoRA with VeRA. In particular, DV oRA merges the advantageous qualities of DoRA and VeRA, attaining scores that are on par with or even sur- pass those of LoRA, yet with significantly fewer parame- ters. For example, DV oRA outperforms VeRA by 0.7/0.5 points and achieves the same level of accuracy as LoRA on LLaMA-7B and DoRA on LLaMA2-7B, respectively. Ad- ditionally, we present a selection of questions chosen from MT-Bench, accompanied by the responses from LLaMA2- 7B fine-tuned using DV oRA and VeRA in the appendix (Table 13 and 14) where we can observe that the answers given by DV oRA tend to be more precise and structural. Next, to further assess DoRA’s ability to remain competitive under varying amounts of training data, considering that in practical situations, access to extensive fine-tuning datasets is frequently limited. We compare DoRA to LoRA and DV oRA to VeRA for fine-tuning LLaMA2-7B/LLaMA-7B with a range of instruction-tuning sample sizes, specifically 1000, 4000, 7000, 10000, with 10000 being the setting of 1000 4000 7000 10000 Number of instruction tuning training samples 5.0 5.2 5.4 5.6 5.8 6.0Score LLaMA2-7B DoRA LoRA DV oRA VeRA Figure 4.Performance of fine-tuned LLaMA2-7B on MT-Bench using different numbers of Alpaca training samples. (Kopiczko et al., 2024). We visualize the average perfor- mance of each method on LLaMA2-7B in Figure 4, and on LLaMA-7B in Figure 9 in the appendix. The result shows that DoRA and DV oRA consistently outperform LoRA and VeRA across all training sample sizes. For instance, with 7000 training samples, DoRA and DV oRA surpass LoRA and VeRA by margins of 0.3 and 0.33, respectively. Even when the sample size is reduced to 1000, DoRA and DV oRA maintain their lead with advantages of 0.29 and 0.22 over LoRA and VeRA, respectively. This demonstrates that our methods persistently enhance performance over LoRA and VeRA, regardless of the training sample volume. 5.5. Robustness of DoRA towards different rank settings 4 8 16 32 64 rank r 40 45 50 55 60 65 70 75 80Avg. Accuracy (+ 37.2%) (+ 22.4%) LLaMA-7B DoRA LoRA Figure 5.Average accuracy of LoRA and DoRA for varying ranks for LLaMA-7B on the commonsense reasoning tasks. This section explores the impact of various rank configura- tions on DoRA and LoRA by adjustingr within the set {4, 8, 16, 32, 64} and assessing the performance of the fine-tuned LLaMA-7B on commonsense reasoning tasks as outlined in Sec 5.1. The average accuracies of LoRA and DoRA across different ranks are depicted in Figure 5, with detailed num- bers presented in Table 15. From Figure 5, we can observe that DoRA consistently surpasses LoRA across all rank con- figurations. Notably, the performance gap widens for ranks 8DoRA: Weight-Decomposed Low-Rank Adaptation Table 6.Accuracy comparison of LLaMA 7B/13B with two differ- ent tuning granularity of DoRA. Columns m and V designate the modules with tunable magnitude and directional components, re- spectively. Each module is represented by its first letter as follows: (Q)uery, (K)ey, (V)alue, (O)utput, (G)ate, (U)p, (D)own. Model PEFT Method # Params (%) m V Avg. LLaMA-7B LoRA 0.83 - - 74.7 DoRA (Ours) 0.84 QKVUD QKVUD 78.1 DoRA (Ours) 0.39 QKVOGUD QKV 77.5 LLaMA-13B LoRA 0.67 - - 80.5 DoRA (Ours) 0.68 QKVUD QKVUD 81.5 DoRA (Ours) 0.31 QKVOGUD QKV 81.3 below 8, where LoRA’s average accuracies drop to 40.74% for r = 8 and 39.49% for r = 4. In contrast, DoRA retains a notable accuracy of 77.96% for r = 8 and 61.89% for r = 4, demonstrating its resilience and consistently superior performance over LoRA regardless of the rank setting. 5.6. Tuning Granularity Analysis The visualization in Figure 2 indicates that significant changes in magnitude often result in relatively smaller di- rectional changes. Given this observation and the fact that directional updates account for most of the trainable param- eters, it prompts an investigation into whether it is possible to decrease the number of trainable parameters by updating only the magnitude components of specific modules while continuing to update both the magnitude and directional components for the remaining linear modules. Our findings indicate that, in contrast to the original con- figuration suggested for LoRA in (Hu et al., 2023), which requires updates to both the Multi-head Attention and MLP layers for optimal performance, DoRA can already achieve superior accuracy by updating only the directional and mag- nitude components of the multi-head layers and the magni- tude of the MLP layers. Specifically, as shown in Table 6, by updating the directional and magnitude components of the QKV modules and only the magnitude of the rest of the layers, DoRA surpasses LoRA by 2.8% on LLaMA-7B and 0.8% on LLaMA-13B, while utilizing only less than half of the trainable parameters compared to LoRA. 6. Broader Impacts 6.1. QDoRA: Enhancements to QLoRA While finetuning LLMs with PEFT significantly reduces training memory overhead, a considerable amount of GPU memory is still required to initially load the model weights onto the GPUs. To further decrease the memory demands of finetuning, QLoRA (Dettmers et al., 2023) suggests quan- tizing the pretrained model to 4-bit and finetuning LoRA on top of the frozen low-bit backbone. With our porposed 0.0 0.1 0.2 0.3 0.4 0.5 Exact match score (Eval size: 500) Zero-shot Five-shot Full Finetune QLoRA QDoRA 0.23 0.27 0.51 0.32 0.56 0.07 0.08 0.26 0.12 0.31 100k Orca-Math finetuning results LLaMA2-7B LLaMA3-8B Figure 6.Accuracy comparison of LLaMA2-7B/LLaMA3-8B with QDoRA, QLoRA and FT on Orca-Math (Mitra et al., 2024). DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework. Recently, (Kerem Turgutlu, 2024) launch a project that substitutes the LoRA component in QLoRA with DoRA, dubbing it QDoRA, and incorporate the training pipeline with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) to enable model splitting and parallel training across multiple GPUs. They conducted experiments on fine-tuning LLaMA2- 7B/LLaMA3-8B using the Orca-Math(Mitra et al., 2024) dataset with QDoRA, QLoRA, and FT. The training set included 100k samples, with 500 reserved for evaluation using the exact match score as the metric. In addition to the fine-tuned models, they also reported results from zero-shot, few-shot, and FT with post-training quantization (PTQ), where the FT model is quantized to the BnB NF4 format after training. According to Figure 6, QDoRA not only significantly surpasses QLoRA by 0.19/0.23 on LLaMA2- 7B and LLaMA3-8B, but it also slightly outperforms FT on both models, while using considerably less memory. This in- dicates that QDoRA can effectively combines the parameter efficiency of QLoRA with the more granular optimization of full finetuning. These initial findings suggest that QDoRA holds considerable promise and could hugely benefit the opensoure community by substantially lowering the GPU memory requirements for fine-tuning large language mod- els. 6.2. Text-to-Image Generation Recently, as diffusion models have expanded in size, LoRA has become a popular method for efficiently fine-tuning large stable diffusion models. In this section, we aim to explore whether DoRA’s advantages over LoRA extend to the task of text-to-image generation. We follow the training pipeline of DreamBooth (Ruiz et al., 2023) for fine-tuning 9DoRA: Weight-Decomposed Low-Rank Adaptation SDXL (Podell et al., 2023), utilizing the advanced train- ing scripts developed by HuggingFace. The hyperparameter settings for LoRA and DoRA are kept the same, and we fine- tune the model using two challenging datasets: 3D icons and Lego sets. The sample seeds for generating the images are kept the same for LoRA and DoRA for fair comparison. The generated images are shown in Figure 10 and 11 in the appendix. The results indicate that DoRA achieves signif- icantly better personalization than LoRA when using the same training settings, and more accurately reflects the train- ing targets. For example, in Figure 10, the first sub-figure of DoRA’s output features a unique round square around the image, which is a feature common to all the training targets. In contrast, this feature is absent in all the LoRA outputs. A similar observation could be found with the Lego training targets, where only the DoRA outputs consistently incorporate the Lego logo in the generated images. 7. Conclusion In this work, we first conduct a novel weight decomposi- tion analysis to reveal the distinct learning patterns between LoRA and FT. Building on these insights, we introduce DoRA, a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT’s learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Specifically, DoRA improves upon LoRA in commonsense reasoning and visual instruction tuning tasks. Furthermore, DoRA also shows compatibility with VeRA on the Alpaca instruction tuning task. Moreover, DoRA can be considered as a costless alternative to LoRA, as its decomposed mag- nitude and direction components can be merged back into the pre-trained weight after the training, ensuring that there is no extra inference overhead. For future work, we wish to explore the generalizability of DoRA in domains beyond language and vision, particularly in the field of audio. Acknowledgements We extend our gratitude to Benjamin Bossan, Younes Belkada, and Sourab Mangrulkar from Hugging Face for their assistance in integrating DoRA into the PEFT pack- age, thus making our work more accessible to the broader public. We thank Kerem Turgutlu, Jonathan Whitaker, and Jeremy Howard from Answer.AI for their work on the imple- mentation and experiments of QDoRA/FSDP, which makes fine-tuning of large language models with DoRA on con- sumer GPUs a lot more feasible. We also thank Sebastian Raschka for his well-written tutorial on DoRA which offers a thorough overview of the background knowledge neces- sary to comprehend DoRA. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Chen, X., Fang, H., Lin, T.-Y ., Vedantam, R., Gupta, S., Doll´ar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 10088–10115. Cur- ran Associates, Inc., 2023. Goyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617, 2018. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient trans- fer learning. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on 10DoRA: Weight-Decomposed Low-Rank Adaptation imagenet classification. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 1026–1034, 2015. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pp. 2790–2799, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Hu, Z., Wang, L., Lan, Y ., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large lan- guage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700– 6709, 2019. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low- rank hadamard product for communication-efficient fed- erated learning. In International Conference on Learning Representations, 2022. Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 565–576, 2021. Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014. Kerem Turgutlu, Jonathan Whitaker, J. H. Ef- ficient finetuning of llama 3 with fsdp qdora. https://www.answer.ai/posts/ 2024-04-26-fsdp-qdora-llama3.html , 2024. Kopiczko, D. J., Blankevoort, T., and Asano, Y . M. Vera: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024. Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, pp. 32–73, 2017. Lei, J., Yu, L., Bansal, M., and Berg, T. TVQA: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1369–1379, 2018. Lei, J., Yu, L., Berg, T. L., and Bansal, M. Tvr: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision, pp. 447–463, 2020. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pp. 7871–7880, 2020. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Confer- ence on Machine Learning, pp. 12888–12900, 2022. Li, L., Chen, Y .-C., Cheng, Y ., Gan, Z., Yu, L., and Liu, J. HERO: Hierarchical encoder for Video+Language omni- representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2046–2065, 2020. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y .-C., Pillai, R., Cheng, Y ., Zhou, L., Wang, X. E., Wang, W. Y ., et al. Value: A multi-task benchmark for video-and-language understand- ing evaluation. In Thirty-fifth Conference on Neural In- formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582– 4597, 2021. Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language 11DoRA: Weight-Decomposed Low-Rank Adaptation models. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing , pp. 292–305, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun- ing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Liu, W., Qiu, Z., Feng, Y ., Xiu, Y ., Xue, Y ., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W., Yuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Infor- mation Processing Systems, pp. 2507–2521, 2022. mahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural Information Processing Systems, 2021. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unam- biguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11–20, 2016. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requir- ing external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195–3204, 2019. Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952, 2019. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. OpenAI. Welcome to the openai platform, 2023. URL https://platform.openai.com/docs/ introduction. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im- proving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining modular skills in multitask learning.arXiv preprint arXiv:2202.13914, 2022. Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339–1384, 2023. Qiu, Z., Liu, W., Feng, H., Xue, Y ., Feng, Y ., Liu, Z., Zhang, D., Weller, A., and Sch¨olkopf, B. Controlling text- to-image diffusion by orthogonal finetuning. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763, 2021. Razdaibiedina, A., Mao, Y ., Khabsa, M., Lewis, M., Hou, R., Ba, J., and Almahairi, A. Residual prompt tuning: improving prompt tuning with residual reparameteriza- tion. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6740–6757, 2023. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023. Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image dif- fusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023. Salimans, T. and Kingma, D. P. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 901–909, 2016. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual ques- tion answering using world knowledge. In European Conference on Computer Vision, pp. 146–162, 2022. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehen- sion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part II 16, pp. 742–758, 2020. 12DoRA: Weight-Decomposed Low-Rank Adaptation Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317–8326, 2019. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y . A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418–6428, 2019. Sung, Y .-L., Cho, J., and Bansal, M. Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 5227–5237, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stan- ford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/ stanford_alpaca. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wang, Y ., Wu, J., Dabral, T., Zhang, J., Brown, G., Lu, C.-T., Liu, F., Liang, Y ., Pang, B., Bendersky, M., et al. Non- intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. arXiv preprint arXiv:2310.12100, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y ., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Interna- tional Conference on Learning Representations, 2023. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In Proceed- ings of the AAAI Conference on Artificial Intelligence , 2018. 13DoRA: Weight-Decomposed Low-Rank Adaptation A. Appendix A.1. Weight decomposition analysis on the value weight matrix In this section, we illustrate the changes in magnitude and direction within the value weight matrix for FT, LoRA, and DoRA across different training steps and layers, as shown in Figure 7. This reveals patterns similar to those seen in the query weight matrix depicted in Figure 2, indicating that DoRA is capable of displaying learning behaviors that closely mirror those of FT across various modules. 0.02 0.03 0.04 0.05 0.06 0.07 0.08 D (a) 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325 D (b) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 D (c) 0.4 0.6 0.8 1.0 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 7.Magnitude and Directional changes of FT (a), LoRA (b), and DoRA (c) of the V weight matrices across different layers and intermediate steps. A.2. Ablation study for the modification to reduce DoRA training cost Table 7 presents the GPU cost and the average accuracy of DoRA with and without the proposed modification for commonsense reasoning tasks and image-text understanding tasks. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains unchanged for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. Table 7.GPU cost and accuracy of DoRA with or without the modification on the commonsense reasoning tasks and image-text understanding tasks. Model PEFT Method Accumulation steps Batch Size GPU Memory Cost (GB) # Params (%) Avg. LLaMA-7B DoRA w/o modification 4 16 37.3 0.84 78.3 DoRA 28.2 (-24.4%) 0.84 78.1 VL-BART DoRA w/o modification - 300 23.4 5.96 77.3 DoRA 20.5 (-12.4%) 5.96 77.4 14DoRA: Weight-Decomposed Low-Rank Adaptation A.3. Hyperparameters Table 8.Hyperparameter configurations of DoRA for LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B on the commonsense reasoning tasks. Hyperparameters (DoRA) LLaMA-7B LLaMA-13B LLaMA2-7B LLaMA3-8B Rank r 16 32 16 32 16 32 16 32 α 32 64 32 64 32 64 32 64 Dropout 0.05 Optimizer AdamW LR 2e-4 1e-4 3e-4 2e-4 2e-4 2e-4 1e-4 1e-4 LR Scheduler Linear Batch size 16 Warmup Steps 100 Epochs 3 Where Q,K,V ,Up,Down Table 9.Hyperparameter configurations of DoRA for fine-tuning VL-Bart on image/video-text tasks. Hyperparameters (DoRA) image-text video-text Rank r 128 α 128 Dropout 0.0 Optimizer AdamW LR 1e-3 3e-4 LR Scheduler Linear Batch size 300 40 Warmup ratio 0.1 Epochs 20 7 Where Q,K Table 10.Hyperparameter configurations of DoRA and LoRA for fine-tuning LLaV A-1.5-7B with visual instruction tuning datasets. Hyperparameters DoRA LoRA Rank r 128 α 256 Dropout 0.05 Optimizer AdamW LR 2e-4 LR Scheduler Cosine decay Batch size 16 Warmup ratio 0.03 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 15DoRA: Weight-Decomposed Low-Rank Adaptation Table 11.Hyperparameter configurations of DoRA and DV oRA for fine-tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset. Hyperparameters (DoRA) LLaMA-7B LLaMA2-7B Rank r 64 Dropout 0.0 Optimizer AdamW LR 4e-4 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate Hyperparameters (DVoRA) LLaMA-7B LLaMA2-7B Rank r 1024 Dropout 0.0 Optimizer AdamW LR 4e-3 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 16DoRA: Weight-Decomposed Low-Rank Adaptation A.4. Magnitude and Direction difference between DoRA/LoRA fine-tuned weight and the pre-triained weight of LLaMA2-7B for the commonsesne reasoning tasks Figure 8 depicts the magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. The figure shows that the DoRA fine-tuned weights deviate less from the pre-trained weights in both magnitude and direction, supporting our hypothesis that a robust foundation model does not need substantial changes for effective downstream adaptation. (a) (b) (d) (c) (f) (e) Figure 8.Magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. 17DoRA: Weight-Decomposed Low-Rank Adaptation A.5. Visual instruction tuning evaluation result Table 12 presents the evaluation result of LLaV A-1.5-7B fine-tuned using DoRA, LoRA, and FT with visual instruction tuning data. The evaluation is conducted on an array of seven different vision-language benchmarks. From the table, we can see that DoRA consistently outmatch FT, and can achieve an average accuracy better than LoRA. Table 12.Visual instruction tuning evaluation result of DoRA, LoRA, and FT for LLaV A-1.5-7B on a wide range of 7 vision-language tasks. Method # Params(%) VQAv2 GQA VisWiz SQA VQA T POPE MMBench Avg. FT 100 78.5 61.9 50.0 66.8 58.2 85.9 64.3 66.5 LoRA 4.61 79.1 62.9 47.8 68.4 58.2 86.4 66.1 66.9 DoRA (Ours) 4.63 78.6 62.9 52.2 69.9 57 87.2 66.1 67.6 18DoRA: Weight-Decomposed Low-Rank Adaptation A.6. Instruction-tuning evaluation examples We sample two questions from the MT-Bench question set and present the respective responses from DV oRA and VeRA, including the scores and comments provided by GPT-4. From these examples, we can observe that responses from DV oRA are generally more accurate and well-organized compared to those from VeRA. For instance, in response to Question 24, DV oRA leverages a well-known superhero story that is familiar to children, whereas VeRA’s response begins with an uncertain tone, potentially making it less engaging than DV oRA’s reply. Table 13.Question 24 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 24 As a superhero, how would you explain your origin story to a curious child? VeRA Well, I’m not really sure where I come from. All I know is that one day, I woke up with special powers and abilities that allowed me to help people. I decided to use my powers to fight crime and protect the innocent, and that’s how I became a superhero DV oRA As a superhero, I have the ability to use my super powers to protect people and fight against evil. My origin story begins when I was just a regular kid like you, but one day, I was bitten by a radioactive spider, and it gave me incredible powers. I could climb walls, shoot webs, and see in the dark. I decided to use my powers to help people, and that’s how I became your friendly neighborhood Spider-Man. GPT-4 Evaluation Score: 5 7. Assistant 1’s response starts with an uncertain tone, which might not be very engaging for a child. The explanation is quite vague and doesn’t offer a clear or imaginative origin story that could captivate a child’s interest. It does, however, maintain the theme of using powers for good, which is a positive message. Assistant 2’s response is more detailed and uses a familiar superhero origin story (Spider-Man’s) that many children can relate to and understand. It’s engaging and provides a clear narrative of how an ordinary person can become a superhero. However, it directly references Spider-Man, which is not an original origin story and could be misleading if the assistant is supposed to be a unique superhero. Despite this, the response is more likely to resonate with a child due to its clarity and reliability. Both responses could be improved by providing an original and imaginative origin story that doesn’t rely on existing superhero narratives. They should also ensure that the story is simple enough for a child to understand while being engaging and inspiring. 19DoRA: Weight-Decomposed Low-Rank Adaptation Table 14.Question 33 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 33 Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? VeRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Some people may prefer the physical act of navigating with a paper map, as it can be more tactile and interactive. Others may prefer to have a hard copy of the map in case the GPS or app fails or the battery dies. Additionally, some people may not have access to a GPS device or smartphone, or may not want to rely on technology for navigation. Finally, some people may simply prefer the security and peace of mind that comes with knowing exactly where they are and how to get to their destination DV oRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Here are some of them: 1. Navigation accuracy: GPS devices and smartphone apps rely on satellite signals to pinpoint your location, but this technology is not foolproof. GPS signals can be blocked by tall buildings, tunnels, or other obstacles, resulting in inaccurate or missing information. Paper maps, on the other hand, are based on accurate, up-to-date information that is not subject to these limitations. 2. Cost: GPS devices and smartphone apps can be expensive to purchase and maintain. Paper maps are free to use and do not require an ongoing subscription or data plan. 3. Personal preference: Some people prefer the tactile experience of using a paper map, which can be easier to read and navigate than a screen. Others enjoy the challenge of navigating without technology, which can help them learn more about the area they are visiting. 4. Battery life: GPS devices and smartphone apps can drain batteries quickly, especially when used for navigation. Paper maps do not require power, so they can be used for extended periods without recharging. 5. Offline use: GPS devices and smartphone apps rely on an internet connection to function properly. If there is no signal, the device will not be able to provide accurate information. Paper maps, on the other hand, can be used offline, making them ideal for use in remote or unpopulated areas. In summary, there are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. These include navigation accuracy, cost, personal preference, battery life, and offline use. GPT-4 Evaluation Score: 6 8. Assistant 1’s response is somewhat brief and lacks specific details that could be helpful to the user. It mentions the tactile nature of paper maps, the possibility of technology failure, lack of access to technology, and a general preference for security and peace of mind. However, it does not delve into the practical considerations such as GPS signal limitations, cost, battery life, or offline use, which are important factors in deciding between paper maps and digital navigation tools. Assistant 2’s response is more comprehensive and provides a structured list of reasons, including navigation accuracy, cost, personal preference, battery life, and offline use. Each point is explained clearly, giving the user a better understanding of the potential advantages of paper maps over GPS devices or smartphone apps. The response is relevant, accurate, and detailed, making it more helpful to the user. Both assistants provided relevant and accurate information, but Assistant 2’s answer is more detailed and structured, which is why it receives a higher score. The scores reflect the level of detail and the usefulness of the information provided to the user, with Assistant 2 offering a more complete answer to the question 20DoRA: Weight-Decomposed Low-Rank Adaptation A.7. Robustness towards different numbers of instruction tunning examples Figure 9 depicts the score of DoRA, LoRA, DV oRA, and VeRA for fine-tuning LLaMA-7B with varying instruction-tuning sample sizes, including 1000, 4000, 7000, and 10000. From the figure, we can observe that similar to that of the result on LLaMA2-7B, both DoRA and DV oRA consistently outperform LoRA and VeRA across the entire range of training sample sizes. 1000 4000 7000 10000 Number of instruction tuning training samples 3.0 3.5 4.0 4.5 5.0 5.5Score LLaMA-7B DoRA LoRA DV oRA VeRA Figure 9.Performance of LLaMA-7B fine-tuned with LoRA, DoRA, VeRA, and DV oRA on MT-Bench using different numbers of training samples from Alpaca dataset (Taori et al., 2023). 21DoRA: Weight-Decomposed Low-Rank Adaptation A.8. Robustness of DoRA towards different rank Table 15 shows a comparison of the average accuracy between LoRA and DoRA method across various rank settings for commonsense reasoning tasks. DoRA consistently outperforms LoRA at all rank settings, with the performance gap widening as the rank decreases. This suggests that our method effectively enhances the learning capacity of LoRA, enabling it to achieve better accuracy with fewer trainable parameters. Table 15.Accuracy comparison of LoRA and DoRA with varying ranks for LLaMA-7B on the commonsense reasoning tasks. PEFT Method rank r # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. LoRA 4 0.10 2.3 46.1 18.3 19.7 55.2 65.4 51.9 57 39.5 8 0.21 31.3 57.0 44.0 11.8 43.3 45.7 39.2 53.8 40.7 16 0.42 69.9 77.8 75.1 72.1 55.8 77.1 62.2 78.0 70.9 32 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 64 1.64 66.7 79.1 75.7 17.6 78.8 73.3 59.6 75.2 65.8 DoRA (Ours) 4 0.11 51.3 42.2 77.8 25.4 78.8 78.7 62.5 78.6 61.9 8 0.22 69.9 81.8 79.7 85.2 80.1 81.5 65.7 79.8 77.9 16 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 32 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 64 1.65 69.9 81.4 79.1 40.7 80.0 80.9 65.5 79.4 72.1 22DoRA: Weight-Decomposed Low-Rank Adaptation A.9. Text-to-Image Generation Figures 10 and 11 show the images produced by SDXL fine-tuned with DoRAand LoRA via DreamBooth (Ruiz et al., 2023) personalization techniques on two distinct training sets: 3D Icon 1 and Lego2. The results reveal that DoRA can achieve considerably better personalization than LoRA with identical training configurations, more closely matching the training target. Figure 10.Images generated with SDXL finetuned with LoRA and DoRA on the 3D Icon training sets. Figure 11.Images generated with SDXL finetuned with LoRA and DoRA on the Lego training sets. 1https://huggingface.co/datasets/linoyts/3d_icon 2https://huggingface.co/datasets/merve/lego_sets_latest 23",
      "meta_data": {
        "arxiv_id": "2402.09353v6",
        "authors": [
          "Shih-Yang Liu",
          "Chien-Yi Wang",
          "Hongxu Yin",
          "Pavlo Molchanov",
          "Yu-Chiang Frank Wang",
          "Kwang-Ting Cheng",
          "Min-Hung Chen"
        ],
        "published_date": "2024-02-14T17:59:34Z",
        "pdf_url": "https://arxiv.org/pdf/2402.09353v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel parameter-efficient fine-tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and full fine-tuning (FT) without incurring additional inference costs. It achieves this by decomposing pre-trained weights into magnitude and direction components, with LoRA applied specifically for directional updates. DoRA consistently outperforms LoRA across various tasks and models, including LLaMA, LLaVA, and VL-BART, and is compatible with other LoRA variants like VeRA. The work also presents a novel weight decomposition analysis to identify fundamental differences in the learning patterns of FT and existing PEFT methods.",
        "methodology": "DoRA reparameterizes pre-trained model weights (W) into a magnitude vector (m) and a directional matrix (V), such that W = m * (V / ||V||c). During fine-tuning, DoRA fine-tunes both components, with the directional component updated using a low-rank adaptation (LoRA) mechanism (∆V = BA). The magnitude component (m) is a trainable vector, while the base directional component (W0) remains frozen. A gradient analysis is performed to demonstrate how this decomposition enhances optimization stability and aligns DoRA's learning pattern, characterized by a negative slope between magnitude and direction updates, more closely with FT. To reduce training overhead, a modification is introduced where the norm factor ||V + ∆V ||c is treated as a constant during backpropagation, significantly reducing memory consumption without notable accuracy loss.",
        "experimental_setup": "DoRA's efficacy is validated across diverse tasks and models. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B are fine-tuned on 8 sub-tasks, comparing DoRA against LoRA, Prefix, Series, and Parallel adapters, and ChatGPT. For image/video-text understanding, VL-BART is used with image-text tasks (VQAv2, GQA, NLVR2, MSCOCO) and video-text tasks (TVQA, How2QA, TVC, YC2C). Visual instruction tuning is performed on LLaVA-1.5-7B, evaluated on seven vision-language benchmarks (VQAv2, GQA, VisWiz, SQA, VQAT, POPE, MMBench). Compatibility with LoRA variants is shown by combining DoRA with VeRA (DV oRA) for instruction tuning on LLaMA-7B/LLaMA2-7B using the Alpaca dataset, evaluated on MT-Bench. Ablation studies cover varying rank settings and training sample sizes. QDoRA (DoRA on QLoRA) is evaluated on Orca-Math with LLaMA2-7B/LLaMA3-8B. Text-to-image generation with SDXL is explored using DreamBooth with 3D icons and Lego sets.",
        "limitations": "The paper notes that in scenarios where full fine-tuning (FT) performs worse than LoRA, DoRA's improvements over LoRA might be less pronounced compared to situations where FT generally outperforms LoRA. This suggests that DoRA's performance uplift is somewhat conditional on the underlying capacity for improvement relative to FT. Additionally, while inference overhead is avoided, the training mechanism for DoRA is implicitly more complex than vanilla LoRA due to the weight decomposition and the need for specific gradient handling to reduce memory during backpropagation, although this was successfully mitigated.",
        "future_research_directions": "The authors intend to explore the generalizability of DoRA in domains beyond language and vision, specifically mentioning the field of audio as a promising area for future research."
      }
    },
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
      "full_text": "DoRA: Weight-Decomposed Low-Rank Adaptation Shih-Yang Liu1 2 Chien-Yi Wang1 Hongxu Yin 1 Pavlo Molchanov1 Yu-Chiang Frank Wang1 Kwang-Ting Cheng 2 Min-Hung Chen 1 Abstract Among the widely used parameter-efficient fine- tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decom- position analysis to investigate the inherent dif- ferences between FT and LoRA. Aiming to re- semble the learning capacity of FT from the findings, we propose Weight-Decomposed Low- Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, mag- nitude and direction, for fine-tuning, specifically employing LoRA for directional updates to ef- ficiently minimize the number of trainable pa- rameters. By employing DoRA, we enhance both the learning capacity and training stabil- ity of LoRA while avoiding any additional in- ference overhead. DoRA consistently outper- forms LoRA on fine-tuning LLaMA, LLaV A, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruc- tion tuning, and image/video-text understanding. Code is available at https://github.com/ NVlabs/DoRA. 1. Introduction Models that are pre-trained with extensive general domain datasets have demonstrated remarkable generalization abil- ities, significantly benefiting a wide array of applications, from natural language processing (NLP) tasks (Qin et al., 2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022; Liu et al., 2023a). To tailor these general models for spe- cific downstream tasks, full fine-tuning (FT) is commonly 1NVIDIA 2HKUST. Correspondence to: Shih-Yang Liu <shi- hyangl@nvidia.com, sliuau@connect.ust.hk>, Min-Hung Chen <minhungc@nvidia.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Magnitude B  A  Pretrained Weight Merged Weight Pretrained Weight Pretrained Weight Adapt Frozen Trainable Magnitude Direction Direction Decompose (Initialize) Merge Figure 1.An overview of our proposed DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that || · ||c denotes the vector-wise norm of a matrix across each column vector. employed, involving the retraining of all model parameters. Nevertheless, as the size of models and datasets expand in scale, the expense associated with fine-tuning the entire model becomes prohibitively large. To address this issue, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019) have been introduced to fine- tune the pre-trained models with only a minimal number of parameters. Among these, LoRA (Hu et al., 2022), which does not change the model architecture, has become notably popular for its simplicity and efficacy. Nevertheless, there is still a capacity gap between LoRA and FT, which is often attributed to the limited number of trainable parameters without further exploration of other underlying causes (Hu et al., 2022; Kopiczko et al., 2024). Drawing on Weight Normalization (Salimans & Kingma, 2016), which achieves faster convergence via improving the conditioning of the gradient with weight reparameterization, we introduce a novel weight decomposition analysis that ini- tially reparameterizes model weights into magnitude and di- rectional components, subsequently examining the changes 1 arXiv:2402.09353v6  [cs.CL]  9 Jul 2024DoRA: Weight-Decomposed Low-Rank Adaptation in magnitude and direction introduced by LoRA and FT. Our analysis reveals that LoRA and FT exhibit markedly distinct patterns of updates, leading us to surmise that these variations mirror the learning capability of each method. Inspired by our findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA), which begins by decompos- ing the pre-trained weight into its magnitude and directional components, then fine-tunes both. Given the substantial size of the directional component in terms of parameters, we exploit LoRA for the directional adaptation to enable efficient fine-tuning, as illustrated in Figure.1. Moreover, by showing a learning behavior similar to FT both empir- ically and mathematically, suggesting a learning capacity closely resembling FT, we have validated DoRA across a wide variety of tasks, from NLP to Vision-Language, and over various backbones, including LLM and LVLM. The experimental results show that DoRA consistently outper- forms LoRA without sacrificing inference efficiency, such as commonsense reasoning (+3.7/+1.0 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +4.4 on LLaMA3-8B), visual in- struction tuning (+0.6 on LLaV A-7B), and image/video-text understanding (+0.9/+1.9 on VL-BART). The summary of our contributions is as follows: • We introduce DoRA, a novel PEFT method that incor- porates weight decomposition, achieving a learning capacity closely resembling FT without any additional inference latency over LoRA. • We introduce a novel weight decomposition analysis to uncover the fundamental differences in the learning patterns of FT and different PEFT methods. • DoRA consistently surpasses LoRA on various tasks, from NLP to Vision-Language benchmarks and across various backbones, including LLM and LVLM. 2. Related Works Parameter-Efficient Fine-Tuning (PEFT)methods are de- signed to reduce the high expense of fine-tuning large-scale models. They achieve this by training a relatively small subset of parameters, compared to the total number of pa- rameters, for adapting to downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as Adapter-based methods, which involve introducing additional trainable modules into the original frozen backbone, such as (Houlsby et al., 2019; He et al., 2021; Karimi Mahabadi et al., 2021; mahabadi et al., 2021). For example, (Houlsby et al., 2019) proposes adding linear modules in sequence to the existing layer, whereas (He et al., 2021) advocates for integrating these modules in parallel with the original layer to enhance performance. The second category is Prompt-based methods. These methods add extra soft tokens (prompts) to the initial input and fo- cus solely on fine-tuning these trainable vectors, as seen in works like (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023). However, these approaches typically face challenges due to their sensitivity to initialization, af- fecting their overall effectiveness. These first two categories, whether altering the model’s input or architecture, result in increased inference latency compared to the baseline model. LoRA (Hu et al., 2022) and its variants are among the third category of PEFT, notable for not adding any extra inference burden. These methods apply low-rank matrices to approximate weight changes during fine-tuning and can merge with pre-trained weights prior to inference. For ex- ample, (Zhang et al., 2023) employs SVD decomposition and prunes less significant singular values for more efficient updates. (Hyeon-Woo et al., 2022) focuses on low-rank Hadamard product for federated learning. (Qiu et al., 2023; Liu et al., 2023b) exploit orthogonal factorization in fine- tuning diffusion models. (Renduchintala et al., 2023) uses weight tying to further reduce the trainable parameters. (Yeh et al., 2023) introduces a unified LoRA family framework for Stable diffusion. (Ponti et al., 2022) chooses different combinations of LoRAs from the inventory with a routing function for different tasks. (Kopiczko et al., 2024) imple- ments learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Our research also falls within this third category, and we validate the efficacy of our proposed method alongside LoRA and its variants through comprehensive experimentation. 3. Pattern Analysis of LoRA and FT 3.1. Low-Rank Adaptation (LoRA) Building upon the hypothesis that updates made during the fine-tuning exhibit a low “intrinsic rank”, LoRA (Hu et al., 2022) proposes using the product of two low-rank matri- ces to update the pre-trained weights incrementally. For a pre-trained weight matrix W0 ∈ Rd×k, LoRA models the weight update ∆W ∈ Rd×k utilizing a low-rank decompo- sition, expressed as BA, where B ∈ Rd×r and A ∈ Rr×k represent two low-rank matrices, withr ≪ min(d, k). Con- sequently, the fine-tuned weight W′ can be represented as: W′ = W0 + ∆W = W0 + BA (1) where W0 remains static during the fine-tuning process, and the underlined parameters are being trained. The ma- trix A is initialized with uniform Kaiming distribution (He et al., 2015), while B is initially set to zero, resulting in ∆W = BA being zero at the start of training. Notably, this decomposition of ∆W can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Ad- ditionally, based on Eq. (1), we can merge the learned ∆W with the pre-trained weight W0 and obtain W′ in advance 2DoRA: Weight-Decomposed Low-Rank Adaptation of deployment, and given that both W′ and W0 both fall within the dimensionality of Rd×k, LoRA and its related variants do not introduce any extra latency during the infer- ence compared to the original model. 3.2. Weight Decomposition Analysis The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank r of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Con- sequently, many previous studies have attributed the dis- crepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans & Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two sep- arate components, magnitude and direction, to reveal the inherent differences in LoRA and FT learning patterns. Analysis Method: This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of W ∈ Rd×k can be formulated as: W = m V ||V ||c = ||W||c W ||W||c (2) where m ∈ R1×k is the magnitude vector, V ∈ Rd×k is the directional matrix, with ||·|| c being the vector-wise norm of a matrix across each column. This decomposition ensures that each column of V/||V ||c remains a unit vector, and the corresponding scalar in m defines the magnitude of each vector. For our weight decomposition analysis, we select the VL- BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre- trained weight W0, the full fine-tuned weight WFT, and the merged LoRA weight WLoRA of query/value weight matrix using Eq. (2). The magnitude and directional variations between W0 and WFT can be defined as follows: ∆Mt FT = Pk n=1 |mn,t FT − mn 0 | k (3) ∆Dt FT = Pk n=1(1 − cos(V n,t FT , Wn 0 )) k (4) Here, ∆Mt FT and and ∆Dt FT represent the magnitude dif- ference and directional difference between W0 and WFT at t training step respectively, with cos(·, ·) being the cosine similarity function. Mn,t FT and Mn 0 are the nth scalars in their respective magnitude vectors, while V n,t FT and Wn 0 are the nth columns in V t FT and W0. The magnitude and direc- tional differences between WLoRA and W0 are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition anal- ysis on each of these checkpoints to determine the ∆M and ∆D throughout different layers. Analysis Results: Figure 2 (a) and (b) illustrate the alter- ations in the query weight matrix of FT and LoRA, with each point representing a ( ∆Dt, ∆Mt) pair from query weight matrices across different layers and training steps. Similarly, Figure 7 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively neg- ative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direc- tion updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and direc- tional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resem- bling that of FT, and can improve the learning capacity over LoRA. 4. Method 4.1. Weight-Decomposed Low-Rank Adaptation Drawing from the insights of our weight decomposition analysis, we introduce Weight- Decomposed L ow-Rank Adaptation (DoRA). DoRA initially decomposes the pre- trained weight into its magnitude and directional compo- nents and finetunes both of them. Because the directional component is large in terms of parameter numbers, we fur- ther decompose it with LoRA for efficient finetuning. Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tun- able simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag- 3DoRA: Weight-Decomposed Low-Rank Adaptation 0.014 0.016 0.018 0.020 0.022 D (a) 0.05 0.06 0.07 0.08 0.09 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.10 0.11 0.12 0.13 0.14 0.15 0.16 D (b) 0.25 0.30 0.35 0.40 0.45 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 D (c) 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 2.Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer. nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinc- tion between DoRA and weight normalization (Salimans & Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, mak- ing the method sensitive to different initializations. Con- versely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight W0 as outlined in Eq. (2), where m = ||W0||c and V = W0 after initialization. We then keep V frozen and m a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as: W′ = m V + ∆V ||V + ∆V ||c = m W0 + BA ||W0 + BA||c (5) where ∆V is the incremental directional update learned by multiplying two low-rank matrices B and A, and the underlined parameters denote the trainable parameters. The matrices B ∈ Rd×r and A ∈ Rr×k are initialized in line with LoRA’s strategy to ensure that W′ equals W0 before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency. We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and W0 in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for (∆D, ∆M) of both DoRA and FT, we reveal that in contrast to LoRA’s pattern, DoRA, and FT are characterized by a distinct neg- ative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substan- tial knowledge suitable for various downstream tasks. There- fore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally com- pute the correlation between ∆D and ∆M for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT’s signifies its superior learning capacity over LoRA. 4.2. Gradient Analysis of DoRA In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the opti- mization of ∆V . Subsequently, we analyze from the gradi- ent’s perspective to explicate the learning pattern of DoRA, which tends to have a negative slope. From Eq. (5), we can obtain the gradient of Loss L with respect to m and V ′ = V + ∆V as: ∇V ′L = m ||V ′||c \u0012 I − V ′V ′T ||V ′||2c \u0013 ∇W′L (6) ∇mL = ∇W′L ·V ′ ||V ′||c (7) Eq. (6) reveals that the weight gradient ∇W′L is scaled by m/||V ′||c and is projected away from the current weight matrix. These two effects contribute to aligning the gra- dient’s covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans & Kingma, 2016). Additionally, given that V ′ = V + ∆V , the gradient ∇V ′L is equivalent to ∇∆V L. Therefore, the optimization benefits derived from this decomposition are fully transferred to ∆V , enhancing the learning stability of LoRA. We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent dis- cussion, we represent vectors using lower-case letters in- stead of the previous matrix form notation. Consider w′′ = w′ + ∆w as the parameter update for a weight vector, where ∆w ∝ ∇w′L. In two hypothetical update 4DoRA: Weight-Decomposed Low-Rank Adaptation scenarios, S1 and S2, S1 involves a smaller directional update (∆DS1), while S2 involves a larger one ( ∆DS2). Assuming ||∆wS1|| = ||∆wS2||, and at time 0, we have ∆v = 0 and v′ = v. From ∆DS1 < ∆DS2, it follows that |cos(∆wS1, w′)| > |cos(∆wS2, w′)|. Since ∆w ∝ ∇w′L, it implies |cos(∇S1 w′ L, w′)| > |cos(∇S2 w′ L, w′)|. From Sec 4.1, with v initialized as v0 and w′ = w0 at time 0, we get |cos(∇w′L, w′)| = |cos(∇w′L, v′)| = |cos(∇w′L, v)|. Using the cosine similarity equation with ∆v = 0: cos(∇w′L, v′) = cos(∇w′L, v) = ∇w′L ·v ||∇w′L||||v|| (8) denote m∗ as the magnitude scalar of vector w′ then Eq. (7) w.r.t m∗ can be rewritten to: ∇m∗L = ∇w′L ·v′ ||v′|| = ||∇w′L|| ·cos(∇w′L, v) (9) Given that ||∆wS1|| = ||∆wS2|| for S1 and S2, and ||∇S1 w′ L|| = ||∇S2 w′ L||. Therefore, with: ||∇S1 w′ L|| · |cos(∇S1 w′ L, v)| > ||∇S2 w′ L|| · |cos(∇S2 w′ L, v)| (10) it can be inferred that |∇S1 m∗L| > |∇S2 m∗L| which indicate that S1 has larger magnitude updates over S2 while having smaller directional alteration than that of S2. Our conclu- sion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT. 4.3. Reduction of Training Overhead In Eq. (1), the gradients of W′ and ∆W are the same. How- ever, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low- rank updates differs from that ofW′, as illustrated in Eq. (6). This divergence necessitates extra memory during backprop- agation. To address this, we suggest treating||V + ∆V ||c in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while ||V + ∆V ||c dynamically reflects the updates of ∆V , it won’t receive any gradient during backpropagation. With this modification, the gra- dient w.r.t m remains unchanged, and ∇V ′L is redefined as: ∇V ′L = m C ∇W′L where C = ||V ′||c (11) This approach reduces the gradient graph memory consump- tion drastically without a noticeable difference in accuracy. We conduct an ablation study to evaluate the impact of the proposed modification on fine-tuning LLaMA-7B and VL- BART. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains un- changed for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. For a comprehensive comparison of training memory usage and accuracy differences, please see Table 7 in the appendix. Consequently, all subsequent experiments with DoRA incorporate this adjustment. 5. Experiments We conduct a variety of experiments to showcase the effi- cacy of DoRA on various tasks including language, image, and video domains. Firstly, we evaluate DoRA against sev- eral Parameter-Efficient Fine-Tuning (PEFT) methods by fine-tuning LLaMA-7B/13B, LLaMA2-7B, and LLaMA3- 8B on commonsense reasoning tasks. Subsequently, we extend from single modality to multimodality. We compare DoRA with LoRA across multi-task image/video-text under- standing tasks using VL-BART and visual instruction tuning with LLaV A-1.5-7B. Following this, we explore the com- patibility of DoRA with LoRA and VeRA (Kopiczko et al., 2024) for instruction-tuning on LLaMA-7B and LLaMA2- 7B. Furthermore, we perform a series of ablation studies to illustrate that DoRA surpasses LoRA in performance, irre- spective of the number of fine-tuning training samples and rank variations. Lastly, We analyze the tuning granularity of DoRA, and show that DoRA can achieve better accuracy than LoRA with fewer trainable parameters by selectively updating only the directional components of certain mod- ules. 5.1. Commonsense Reasoning We evaluate DoRA against LoRA and several baseline meth- ods which include Prompt learning (Prefix) (Li & Liang, 2021), Series adapter (Series) (Houlsby et al., 2019), and Parallel adapter (Parallel) (He et al., 2021) on LLaMA- 7B/13B (Touvron et al., 2023) for commonsense reasoning tasks. We also include ChatGPT’s accuracy obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Ope- nAI, 2023; Wei et al., 2022). The commonsense reasoning tasks comprise 8 sub-tasks, each with a predefined training and testing set. We follow the setting of (Hu et al., 2023) and amalgamate the training datasets from all 8 tasks to create the final training dataset and conduct evaluations on the individual testing dataset for each task. To ensure a fair comparison, we initially fine-tuned models with DoRA following the LoRA config- uration, maintaining the same rank while adjusting only the learning rate. The marginal increase of 0.01% in the number of trainable parameters for DoRA over LoRA, as detailed in Table 1, arises from the inclusion of learnable magnitude components (parameter of size 1 × k). Then, we further halve the rank used in DoRA compared to LoRA 5DoRA: Weight-Decomposed Low-Rank Adaptation Table 1.Accuracy comparison of LLaMA 7B/13B, LLaMA2 7B, and LLaMA3 8B with various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods on LLaMA 7B/13B are taken from (Hu et al., 2023). Results of LoRA on LLaMA2 7B and LLaMA3 8B are obtained using the hyperparameters described in (Hu et al., 2023). DoRA†: the adjusted version of DoRA with the rank halved. Model PEFT Method # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B Prefix 0.11 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series 0.99 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel 3.54 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2 LoRA 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 DoRA† (Ours) 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA (Ours) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 LLaMA-13B Prefix 0.03 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 Series 0.80 71.8 83 79.2 88.1 82.4 82.5 67.3 81.8 79.5 Parallel 2.89 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4 LoRA 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5 DoRA† (Ours) 0.35 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8 DoRA (Ours) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LLaMA2-7B LoRA 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6 DoRA† (Ours) 0.43 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5 DoRA (Ours) 0.84 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 LLaMA3-8B LoRA 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA† (Ours) 0.35 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA (Ours) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 and denote this adjusted configuration as DoRA†. See Table 8 for details on the hyperparameters used. Table 1 demonstrates that DoRA consistently surpasses all baseline methods across both LLaMA-7B/13B, LLaMA2- 7B and LLaMA3-8B. Notably, in the LLaMA-7B model, where LoRA exceeds the performance of other baselines, DoRA further enhances accuracy by 3.7%, outstripping ChatGPT’s accuracy levels. Conversely, for LLaMA-13B, where LoRA’s effectiveness is inferior to the Parallel adapter, DoRA achieves superior accuracy over LoRA by 1% and comparable accuracy to the Parallel adapter, with only a quarter of the trainable parameters required by the Parallel adapter and without adding any extra inference overhead as the Parallel adapter. Additionally, DoRA consistently surpasses LoRA on both LLaMA2-7B and LLaMA3-8B by 2.1% and 4.4%, respectively. Furthermore, DoRA† exceeds LoRA’s performance on LLaMA-7B by 2.8%, on LLaMA- 13B by 1%, on LLaMA2-7B by 2.9%, and on LLaMA3-8B by 4.2%, despite having only half as many trainable parame- ters as LoRA. This outcome suggests that the integration of DoRA enhances the learning capability of LoRA, thereby reducing the need for a higher rank to surpass LoRA in terms of accuracy. Additionally, in previous sections, we hypothesize that a negative correlation between the magnitude update and di- rectional update is more optimal than a positive correlation. This is because pre-trained weights already contain sub- stantial knowledge suitable for downstream tasks, and a larger magnitude or direction alteration alone is sufficient Figure 3.Magnitude (a) and direction (b) difference of LoRA/DoRA and the pre-trained weight of the query matrices across different layers. for downstream adaptation. To further validate our hypoth- esis, we used LLaMA2-7B fine-tuned with DoRA/LoRA on commonsense reasoning datasets as a case study. We visualized the magnitude (∆M) and directional difference (∆D) between the DoRA/LoRA weights and the pre-trained model weights across different modules and layers. In Fig- ure 3 (a) and (b), we observe that the DoRA fine-tuned weights show less deviation from the pre-trained weights in both magnitude and direction, while the differences for the LoRA fine-tuned weights are significantly larger. Coupled with the experimental results that DoRA significantly out- performs LoRA, we can conclude that our earlier hypothesis is valid: a robust foundation model does not require signif- icant alterations for effective downstream adaptation and having the ability to perform more fine-grained magnitude and directional update explains the superiority of DoRA over LoRA. We leave the visualization of the value and key 6DoRA: Weight-Decomposed Low-Rank Adaptation weight matrices in the appendix. 5.2. Image/Video-Text Understanding Table 2.The multi-task evaluation results on VQA, GQA, NVLR2 and COCO Caption with the VL-BART backbone. Method #Params (%) VQAv2 GQA NVLR2 COCO Cap Avg. FT 100 66.9 56.7 73.7 112.0 77.3 LoRA 5.93 65.2 53.6 71.9 115.3 76.5 DoRA (Ours) 5.96 65.8 54.7 73.1 115.9 77.4 Table 3.The multi-task evaluation results on TVQA, How2QA, TVC, and YC2C with the VL-BART backbone. Method #Params (%) TVQA How2QA TVC YC2C Avg. FT 100 76.3 73.9 45.7 154 87.5 LoRA 5.17 75.5 72.9 44.6 140.9 83.5 DoRA (Ours) 5.19 76.3 74.1 45.8 145.4 85.4 Having shown that DoRA can consistently achieve bet- ter accuracy on fine-tuning LLM, we would like to see if DoRA can remain competitive on multi-modality fine- tuning tasks. We compare DoRA with LoRA and full fine- tuning on VL-BART which comprises a vision encoder (CLIP-ResNet101 (Radford et al., 2021)) and an encoder- decoder language model ( BARTBase (Lewis et al., 2020)) across four different image-text tasks: VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for visual ques- tion answering, NLVR2 (Suhr et al., 2019) for visual reason- ing, and MSCOCO (Chen et al., 2015) for image captioning, and four different video-text tasks from the V ALUE (Li et al., 2021) Benchmark: TVQA (Lei et al., 2018) and How2QA (Li et al., 2020) for video question answering, TVC (Lei et al., 2020) and YC2C (Zhou et al., 2018) for video captioning. We follow the same framework as (Sung et al., 2022) and fine-tuned VL-BART within a multi-task framework for both image/video-text tasks. We adopt the same setup as that of LoRA outlined in (Sung et al., 2022) when ap- plying DoRA. See Table 9 for the complete hyperparam- eters. The result of LoRA and FT for both image/video- text tasks is directly quoted from (Sung et al., 2022). We can see that DoRA uniformly surpasses LoRA in accuracy while maintaining a similar count of trainable parameters in both Table 2 and Table 3. In particular, DoRA exceeds LoRA’s performance by nearly 1% in image-text under- standing tasks, reaching the accuracy level of FT. Moreover, DoRA achieves roughly 2% higher accuracy than LoRA in video-text understanding tasks. 5.3. Visual Instruction Tuning We further scale up the model size and compare DoRA to LoRA and FT on the visual instruction tuning tasks with Table 4.Visual instruction tuning evaluation results for LLaV A- 1.5-7B on a wide range of seven vision-language tasks. We directly use checkpoints from (Liu et al., 2023a) to reproduce their results. Method # Params(%) Avg. FT 100 66.5 LoRA 4.61 66.9 DoRA (Ours) 4.63 67.6 LLaV A-1.5-7B (Liu et al., 2023a) which is composed of a language model, Vicuna-1.5-7B (Peng et al., 2023), and a vision encoder, CLIP ViT-L/336px (Radford et al., 2021). The training datasets contain several datasets from VQA (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Schwenk et al., 2022), OCR (Mishra et al., 2019; Sidorov et al., 2020), region-level VQA (Kazemzadeh et al., 2014; Krishna et al., 2017; Mao et al., 2016), visual conversation (Liu et al., 2023a), and language conversation data. We follow the setting of (Liu et al., 2023a) to filter the training data and construct the tunning prompt format. For a fair comparison, DoRA follows the same configuration as the LoRA configuration provided by (Liu et al., 2023a). The fine-tuned models are then evaluated on seven vision- language benchmarks: VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018) SQA (Lu et al., 2022), VQAT (Singh et al., 2019), POPE (Li et al., 2023), and MMBench (Liu et al., 2023c). From Table 4, we can observe that the average accuracy of LoRA already surpasses FT, which could imply that FT might be experiencing issues with overfitting. Given that DoRA is designed to enhance LoRA’s performance to more closely resemble that of FT, in scenarios where FT is infe- rior to LoRA, DoRA’s improvement over LoRA might not be as pronounced as observed in other experiments where FT usually outperforms LoRA. Nonetheless, DoRA still demonstrates superior performance over both LoRA and FT, with an average improvement of 0.7% over LoRA and 1.1% over FT. See Table 10 for the hyperparameters setting and Table 12 for the score of each evaluation benchmark. 5.4. Compatibility of DoRA with other LoRA variants Recall from Equation.(1) that ∆W can be adapted by differ- ent LoRA variants. With DoRA, the concept of incremental directional update ∆V introduced in Equation.(5) can like- wise be replaced with alternative LoRA variants. In this section, we select VeRA (Kopiczko et al., 2024) as a case study to explore DoRA’s compatibility with other LoRA variants. VeRA suggests freezing a unique pair of random low-rank matrices to be shared across all layers, employ- ing only minimal layer-specific trainable scaling vectors to capture each layer’s incremental updates. This approach allows VeRA to reduce trainable parameters significantly 7DoRA: Weight-Decomposed Low-Rank Adaptation Table 5.Average scores on MT-Bench assigned by GPT-4 to the answers generated by fine-tuned LLaMA-7B/LLaMA2-7B. Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA (Ours) 2.33 5.5 VeRA 0.02 4.3 DV oRA (Ours) 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA (Ours) 2.33 6.0 VeRA 0.02 5.5 DV oRA (Ours) 0.04 6.0 by 10x compared to LoRA, with only a minimal impact on accuracy. We apply VeRA for the directional update in DoRA and name such combination DV oRA. We assess the effectiveness of both DV oRA and DoRA compared to VeRA and LoRA across LLaMA-7B and LLaMA2-7B, focusing on instruction tuning with the 10K subset of cleaned Alpaca dataset (Taori et al., 2023). We utilize the official imple- mentation of VeRA to obtain the results of VeRA and LoRA and fine-tune the model with DV oRA and DoRA using the identical training settings as VeRA and LoRA (see Table 11 in the appendix for more details). The performance of the fine-tuned models is then evaluated on the MT-Bench bench- mark (Zheng et al., 2023) by generating model responses to a pre-defined set of 80 multi-turn questions. These re- sponses are then evaluated by GPT-4, which reviews each answer and assigns a numerical score out of 10. Table 5 presents the average scores for DV oRA, DoRA, VeRA, and LoRA, demonstrating that our proposed method exhibits consistent improvements over VeRA and LoRA for both LLaMA-7B and LLaMA2-7B. This effectively show- cases the compatibility of DoRA with VeRA. In particular, DV oRA merges the advantageous qualities of DoRA and VeRA, attaining scores that are on par with or even sur- pass those of LoRA, yet with significantly fewer parame- ters. For example, DV oRA outperforms VeRA by 0.7/0.5 points and achieves the same level of accuracy as LoRA on LLaMA-7B and DoRA on LLaMA2-7B, respectively. Ad- ditionally, we present a selection of questions chosen from MT-Bench, accompanied by the responses from LLaMA2- 7B fine-tuned using DV oRA and VeRA in the appendix (Table 13 and 14) where we can observe that the answers given by DV oRA tend to be more precise and structural. Next, to further assess DoRA’s ability to remain competitive under varying amounts of training data, considering that in practical situations, access to extensive fine-tuning datasets is frequently limited. We compare DoRA to LoRA and DV oRA to VeRA for fine-tuning LLaMA2-7B/LLaMA-7B with a range of instruction-tuning sample sizes, specifically 1000, 4000, 7000, 10000, with 10000 being the setting of 1000 4000 7000 10000 Number of instruction tuning training samples 5.0 5.2 5.4 5.6 5.8 6.0Score LLaMA2-7B DoRA LoRA DV oRA VeRA Figure 4.Performance of fine-tuned LLaMA2-7B on MT-Bench using different numbers of Alpaca training samples. (Kopiczko et al., 2024). We visualize the average perfor- mance of each method on LLaMA2-7B in Figure 4, and on LLaMA-7B in Figure 9 in the appendix. The result shows that DoRA and DV oRA consistently outperform LoRA and VeRA across all training sample sizes. For instance, with 7000 training samples, DoRA and DV oRA surpass LoRA and VeRA by margins of 0.3 and 0.33, respectively. Even when the sample size is reduced to 1000, DoRA and DV oRA maintain their lead with advantages of 0.29 and 0.22 over LoRA and VeRA, respectively. This demonstrates that our methods persistently enhance performance over LoRA and VeRA, regardless of the training sample volume. 5.5. Robustness of DoRA towards different rank settings 4 8 16 32 64 rank r 40 45 50 55 60 65 70 75 80Avg. Accuracy (+ 37.2%) (+ 22.4%) LLaMA-7B DoRA LoRA Figure 5.Average accuracy of LoRA and DoRA for varying ranks for LLaMA-7B on the commonsense reasoning tasks. This section explores the impact of various rank configura- tions on DoRA and LoRA by adjustingr within the set {4, 8, 16, 32, 64} and assessing the performance of the fine-tuned LLaMA-7B on commonsense reasoning tasks as outlined in Sec 5.1. The average accuracies of LoRA and DoRA across different ranks are depicted in Figure 5, with detailed num- bers presented in Table 15. From Figure 5, we can observe that DoRA consistently surpasses LoRA across all rank con- figurations. Notably, the performance gap widens for ranks 8DoRA: Weight-Decomposed Low-Rank Adaptation Table 6.Accuracy comparison of LLaMA 7B/13B with two differ- ent tuning granularity of DoRA. Columns m and V designate the modules with tunable magnitude and directional components, re- spectively. Each module is represented by its first letter as follows: (Q)uery, (K)ey, (V)alue, (O)utput, (G)ate, (U)p, (D)own. Model PEFT Method # Params (%) m V Avg. LLaMA-7B LoRA 0.83 - - 74.7 DoRA (Ours) 0.84 QKVUD QKVUD 78.1 DoRA (Ours) 0.39 QKVOGUD QKV 77.5 LLaMA-13B LoRA 0.67 - - 80.5 DoRA (Ours) 0.68 QKVUD QKVUD 81.5 DoRA (Ours) 0.31 QKVOGUD QKV 81.3 below 8, where LoRA’s average accuracies drop to 40.74% for r = 8 and 39.49% for r = 4. In contrast, DoRA retains a notable accuracy of 77.96% for r = 8 and 61.89% for r = 4, demonstrating its resilience and consistently superior performance over LoRA regardless of the rank setting. 5.6. Tuning Granularity Analysis The visualization in Figure 2 indicates that significant changes in magnitude often result in relatively smaller di- rectional changes. Given this observation and the fact that directional updates account for most of the trainable param- eters, it prompts an investigation into whether it is possible to decrease the number of trainable parameters by updating only the magnitude components of specific modules while continuing to update both the magnitude and directional components for the remaining linear modules. Our findings indicate that, in contrast to the original con- figuration suggested for LoRA in (Hu et al., 2023), which requires updates to both the Multi-head Attention and MLP layers for optimal performance, DoRA can already achieve superior accuracy by updating only the directional and mag- nitude components of the multi-head layers and the magni- tude of the MLP layers. Specifically, as shown in Table 6, by updating the directional and magnitude components of the QKV modules and only the magnitude of the rest of the layers, DoRA surpasses LoRA by 2.8% on LLaMA-7B and 0.8% on LLaMA-13B, while utilizing only less than half of the trainable parameters compared to LoRA. 6. Broader Impacts 6.1. QDoRA: Enhancements to QLoRA While finetuning LLMs with PEFT significantly reduces training memory overhead, a considerable amount of GPU memory is still required to initially load the model weights onto the GPUs. To further decrease the memory demands of finetuning, QLoRA (Dettmers et al., 2023) suggests quan- tizing the pretrained model to 4-bit and finetuning LoRA on top of the frozen low-bit backbone. With our porposed 0.0 0.1 0.2 0.3 0.4 0.5 Exact match score (Eval size: 500) Zero-shot Five-shot Full Finetune QLoRA QDoRA 0.23 0.27 0.51 0.32 0.56 0.07 0.08 0.26 0.12 0.31 100k Orca-Math finetuning results LLaMA2-7B LLaMA3-8B Figure 6.Accuracy comparison of LLaMA2-7B/LLaMA3-8B with QDoRA, QLoRA and FT on Orca-Math (Mitra et al., 2024). DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework. Recently, (Kerem Turgutlu, 2024) launch a project that substitutes the LoRA component in QLoRA with DoRA, dubbing it QDoRA, and incorporate the training pipeline with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) to enable model splitting and parallel training across multiple GPUs. They conducted experiments on fine-tuning LLaMA2- 7B/LLaMA3-8B using the Orca-Math(Mitra et al., 2024) dataset with QDoRA, QLoRA, and FT. The training set included 100k samples, with 500 reserved for evaluation using the exact match score as the metric. In addition to the fine-tuned models, they also reported results from zero-shot, few-shot, and FT with post-training quantization (PTQ), where the FT model is quantized to the BnB NF4 format after training. According to Figure 6, QDoRA not only significantly surpasses QLoRA by 0.19/0.23 on LLaMA2- 7B and LLaMA3-8B, but it also slightly outperforms FT on both models, while using considerably less memory. This in- dicates that QDoRA can effectively combines the parameter efficiency of QLoRA with the more granular optimization of full finetuning. These initial findings suggest that QDoRA holds considerable promise and could hugely benefit the opensoure community by substantially lowering the GPU memory requirements for fine-tuning large language mod- els. 6.2. Text-to-Image Generation Recently, as diffusion models have expanded in size, LoRA has become a popular method for efficiently fine-tuning large stable diffusion models. In this section, we aim to explore whether DoRA’s advantages over LoRA extend to the task of text-to-image generation. We follow the training pipeline of DreamBooth (Ruiz et al., 2023) for fine-tuning 9DoRA: Weight-Decomposed Low-Rank Adaptation SDXL (Podell et al., 2023), utilizing the advanced train- ing scripts developed by HuggingFace. The hyperparameter settings for LoRA and DoRA are kept the same, and we fine- tune the model using two challenging datasets: 3D icons and Lego sets. The sample seeds for generating the images are kept the same for LoRA and DoRA for fair comparison. The generated images are shown in Figure 10 and 11 in the appendix. The results indicate that DoRA achieves signif- icantly better personalization than LoRA when using the same training settings, and more accurately reflects the train- ing targets. For example, in Figure 10, the first sub-figure of DoRA’s output features a unique round square around the image, which is a feature common to all the training targets. In contrast, this feature is absent in all the LoRA outputs. A similar observation could be found with the Lego training targets, where only the DoRA outputs consistently incorporate the Lego logo in the generated images. 7. Conclusion In this work, we first conduct a novel weight decomposi- tion analysis to reveal the distinct learning patterns between LoRA and FT. Building on these insights, we introduce DoRA, a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT’s learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Specifically, DoRA improves upon LoRA in commonsense reasoning and visual instruction tuning tasks. Furthermore, DoRA also shows compatibility with VeRA on the Alpaca instruction tuning task. Moreover, DoRA can be considered as a costless alternative to LoRA, as its decomposed mag- nitude and direction components can be merged back into the pre-trained weight after the training, ensuring that there is no extra inference overhead. For future work, we wish to explore the generalizability of DoRA in domains beyond language and vision, particularly in the field of audio. Acknowledgements We extend our gratitude to Benjamin Bossan, Younes Belkada, and Sourab Mangrulkar from Hugging Face for their assistance in integrating DoRA into the PEFT pack- age, thus making our work more accessible to the broader public. We thank Kerem Turgutlu, Jonathan Whitaker, and Jeremy Howard from Answer.AI for their work on the imple- mentation and experiments of QDoRA/FSDP, which makes fine-tuning of large language models with DoRA on con- sumer GPUs a lot more feasible. We also thank Sebastian Raschka for his well-written tutorial on DoRA which offers a thorough overview of the background knowledge neces- sary to comprehend DoRA. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Chen, X., Fang, H., Lin, T.-Y ., Vedantam, R., Gupta, S., Doll´ar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 10088–10115. Cur- ran Associates, Inc., 2023. Goyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617, 2018. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient trans- fer learning. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on 10DoRA: Weight-Decomposed Low-Rank Adaptation imagenet classification. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 1026–1034, 2015. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pp. 2790–2799, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Hu, Z., Wang, L., Lan, Y ., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large lan- guage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700– 6709, 2019. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low- rank hadamard product for communication-efficient fed- erated learning. In International Conference on Learning Representations, 2022. Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 565–576, 2021. Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014. Kerem Turgutlu, Jonathan Whitaker, J. H. Ef- ficient finetuning of llama 3 with fsdp qdora. https://www.answer.ai/posts/ 2024-04-26-fsdp-qdora-llama3.html , 2024. Kopiczko, D. J., Blankevoort, T., and Asano, Y . M. Vera: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024. Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, pp. 32–73, 2017. Lei, J., Yu, L., Bansal, M., and Berg, T. TVQA: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1369–1379, 2018. Lei, J., Yu, L., Berg, T. L., and Bansal, M. Tvr: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision, pp. 447–463, 2020. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pp. 7871–7880, 2020. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Confer- ence on Machine Learning, pp. 12888–12900, 2022. Li, L., Chen, Y .-C., Cheng, Y ., Gan, Z., Yu, L., and Liu, J. HERO: Hierarchical encoder for Video+Language omni- representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2046–2065, 2020. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y .-C., Pillai, R., Cheng, Y ., Zhou, L., Wang, X. E., Wang, W. Y ., et al. Value: A multi-task benchmark for video-and-language understand- ing evaluation. In Thirty-fifth Conference on Neural In- formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582– 4597, 2021. Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language 11DoRA: Weight-Decomposed Low-Rank Adaptation models. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing , pp. 292–305, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun- ing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Liu, W., Qiu, Z., Feng, Y ., Xiu, Y ., Xue, Y ., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W., Yuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Infor- mation Processing Systems, pp. 2507–2521, 2022. mahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural Information Processing Systems, 2021. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unam- biguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11–20, 2016. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requir- ing external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195–3204, 2019. Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952, 2019. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. OpenAI. Welcome to the openai platform, 2023. URL https://platform.openai.com/docs/ introduction. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im- proving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining modular skills in multitask learning.arXiv preprint arXiv:2202.13914, 2022. Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339–1384, 2023. Qiu, Z., Liu, W., Feng, H., Xue, Y ., Feng, Y ., Liu, Z., Zhang, D., Weller, A., and Sch¨olkopf, B. Controlling text- to-image diffusion by orthogonal finetuning. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763, 2021. Razdaibiedina, A., Mao, Y ., Khabsa, M., Lewis, M., Hou, R., Ba, J., and Almahairi, A. Residual prompt tuning: improving prompt tuning with residual reparameteriza- tion. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6740–6757, 2023. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023. Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image dif- fusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023. Salimans, T. and Kingma, D. P. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 901–909, 2016. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual ques- tion answering using world knowledge. In European Conference on Computer Vision, pp. 146–162, 2022. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehen- sion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part II 16, pp. 742–758, 2020. 12DoRA: Weight-Decomposed Low-Rank Adaptation Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317–8326, 2019. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y . A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418–6428, 2019. Sung, Y .-L., Cho, J., and Bansal, M. Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 5227–5237, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stan- ford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/ stanford_alpaca. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wang, Y ., Wu, J., Dabral, T., Zhang, J., Brown, G., Lu, C.-T., Liu, F., Liang, Y ., Pang, B., Bendersky, M., et al. Non- intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. arXiv preprint arXiv:2310.12100, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y ., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Interna- tional Conference on Learning Representations, 2023. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In Proceed- ings of the AAAI Conference on Artificial Intelligence , 2018. 13DoRA: Weight-Decomposed Low-Rank Adaptation A. Appendix A.1. Weight decomposition analysis on the value weight matrix In this section, we illustrate the changes in magnitude and direction within the value weight matrix for FT, LoRA, and DoRA across different training steps and layers, as shown in Figure 7. This reveals patterns similar to those seen in the query weight matrix depicted in Figure 2, indicating that DoRA is capable of displaying learning behaviors that closely mirror those of FT across various modules. 0.02 0.03 0.04 0.05 0.06 0.07 0.08 D (a) 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325 D (b) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 D (c) 0.4 0.6 0.8 1.0 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 7.Magnitude and Directional changes of FT (a), LoRA (b), and DoRA (c) of the V weight matrices across different layers and intermediate steps. A.2. Ablation study for the modification to reduce DoRA training cost Table 7 presents the GPU cost and the average accuracy of DoRA with and without the proposed modification for commonsense reasoning tasks and image-text understanding tasks. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains unchanged for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. Table 7.GPU cost and accuracy of DoRA with or without the modification on the commonsense reasoning tasks and image-text understanding tasks. Model PEFT Method Accumulation steps Batch Size GPU Memory Cost (GB) # Params (%) Avg. LLaMA-7B DoRA w/o modification 4 16 37.3 0.84 78.3 DoRA 28.2 (-24.4%) 0.84 78.1 VL-BART DoRA w/o modification - 300 23.4 5.96 77.3 DoRA 20.5 (-12.4%) 5.96 77.4 14DoRA: Weight-Decomposed Low-Rank Adaptation A.3. Hyperparameters Table 8.Hyperparameter configurations of DoRA for LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B on the commonsense reasoning tasks. Hyperparameters (DoRA) LLaMA-7B LLaMA-13B LLaMA2-7B LLaMA3-8B Rank r 16 32 16 32 16 32 16 32 α 32 64 32 64 32 64 32 64 Dropout 0.05 Optimizer AdamW LR 2e-4 1e-4 3e-4 2e-4 2e-4 2e-4 1e-4 1e-4 LR Scheduler Linear Batch size 16 Warmup Steps 100 Epochs 3 Where Q,K,V ,Up,Down Table 9.Hyperparameter configurations of DoRA for fine-tuning VL-Bart on image/video-text tasks. Hyperparameters (DoRA) image-text video-text Rank r 128 α 128 Dropout 0.0 Optimizer AdamW LR 1e-3 3e-4 LR Scheduler Linear Batch size 300 40 Warmup ratio 0.1 Epochs 20 7 Where Q,K Table 10.Hyperparameter configurations of DoRA and LoRA for fine-tuning LLaV A-1.5-7B with visual instruction tuning datasets. Hyperparameters DoRA LoRA Rank r 128 α 256 Dropout 0.05 Optimizer AdamW LR 2e-4 LR Scheduler Cosine decay Batch size 16 Warmup ratio 0.03 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 15DoRA: Weight-Decomposed Low-Rank Adaptation Table 11.Hyperparameter configurations of DoRA and DV oRA for fine-tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset. Hyperparameters (DoRA) LLaMA-7B LLaMA2-7B Rank r 64 Dropout 0.0 Optimizer AdamW LR 4e-4 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate Hyperparameters (DVoRA) LLaMA-7B LLaMA2-7B Rank r 1024 Dropout 0.0 Optimizer AdamW LR 4e-3 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 16DoRA: Weight-Decomposed Low-Rank Adaptation A.4. Magnitude and Direction difference between DoRA/LoRA fine-tuned weight and the pre-triained weight of LLaMA2-7B for the commonsesne reasoning tasks Figure 8 depicts the magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. The figure shows that the DoRA fine-tuned weights deviate less from the pre-trained weights in both magnitude and direction, supporting our hypothesis that a robust foundation model does not need substantial changes for effective downstream adaptation. (a) (b) (d) (c) (f) (e) Figure 8.Magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. 17DoRA: Weight-Decomposed Low-Rank Adaptation A.5. Visual instruction tuning evaluation result Table 12 presents the evaluation result of LLaV A-1.5-7B fine-tuned using DoRA, LoRA, and FT with visual instruction tuning data. The evaluation is conducted on an array of seven different vision-language benchmarks. From the table, we can see that DoRA consistently outmatch FT, and can achieve an average accuracy better than LoRA. Table 12.Visual instruction tuning evaluation result of DoRA, LoRA, and FT for LLaV A-1.5-7B on a wide range of 7 vision-language tasks. Method # Params(%) VQAv2 GQA VisWiz SQA VQA T POPE MMBench Avg. FT 100 78.5 61.9 50.0 66.8 58.2 85.9 64.3 66.5 LoRA 4.61 79.1 62.9 47.8 68.4 58.2 86.4 66.1 66.9 DoRA (Ours) 4.63 78.6 62.9 52.2 69.9 57 87.2 66.1 67.6 18DoRA: Weight-Decomposed Low-Rank Adaptation A.6. Instruction-tuning evaluation examples We sample two questions from the MT-Bench question set and present the respective responses from DV oRA and VeRA, including the scores and comments provided by GPT-4. From these examples, we can observe that responses from DV oRA are generally more accurate and well-organized compared to those from VeRA. For instance, in response to Question 24, DV oRA leverages a well-known superhero story that is familiar to children, whereas VeRA’s response begins with an uncertain tone, potentially making it less engaging than DV oRA’s reply. Table 13.Question 24 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 24 As a superhero, how would you explain your origin story to a curious child? VeRA Well, I’m not really sure where I come from. All I know is that one day, I woke up with special powers and abilities that allowed me to help people. I decided to use my powers to fight crime and protect the innocent, and that’s how I became a superhero DV oRA As a superhero, I have the ability to use my super powers to protect people and fight against evil. My origin story begins when I was just a regular kid like you, but one day, I was bitten by a radioactive spider, and it gave me incredible powers. I could climb walls, shoot webs, and see in the dark. I decided to use my powers to help people, and that’s how I became your friendly neighborhood Spider-Man. GPT-4 Evaluation Score: 5 7. Assistant 1’s response starts with an uncertain tone, which might not be very engaging for a child. The explanation is quite vague and doesn’t offer a clear or imaginative origin story that could captivate a child’s interest. It does, however, maintain the theme of using powers for good, which is a positive message. Assistant 2’s response is more detailed and uses a familiar superhero origin story (Spider-Man’s) that many children can relate to and understand. It’s engaging and provides a clear narrative of how an ordinary person can become a superhero. However, it directly references Spider-Man, which is not an original origin story and could be misleading if the assistant is supposed to be a unique superhero. Despite this, the response is more likely to resonate with a child due to its clarity and reliability. Both responses could be improved by providing an original and imaginative origin story that doesn’t rely on existing superhero narratives. They should also ensure that the story is simple enough for a child to understand while being engaging and inspiring. 19DoRA: Weight-Decomposed Low-Rank Adaptation Table 14.Question 33 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 33 Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? VeRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Some people may prefer the physical act of navigating with a paper map, as it can be more tactile and interactive. Others may prefer to have a hard copy of the map in case the GPS or app fails or the battery dies. Additionally, some people may not have access to a GPS device or smartphone, or may not want to rely on technology for navigation. Finally, some people may simply prefer the security and peace of mind that comes with knowing exactly where they are and how to get to their destination DV oRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Here are some of them: 1. Navigation accuracy: GPS devices and smartphone apps rely on satellite signals to pinpoint your location, but this technology is not foolproof. GPS signals can be blocked by tall buildings, tunnels, or other obstacles, resulting in inaccurate or missing information. Paper maps, on the other hand, are based on accurate, up-to-date information that is not subject to these limitations. 2. Cost: GPS devices and smartphone apps can be expensive to purchase and maintain. Paper maps are free to use and do not require an ongoing subscription or data plan. 3. Personal preference: Some people prefer the tactile experience of using a paper map, which can be easier to read and navigate than a screen. Others enjoy the challenge of navigating without technology, which can help them learn more about the area they are visiting. 4. Battery life: GPS devices and smartphone apps can drain batteries quickly, especially when used for navigation. Paper maps do not require power, so they can be used for extended periods without recharging. 5. Offline use: GPS devices and smartphone apps rely on an internet connection to function properly. If there is no signal, the device will not be able to provide accurate information. Paper maps, on the other hand, can be used offline, making them ideal for use in remote or unpopulated areas. In summary, there are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. These include navigation accuracy, cost, personal preference, battery life, and offline use. GPT-4 Evaluation Score: 6 8. Assistant 1’s response is somewhat brief and lacks specific details that could be helpful to the user. It mentions the tactile nature of paper maps, the possibility of technology failure, lack of access to technology, and a general preference for security and peace of mind. However, it does not delve into the practical considerations such as GPS signal limitations, cost, battery life, or offline use, which are important factors in deciding between paper maps and digital navigation tools. Assistant 2’s response is more comprehensive and provides a structured list of reasons, including navigation accuracy, cost, personal preference, battery life, and offline use. Each point is explained clearly, giving the user a better understanding of the potential advantages of paper maps over GPS devices or smartphone apps. The response is relevant, accurate, and detailed, making it more helpful to the user. Both assistants provided relevant and accurate information, but Assistant 2’s answer is more detailed and structured, which is why it receives a higher score. The scores reflect the level of detail and the usefulness of the information provided to the user, with Assistant 2 offering a more complete answer to the question 20DoRA: Weight-Decomposed Low-Rank Adaptation A.7. Robustness towards different numbers of instruction tunning examples Figure 9 depicts the score of DoRA, LoRA, DV oRA, and VeRA for fine-tuning LLaMA-7B with varying instruction-tuning sample sizes, including 1000, 4000, 7000, and 10000. From the figure, we can observe that similar to that of the result on LLaMA2-7B, both DoRA and DV oRA consistently outperform LoRA and VeRA across the entire range of training sample sizes. 1000 4000 7000 10000 Number of instruction tuning training samples 3.0 3.5 4.0 4.5 5.0 5.5Score LLaMA-7B DoRA LoRA DV oRA VeRA Figure 9.Performance of LLaMA-7B fine-tuned with LoRA, DoRA, VeRA, and DV oRA on MT-Bench using different numbers of training samples from Alpaca dataset (Taori et al., 2023). 21DoRA: Weight-Decomposed Low-Rank Adaptation A.8. Robustness of DoRA towards different rank Table 15 shows a comparison of the average accuracy between LoRA and DoRA method across various rank settings for commonsense reasoning tasks. DoRA consistently outperforms LoRA at all rank settings, with the performance gap widening as the rank decreases. This suggests that our method effectively enhances the learning capacity of LoRA, enabling it to achieve better accuracy with fewer trainable parameters. Table 15.Accuracy comparison of LoRA and DoRA with varying ranks for LLaMA-7B on the commonsense reasoning tasks. PEFT Method rank r # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. LoRA 4 0.10 2.3 46.1 18.3 19.7 55.2 65.4 51.9 57 39.5 8 0.21 31.3 57.0 44.0 11.8 43.3 45.7 39.2 53.8 40.7 16 0.42 69.9 77.8 75.1 72.1 55.8 77.1 62.2 78.0 70.9 32 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 64 1.64 66.7 79.1 75.7 17.6 78.8 73.3 59.6 75.2 65.8 DoRA (Ours) 4 0.11 51.3 42.2 77.8 25.4 78.8 78.7 62.5 78.6 61.9 8 0.22 69.9 81.8 79.7 85.2 80.1 81.5 65.7 79.8 77.9 16 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 32 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 64 1.65 69.9 81.4 79.1 40.7 80.0 80.9 65.5 79.4 72.1 22DoRA: Weight-Decomposed Low-Rank Adaptation A.9. Text-to-Image Generation Figures 10 and 11 show the images produced by SDXL fine-tuned with DoRAand LoRA via DreamBooth (Ruiz et al., 2023) personalization techniques on two distinct training sets: 3D Icon 1 and Lego2. The results reveal that DoRA can achieve considerably better personalization than LoRA with identical training configurations, more closely matching the training target. Figure 10.Images generated with SDXL finetuned with LoRA and DoRA on the 3D Icon training sets. Figure 11.Images generated with SDXL finetuned with LoRA and DoRA on the Lego training sets. 1https://huggingface.co/datasets/linoyts/3d_icon 2https://huggingface.co/datasets/merve/lego_sets_latest 23",
      "meta_data": {
        "arxiv_id": "2402.09353v6",
        "authors": [
          "Shih-Yang Liu",
          "Chien-Yi Wang",
          "Hongxu Yin",
          "Pavlo Molchanov",
          "Yu-Chiang Frank Wang",
          "Kwang-Ting Cheng",
          "Min-Hung Chen"
        ],
        "published_date": "2024-02-14T17:59:34Z",
        "pdf_url": "https://arxiv.org/pdf/2402.09353v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel parameter-efficient fine-tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and full fine-tuning (FT) while avoiding additional inference costs. DoRA achieves this by decomposing pre-trained weights into magnitude and directional components, applying LoRA specifically for directional updates and allowing magnitude components to be tunable. The paper also presents a novel weight decomposition analysis to understand the fundamental differences in learning patterns between FT and existing PEFT methods like LoRA. DoRA consistently outperforms LoRA across various tasks and model architectures, including LLaMA, LLaVA, and VL-BART, and demonstrates compatibility with other LoRA variants like VeRA.",
        "methodology": "The core methodology involves a novel weight decomposition analysis, which reparameterizes model weights into magnitude and directional components to study LoRA and FT learning patterns. Based on these insights, DoRA is proposed, which first decomposes the pre-trained weight W0 into a magnitude vector m and a directional matrix V. For fine-tuning, DoRA keeps m as a trainable vector and updates the directional component V using a low-rank adaptation (LoRA) formulation (∆V = BA), resulting in W′ = m * (W0 + BA) / ||W0 + BA||c. This approach simplifies the learning task for LoRA by concentrating it on directional adaptation and enhances training stability. A gradient analysis is performed to explain DoRA's learning pattern. To reduce training overhead, a modification is introduced where ||V + ∆V||c is treated as a constant during backpropagation, drastically reducing gradient graph memory consumption with negligible accuracy impact.",
        "experimental_setup": "DoRA's efficacy is evaluated across diverse tasks and models. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B are fine-tuned on eight sub-tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA) and compared against LoRA, Prefix, Series, Parallel adapters, and ChatGPT. For image/video-text understanding, VL-BART is used with image-text tasks (VQAv2, GQA, NLVR2, MSCOCO Caption) and video-text tasks (TVQA, How2QA, TVC, YC2C). Visual instruction tuning is performed on LLaVA-1.5-7B, evaluated on VQAv2, GQA, VisWiz, SQA, VQAT, POPE, and MMBench. Compatibility with other LoRA variants is tested by creating DVoRA (DoRA with VeRA) and evaluating on LLaMA-7B/LLaMA2-7B with a 10K subset of Alpaca dataset using MT-Bench (GPT-4 scoring). Ablation studies investigate robustness to varying training sample sizes and rank settings. QDoRA (DoRA with QLoRA) is tested on LLaMA2-7B/LLaMA3-8B with Orca-Math dataset. Text-to-image generation is explored by fine-tuning SDXL via DreamBooth on 3D icons and Lego sets.",
        "limitations": "The paper does not explicitly list limitations of DoRA. It mentions that in scenarios where full fine-tuning (FT) is inferior to LoRA, DoRA's improvement over LoRA might not be as pronounced, as DoRA is designed to align more closely with FT's learning behavior. A practical consideration is the marginal increase of 0.01% in trainable parameters for DoRA over LoRA due to the learnable magnitude components. Additionally, the proposed memory reduction technique for training overhead involves treating a component as constant during backpropagation, which is an approximation.",
        "future_research_directions": "The authors express interest in exploring the generalizability of DoRA in domains beyond language and vision, specifically mentioning the field of audio as a promising area for future research."
      }
    },
    {
      "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
      "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .",
      "full_text": "Published as a conference paper at ICLR 2023 ADALORA: A DAPTIVE BUDGET ALLOCATION FOR PARAMETER -EFFICIENT FINE -TUNING Qingru Zhang†∗, Minshuo Chen‡, Alexander Bukharin†, Nikos Karampatziakis⋄, Pengcheng He⋄, Yu Cheng⋄, Weizhu Chen⋄ and Tuo Zhao† †Georgia Institute of Technology ‡Princeton University ⋄Microsoft Azure AI {qingru.zhang,abukharin3,tourzhao}@gatech.edu mc0750@princeton.edu {nikosk,penhe,yu.cheng,wzchen}@microsoft.com ABSTRACT Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine- tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/ QingruZhang/AdaLoRA. 1 I NTRODUCTION Pre-trained language models (PLMs) have manifested superior performance in various natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2021b; Radford et al., 2019; Brown et al., 2020). The most common way to adapt pre-trained models to down-stream tasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)). However, pre-trained models typically incurs large memory footprint. For example, BERT model (Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up to 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters. When building a NLP system upon these pre-trained models, we usually handle multiple tasks that arrive simultaneously (Radford et al., 2019). Given a large number of down-stream tasks, full fine-tuning requires that each task maintains a separated copy of large models. The resulting memory consumption is prohibitively expensive. To address this issue, researchers have proposed two main lines of research to reduce the fine-tuning parameters, while maintaining or even improving the performance of PLMs. Specifically, one line of research focuses on adding small neural modules to PLMs and fine-tune only these modules for each task – the base model is kept frozen and shared across tasks. In this way, only a small number of task-specific parameters are introduced and updated, greatly enhancing the practicality of large models. For example, adapter tuning (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2020; ∗Work was done during Qingru Zhang’s internship at Microsoft Azure AI. 1 arXiv:2303.10512v2  [cs.CL]  20 Dec 2023Published as a conference paper at ICLR 2023 Wq Wk Wv Wo Wf1 Wf2 88.50 88.75 89.00 89.25 89.50 89.75 90.00 MNLI Matched Acc 88.58 88.98 89.36 89.28 89.91 89.99 (a) Selected weight matrix 1,2,3 4,5,6 7,8,9 10,11,12 78 80 82 84 86 88MNLI Matched Acc 77.87 85.82 88.15 88.6 (b) Selected layers Figure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left) or selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m. Figure 1a: we only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value projection (Wq, Wk, Wv), output projection (Wo) in the self-attention, and two weight matrices (Wf1 , Wf2 ) in two-layer FFNs. In Figure 1b, we apply LoRA to every weight matrix of the selected layers. He et al., 2022) inserts small neural modules called adapters between the layers of the base model. Prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable prefix tokens to the input or hidden layers of the base model. These methods have shown to achieve comparable performance to full fine-tuning, while only updating less than 1% of the original model parameters, significantly releasing the memory consumption. Another line of research proposes to model the incremental update of the pre-trained weights in a parameter-efficient way, without modifying the model architecture (Zaken et al., 2021; Guo et al., 2020; Hu et al., 2022). Given a pre-trained weight matrix1 W(0), for example, diff pruning (Guo et al., 2020) models its incremental update ∆ as a sparse matrix. Diff pruning initializes ∆ as the same dimension as W(0) and then prunes ∆ element-wise based on the magnitude of the entries. As such, diff pruning can increase the parameter efficiency substantially by adaptively retaining important updates and pruning unimportant ones. Nonetheless, diff pruning has several limitations. First, it relies on low-level implementation to speed up the computation of unstructured sparse matrices, which is not well supported by existing deep learning frameworks. Therefore, we have to store ∆ as a dense matrix during training. Second, it needs to update every entry of ∆ with their gradients and then prune them. This results in similar computational cost as full fine-tuning (Guo et al., 2020). To overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes ∆ as a low-rank matrix by the product of two much smaller matrices: W = W(0) + ∆ = W(0) + BA, (1) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. During fine-tuning, only A and B are updated. The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8 when d1 = d2 = 1024). With less than 0.5% additional trainable parameters, the training overhead can be reduced up to 70%, compared to full fine-tuning. However, LoRA achieves comparable or even better performance than full fine-tuning (Hu et al., 2022). Meanwhile, the product of two samll matrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning. LoRA still has limitations as it prespecifies the rank r of each incremental matrix ∆ identical. This ignores the fact that the importance of weight matrices varies significantly across modules and layers when fine-tuning pre-trained models. To illustrate this point, we present an concrete example in Figure 1. We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers. Adding more trainable parameters to the critical weight matrices can lead to better model performance. In contrast, adding more parameters to those less important weight matrices yields very marginal gains or even hurt model performance. Given the parameter budget, i.e., the number of total trainable parameters, we always prefer to allocate more parameters to those important modules. Distributing the budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often gives suboptimal performance. To this end, a natural question is: How can we allocate the parameter budget adaptively according to importance of modules to improve the performance of parameter-efficient fine-tuning? 1Unless specified otherwise, we use W(0) to denote any pre-trained weight matrix. 2Published as a conference paper at ICLR 2023 To answer this question, we propose a new method –AdaLoRA (Adaptive Low-Rank Adaptation), which dynamically allocates the parameter budget among weight matrices during LoRA-alike fine- tuning. Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget. Critical incremental matrices are assigned with high rank such that they can capture more fine-grained and task-specific information. Less importance ones are pruned to have lower rank to prevent overfitting and save the computational budget. There are some methods to control the rank of matrices in the existing literature of matrix approximation (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). Most of them directly compute singular value decomposition (SVD) of a matrix and then truncate the smallest singular values. Such an operation can manipulate the rank explicitly and, more importantly, minimize the difference between the resulting matrix and the original matrix. However, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD for a large number of high-dimensional weight matrices. Therefore, instead of computing SVD exactly, we parameterize ∆ as ∆ = PΛQ to mimic SVD. The diagonal matrix Λ contains singular values while the orthogonal matrices P and Q represent left/right singular vectors of ∆. To regularize the orthogonality of P and Q, an additional penalty is added to training loss. Such a parameterization avoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the unimportant singular values while the singular vectors are maintained. This preserves the possibility of future recovery and stabilizes the training. See a detailed comparison to LoRA in Section 3. Based on our SVD parameterization, AdaLoRA dynamically adjusts the rank of ∆ = PΛQ by importance scoring. Specifically, we divide the incremental matrix PΛQ into triplets, where each triplet Gi contains the i-th singular value and the corresponding singular vectors. To quantify the importance of triplets, we propose a novel importance metric, which takes account of the contribution of every entry in Gi to the model performance (Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). Triplets with low importance scores are granted low priority and hence the singular values are zeroed out. Triplets with high importance are retained for fine-tuning. Moreover, we also propose a global budget scheduler to facilitate the training. In particular, we start from an initial parameter budget, which is slightly higher than the final budget, and then gradually reduce it until matching the target. Such a scheduler can improve the training stability and model performance. Please see Section 3 for a detailed description of our importance metric and budget scheduler. We conduct extensive experiments on a wide range of tasks and models to demonstrate the effec- tiveness of AdaLoRA. Specifically, we evaluate the performance using DeBERTaV3-base (He et al., 2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets. We also apply our methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks. We show AdaLoRA consistently outperforms the baseline, especially under low budget settings. For example, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1 improvement on the SQuAD2.0 dataset compared with state-of-the-art approaches. 2 B ACKGROUND Transformer-based Models. A typical transformer model consists of L stacked blocks, where each block contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the input sequence X ∈ Rn×d, MHA performs the attention function in parallel h heads: MHA (X) = Concat(head1, ...,headh)Wo, headi = Softmax \u0010 XWqi (XWki )⊤/ p dh \u0011 XWvi , where Wo ∈ Rd×d is an output projection and Wqi , Wki , Wvi ∈ Rd×dh are query, key and value projections of head i. dh is typically set to d/h. The other important module is a FFN which consists of two linear transformations with a ReLU activation in between: FFN(X) = ReLU(XWf1 + b1)Wf2 + b2, where Wf1 ∈ Rd×dm and Wf2 ∈ Rdm×d. Finally, a residual connection is used followed by a layer normalization (Ba et al., 2016). Low Rank Adaptation.LoRA (Hu et al., 2022) models the incremental update of the pre-trained weights by the product of two small matrices. For h = W(0)x, the modified forward pass is: h = W(0)x + ∆x = W(0)x + BAx, (2) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. A typically adopts a random Gaussion initialization while B is initialized with zero to have ∆ = 0 at the beginning of 3Published as a conference paper at ICLR 2023 training. We further denoteAi∗ as the i-th row ofA, B∗i as the i-th column ofB, and Gi = {Ai∗, B∗i} as the i-th doublet. Hu et al. (2022) only apply LoRA to query and value projections (i.e, Wq and Wv) in the MHAs. He et al. (2022) extend it to weight matrices of FFNs (i.e, Wf1 and Wf2 ), leading to the performance improvement . Meanwhile, they propose a unified view of various efficient tuning methods including adapter tuning, prefix tuning and LoRA. 3 A DALORA M ETHOD Our method contains two important components: (i) SVD-based adaptation, which formulates the incremental matrices in the form of singular value decomposition; (ii) Importance-aware rank allocation, which prunes redundant singular values based on our newly-designed importance metric. 3.1 SVD-B ASED ADAPTATION As mentioned in Section 1, we propose to parameterize the incremental updates of the pre-trained weight matrices in the form of singular value decomposition: W = W(0) + ∆ = W(0) + PΛQ, (3) where P ∈ Rd1×r and Q ∈ Rr×d2 represent the left/right singular vectors of ∆ and the diagonal matrix Λ ∈ Rr×r contains the singular values {λi}1≤i≤r with r ≪ min(d1, d2). We further denote Gi = {P∗i, λi, Qi∗} as the triplet containing the i-th singular value and vectors. In practice, since Λ is diagonal, we only need to save it as a vector in Rr. Λ is initialized with zero while P and Q adopt a random Gaussian initialization to ensure ∆ = 0 at the beginning of training. To enforce the orthogonality of P and Q, i.e., P⊤P = QQ⊤ = I, we utilize the following regularizer2: R(P, Q) = ∥P⊤P − I∥2 F + ∥QQ⊤ − I∥2 F. (4) In our method, Λ is iteratively pruned to adjust the rank after each gradient decent step. As mentioned in Section 1, one can directly compute SVD for every ∆ to manipulate singular values. The computational complexity, however, is O(min(d1, d2)d1d2). It becomes extremely expensive to iteratively apply SVD for a large number of high-dimensional incremental matrices. In contrast, our parameterization avoids intensive SVD computation, greatly releasing the computational overhead. We remark that one can also apply structured pruning to LoRA to control the rank (i.e., pruneBA doublet-wise in (1)), whereas it has the following disadvantages. First, when a doublet is measured as unimportant, we have to prune all of its elements. It makes scarcely possible to reactivate the pruned doublets as their entries are all zeroed out and not trained. In contrast, AdaLoRA only masks out the singular values based on (3) while the singular vectors are always maintained. It preserves the potential of future recovery for the triplets dropped by mistake. Second, A and B of LoRA are not orthogonal, meaning the doublets can be dependent with each other. Discarding the doublets can incur larger variation from the original matrix than truncating the smallest singular values. Therefore, the incremental matrices are often altered dramatically after each step of rank allocation, which causes training instability and even hurts generalization. To demonstrate this point, we present an ablation study in Section 4.4, which compares AdaLoRA with structured pruning for LoRA. 3.2 I MPORTANCE -AWARE RANK ALLOCATION We apply the SVD-based adaptation (3) to every weight matrix including Wq, Wk, Wv, Wf1 and Wf2 of each transformer layer. In order to control the budget, we iteratively prune singular values in correspondence to their importance score during the training. For clear reference, we use k to index the incremental matrix, i.e., ∆k = PkΛkQk for k = 1 , . . . , n, where n is the number of adapted weight matrices. We denote the i-th triplet of ∆k as Gk,i = {Pk,∗i, λk,i, Qk,i∗} and its importance score as Sk,i. We further denote the parameter sets P = {Pk}n k=1, E = {Λk}n k=1, Q = {Qk}n k=1 and training cost as C(P, E, Q). With the regularization (4), the training objective is given by L(P, E, Q) = C(P, E, Q) + γ Pn k=1 R(Pk, Qk), where γ > 0 is the regularization coefficient. At the t-th step, we first take a stochastic gradient step to update P(t) k , Λ(t) k and Q(t) k for k = 1, . . . , n. Specifically, for Λ(t) k ˜Λ(t) k = Λ(t) k − η∇Λk L(P(t), E(t), Q(t)), (5) 2We present the experiments in Appendix G to verify the effectiveness of the regularization. 4Published as a conference paper at ICLR 2023 where η > 0 is learning rate. Then, given importance score S(t) k , the singular values are pruned following Λ(t+1) k = T (˜Λ(t) k , S(t) k ), with T (˜Λ(t) k , S(t) k )ii = \u001a ˜Λ(t) k,ii S(t) k,i is in the top-b(t) of S(t), 0 otherwise, (6) where S(t) = {S(t) k,i}1≤k≤n,1≤i≤r contains the importance score of all triplets. Here b(t) is the budget of remaining singular values at the t-th step, which we explain more in Section 3.3. In this way, we leave more budget to the incremental matrices of higher priority by pruning the singular values of less important ones. In the sequel, we introduce several options to design the importance score. Magnitude of singular valuesis the most straightforward way to quantify the importance of every triplet, i.e., Sk,i = |λk,i|. In this way, only the least significant singular values are discarded. It minimizes the deviation from the original matrix and further stabilizes the training. Many existing methods use this criterion to control the rank of matrix (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). However, we remark that such a simple metric cannot properly quantify the contribution of parameters to model performance. Sensitivity-based importanceis another option for importance scoring, which quantifies the sensi- tivity of parameters to the training loss (Molchanov et al., 2019; Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). The prior work, however, leverages the sensitivity to quantify the importance of single entries and applies it for unstructured pruning that prunes weights element-wise. When it turns to our case, we have to design a new metric as the triplets are discarded group-wise. Every entry’s sensitivity ought to be considered and properly combined to quantify the overall contribution of the triplet to model performance. Therefore, we propose a newly-designed importance metric in account of both the singular value and vectors in triplet Gk,i: Sk,i = s(λk,i) + 1 d1 d1X j=1 s(Pk,ji) + 1 d2 d2X j=1 s(Qk,ij), (7) where we calculate the mean importance of Pk,∗i and Qk,i∗ such that Sk,i does not scale with the number of parameters in Gk,i. Here s(·) is a specific importance function for single entries. We can adopt the sensitivity for s(·), which is defined as the magnitude of the gradient-weight product: I(wij) = |wij∇wij L|, (8) where wij is any trainable parameter. (8) essentially approximates the change in loss when a parameter is zeroed out. If the removal of a parameter has a large influence, then the model is sensitive to it and we should retain it (Molchanov et al., 2019; Liang et al., 2021; Zhang et al., 2022). However, Zhang et al. (2022) point out that the sensitivity in (8) is not yet a reliable importance indi- cator. Such a score is estimated on the sampled mini batch. The stochastic sampling and complicated training dynamics incur high variability and large uncertainty for estimating the sensitivity with (8). Therefore, Zhang et al. (2022) propose to resolve this issue by sensitivity smoothing and uncertainty quantification: I (t) (wij) =β1I (t−1) (wij) + (1− β1)I(t)(wij) (9) U (t) (wij) =β2U (t−1) (wij) + (1− β2) \f\f\fI(t)(wij) − I (t) (wij) \f\f\f, (10) where 0 < β1, β2 < 1. I (t) is the smoothed sensitivity by exponential moving average and U (t) is the uncertainty term quantified by the local variation between I(t) and I (t) . Then they define the importance as the product between I (t) and U (t) , which can be another option for s(·): s(t)(wij) = I (t) (wij) · U (t) (wij). (11) We present a detailed ablation study in Section 4.4 to compare the performance of different importance metrics. We find the proposed metric (7) based on the sensitivity variant (11) generally performs best. We summarize the detailed algorithm in Algorithm 1. 5Published as a conference paper at ICLR 2023 Algorithm 1AdaLoRA 1: Input: Dataset D; total iterations T; budget schedule {b(t)}T t=0; hyperparameters η, γ, β1, β2. 2: for t = 1, . . . , Tdo 3: Sample a mini-batch from D and compute the gradient ∇L(P, E, Q); 4: Compute the sensitivity I(t) in (8) for every parameter in {P, E, Q}; 5: Update I (t) as (9) and U (t) as (10) for every parameter in {P, E, Q}; 6: Compute S(t) k,i by (7), for k = 1, . . . , nand i = 1, . . . , r; 7: Update P(t+1) k = P(t) k − η∇Pk L(P, E, Q) and Q(t+1) k = Q(t) k − η∇Qk L(P, E, Q); 8: Update Λ(t+1) k = T (Λ(t) k − η∇Λk L(P, E, Q), S(t) k ) given the budget b(t). 9: end for 10: Output: The fine-tuned parameters {P(T), E(T), Q(T)}. 3.3 G LOBAL BUDGET SCHEDULER As mentioned in Section 1, adjusting the rank is naturally to control the parameter budget in the context of low-rank adaptation. Hence we define the budget b(t) as the total rank of all incremental matrices, i.e., the number of total singular values. Recall that the budget allocation is iteratively conducted during the fine-tuning. To facilitate the training, we propose a global budget scheduler. Specifically, we start from an initial budgetb(0) that is slightly higher than the target budgetb(T) (e.g., 1.5 times of b(T)). We set the initial rank of each incremental matrix as r = b(0)/n. We warm up the training for ti steps, and then follow a cubic schedule to decrease the budget b(t) until it reaches b(T). Finally, we fix the resulting budget distribution and fine-tune the model for tf steps. The exact equation for the budget schedule is presented in Appendix A. This allows AdaLoRA to explore the parameter space first and then focus on the most important weights later. 4 E XPERIMENTS We implement AdaLoRA for fine-tuning DeBERTaV3-base (He et al., 2021a) and BART-large (Lewis et al., 2019). We evaluate the effectiveness of the proposed algorithm on natural language understanding (GLUE, Wang et al. (2019)), question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)), and natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail Hermann et al. (2015)). All the gains have passed significant tests with p <0.05. Implementation Details. We use PyTorch(Paszke et al., 2019) to implement all the algorithms. Our implementation is based on the publicly available Huggingface Transformers3 (Wolf et al., 2019) code-base. All the experiments are conducted on NVIDIA V100 GPUs. LoRA scales ∆x by α/r where α is a constant in r. As a result, the magnitude of output can be consistent given different r. It reduces the efforts of retuning learning rate when varying r. Typically α is set as 16 or 32 and never tuned (Hu et al., 2022; Yang & Hu, 2020). Following LoRA, we add the same scaling for (3) and fix α as LoRA. Besides, in Algorithm 1, we prune singular values every ∆T steps (e.g., ∆T = 100) such that the pruned triplets can still get updated within these intervals and possibly reactivated in future iterations. Baselines. We compare AdaLoRA with the following methods: • Full fine-tuning is the most common approach for adaptation. During fine-tuning, the model is initialized with pre-trained weights and biases, and all model parameters undergo gradient updates. • Bitfit (Zaken et al., 2021) is an effective parameter-efficient fine-tuning method. The method only fine-tunes bias vectors in the pre-trained model. • Adapter tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) inserts two-layer adapters between transformer blocks. We compare with two types of adapter. Houlsby adapter as proposed in Houlsby et al. (2019) is inserted between the self-attention module and the FFN module followed by a subsequent residual connection. Recently, Pfeiffer et al. (2020) propose a more efficient design with adapters only applied after FFN modules and LayerNorm modules (Ba et al., 2016), which we call 3https://github.com/huggingface/transformers 6Published as a conference paper at ICLR 2023 Table 1: Results with DeBERTaV3-base on GLUE development set. The best results on each dataset are shown in bold. We report the average correlation for STS-B.Full FT, HAdapter and PAdapter represent full fine-tuning, Houlsby adapter, and Pfeiffer adapter respectively. We report mean of5 runs using different random seeds. Method # Params MNLI SST-2 CoLA QQP QNLI RTE MRPC STS-B All m/mm Acc Mcc Acc/F1 Acc Acc Acc Corr Ave. Full FT 184M 89.90/90.12 95.63 69.19 92.40/89.80 94.03 83.75 89.46 91.60 88.09 BitFit 0.1M 89.37/89.91 94.84 66.96 88.41/84.95 92.24 78.70 87.75 91.35 86.02 HAdapter 1.22M 90.13/90.17 95.53 68.64 91.91/89.27 94.11 84.48 89.95 91.48 88.12 PAdapter 1.18M 90.33/90.39 95.61 68.77 92.04/89.40 94.29 85.20 89.46 91.54 88.24 LoRAr=8 1.33M 90.65/90.69 94.95 69.82 91.99/89.38 93.87 85.20 89.95 91.60 88.34 AdaLoRA 1.27M 90.76/90.79 96.10 71.45 92.23/89.74 94.55 88.09 90.69 91.84 89.31 HAdapter 0.61M 90.12/90.23 95.30 67.87 91.65/88.95 93.76 85.56 89.22 91.30 87.93 PAdapter 0.60M 90.15/90.28 95.53 69.48 91.62/88.86 93.98 84.12 89.22 91.52 88.04 HAdapter 0.31M 90.10/90.02 95.41 67.65 91.54/88.81 93.52 83.39 89.25 91.31 87.60 PAdapter 0.30M 89.89/90.06 94.72 69.06 91.40/88.62 93.87 84.48 89.71 91.38 87.90 LoRAr=2 0.33M 90.30/90.38 94.95 68.71 91.61/88.91 94.03 85.56 89.71 91.68 88.15 AdaLoRA 0.32M 90.66/90.70 95.80 70.04 91.78/89.16 94.49 87.36 90.44 91.63 88.86 Pfeiffer adapter. The number of trainable parameters is determined by the number of layers, the hidden dimension of adapters and the dimension of their inputs. • LoRA (Hu et al., 2022) is a state-of-the-art method for parameter-efficient fine-tuning. The method parameterizes incremental updates by two small matrices and only fine-tune them. The number of trainable parameter is controlled by the rank r and the number of adapted weight matrices n. Hu et al. (2022) apply LoRA to query and value projections only. In empirical, we find that applying LoRA to all weight matrices, i.e., Wq, Wk, Wv, Wf1 and Wf2 , can further improve its performance (Please see Appendix F). Hence, we compare with this generalized LoRA to maximize its performance. We use publicly available implementation 4 to run all the baselines. Please refer to Hu et al. (2022) and reference therein for details. 4.1 N ATURAL LANGUAGE UNDERSTANDING Models and Datasets.We evaluate the fine-tuning performance of DeBERTaV3-base (He et al., 2021a) using the proposed algorithm. We conduct experiments on the General Language Understand- ing Evaluation (GLUE, Wang et al. 2019) benchmark. The benchmark includes two single-sentence classification tasks, three similarity and paraphrase tasks and four natural language inference tasks. Dataset details are summarized in Appendix B. Implementation Details. DeBERTaV3-base consists of 183 millions parameters. We compare AdaLoRA with the baselines under different budget levels, for example, given the total trainable parameters as 0.3/0.6/1.2 million. In order to match the parameter budget, we select the hidden dimensions of adapters from {8, 16, 32, 64}, set the rank r of LoRA as {2, 4, 8}, and choose the final budget b(T) of AdaLoRA from {144, 288, 576}. Then we set b(0) as 1.5 times of b(T) for AdaLoRA and select the regularization coefficient γ from {0.1, 0.3, 0.5}. We set the exponential moving average parameters β1 and β2 as their default value 0.85. We select the learning rate from {5 × 10−5, 8 × 10−5, 1 × 10−4, 2 × 10−4}. More details are presented in Appendix C. Main results.We compare AdaLoRA with the baseline methods under different budget settings. Table 1 shows experimental results on the GLUE development set. We see that AdaLoRA achieves better or on par performance compared with existing approaches on all datasets under all budget levels. For example, when the parameter budget is 0.3M, AdaLoRA achieves 87.36% accuracy on RTE, which is 1.8% higher than the best-performing baseline. Besides, AdaLoRA with extreme low budget can often perform better than the baselines with higher budget. For example, AdaLoRA achieve 70.04% Mcc. score on CoLA with 0.3M fine-tuning parameters, which is higher than all baseline methods with lager budget (e.g., 0.6M and 1.2M). 4.2 Q UESTION ANSWERING Models and Datasets.We evaluate performance of the proposed algorithm on two question answering (QA) datasets: SQuAD v1.1 (Rajpurkar et al., 2016) and SQuADv2.0 (Rajpurkar et al., 2018), where 4https://github.com/microsoft/LoRA 7Published as a conference paper at ICLR 2023 Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. Here # Params is the number of trainable parameters relative to that in full fine-tuning. We report EM/F1. The best results in each setting are shown in bold. SQuADv1.1 SQuADv2.0 Full FT 86.0 / 92.7 85.4 / 88.4 # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% HAdapter 84.4/91.5 85.3/92.1 86.1/92.7 86.7/92.9 83.4/86.6 84.3/87.3 84.9/87.9 85.4/88.3 PAdapter 84.4/91.7 85.9/92.5 86.2/92.8 86.6/93.0 84.2/87.2 84.5/87.6 84.9/87.8 84.5/87.5 LoRA 86.4/92.8 86.6/92.9 86.7/93.1 86.7/93.1 84.7/87.5 83.6/86.7 84.5/87.4 85.0/88.0 AdaLoRA 87.2/93.4 87.5/93.6 87.5/93.7 87.6/93.7 85.6/88.7 85.7/88.8 85.5/88.6 86.0/88.9 we use AdaLoRA to fine-tune DeBERTaV3-base. These tasks are treated as a sequence labeling problem, where we predict the probability of each token being the start and end of the answer span. Dataset details can be found in Appendix D. Implementation Details.We compare AdaLoRA with the baseline methods under different parameter budgets. That is we have the number of trainable parameters as 0.08%/0.16%/0.32%/0.65% of total pre-trained parameters. To match the budget requirements, we select the hidden dimensions of adapters from {4, 8, 16, 32, 64}, set the rank r of LoRA as {1, 2, 4, 8} and choose the final total rank b(T) of AdaLoRA from {72, 144, 288, 576}. We set the batch size as 16. We use AdamW (Loshchilov & Hutter, 2019) as the optimizer and we set the learning rate as 1 × 10−3 for AdaLoRA. Please refer to Appendix D for more details. Main Results. Table 2 summarizes experimental results when we fine-tune DeBERTaV3-base under 4 different budget settings: 0.08%, 0.16%, 0.32% and 0.65% of total pre-trained parameters. From the result, we see that AdaLoRA consistently outperforms existing approaches under all the budget levels in term of two evaluation metrics: exact match (EM) and F1. Notice that the performance of Houlsby adapter and Pfeiffer adapter are notably decreased when we reduce the parameter budget. In contrast, our method shows the consistent performance under different budget levels. For example, AdaLoRA achieves 88.7% F1 on SQuADv2.0 with the smallest budget 0.08%. It is close to its performance under the high budget and it is also 1.2% higher than the best-performing baseline. 4.3 N ATURAL LANGUAGE GENERATION Table 3: Results with BART-large on XSum and CNN/DailyMail. Here# Params is the number of trainable parameters relative to that in full fine-tuning. We report R-1/2/L. The best results are shown in bold. # Params Method XSum CNN/DailyMail 100% Full FT 45.49 / 22.33 / 37.26 44.16 / 21.28 / 40.90 2.20% LoRA 43.95 / 20.72 / 35.68 45.03 / 21.84 / 42.15 AdaLoRA 44.72 / 21.46 / 36.4645.00 / 21.89 / 42.16 1.10% LoRA 43.40 / 20.20 / 35.20 44.72 / 21.58 / 41.84 AdaLoRA 44.35 / 21.13 / 36.1344.96 / 21.77 / 42.09 0.26% LoRA 43.18 / 19.89 / 34.92 43.95 / 20.91 / 40.98 AdaLoRA 43.55 / 20.17 / 35.2044.39 / 21.28 / 41.50 0.13% LoRA 42.81 / 19.68 / 34.73 43.68 / 20.63 / 40.71 AdaLoRA 43.29 / 19.95 / 35.0443.94 / 20.83 / 40.96 Models and Datasets.To provide a comparison with the state-of-the-art in natural language gener- ation (NLG) tasks, we apply AdaLoRA to fine-tune a BART-large model (Lewis et al., 2019). We evaluate model performance on two datasets: XSum (Narayan et al., 2018) and CNN/DailyMail (Hermann et al., 2015). Implementation Details.Similarly as DeBERTav3-base, we apply low-rank/SVD-based adaptation to every weight matrix of both encoder and decoder layers. We report ROUGE 1/2/L scores (R-1/2/L, Lin (2004)). We set the training epochs as 15. For XSum, we set the beam length as 8 and batch size 8Published as a conference paper at ICLR 2023 as 64. For CNN/DailyMail, we set the beam length as 4 and batch size as 32. Please see Appendix E for the detailed configuration. Main Results.Experimental results are summarized in Table 3, where we compare the fine-tuning performance under four budget levels: the number of trainable parameters is 0.13%, 0.26%, 1.10% and 2.20% of total pre-trained parameters. We see that AdaLoRA achieves better or on par performance compared with the baseline on both datasets (XSum and CNN/DailyMail) under all the budget levels. For example, AdaLoRA achieves 21.13 R-2 score when budget level is 1.10%, compared with 19.89 for LoRA. 4.4 A NALYSIS Different budget levels.Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels. We see that on all the three datasets (MNLI-m, SQuADv2.0 and XSum), AdaLoRA achieves consistent performance improvement under all the budget levels compared with the baseline. The performance gain is more significant when increasing the budget for the XSum task, suggesting a high budget can help NLG tasks. Note that on the MNLI and SQuADv2.0 datasets, the performance of AdaLoRA under low budget levels (≤ 1%) can match the results of high budget settings. For example, AdaLoRA achieves 88.78% F1 on SQuADv2.0 when the budget is 0.16%. It is close to the performance (88.89% F1) of the highest budget (4.65%) with a more significant gain over the baseline. 0.08 0.16 0.32 0.65 0.96 1.30 1.95 2.88 # Params (%) 90.2 90.3 90.4 90.5 90.6 90.7Acc (MNLI-m) LoRA  AdaLoRA (a) MNLI 0.08 0.16 0.32 0.65 1.30 2.70 4.65 # Params (%) 87.0 87.5 88.0 88.5 89.0 F1  (b) SQuADv2.0 0.13 0.26 1.1 2.2 4.5 7.9 12.5 # Params (%) 20.0 20.5 21.0 21.5 ROUGE-2  (c) XSum Figure 2: Fine-tuning performance under different budget levels. We compare AdaLoRA with the generalized LoRA that applies to every weight matrix. Comparison to low-rank parameterization.As mentioned in Section 3.1, one can alternatively prune LoRA doublet-wise to conduct the rank allocation. In this case, the doublets are zeroed out entirely, raising the barrier to reactivate them. It can cause training instability and hurt the generalization when some crucial doublets are pruned by mistake. In Table 4, we compare AdaLoRA with pruning LoRA on three datasets (SST-2, RTE, and CoLA) to illustrate this point. We apply the same importance score, budget scheduler and training setups as Section 4.1 for pruning LoRA. We can see that AdaLoRA outperforms pruning LoRA on all the datasets under all the budget levels. Table 4: We present two ablation studies in this table: (i) Comparison between AdaLoRA and structured pruning on LoRA. (ii) Comparison of different importance metrics for AdaLoRA. SST-2 RTE CoLA # Params 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% Prune LoRA 94.84 94.50 94.95 86.28 86.15 87.00 66.71 69.29 69.57 AdaLoRA 95.52 95.80 96.10 87.36 87.73 88.09 70.21 70.04 71.45 s(·) = I(·) 94.61 95.30 95.64 87.36 87.71 88.10 66.71 68.83 70.19 Si = |λi| 95.41 95.41 95.87 87.00 86.28 88.00 67.67 68.44 70.38 Variants of the importance score.Recall that in AdaLoRA, the importance score is defined by the sensitivity and uncertainty of every entry in the triplet (7). In Table 4, we examine two variants of the importance score: (i) changing s(·) in (7) to sensitivity-only; (ii) directly defining Si as |λi|. From the results, we can see that the proposed importance score generally performs best. The other two variants can degenerate the model performance up to 0.9%. The role of two components.We remark that both two components of our method - SVD adaptation and adaptive budget allocation, play vital roles for the performance gain. To demonstrate it, we 9Published as a conference paper at ICLR 2023 compare AdaLoRA with the following variants: (i) SVD-LoRA: fine-tuning only with the proposed SVD-based adaptation in (3) and (4); (ii) LoRA regu: LoRA with orthogonal regularization (4) on A and B; (iii) AdaLoRAγ = 0: AdaLoRA without orthogonal regularization (4). Table 5 present the results when fine-tuning DeBERTaVe-base on SST-2 and MNLI. We can see that fine-tuning only with SVD adaptation shows an improvement over LoRA but cannot match the performance of AdaLoRA. Meanwhile, without SVD orthogonal regularization, the performance of AdaLoRA can degenerate. These results validate that both components contribute to the model performance. Table 5: We present ablation studies about SVD-based adaptation, orthogonal regularization, and budget allocation in this table. For MNLI, we report the average score of m/mm acc. SST-2 MNLI # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% LoRA 94.38 94.95 - 94.95 90.19 90.34 - 90.57 LoRAregu - 94.61 94.72 94.61 - 90.30 90.40 90.66 SVD-LoRA 95.33 95.18 95.07 95.53 90.28 90.25 90.52 90.62 AdaLoRAγ = 0 95.41 95.10 95.30 95.10 90.37 90.34 90.56 90.43 AdaLoRA 95.64 95.80 96.10 96.10 90.65 90.68 90.66 90.77 The resulting budget distribution.Figure 3 shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in Figure 1 that weight matrices of FFN moduels and top layers are more important for model performance. Hence, it validates that our proposed importance metric can guide AdaLoRA to focus on crucial modules. Meanwhile, the rank distribution generated by AdaLoRA is consistent across different budget levels, tasks and models. It means the number of remaining parameters is linearly scaled with b(T) and hence we can tune b(T) to control the remaining parameters. 1 2 3 4 5 6 7 8 9 10 11 12 Layer Wf2 Wf1 Wo Wv Wk Wq 4 1 5 2 3 5 5 6 10 5 5 0 6 9 9 9 12 11 12 12 12 12 12 2 7 3 5 8 8 10 12 12 12 12 12 5 6 6 10 6 10 11 11 11 12 12 11 9 5 4 5 5 10 9 9 11 12 12 12 12 3 2 5 4 7 7 7 10 11 11 10 3 0 2 4 6 8 10 12 The ﬁnal rank Figure 3: The resulting rank of each incremental matrix when fine-tuning DeBERTaV3-base on MNLI with AdaLoRA. Here the x-axis is the layer index and the y-axis represents different types of adapted weight matrices. 5 C ONCLUSION We propose a parameter-efficient fine-tuning method – AdaLoRA that adaptively allocates the parameter budget according to importance scoring. In AdaLoRA, we parameterize the incremental updates of weight matrices in the form of singular value decomposition. Then, we dynamically allocate the parameter budget among incremental matrices by manipulating the singular values based on a new importance metric. Such an a pproach effectively improves the model performance and parameter efficiency. We conduct extensive experiments on natural language processing, question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches. 10Published as a conference paper at ICLR 2023 REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021a. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2021b. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302–2329, 2011. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https: //aclanthology.org/2021.emnlp-main.243. 11Published as a conference paper at ICLR 2023 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/v1/2021. acl-long.353. Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compression to improving generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6524–6538, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.510. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 11264–11272. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01152. Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum- mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e- Buc, Emily B. Fox, and Roman Garnett (eds.),Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter- fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872–1897, 2020. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. 12Published as a conference paper at ICLR 2023 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by fine-tuning. 2020. Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of optimization, 6(615-640):15, 2010. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. In International Conference on Machine Learning, pp. 26809–26823. PMLR, 2022. 13Published as a conference paper at ICLR 2023 A G LOBAL BUDGET SCHEDULE As mentioned in Section 3.3, we propose a global budget scheduler to gradually decrease the budget b(t) following a cubic schedule. The detailed equation is given as follows: b(t) =    b(0) 0 ≤ t < ti b(T) + \u0000 b(0) − b(T)\u0001\u0010 1 − t−ti−tf T−ti−tf \u00113 ti ≤ t < T− tf b(T) o.w. . (12) B GLUE D ATASET STATISTICS We present the dataset statistics of GLUE (Wang et al., 2019) in the following table. Table 6: Summary of the GLUE benchmark. Corpus Task #Train #Dev #Test #Label Metrics Single-Sentence Classification (GLUE) CoLA Acceptability 8.5k 1k 1k 2 Matthews corr SST Sentiment 67k 872 1.8k 2 Accuracy Pairwise Text Classification (GLUE) MNLI NLI 393k 20k 20k 3 Accuracy RTE NLI 2.5k 276 3k 2 Accuracy QQP Paraphrase 364k 40k 391k 2 Accuracy/F1 MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1 QNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy Text Similarity (GLUE) STS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr C N ATURAL LANGUAGE UNDERSTANDING C.1 B UDGET CONFIGURATION For each budget level, we tune the final budget b(T) for AdaLoRA, the rank r for LoRA, the hidden dimension d for two adapters to match the budget requirements. Table 7: Detailed budget setup for GLUE benchmark. # Params Houlsby Adapter (d) Pfeiffer Adapter ( d) LoRA ( r) AdaLoRA ( b(T)) 1.2M 32 64 8 576 0.6M 16 32 4 288 0.3M 8 16 2 144 Alternatively, we can also set the final average rank ¯r(T) = b(T)/n for AdaLoRA to control the budget, which is set as 2, 4, and 8 given the final budget as 144, 288, and 576 respectively. Then we select the initial rank r from {4, 6, 12} for the final average rank {2, 4, 8} respectively. C.2 T RAINING DETAILS We tune the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best learning rate for every method. For each dataset, the batch size is set as identical for every method. 14Published as a conference paper at ICLR 2023 Table 8: Hyper-parameter setup of AdaLoRA for GLUE benchmark. Dataset learning rate batch size # epochs γ t i ∆T tf MNLI 5 × 10−4 32 7 0.1 8000 100 50000 RTE 1.2 × 10−3 32 50 0.3 600 1 1800 QNLI 1.2 × 10−3 32 5 0.1 2000 100 8000 MRPC 1 × 10−3 32 30 0.1 600 1 1800 QQP 5 × 10−4 32 5 0.1 8000 100 25000 SST-2 8 × 10−4 32 24 0.1 6000 100 22000 CoLA 5 × 10−4 32 25 0.5 800 10 3500 STS-B 2.2 × 10−3 32 25 0.1 800 10 2000 D Q UESTION ANSWERING D.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. Table 9: Detailed budget setup for question answering. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 D.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 10: Hyper-parameter setup of AdaLoRA for question answering tasks. Dataset learning rate batch size # epochs γ t i ∆T tf SQuADv1.1 1 × 10−3 16 10 0.1 5000 100 25000 SQuADv2.0 1 × 10−3 16 12 0.1 5000 100 50000 D.3 D ATASET The statistics of question answering datasets are summarized in Table 11. Table 11: Statistics of the SQuAD dataset. # Train # Validation SQuAD v1.1 87,599 10,570 SQuAD v2.0 130,319 11,873 E N ATURAL LANGUAGE GENERATION E.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. 15Published as a conference paper at ICLR 2023 Table 12: Detailed budget setup for summarization tasks. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 E.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 13: Hyper-parameter setup of AdaLoRA for summarization tasks. Dataset learning rate batch size # epochs γ t i ∆T tf XSum 5 × 10−4 64 25 0.1 6000 100 50000 CNN/DailyMail 5 × 10−4 32 15 0.1 5000 100 85000 F A BLATION STUDY FOR LORA As mentioned in Section 4, we find that the performance of LoRA can be further improved when applying it to every weight matrix, compared to fine-tuning Wq and Wv only (Hu et al., 2022). This observation aligns with the empirical results of He et al. (2022). In Table 14, we follow the same training configuration as Section 4.1 and present an ablation study to illustrate this point. Table 14: We compare the fine-tuning performance when apply LoRA to every weight matrix or Wq, Wv only. The parameter budget is fixed as 0.3M. We report accuracy for QQP and MRPC, accuracy(m) for MNLI, and average correlation for STS-B. MNLI QQP CoLA RTE QNLI SST-2 MRPC STS-B LoRA (Wq, Wk) 89.80 90.48 67.04 83.75 93.69 94.84 90.20 91.05 LoRA (all) 90.30 91.61 68.71 85.56 94.31 94.95 90.44 91.68 G O RTHOGONAL REGULARIZATION To verify the effectiveness of (4), we plot ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F to show whether P and Q are regularized to be orthogonal. We fine-tune a DeBERTaV3-base model on SST-2 with AdaLoRA and follow the same training configuration as Section 4.1. We set γ as 0.1 and plot the two terms along the training horizon. From Figure 4, we can see that two regularization terms can be optimized to a very small value (e.g., 0.001) at the beginning of training. Therefore, both P and Q can be enforced to be orthogonal quickly during the initial warm-up of AdaLoRA. It ensures that the triplets are not dependent with each other. H C OMPARISON OF TRAINING COST We compare the training cost between AdaLoRA and LoRA in the following table. We use two methods to fine-tune DeBERTaV3-base on a single NVIDIA V100 GPU. We do training only and set hyperparameters, e.g., batch size and training epochs, the same as in Section 4. Table 15 shows that AdaLoRA incurs 11% additional training time on MNLI and 16% on SQuADv2 under different budgets. The memory footprint of two methods are quite close. Such results demonstrate that AdaLoRA does not incur significant training overheads. The reason behind is that 16Published as a conference paper at ICLR 2023 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (a) P of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||QQ⊤ −I||2 F (b) Q of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (c) P of Wf2 at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 ||QQ⊤ −I||2 F (d) Q of Wf2 at the first layer Figure 4: We plot the ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F when fine-tuning DeBERTaV3-base on SST-2. Table 15: Comparison of practical training cost between AdaLoRA and LoRA. Dataset # Param Method GPU Mem Time/epoch MNLI 0.08% LoRA 11.094 GB 105 min AdaLoRA 11.104 GB 116 min 0.16% LoRA 11.098 GB 105 min AdaLoRA 11.110 GB 117 min 0.65% LoRA 11.128 GB 105 min AdaLoRA 11.188 GB 117 min SST-2 0.08% LoRA 13.138 GB 60 min AdaLoRA 13.148 GB 71 min 0.16% LoRA 13.142 GB 61 min AdaLoRA 13.164 GB 71 min 0.65% LoRA 13.170 GB 61 min AdaLoRA 13.226 GB 71 min we only evaluate the importance score for small incremental matrices PΛQ. Their total number of parameters is usually less than 1% of pre-trained weights. Therefore, it does not lead to significant computational cost to update the importance scores of these well-structured small matrices, compared to forward-backward pass of full model. 17",
      "meta_data": {
        "arxiv_id": "2303.10512v2",
        "authors": [
          "Qingru Zhang",
          "Minshuo Chen",
          "Alexander Bukharin",
          "Nikos Karampatziakis",
          "Pengcheng He",
          "Yu Cheng",
          "Weizhu Chen",
          "Tuo Zhao"
        ],
        "published_date": "2023-03-18T22:36:25Z",
        "pdf_url": "https://arxiv.org/pdf/2303.10512v2.pdf",
        "github_url": "https://github.com/microsoft/LoRA"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addresses the suboptimal performance of parameter-efficient fine-tuning methods for large pre-trained language models (PLMs) due to their even distribution of incremental update budgets across weight matrices, overlooking their varying importance. The key contribution is AdaLoRA, a novel method that adaptively allocates the parameter budget among weight matrices based on their importance score. AdaLoRA parameterizes incremental updates using singular value decomposition (SVD) to effectively prune singular values of unimportant updates, reducing parameter budget without intensive SVD computations. It significantly improves performance, especially in low-budget settings, across natural language processing, question answering, and natural language generation tasks.",
        "methodology": "AdaLoRA's methodology consists of two main components: (i) SVD-based adaptation and (ii) Importance-aware rank allocation. For SVD-based adaptation, incremental updates (Δ) are parameterized as Δ = PΛQ, where P and Q are orthogonal matrices representing left/right singular vectors and Λ is a diagonal matrix containing singular values. An orthogonality regularizer R(P, Q) = ||PᵀP - I||F² + ||QQᵀ - I||F² is added to the training loss. For importance-aware rank allocation, singular values are pruned iteratively based on an importance score for each triplet Gk,i = {Pk,∗i, λk,i, Qk,i∗}. The importance metric Sk,i combines smoothed sensitivity and uncertainty of each entry (λk,i, Pk,∗i, Qk,i∗), calculated as I⁽ᵗ⁾(wij) = β₁I⁽ᵗ⁻¹⁾(wij) + (1-β₁)I(t)(wij) and U⁽ᵗ⁾(wij) = β₂U⁽ᵗ⁻¹⁾(wij) + (1-β₂) |I⁽ᵗ⁾(wij) - I⁽ᵗ⁾(wij)|, with s⁽ᵗ⁾(wij) = I⁽ᵗ⁾(wij) · U⁽ᵗ⁾(wij). A global budget scheduler gradually reduces the total rank b(t) from an initial higher budget to the target budget following a cubic schedule, allowing for parameter space exploration before focusing on important weights.",
        "experimental_setup": "AdaLoRA was implemented using PyTorch and the Huggingface Transformers codebase, running on NVIDIA V100 GPUs. Experiments were conducted with DeBERTaV3-base on natural language understanding (GLUE benchmark: MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B) and question answering (SQuADv1.1, SQuADv2.0) tasks. BART-large was used for natural language generation (XSum, CNN/DailyMail). Baselines included Full fine-tuning, Bitfit, Adapter tuning (Houlsby and Pfeiffer adapters), and LoRA (generalized to all weight matrices: Wq, Wk, Wv, Wf1, Wf2). Evaluation metrics included accuracy, F1, EM, and ROUGE 1/2/L scores. Experiments compared methods under various parameter budget levels (e.g., 0.08% to 2.20% of total pre-trained parameters) by tuning adapter hidden dimensions, LoRA rank r, and AdaLoRA's final total rank b(T). The learning rate was selected from a range {5e-5 to 1e-3}, and exponential moving average parameters β1, β2 were set to 0.85.",
        "limitations": "The paper does not explicitly state significant limitations of AdaLoRA itself. However, it notes that AdaLoRA incurs an additional training time overhead, although it is considered not significant (e.g., 11-16% more training time compared to LoRA on MNLI and SQuADv2.0 datasets). It also highlights the limitations of prior methods like diff pruning (reliance on low-level implementation for unstructured sparse matrices, high computational cost) and LoRA (prespecifying identical rank for all incremental matrices, leading to suboptimal performance by ignoring varying importance, and potential training instability with doublet-wise pruning). AdaLoRA is designed to overcome these limitations.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import loralib as lora\n# ...\nclass Attention(nn.Module):\n    # ...\n    self.c_attn = lora.MergedLinear(\n        nx, n_state * 3,\n        r=config.lora_attn_dim,\n        lora_alpha=config.lora_attn_alpha,\n        lora_dropout=config.lora_dropout,\n        enable_lora=[True, False, True],\n        fan_in_fan_out=True,\n        merge_weights=False\n    )\n    # ...\nclass GPT2Config(object):\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n        lora_attn_dim=0,\n        lora_attn_alpha=128,\n        lora_dropout=0.0,\n        lora_r_dropout=0.0,\n        fix_dropout=0.0,\n    ):\n        # ...\n        self.lora_attn_dim = lora_attn_dim\n        self.lora_attn_alpha = lora_attn_alpha\n        self.lora_dropout = lora_dropout\n        self.lora_r_dropout = lora_r_dropout\n        # ...\n\n# In examples/NLG/src/gpt2_ft.py\nimport loralib as lora\n# ...\nparser.add_argument('--lora_dim', type=int, default=0, help='lora attn dimension')\nparser.add_argument('--lora_alpha', type=int, default=128, help='lora attn alpha')\nparser.add_argument('--lora_dropout', default=0.0, type=float, help='dropout probability for lora layers')\n# ...\nif args.lora_dim > 0:\n    lora.mark_only_lora_as_trainable(lm_net)\n# ...\n# Configuration for different model sizes, e.g., for gpt2.sm:\nconfig = GPT2Config(\n    n_embd=768, n_layer=12, n_head=12,\n    lora_attn_dim=args.lora_dim,\n    lora_attn_alpha=args.lora_alpha,\n    lora_dropout=args.lora_dropout,\n)",
        "experimental_info": "The repository implements the underlying LoRA mechanism, which serves as a foundation for AdaLoRA. However, the explicit adaptive components of AdaLoRA, such as SVD-based adaptation (Δ = PΛQ with orthogonality regularizer) and importance-aware rank allocation (singular value pruning based on importance scores and a cubic budget scheduler), are not present in the provided code.\n\nExperimental settings for LoRA include:\n- `lora_attn_dim`: The rank of the low-rank matrices for LoRA. Configurable via `--lora_dim` (default 0).\n- `lora_attn_alpha`: The scaling factor for LoRA. Configurable via `--lora_alpha` (default 128).\n- `lora_dropout`: Dropout probability applied to LoRA layers. Configurable via `--lora_dropout` (default 0.0).\n- `lora_r_dropout`: Recurrent dropout for LoRA layers (not exposed as a command-line argument).\n- `enable_lora=[True, False, True]`: Specifies that LoRA is enabled for the query and value projections within the attention mechanism, but not for the key projection.\n\nDuring training, if `lora_attn_dim > 0`, only the LoRA parameters are made trainable (`lora.mark_only_lora_as_trainable(model)`). The base model parameters (e.g., `n_embd`, `n_layer`, `n_head` for `gpt2.sm`, `gpt2.md`, `gpt2.lg`) are inherited from predefined configurations and augmented with the LoRA-specific parameters."
      }
    },
    {
      "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
      "abstract": "Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.",
      "full_text": "LoRA Training in the NTK Regime has No Spurious Local Minima Uijeong Jang 1 Jason D. Lee 2 Ernest K. Ryu 3 Abstract Low-rank adaptation (LoRA) has become the stan- dard approach for parameter-efficient fine-tuning of large language models (LLM), but our the- oretical understanding of LoRA has been lim- ited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with N data points, showing: (i) full fine- tuning (without LoRA) admits a low-rank solu- tion of rank r ≲ √ N; (ii) using LoRA with rank r ≳ √ N eliminates spurious local minima, allow- ing (stochastic) gradient descent to find the low- rank solutions; (iii) the low-rank solution found using LoRA generalizes well. 1. Introduction The modern methodology of using large language models involves (at least) two phases: self-supervised pre-training on a large corpus followed by supervised fine-tuning to the downstream task. As large language models have grown in scale, pre-training has become out of reach for research groups without access to enormous computational resources. However, supervised fine-tuning remains feasible for such groups. One key strategy facilitating this efficient fine- tuning is Parameter-Efficient Fine-Tuning (PEFT), which freezes most of the pre-trained model’s weights while selec- tively fine-tuning a smaller number of parameters within an adapter module. Among various PEFT methodologies, low- rank adaptation (LoRA) (Hu et al., 2021) has emerged as the standard approach. Given a pre-trained matrixW0 ∈ Rm×n, LoRA trains a low-rank update such that the forward pass evaluates W0x + ∆W x= W0x + BAx 1Department of Mathematical Sciences, Seoul National Uni- versity 2Department of Electrical and Computer Engineering, Princeton University 3Department of Mathematics, University of California, Los Angeles. Correspondence to: Ernest Ryu <eryu@math.ucla.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). where r ≪ min(m, n), A ∈ Rr×n is initialized to be a random Gaussian, and B ∈ Rm×r is initialized to be zero. However, despite the widespread adoption of LoRA, our theoretical understanding of its mechanisms remains limited. One notable prior work is (Zeng & Lee, 2024), which ana- lyzes the expressive power of LoRA, showing that for any given function, there exist weight configurations for LoRA that approximate it. However, their work does not address whether LoRA can efficiently learn such configurations. Additionally, Malladi et al. (2023) experimentally demon- strated that under certain conditions, LoRA fine-tuning is nearly equivalent to a kernel regression, where the A matrix provides random features and is essentially not trained. This regime neglects the possibility of the A matrix learning new features and, consequently, leads to a LoRA rank require- ment of r ≥ Θ(1/ε2), where ε is an approximation toler- ance, originating from the use of the Johnson–Lindenstrauss lemma (Johnson & Lindenstrauss, 1984). Crucially, LoRA’s fundamental nature as a quadratic parameterization has not been considered in the prior analysis of trainability and generalizability. Contribution. In this work, we theoretically analyze LoRA fine-tuning and present results on trainability and generalizability. We consider fine-tuning a deep (trans- former) neural network with K-dimensional outputs using N training (fine-tuning) data points. Assuming that train- ing remains under the NTK regime, which we soon define and justify in Section 2, we show the following. First, full fine-tuning (without LoRA) admits a rank-r solution such that r(r+1) 2 ≤ KN . Second, using LoRA with rank r such that r(r+1) 2 > KN eliminates spurious local minima, al- lowing (stochastic) gradient descent to find the low-rank solutions. Finally, the low-rank solution found using LoRA generalizes well. 1.1. Prior works Theory of neural networks. The question of expressive power addresses whether certain neural networks of interest can approximate a given target function. Starting with the classical universal approximation theorems (Cybenko, 1989; Hornik et al., 1990; Barron, 1993), much research has been conducted in this direction. (Delalleau & Bengio, 2011; 1 arXiv:2402.11867v3  [cs.LG]  28 May 2024LoRA Training in the NTK Regime has No Spurious Local Minima Bengio & Delalleau, 2011; Lu et al., 2017; Duan et al., 2023). These can be thought of as existence results. The question of trainability addresses whether one can com- pute configurations of neural networks that approximate tar- get functions. Ghadimi & Lan (2013); Ge et al. (2015); Du et al. (2017); Jin et al. (2017) studied general convergence results of gradient descent and stochastic gradient descent. Soltanolkotabi et al. (2018); Du & Lee (2018); Allen-Zhu et al. (2019a;b); Du et al. (2019); Zou et al. (2020) stud- ied the loss landscape of neural networks and showed that first-order methods converge to global minima under certain conditions. The question of generalization addresses whether neural net- works trained on finite data can perform well on new unseen data. Classical learning theory (Koltchinskii & Panchenko, 2000; Bartlett et al., 2002; Bousquet & Elisseeff, 2002; Hardt et al., 2016; Bartlett et al., 2017) uses concepts such as uniform stability or the Rademacher complexities to ob- tain generalization bounds. Generalization bounds in the context of modern deep learning often utilize different ap- proaches (Wu et al., 2017; Dinh et al., 2017; Zhang et al., 2021), we use the Rademacher complexity for obtaining our generalization results. Neural tangent kernels. The theory of neural tangent kernel (NTK) concerns the training dynamics of certain infinitely wide neural networks. Jacot et al. (2018) shows that the training of an infinitely wide neural network is equivalent to training a kernel machine. Various studies such as (Arora et al., 2019; Chen et al., 2020) expand the NTK theory to more practical settings. Among these works, Wei et al. (2022a) introduced the concept of empirical NTK (eNTK) and showed that kernel regression with pretrained initialization also performs well on real datasets, providing a background to utilize NTK theory in fine-tuning. Theory of transformers and LLMs. As the transformer architecture (Vaswani et al., 2017) became the state-of-the- art architecture for natural language processing and other modalities, theoretical investigations of transformers have been pursued. Results include that transformers are uni- versal approximators (Yun et al., 2019), that transformers can emulate a certain class of algorithmic instructions (Wei et al., 2022b; Giannou et al., 2023), and that weight matrices in transformers increase their rank during training (Boix- Adsera et al., 2023). Also, (Zhang et al., 2020; Liu et al., 2020) presents improved adaptive optimization methods for transformers. PEFT methods and LoRA. Low-rank adaptation (LoRA) (Hu et al., 2021) has become the standard Parameter- Efficient Fine-Tuning (PEFT) method, and many variants of LoRA have been presented (Fu et al., 2023; Dettmers et al., 2023; Lialin et al., 2023). LoRA has proven to be quite versatile and has been used for convolution layers (Yeh et al., 2024) and for diffusion models (Ryu, 2023; Smith et al., 2023; Choi et al., 2023). Theoretically, Aghajanyan et al. (2021) found an intrinsic low-rank structure is critical for fine-tuning language mod- els, although this finding concerns full fine-tuning, not the setting that uses LoRA. Recently, Zeng & Lee (2024) an- alyzed the expressive power of LoRA. However, we still lack a sufficient theoretical understanding of why LoRA is effective in the sense of optimization and generalization. Matrix factorization. In this work, we utilize techniques developed in prior work on matrix factorization problems. Bach et al. (2008); Haeffele et al. (2014) established the sufficiency of low-rank parameterizations in matrix factor- ization problems, and their techniques have also been used in matrix completion (Ge et al., 2016), matrix sensing (Jin et al., 2023), and semidefinite programming (Bhojanapalli et al., 2018). 1.2. Organization Section 2 introduces the problem setting and reviews rel- evant prior notions and results. Section 3 proves the exis- tence of low-rank solutions. Section 4 proves LoRA has no spurious local minima and, therefore, establishes that (stochastic) gradient descent can find the low-rank global minima. Section 5 shows that the low-rank solution gener- alizes well. Finally, Section 6 presents simple experiments fine-tuning pre-trained models for different modalities. The experimental results validate our theory and provide further experimental insights. 2. Problem setting and preliminaries We primarily consider the setup of pre-trained large lan- guage models fine-tuned with LoRA. However, our theory does generally apply to other setups that utilize pre-training and LoRA fine-tuning, such as diffusion models. Matrix notation. For matrices A and B, let ∥A∥∗ denote the nuclear norm, ∥A∥F the Frobenius norm, and ⟨A, B⟩ = tr(A⊺B) the matrix inner product. We letSn and Sn + for the set of n × n symmetric and positive semi-definite matrices, respectively. Let R(·) and N(·) respectively denote the range and the null-space of a linear operator. Neural network. Let fΘ : X →RK be a neural network (e.g., a transformer-based model) parametrized by Θ, where X is the set of data (e.g., natural language text) and RK is the output (e.g., pre-softmax logits of tokens). K is the output dimension of fΘ, where K = k for k-class classification, K = 1 for binary classification, and K is the 2LoRA Training in the NTK Regime has No Spurious Local Minima dimension of the label Y when using mean square error loss. Assume the model has been pre-trained to Θ = Θ0, i.e., the pre-trained model is fΘ0 . Let W = ( W(1), . . . , W(T)) ⊂ Θ be a subset of the weights (e.g., dense layers in QKV-attention) with size W(i) ∈ Rmi×ni for i = 1, . . . , Tthat we choose to fine- tune. Let W0 = (W(1) 0 , . . . , W(T) 0 ) ⊂ Θ0 be their corre- sponding pre-trained weights. With slight abuse of notation, write fW to denote fΘ, where all parameters ofΘ excluding W are fixed to their corresponding values in Θ0. Fine-tuning loss. Assume we wish to fine-tune the pre- trained model with {(Xi, Yi)}N i=1, where N is the number of (fine-tuning) training data. (In many NLP tasks, it is not uncommon to have N < 100.) Denote δ = (δ(1), . . . , δ(T)) ⊂ Θ to be the change of W after the fine-tuning, i.e., fW0+δ is our fine-tuned model. We use the empirical risk ˆL(δ) = 1 N NX i=1 ℓ(fW0+δ(Xi), Yi), with some loss functionℓ. We assumeℓ(x, y) is convex, non- negative, and twice-differentiable with respect to x for any y. (This assumption holds for the cross-entropy loss and the mean squared error loss.) The empirical risk approximates the true risk L(δ) = E (X,Y )∼P \u0002 ℓ(fW0+δ(X), Y) \u0003 with some data distribution P. NTK regime. Under the NTK regime (also referred to as the lazy-training regime), the change of the network can be approximated by its first-order Taylor expansion fW0+δ(X) ≈ fW0 (X) + ⟨∇fW0 (X), δ⟩ (1) sufficiently well throughout (fine-tuning) training. To clarify, fW0+δ(X) ∈ RK, so the NTK regime requires the first- order Taylor expansion to be accurate for all coordinates: f(j) W0+δ(X) ≈ f(j) W0 (X) + ⟨∇f(j) W0 (X), δ⟩, where f(j) W is the j-th coordinate of fW for j = 1, . . . , K. The NTK regime is a reasonable assumption in fine-tuning if δ is small, and this assertion is supported by the empirical evidence of (Malladi et al., 2023). This prior work provides extensive experiments on various NLP tasks to validate that fine-tuning happens within the NTK regime for many, although not all, NLP tasks. Observation 2.1 (Malladi et al. (2023)) . When prompt- based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) is used, fine-tuning a pre-trained language model stays within the NTK regime. Motivated by this empirical observation, we define lin- earized losses ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001 ≈ ˆL(δ) and L(δ) = E (X,Y )∼P h ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001i ≈ L(δ). LoRA. We use the low-rank parameterization δ(i) = u(i)(v(i))⊺ ∈ Rmi×ni, where u(i) ∈ Rmi×r, v(i) ∈ Rni×r, for i ∈ {1, ··· , T}. Under the NTK regime, the empirical risk can be approxi- mated as ˆL(uv⊺) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 , where u =   u(1) ... u(T)   ∈ Rm×r, v =   v(1) ... v(T)   ∈ Rn×r with m = PT i=1 mi and n = PT i=1 ni, and G(Xi) = diag \u0000 ∇W(1) fW0 (Xi), . . . ,∇W(T) fW0 (Xi) \u0001 is an collection ofK m×n block diagonal matrices. To clar- ify, G(Xi) ∈ RK×m×n, so ⟨G(Xi), uv⊺⟩ ∈RK should be interpreted as K inner products of m ×n matrices where each matrices correspond to each coordinates of f. More specifically, G(j)(Xi) ∈ Rm×n and \u0000 ⟨G(Xi), uv⊺⟩ \u0001 j = ⟨G(j)(Xi), uv⊺⟩ for j = 1, . . . , K. Note that ˆL(uv⊺) under the NTK regime is non-convex in (u, v) so SGD-training does not converge to the global minimizer, in general. Weight decay on LoRA is nuclear norm regularization. The LoRA training of optimizing ˆL is often conducted with weight decay (Hu et al., 2021; Dettmers et al., 2023), which can be interpreted as solving minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , 3LoRA Training in the NTK Regime has No Spurious Local Minima with regularization parameter λ ≥ 0. This problem is equiv- alent to the rank-constrained nuclear-norm regularized prob- lem minimize δ, rankδ≤r ˆLλ(δ) ≜ ˆL(δ) + λ∥δ∥∗. This is due to the following lemma. Lemma 2.2 (Lemma 5.1 of (Recht et al., 2010)). Let r >0. For δ ∈ Rm×n such that rank(δ) ≤ r, ∥δ∥∗ = 1 2 min uv⊺=δ {∥u∥2 F + ∥v∥2 F |u ∈ Rm×r, v ∈ Rn×r}. (The connection between weight decay on Burer–Monteiro style low-rank factorization and nuclear norm regularization has been previously in different contexts not directly related to LoRA (Cabral et al., 2013; Pilanci & Ergen, 2020).) Second-order stationary points. Let ˆL: Rm×n → R be twice-continuously differentiable. We say U ∈ Rm×n is a (first-order) stationary point if ∇ˆL(U) = 0. We say U ∈ Rm×n is a second-order stationary point (SOSP) if ∇ˆL(U) = 0, ∇2 ˆL(U)[V, V] ≥ 0, for any direction V ∈ Rm×n. We say U is strict saddle if U is a first- but not second-order stationary point. Lastly, we say U ∈ Rm×n is a local minimum if there exists an open ball B that contains U and ˆL(U) ≤ ˆL(U′) for any U′ ∈ B. It follows that a local minimum is an SOSP. The following results, roughly speaking, establish that (stochastic) gradient descent only converges to SOSPs when a loss function is twice-continuously differentiable. Theorem 2.3 (Theorem 4.1 of (Lee et al., 2016)). Gradient descent on twice-differentiable function with random ini- tialization, almost surely, does not converge to strict saddle points. I.e., if gradient descent converges, it converges to an SOSP , almost surely. Theorem 2.4 (Informal, Theorem 1 of (Ge et al., 2015)). Stochastic gradient descent with noise on twice- differentiable strict saddle function (i.e., every stationary point is either a local minimum or a strict saddle) does not converge to strict saddle points with high probability. I.e., if stochastic gradient descent with noise converges, it converges to an SOSP with high probability. Therefore, if we can show that all SOSPs are global minima in our setup of interest, then (stochastic) gradient descent will only converge to global minima. 3. Low-rank solution exists In this section, we show that full fine-tuning in the NTK regime admits a low-rank solution of rank r ≲ √ N. The existence of a low-rank solution provides theoretical legit- imacy to using the low-rank parameterization of LoRA, which, of course, can only find low-rank solutions. Theorem 3.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global minimizer (not necessarily unique). Then there is a rank-r solution such that r(r+1) 2 ≤ KN . The assumption that ˆLλ(δ) has a global minimum is very mild; it is automatically satisfied if λ >0. When λ = 0, the assumption holds if ℓ is the mean squared error loss. The inspiration for Theorem 3.1 comes from the classical results of (Barvinok, 1995; Pataki, 1998; 2000) that establish that semi-definite programs (which have symmetric positive semi-definite matrices as optimization variables) admit low- rank solutions. We clarify that Theorem 3.1 does not require δ to be symmetric nor any notion of “semi-definiteness” (δ is not even square). Proof sketch of Theorem 3.1. We quickly outline the key ideas of the proof while deferring the details to Appendix A. We can show that finding δ⋆ λ ∈ argminδ ˆLλ(δ) with rank(δ⋆ λ) = r is equivalent to finding a rank-r global mini- mum of F : S(m+n) + → R where F(Z) = ˆL( ¯Z) + λ 2 tr(Z) and ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of Z such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . (2) Now suppose Z⋆ ∈ S(m+n) + is a global minimizer of F. Define S(Z⋆) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆)} and a linear operator A: S(m+n) → RKN as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Now let rank(Z⋆) = r and assume {0} = S(Z⋆) ∩ N(A). Then by dimension counting, we have the following inequal- 4LoRA Training in the NTK Regime has No Spurious Local Minima Figure 1.Geometric intuition of Theorem 3.1. The three dimensional space describes the space of 2 by 2 matrices \u0014 1 x y z \u0015 . The surface z = xy represents the rank 1 matrices. The blue region on the surface correspond to the region of smaller objective values, and the set of global minima are depicted with purple. (Left) Plot of (a) with N = 1. The set of global minima is a plane, and the intersection with the surface z = xy (curve) is the set of rank-1 global minima. (Middle) Plot of (b) with N = 2. the set of global minima is a line, and the intersection with the surface (two dots) is the set of rank 1 global minima. (Right) Plot of (c) with N = 3. The set of global minima is a line, and there is no intersection with the surface, i.e., there is no global minimum of rank-1 but admits a rank-2 global minima. ity. 0 = dimS(Z⋆) + dimN(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(m+n)) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆) − KN If there exists nonzero Z ∈ S(m+n) such that Z ∈ S(Z⋆) ∩ N(A), then we can show that there exists nonzero t ∈ R such that Z⋆ + tZ is also a global minimizer of F with strictly lower rank. Replace Z⋆ with Z⋆ + tZ and repeat this process until we find a solution Z⋆ with {0} = S(Z⋆) ∩ N(A). Together with the fact that dimS(Z⋆) = r(r+1) 2 , we have the desired result. Illustration of Theorem 3.1. The following toy example illustrates the geometric intuition of Theorem 3.1. Let ℓ be the mean square error loss, K = 1, δ = \u0014w x y z \u0015 , and λ = 0 (no regularization). Then consider the following objective functions each for N = 1, 2, and 3: ˆL0(δ) = (x + y)2 (a) ˆL0(δ) = 1 2(z + 4)2 + 1 2(x + y)2 (b) ˆL0(δ) = 1 3(w − 1)2 + 1 3(z − 4)2 + 1 3( √ 3x + √ 3y)2 (c) The set of low-rank (rank-1) solutions for the three objec- tives are depicted in Figure 1. 4. GD and LoRA finds low-rank solution In this section, we show that the optimization landscape with LoRA in the NTK regime has no spurious local minima if the LoRA parameterization uses rank r ≳ √ N and if we consider an ε-perturbed loss. This implies that optimizers such as stochastic gradient descent only converge to the low-rank global minimizers. Theorem 4.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global min- imizer (not necessarily unique) and r(r+1) 2 > KN. Con- sider the perturbed loss function ˆLλ,P defined as ˆLλ,P (u, v) ≜ ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 ∈ R(m+n)×r and P ∈ S(m+n) + is pos- itive semi-definite. Then, for almost all nonzero P (with respect to the Lebesgue measure on S(m+n) + ⊂ S(m+n) ∼= R (m+n)(m+n+1) 2 ), all SOSPs of ˆLλ,P are global minimizers of ˆLλ,P . To clarify, the conclusion that ‘all SOSPs are global mini- mizers’ holds with probability 1 even if the distribution of P is supported on {P ∈ S(m+n) + : ∥P∥ ≤ε} for arbitrarily small ε >0. In the practical LoRA fine-tuning setup where no perturbation is used and P = 0 is set deterministically, Theorem 4.1 does not apply. However, we can neverthe- less interpret the result of Theorem 4.1 to show that LoRA fine-tuning generically has no spurious local minima. 5LoRA Training in the NTK Regime has No Spurious Local Minima If we do use a randomly generated small perturbation P so that Theorem 4.1 applies, the solution to the perturbed problem with small P does not differ much from that of the unperturbed problem with P = 0 in the following sense. Corollary 4.2. Consider the setup of Theorem 4.1 and let ε > 0. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). Assume P is randomly sampled with a probability distribution supported in {P ∈ S(m+n) + : ∥P∥F < ε} and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Then for any SOSP (ˆu, ˆv) of ˆLλ,P ˆLλ(ˆuˆv⊺) ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗ = min δ ˆLλ(δ) + 2ε∥δ⋆ λ∥∗. I.e., if (ˆu, ˆv) is an SOSP (and thus a global minimizer by Theorem 4.1) of the perturbed loss ˆLλ,P , then it is an ε- approximate minimizer of the unperturbed loss ˆLλ. So if r(r+1) 2 > KN, then Theorem 2.3, Theorem 2.4, and Corollary 4.2 together establish that (stochastic) gradient descent finds a ˆuˆv⊺ such that its unperturbed empirical risk is ε-close to the the minimum unperturbed empirical risk. 4.1. Proof outlines The proof is done by continuing our analysis of global mini- mum of ˆLλ(δ). Given that low-rank solution exists, which we proved in the previous section, recall that LoRA training with weight decay is equivalent to solving argmin u,v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F . In this section, we relate SOSPs with global minimum, which opens the chance to find a global minimum by using gradient-based optimization methods. We start the analysis from the following lemma, which is a prior characterization of SOSPs in the matrix factorization. Lemma 4.3. (Theorem 2 of (Haeffele et al., 2014)) Let G: S(m+n) + → R be a twice differentiable convex function with compact level sets, H : S(m+n) + → R be a proper con- vex lower semi-continuous function, and r > 0. If the function F : U 7→ G(UU ⊺) +H(UU ⊺) defined over matri- ces U ∈ R(m+n)×r has a second order staionary point at a rank-deficient matrix U, then UU ⊺ is a global minimum of G + H. We build our analysis upon Lemma 4.3. However, Lemma 4.3 is not directly applicable to our setting since it requires that the SOSP must be rank-deficient. However, this can be effectively circumvented by employing a per- turbed empirical risk: minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 , and P is a positive semi-definite matrix. Now we get the following lemma by applying Lemma 4.3 to the perturbed empricial risk. Lemma 4.4. Fix λ ≥ 0. Assume ˆLλ(δ) has a global min- imum (not necessarily unique), P ∈ S(m+n) + is nonzero positive semi-definite, and r >0. If ˆQ = \u0014ˆu ˆv \u0015 ∈ R(m+n)×r is a rank deficient SOSP of ˆLλ,P (u, v) = ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, then ˆQ is a global minimum of ˆLλ,P (u, v). Proof. Define G, H: S(m+n) + → R to be G(X) = λ 2 tr(X) + ⟨P, X⟩, H (X) = ˆL( ¯X) where ¯X is the off-diagonal submatrix of X defined in (2). Note that G has compact level set for every λ ≥ 0 since tr(X) ≥ 0 and P, Xare positive semi-definite, concluding that ˆQλ,P is a global minimum of F(Q) ≜ G(QQ⊺) + H(QQ⊺) = ˆLλ,P (u, v). We now give a detailed analysis of the proof of Theorem 4.1. The structure of the proof is inspired by the original work of Pataki (1998) and followed by Burer & Monteiro (2003); Boumal et al. (2016); Du & Lee (2018). The proof uses an application of Sard’s theorem of differential geometry. The argument is captured in Lemma 4.5, and its proof is deferred to Appendix B. Lemma 4.5. Let M be m-dimensional smooth manifold embedded in Rd and V be a linear subspace of Rd with dimension n. If m + n < d, then the set M + V = {p + v : p ∈ M, v∈ V } has Lebesgue measure zero in Rd. Proof of Theorem 4.1. We show that second-order station- ary point ˆQλ,P = \u0014ˆu ˆv \u0015 is rank-deficient for almost all posi- tive semi-definite P, then use Lemma 4.4 to complete the proof. Denote f(j) for the j-th coordinate of f. For simplic- ity of notations, define ˆY (j) i ≜ f(j) W0 (Xi) + ⟨G(j)(Xi), uv⊺⟩, 6LoRA Training in the NTK Regime has No Spurious Local Minima and v(j) i ≜ 1 N ∂ ∂ ˆY (j) i ℓ( ˆYi, Yi) for 1 ≤ i ≤ N and 1 ≤ j ≤ K, which depends on u and v. Then for v = {v(j) i } ∈RKN define S(v) ≜ NX i=1 KX j=1 v(j) i G(j)(Xi) ∈ Rm×n. Then by first-order gradient condition, we have  \u0014 0 S(v) S(v)⊺ 0 \u0015 + λI + P | {z } ≜M ! ˆQλ,P = 0 We observe that the range of ˆQλ,P ∈ R(m+n)×r is in the nullspace of M ∈ S(m+n). We now suppose ˆQλ,P has full rank, i.e., rank( ˆQλ,P ) = r. Hence, we have the following inequality: r = rank( ˆQλ,P ) ≤ dim N(M) ≤ m + n Now for r ≤ s ≤ m + n and s ∈ Z, define As = n P : P = M − λI, M∈ S(m+n), dimN(M) = s o . Then from Proposition 2.1 of (Helmke & Shayman, 1995), As is a smooth manifold embedded in R (m+n)(m+n+1) 2 ∼= S(m+n) with dimension dimAs = (m + n + 1)(m + n) 2 − s(s + 1) 2 . Now by definition of P, we know that P ∈ m+n[ s=r (As + R(S)) where “ + ”is the set-sum (Minkowski sum) and R(S) is the range of S(v) in R (m+n)(m+n+1) 2 for any v ∈ RKN . The dimensions can be bounded by dimAs ≤ (m + n)(m + n + 1) 2 − r(r + 1) 2 for r ≤ s ≤ m + n and dimR(S) ≤ KN. Therefore given that r(r+1) 2 > KN, we have dimAs + dimR(S) < (m + n)(m + n + 1) 2 . Then, by Lemma 4.5, which is effectively an application of Sard’s theorem, we can conclude As + R(S) is a measure- zero set, and the finite union of such measure-zero sets is measure-zero. This implies that every P that makes ˆQλ,P to be of full rank must be chosen from measure-zero subset of S(m+m) + ⊂ S(m+n). Therefore we may conclude that rank( ˆQλ,P ) < rfor almost every nonzero positive semi- definite P. Proof of Corollary 4.2. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). We observe the following chain of inequalities. ˆL(ˆδ) + λ∥ˆδ∥∗ ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F + ⟨P, ˆQ ˆQ⊺⟩ = ˆLλ,P (ˆu, ˆv), where the first inequality of is from Lemma 2.2, the second is from P and ˆQ ˆQ⊺ being positive semi-definite. On the other hand, we can find u⋆ and v⋆ such that δ⋆ λ = u⋆v⋆⊺ and ∥δ⋆ λ∥∗ = 1 2 (∥u⋆∥2 F + ∥v⋆∥2 F ) by using Lemma 2.2. Now take Q⋆ = \u0014u⋆ v⋆ \u0015 , then we get ˆLλ,P (u⋆, v⋆) = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ⟨P, Q⋆Q⋆⊺⟩ ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆Q⋆⊺∥F ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥u⋆∥2 F + ε∥v⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗, where the first inequality is Cauchy–Schwartz inequality, and the second inequality is from sub-multiplicativity of ∥ · ∥F . Moreover by Theorem 4.1, ˆLλ,P (ˆu⋆, ˆv⋆) ≤ ˆLλ,P (u⋆, v⋆), and this happens for almost sure, since we sampled P from a probability distribution which is absolutely continuous with respect to the Lebesgue measure on R (m+n)(m+n+1) 2 ∼= S(m+n). 5. Low-rank LoRA solution generalizes well In this section, we establish a generalization guarantee for the low-rank solution obtained by minimizing the perturbed loss ˆLλ,P of Theorem 4.1. For simplicity, we restrict the following main result to the cross-entropy loss. General- ization guarantees for general convex, non-negative, and twice continuously differentiable losses, are provided as Theorem C.6 in Appendix C. Theorem 5.1. Assume ℓ is cross-entropy loss. Assume the population risk L has a minimizer (not necessarily unique) and denote it as δ⋆ true ∈ argminδ L(δ). Assume δ⋆ true ̸= 0. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely 7LoRA Training in the NTK Regime has No Spurious Local Minima (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 2.Training curves (training loss vs. epochs) on different NLP tasks. with respect to the random data X ∼ P. Let ε >0, η ∈ (0, 1), and λ = 2(2 + ε) √ KR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ). Consider the setup of Corollary 4.2 withP randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺)−L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N \u0012 2 + r log 1 η \u0013 . In the context of fine-tuning, where the target task is closely related to the pre-training task, it is natural to assume that δ⋆ true in Theorem 5.1 is “small”. The proof, deferred to Appendix C, utilizes standard arguments with Rademacher complexity. 6. Experiments In this section, we conduct simple experiments on fine- tuning linearized pre-trained models to validate our theory.1 1Code available at https://github.com/UijeongJang/LoRA-NTK. Experimental setup on NLP tasks. We use prompt-based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) and consider the same architecture and dataset as in (Malladi et al., 2023), which empirically verifies that with prompt- based fine-tuning, the fine-tuning dynamics stay within the NTK regime. We present the results of six NLP tasks that were also considered in (Malladi et al., 2023): sen- timent analysis (SST-2, MR, CR), natural language infer- ence (QNLI), subjectivity (Subj), and paraphrase detection (QQP). We optimize a linearized RoBERTa-base (Liu et al., 2019) model with dataset of size 32 ( N = 32) with two labels (K = 2) using cross entropy loss. With LoRA rank r ≥ 11, our theory guarantees that no spurious local min- ima exist. For a baseline comparison, we also perform full fine-tuning (without LoRA) on the linearized model. The training curves are presented in Figure 2, and additional details are provided in Appendix D. Results showing test accuracy are also presented in Appendix D. Experimental setup on image and speech classification tasks. We use a pre-trained vision transformer (Dosovit- skiy et al., 2021) and fine-tune it on the bean disease dataset (Makerere AI Lab, 2020) to perform an image classification task with 3 labels. We use dataset of size 48 with three labels. Similar to our experiments on NLP tasks, we find that training curves converge to the same loss value, where the rates of convergence differ. For speech classification, we use a pre-trained wav2vec2 (Baevski et al., 2020) model and fine-tune it on a SUPERB dataset (Yang et al., 2021) to perform a speech classification 8LoRA Training in the NTK Regime has No Spurious Local Minima (a) Image classification  (b) Speech classification Figure 3.Training curves (training loss vs. epochs) on image and speech classification tasks. task with 4 labels. We use a dataset of size 64 with four labels. We also find that the training curves converge to the same loss value. The details are the same as with the image classification task. The training curves of both image and speech data are pre- sented in Figure 3, and additional details are provided in Appendix D. Empirical observation. The experiments validate our the- ory as the training curves converge to the same globally optimal loss value. However, we do observe that therates of convergence differ. When the LoRA rank is higher or when full fine-tuning is performed and LoRA is not used, fine-tuning converges faster. Indeed, our theory ensures that spurious local minima do not exist, but it says nothing about how convex or favorable the landscape may or may not be. Our intuitive hypothesis is that using lower LoRA rank creates unfavorable regions of the loss landscape, such as plateaus or saddle points, and they slow down the gradient descent dynamics. If this hypothesis is generally true, we face an interesting tradeoff: lower LoRA rank reduces memory cost and per- iteration computation cost but increases the number of it- erations needed for convergence. Then, using a very low LoRA rank may be suboptimal not due to representation power, presence of spurious local minima, or poor general- ization guarantees, but rather due to unfavorable flat training landscapes slowing down convergence. Exploring this phe- nomenon and designing remedies is an interesting direction for future work. 7. Conclusion In this work, we present theoretical guarantees on the train- ability and generalization capabilities of LoRA fine-tuning of pre-trained models. Together with the work of Zeng & Lee (2024), our results represent a first step in theoretically analyzing the LoRA fine-tuning dynamics of pre-trained models by presenting guarantees (upper bounds). For future work, carrying out further refined analyses under more spe- cific assumptions, relaxing the linearization/NTK regime assumption through a local analysis, better understanding the minimum rank requirement through lower bounds, and, motivated by the observation of Section 6, analyzing the tradeoff between training rate and LoRA rank are exciting directions. Acknowledgments UJ and EKR were supported by the Samsung Science and Technology Foundation (Project Number SSTF-BA2101- 02) and the National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIP) [NRF- 2022R1C1C1010010]. JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. We thank Jungsoo Kang for the discussion on the proof of Lemma 4.5. We also thank Jisun Park for providing valuable feedback. Impact statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrin- sic dimensionality explains the effectiveness of language model fine-tuning. Association for Computational Lin- guistics, 2021. Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. International Conference on Machine Learning, 2019a. Allen-Zhu, Z., Li, Y ., and Song, Z. On the convergence rate of training recurrent neural networks. Neural Information Processing Systems, 2019b. Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely 9LoRA Training in the NTK Regime has No Spurious Local Minima wide neural net. Neural Information Processing Systems, 2019. Bach, F. Learning Theory from First Principles . Draft, 2023. Bach, F., Mairal, J., and Ponce, J. Convex sparse matrix factorizations. arXiv preprint arXiv:0812.1869, 2008. Baevski, A., Zhou, Y ., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Neural Information Processing Systems, 2020. Barron, A. R. Universal approximation bounds for super- positions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930–945, 1993. Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. Bartlett, P. L., Boucheron, S., and Lugosi, G. Model selec- tion and error estimation. Machine Learning, 48:85–113, 2002. Bartlett, P. L., Bousquet, O., and Mendelson, S. Local rademacher complexities. The Annals of Statistics, 33(4): 1497–1537, 2005. Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrally- normalized margin bounds for neural networks. Neural Information Processing Systems, 2017. Barvinok, A. I. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computational Geometry, 13:189–202, 1995. Bengio, Y . and Delalleau, O. On the expressive power of deep architectures. Algorithmic Learning Theory, 2011. Bhojanapalli, S., Boumal, N., Jain, P., and Netrapalli, P. Smoothed analysis for low-rank solutions to semidefi- nite programs in quadratic penalty form. Conference On Learning Theory, 2018. Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J. Transformers learn through gradual rank increase. Neural Information Processing Systems, 2023. Boumal, N., V oroninski, V ., and Bandeira, A. The non-convex Burer–Monteiro approach works on smooth semidefinite programs. Neural Information Processing Systems, 29, 2016. Bousquet, O. and Elisseeff, A. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. Burer, S. and Monteiro, R. D. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329– 357, 2003. Cabral, R., De la Torre, F., Costeira, J. P., and Bernardino, A. Unifying nuclear norm and bilinear factorization ap- proaches for low-rank matrix decomposition. Interna- tional Conference on Computer Vision, 2013. Chen, Z., Cao, Y ., Gu, Q., and Zhang, T. A generalized neu- ral tangent kernel analysis for two-layer neural networks. Neural Information Processing Systems, 2020. Choi, J. Y ., Park, J., Park, I., Cho, J., No, A., and Ryu, E. K. LoRA can replace time and class embeddings in diffusion probabilistic models. NeurIPS 2023 Workshop on Diffusion Models, 2023. Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of Control, Signals and Systems, 2(4):303–314, 1989. Delalleau, O. and Bengio, Y . Shallow vs. deep sum-product networks. Neural Information Processing Systems, 2011. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: efficient finetuning of quantized llms. Neural Information Processing Systems, 2023. Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y . Sharp minima can generalize for deep nets. International Con- ference on Machine Learning, 2017. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. Du, S. and Lee, J. On the power of over-parametrization in neural networks with quadratic activation. International Conference on Machine Learning, 2018. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. International Conference on Machine Learning, 2019. Du, S. S., Jin, C., Lee, J. D., Jordan, M. I., Singh, A., and Poczos, B. Gradient descent can take exponential time to escape saddle points. Neural Information Processing Systems, 2017. Duan, Y ., Ji, G., Cai, Y ., et al. Minimum width of leaky- relu neural networks for uniform universal approximation. International Conference on Machine Learning, 2023. 10LoRA Training in the NTK Regime has No Spurious Local Minima Fu, Z., Yang, H., So, A. M.-C., Lam, W., Bing, L., and Collier, N. On the effectiveness of parameter-efficient fine-tuning. AAAI Conference on Artificial Intelligence, 2023. Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. Association for Computational Linguistics, 2021. Ge, R., Huang, F., Jin, C., and Yuan, Y . Escaping from saddle points—online stochastic gradient for tensor de- composition. Conference on Learning Theory, 2015. Ge, R., Lee, J. D., and Ma, T. Matrix completion has no spurious local minimum. Neural Information Processing Systems, 2016. Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as pro- grammable computers. International Conference on Ma- chine Learning, 2023. Haeffele, B., Young, E., and Vidal, R. Structured low-rank matrix factorization: optimality, algorithm, and applica- tions to image processing. International Conference on Machine Learning, 2014. Hardt, M., Recht, B., and Singer, Y . Train faster, generalize better: stability of stochastic gradient descent. Interna- tional Conference on Machine Learning, 2016. Helmke, U. and Shayman, M. A. Critical points of matrix least squares distance functions. Linear Algebra and its Applications, 215:1–19, 1995. Hornik, K., Stinchcombe, M., and White, H. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551–560, 1990. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., Chen, W., et al. LoRA: low-rank adaptation of large language models. International Conference on Learning Representations, 2021. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker- nel: convergence and generalization in neural networks. Neural Information Processing Systems, 2018. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. How to escape saddle points efficiently. Interna- tional Conference on Machine Learning, 2017. Jin, J., Li, Z., Lyu, K., Du, S. S., and Lee, J. D. Under- standing incremental learning of gradient descent: A fine-grained analysis of matrix sensing. International Conference on Machine Learning, 2023. Johnson, W. and Lindenstrauss, J. Extensions of lipschitz maps into a hilbert space. Contemporary Mathematics, 26:189–206, 1984. Koltchinskii, V . and Panchenko, D. Rademacher processes and bounding the risk of function learning. In Gin ´e, E., Mason, D. M., and Wellner, J. A. (eds.), High Dimen- sional Probability II, pp. 443–457. Springer, 2000. Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B. Gra- dient descent only converges to minimizers. Conference on Learning Theory, 2016. Lialin, V ., Muckatira, S., Shivagunde, N., and Rumshisky, A. ReLoRA: high-rank training through low-rank up- dates. Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Re- source Optimization, 2023. Liu, L., Liu, X., Gao, J., Chen, W., and Han, J. Under- standing the difficulty of training transformers. Empirical Methods in Natural Language Processing, 2020. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . RoBERTa: a robustly optimized BERT pretraining ap- proach. arXiv preprint arXiv:1907.11692, 2019. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres- sive power of neural networks: a view from the width. Neural Information Processing Systems, 2017. Makerere AI Lab. Bean disease dataset, 2020. URLhttps: //github.com/AI-Lab-Makerere/ibean/. Malladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S. A kernel-based view of language model fine-tuning. Inter- national Conference on Machine Learning, 2023. Maurer, A. A vector-contraction inequality for rademacher complexities. Algorithmic Learning Theory, 2016. McDiarmid, C. et al. On the method of bounded differences. Surveys in Combinatorics, 141(1):148–188, 1989. Pataki, G. On the rank of extreme matrices in semidefi- nite programs and the multiplicity of optimal eigenval- ues. Mathematics of Operations Research, 23(2):339– 358, 1998. Pataki, G. The geometry of semidefinite programming. In Wolkowicz, H., Saigal, R., and Vandenberghe, L. (eds.), Handbook of Semidefinite Programming: Theory, Algo- rithms, and Applications, pp. 29–65. Springer, 2000. 11LoRA Training in the NTK Regime has No Spurious Local Minima Pilanci, M. and Ergen, T. Neural networks are convex reg- ularizers: exact polynomial-time convex optimization formulations for two-layer networks. International Con- ference on Machine Learning, 2020. Polyak, B. T. Introduction to Optimization . New York, Optimization Software, 1987. Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010. Ryu, S. Low-rank adaptation for fast text-to-image diffu- sion fine-tuning, 2023. URL https://github.com/ cloneofsimo/lora. Schick, T. and Sch¨utze, H. Exploiting cloze questions for few shot text classification and natural language inference. Association for Computational Linguistics, 2021. Smith, J. S., Hsu, Y .-C., Zhang, L., Hua, T., Kira, Z., Shen, Y ., and Jin, H. Continual diffusion: continual customiza- tion of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. Soltanolkotabi, M., Javanmard, A., and Lee, J. D. Theo- retical insights into the optimization landscape of over- parameterized shallow neural networks. IEEE Transac- tions on Information Theory, 65(2):742–769, 2018. Sridharan, K., Shalev-Shwartz, S., and Srebro, N. Fast rates for regularized objectives.Neural Information Processing Systems, 21, 2008. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention is all you need. Neural Information Processing Systems, 2017. Wei, A., Hu, W., and Steinhardt, J. More than a toy: random matrix models predict how real-world neural representa- tions generalize. International Conference on Machine Learning, 2022a. Wei, C., Chen, Y ., and Ma, T. Statistically meaningful approximation: a case study on approximating turing ma- chines with transformers. Neural Information Processing Systems, 2022b. Wu, L., Zhu, Z., et al. Towards understanding generalization of deep learning: perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Yang, S.-w., Chi, P.-H., Chuang, Y .-S., Lai, C.-I. J., Lakho- tia, K., Lin, Y . Y ., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., et al. Superb: Speech processing universal performance benchmark. Interspeech, 2021. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: from LyCORIS fine-tuning to model evaluation. International Conference on Learning Representations, 2024. Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers universal approximators of sequence-to-sequence functions? International Confer- ence on Learning Representations, 2019. Zeng, Y . and Lee, K. The expressive power of low-rank adaptation. International Conference on Learning Repre- sentations, 2024. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107– 115, 2021. Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. Why are adaptive methods good for attention models? Neural Information Processing Systems, 2020. Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. Machine Learning, 109(3):467–492, 2020. 12LoRA Training in the NTK Regime has No Spurious Local Minima A. Omitted proof of Theorem 3.1 Here, we explain the details in the proof of Theorem 3.1. We first prove the equivalence of minimize δ∈Rm×n ˆL(δ) + λ∥δ∥∗ (P) and minimize Z∈S(m+n) + ˆL( ¯Z) + λ 2 tr(Z) (Q) where ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of X such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . Lemma A.1. The following two statements hold. 1. Fix λ ≥ 0 and suppose (P) has a global minimizer (not necessarily unique). Let δ⋆ λ ∈ Rm×n be a global minimizer of (P). Then there exists an Z⋆ λ ∈ S(m+n) + induced from δ⋆ λ such that Z⋆ λ is a global minimizer of (Q), rank(Z⋆ λ) = rank(δ⋆ λ), and has same objective value. 2. Fix λ ≥ 0 and suppose (Q) has a global minimizer (not necessarily unique). Let Z⋆ λ ∈ S(m+n) + be a global minimum of (Q). Then ¯Z⋆ λ ∈ Rm×n is a global minimizer of (P) such that rank( ¯Z⋆ λ) = min(m, n,rank(Z⋆ λ)) and has same objective value. Proof. We prove the two statements at once. Let δ⋆ λ ∈ Rm×n be a global minimizer of (P) and let r = rank(δ⋆ λ). Then by Lemma 2.2, there exists u ∈ Rm×r and v ∈ Rn×r such that ∥δ⋆ λ∥∗ = 1 2 (∥u∥2 F + ∥v∥2 F ) and uv⊺ = δ⋆ λ. Take Z⋆ λ = \u0014u v \u0015\u0002u⊺ v⊺\u0003 = \u0014uu⊺ uv⊺ vu⊺ vv⊺ \u0015 ∈ S(m+n) + . Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = \r\r\r \u0014u v \u0015\r\r\r 2 F = ∥u∥2 F + ∥v∥2 F = 2∥δ⋆ λ∥∗, (Q) with Z⋆ λ has the same objective value with (P) with δ⋆ λ and rank(δ⋆ λ) = rank(Z⋆ λ) = r. Conversely, let Z⋆ λ ∈ S(m+n) + be a global minimizer of (Q) and let rank(Z⋆ λ) = r. Note that r may be larger than m or n. Then there exists Q = \u0014u v \u0015 ∈ R(m+n)×r such that QQ⊺ = Z⋆ λ. Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = ∥Q∥2 F = ∥u∥2 F + ∥v∥2 F ≥ 2∥uv⊺∥∗ = 2∥ ¯Z⋆ λ∥∗, the objective value of (P) with ¯Z⋆ λ ∈ Rm×n has less than or equal to minimum objective value of (Q) and rank( ¯Z⋆ λ) = min(m, n, r). If there exists m × n matrix whose objective value of (P) is strictly less than the minimum objective value of (Q), then we repeat the same step that was applied on δ⋆ λ to induce a solution of (Q) with strictly less objective value, which is a contradiction. Conversely, if there exists positive semi-definite matrix of size m + n whose objective value of (Q) is strictly less than the minimum objective value of (P), then we repeat the same step applied on Z⋆ λ to induce a solution of (P) with strictly less objective value, which is also a contradiction. Therefore if one of (P) and (Q) has a global minimizer, the other must have a global minimizer with same objective value. Next lemma states that if the rank of the global minimizer of (Q) is sufficiently large, then we can find an another solution with strictly less rank. Lemma A.2. Suppose X ∈ Sn + and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists nonzero t∗ ∈ R such that X + t∗Z is positive semi-definite and rank(X + t∗Z) < rank(X). 13LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let r = rank(X). Suppose Q ∈ Rn×r is a matrix where its columns are basis to R(X). Now suppose µ1(Q⊺(X + tZ)Q) > 0 for all t ∈ R where µ1(·) denotes the smallest eigenvalue (note thatµ1(·) is continuous). Then Q⊺(X +tZ)Q ∈ Sr should be positive definite for all t. For contradiction, take v ∈ R(Z) ⊆ R(X) = R(Q) to be an eigenvector of nonzero eigenvalue of Z. Since v⊺Xv >0 and v⊺Zv ̸= 0, there exists some t such that v⊺(X + tZ)v <0. Now take w ∈ Rr such that Qw = v. Then it follows that w⊺(Q⊺(X + tZ)Q)w <0, which is a contradiction. This implies that there exists t⋆ ̸= 0 such that µ1(Q⊺(X + t∗Z)Q) = 0, Hence we have r >rank(Q⊺(X + t∗Z)Q) = rank(X + t∗Z) and Q⊺(X + t∗Z)Q is positive semi-definite. To show thatX + t∗Z is positive semi-definite, take any x ∈ Rn and consider the decomposition x = Qy + z where y ∈ Rr and z ∈ N(Q) = N(X) ⊆ N(Z). Then, we have y⊺(X + t⋆Z)y = (y⊺Q⊺ + z⊺)(X + t∗Z)(Qy + z) = y⊺Q⊺(X + t∗Z)Qy ≥ 0. Finally, the following lemma and its proof are similar to the previous one, but we state it separately for the sake of clarity. It will be used in the proof of Theorem 3.1. Lemma A.3. Suppose X ∈ Sn + which is nonzero and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists t∗ > 0 such that X ± t∗Z is positive semi-definite. Proof. Let rank(X) = r and {y1, . . . , yr} be orthonormal eigenvectors of nonzero eigenvalues of X. Since y⊺ i Xyi > 0 for all yi, i = 1, ··· , r, there exists an interval (−ai, ai) for ai > 0 such that y⊺ i (X ± tZ)yi ≥ 0 for t ∈ (−ai, ai). Take t∗ = min{a1, . . . , ar}. Then t∗ satisfies the statement of the theorem. Now we provide the complete proof of Theorem 3.1. Proof of Theorem 3.1. Suppose Z⋆ λ ∈ S(m+n) + is a global minimizer of F(Z) = ˆL( ¯Z) + λ 2 tr(Z) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), ¯Z⟩, Yi \u0001 + λ 2 tr(Z) which is induced from δ⋆ λ ∈ Rm×n by Lemma A.1. Suppose there exists nonzero symmetric matrix Z such that Z ∈ S(Z⋆ λ) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆ λ)} and ⟨G(Xi), Z⟩ = 0 for 1 ≤ i ≤ N. In other words, Z ∈ S(Z⋆ λ) ∩ N(A) where A: S(m+n) → RKN is a linear operator defined as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Then there exists t >0 such that Z⋆ λ ± tZ is positive semi-definite by Lemma A.3, since Z⋆ must be nonzero. Therefore tr(Z) = 0, otherwise it will contradict the minimality of Z⋆ λ. Also we know that there exists nonzero t∗ ∈ R such that Z⋆ λ + t∗Z is also positive semi-definite with strictly lower rank by Lemma A.2. Since tr(Z) = 0, Z⋆ λ + t∗Z is also a global minimizer of F. Replace Z⋆ λ with Z⋆ λ + tZ and repeat this process until we find a solution Z⋆ λ with {0} = S(Z⋆ λ) ∩ N(A). Now we let rank(Z⋆ λ) = r. Then by dimension counting, we have the following inequality. 0 = dimS(Z⋆ λ) + dimN(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) − KN + dim(S(m+n)) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆ λ) − KN 14LoRA Training in the NTK Regime has No Spurious Local Minima Now we prove that dimS(Z⋆ λ) = r(r+1) 2 to complete the proof. Consider the diagonalization Z⋆ λ = UΛU⊺ where U is a orthogonal matrix. Since the dimension of the subspace is invariant under orthogonal transformations, we have dimS(Z⋆ λ) = dimS(Λ) = dim{Z ∈ S(m+n) : R(Z) ⊆ R(Λ)} where Λ is diagonal matrix with nontrivial entries in the leading principle minor of size r × r. This restricts the symmetric matrix Z to have nontrivial entries only in the leading r × r block. Hence, dimS(Z⋆ λ) = r(r+1) 2 . B. Omitted proof of Lemma 4.5 We prove Lemma 4.5 in this section. Proof of Lemma 4.5. Let ΠV ⊥ : Rd → V ⊥ be the orthogonal projection onto the orthogonal complement of V in Rd. Then, ΠV ⊥|M : M →V ⊥ is a smooth mapping between manifolds. Since dimV ⊥ = d − n > m= dimM, p is singular for all p ∈ M. Therefore ΠV ⊥(M) has measure zero in Rd−n by Sard’s theorem. Note that M + V ⊆ ΠV ⊥(M) + V and the measure of ΠV ⊥(M) + V in Rd is zero. This concludes that M + V is measure-zero in Rd. As a remark, the prior works of (Boumal et al., 2016; Du & Lee, 2018) also use dimension-counting arguments that would warrant the use of Lemma 4.5, but they do not provide a precise justification. Our Theorem 4.1 makes a similar argument, but does so fully rigorous through Lemma 4.5. C. Generalization guarantee In this section, let ℓ(·, ·) be our loss function which is convex, non-negative, and twice-differentiable on the first argument. Then, our empirical risk is ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 . We start the analysis from this non-regularized risk and expand it to regularized ones. We assume that our model is class of affine predictors X 7→ fW0 (X) + ⟨G(X), δ⟩ for given data X. Now we apply the theory of Rademacher complexity to derive the upper bound of the generalization bound. To begin with, we start with introducing the classical result in probability theory from (McDiarmid et al., 1989) without proof. Lemma C.1. (McDiarmid inequality) Let X1, . . . , XN ∈ Xbe i.i.d N random samples from dataset X. Let g : XN → R be a function satisfying the following property with c >0: |g(X1, . . . , Xi−1, Xi, Xi+1, . . . , XN ) − g(X1, . . . , Xi−1, X′ i, Xi+1, . . . , XN )| ≤c for all X1, . . . , XN , X′ i ∈ X. Then, for all ε >0, P(|g(X1, . . . , XN ) − E[g(X1, . . . , XN )]| ≥ε) ≤ exp \u0012 − 2ε2 Nc2 \u0013 . Now, we define the Rademacher complexity of the class of functions H from X to R: RN (H) = Eε,D   sup h∈H 1 N NX i=1 εih(Xi) ! , where {εi}1≤i≤N are independent Rademacher random variables, and D = {X1, . . . , XN } is N random samples from X. In our analysis, we will focus on class of affine predictors Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ and composition of affine predictors with loss Xi 7→ ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi). Rademacher complexities are closely related to upper bounds on generalization bound due to the following lemma. 15LoRA Training in the NTK Regime has No Spurious Local Minima Lemma C.2. Let RN (H) be the Rademacher complexity of the class of functions H from X to R and X1, . . . , XN are N samples from X. Then the following inequality holds. E \" sup h∈H   1 N NX i=1 h(Xi) − E[h(X)] !# ≤ 2RN (H), E \" sup h∈H   E[h(X)] − 1 N NX i=1 h(Xi) !# ≤ 2RN (H). Proof. The proof is by using standard symmetrization arguments. We defer its proof to Theorem 8 of (Bartlett & Mendelson, 2002), or Section 4.5 of (Bach, 2023). The next lemma uses a contraction property to reduce the Rademacher complexity of losses to linear predictors. These type of results are widely used in Rademacher analysis and we use the following specific version of contraction, which was originally introduced in Corollary 4 of (Maurer, 2016) and adapted to our setting. Write ∥ · ∥2 for Euclidean vector norm. Lemma C.3. Let A be the class of functions a : X →RK. For 1 ≤ i ≤ N, let ℓi : RK → R be G-Lipschitz continuous on A with respect to the Euclidean norm in the sense that the following holds: |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ∈ X. Then we have the following inequality for independent Rademacher random variables {σi}1≤i≤N and {εij}1≤i≤N,1≤j≤K: Eσ,D \" sup a∈A 1 N NX i=1 σiℓi(a(Xi)) # ≤ √ 2G · Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)  , where aj denotes the j-th coordinate of a and D = {(Xi, Yi)}i∈{1,...,N} are i.i.d N random samples sampled from X. Proof. We defer the proof to the Section 5 of (Maurer, 2016). In Lemma C.3, if we sample D from a probability distribution P, we can relax the Lipschitz continuity condition to hold for P- almost surely. In other words, |ℓ(a(X1)) − ℓ(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. The next lemma states that the Rademacher complexity of class of bounded affine predictors decays at most O( 1√ N ) rate. Lemma C.4. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Assume AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D >0. Suppose ∥G(j)(Xi)∥F ≤ R almost surely with respect to the random data Xi ∼ P. Then, Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   ≤ RD √ K√ N where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. 16LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Eε  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 f(j) W0 (Xi) + ⟨G(j)(Xi), δ⟩ \u0011   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩ + 1 N NX i=1 KX j=1 εijf(j) W0 (Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   ≤ Eε   sup ∥δ∥F ≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = Eε  D N sup ∥δ∥F ≤1 NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = D N Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F . The inequality is from the fact that ∥ · ∥F ≤ ∥ · ∥∗, hence {δ : ∥δ∥∗ ≤ D} ⊂ {δ : ∥δ∥F ≤ D}. The last equality is from the fact that ∥ · ∥F is self-dual. Next, we can bound Eε \r\r\rPN i=1 PK j=1 εijG(j)(Xi) \r\r\r F by the following inequalities. Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F ≤ vuuutEε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r 2 F = vuutEε NX i=1 KX j=1 \r\rεijG(j)(Xi) \r\r2 F = vuut NX i=1 KX j=1 \r\rG(j)(Xi) \r\r2 F ≤ R √ NK. a.s. The first inequality is from Jensen’s inequality, the equalities are from i.i.d assumption ofεik. We combine the results and take expectation with respect to D to get Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijak(Xi)   ≤ D N · R √ NK = RD √ K√ N . We then combine the previous results to get the following Lemma. Lemma C.5. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Let ˆL is non-regularized empirical risk defined as ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 17LoRA Training in the NTK Regime has No Spurious Local Minima and AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely with respect to the random data Xi ∼ P. For 1 ≤ i ≤ N, suppose ℓi ≜ ℓ(·, Yi) is G-Lipschitz continuous on A on the first argument (with respect to the Euclidean norm) for almost surely with respect to the random data Xi ⊆ D ∼ P. That is, |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. Then for any ∥δ∥∗ ≤ D, fixed δ0 such that ∥δ0∥∗ ≤ D, and η ∈ (0, 1), the following inequality holds with probability greater than 1 − η: ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . Proof. Take g of Lemma C.1 to be g = sup∥δ∥∗≤D(ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)), which is a function of X1, . . . , XN . Since ∥δ∥∗ ≤ D implies ∥δ∥F ≤ D and by the Lipschitz continuity of ℓ(·, Yi), we have the following for any (Xi, Yi) ∈ D: |ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ0⟩, Yi \u0001 − ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 | ≤G∥⟨δ0 − δ, G(Xi)⟩∥2 ≤ G vuut KX j=1 ∥δ0 − δ∥2 F ∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 ∥δ0 − δ∥2∗∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 4D2 · R2 = 2GRD √ K. Hence if we change only one data point (Xi, Yi) of g to (X ′ i, Y ′ i ), the deviation of ˆL(δ0) − ˆL(δ) is at most 2GRD √ K N . Then by Lemma C.1, we have sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) < E \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # + t √ 2KGRD√ N with probability greater than 1 − e−t2 . The expectation on the right hand side can be reduced to ED \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # = ED \" sup ∥δ∥∗≤D (−ˆL(δ) + L(δ)) + ˆL(δ0) − L(δ0) # = ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # Note that L(δ) − ˆL(δ) = L(δ) − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) = E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi), 18LoRA Training in the NTK Regime has No Spurious Local Minima where the expectation is taken over Xi ∼ P. Now apply Lemma C.2 to get ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # = ED \" sup ∥δ∥∗≤D (E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i# − ED \" 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # ≤ 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # where {σ}1≤i≤N are i.i.d Rademacher variables. Then apply Lemma C.3 to get 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # = 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. Finally, use Lemma C.4 to get 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   ≤ 2 √ 2G · RD √ K√ N . Therefore, we conclude that ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N (2 + t) . for ∥δ∥∗ ≤ D with probability greater than 1 − e−t2 . By reparametrization, we get ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . for ∥δ∥∗ ≤ D with probability greater than 1 − η. Now we can extend this generalization guarantee of constrained optimization to regularized optimization, which aligns with our problem of interest. For notational convenience, let Lλ(δ) = L(δ) + λ∥δ∥∗, ˆLλ(δ) = ˆL(δ) + λ∥δ∥∗ We follow the proof structure of (Bach, 2023), which was motivated by (Bartlett et al., 2005) and (Sridharan et al., 2008). Theorem C.6. Fix ε >0 and let 0 ̸= δ⋆ true ∈ argminδ L(δ) be the true optimum of the population risk and consider the setup of Lemma C.5 with D = (2 + ε)∥δ⋆ true∥∗, which is the upper bound on the nuclear norm of the predictors. Let η ∈ (0, 1) and λ = (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ).Consider the setup of Corollary 4.2 with P randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N \u0012 2 + r log 1 η \u0013 . 19LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let ˜ε = ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ and consider the convex set C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) ≤ λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Then for ∥δ∥∗ = 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, δ /∈ intC since the following inequalities hold. Lλ(δ) − Lλ(δ⋆ true) = L(δ) − L(δ⋆ true) + λ∥δ∥∗ − λ∥δ⋆ true∥∗ ≥ λ∥δ∥∗ − λ∥δ⋆ true∥∗ = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Therefore the boundary ∂C of C should be ∂C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Now suppose ˆuˆv⊺ /∈ C. Then since δ⋆ true ∈ C, there exists δ in the segment [ˆuˆv⊺, δ⋆ true] such that δ ∈ ∂C. By the convexity of ˆLλ, we have ˆLλ(δ) ≤ max \u0010 ˆLλ(δ⋆ true), ˆLλ(ˆuˆv⊺) \u0011 . Then we get ˆLλ(δ⋆ true) − ˆLλ(δ) ≥ −2˜ε∥δ⋆ λ∥∗ by Corollary 4.2. Therefore, ˆL(δ⋆ true) − ˆL(δ) − L(δ⋆ true) + L(δ) = ˆLλ(δ⋆ true) − ˆLλ(δ) − Lλ(δ⋆ true) + Lλ(δ) ≥ Lλ(δ) − Lλ(δ⋆) − 2˜ε∥δ⋆ λ∥∗ (3) = λ∥δ⋆ true∥∗ Note that ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗ < (2 + ε)∥δ⋆ true∥∗ and λ∥δ⋆ true∥∗ = ∥δ⋆ true∥∗ (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Then by Lemma C.5, (3) should happen with probability less than η. Then with probability greater than 1 − η, ˆuˆv⊺ ∈ C. In other words, Lλ(ˆuˆv⊺) − Lλ(δ⋆ true) < λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Hence, L(ˆuˆv⊺) + λ∥ˆuˆv⊺∥∗ < Lλ(δ⋆ true) + λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ = L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ ≤ L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + ελ∥δ⋆ true∥∗. Finally, we get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N   2 + r log 1 δ ! . By using the fact that ℓCE is Lipschitz continuous, we can reduce Theorem C.6 to Theorem 5.1. Note that the loss function ℓ may not be Lipschitz continuous in general. However, Lipschitz continuity is a mild assumption when the domain is restricted to a bounded class of predictors AD of Lemma C.5. Proof of Theorem 5.1. If ℓ(·, Y): RK → R is cross entropy loss defined as ℓ(X, Y) = ℓCE (X, Y) = −log   exp X(j) PK i=1 exp X(i) ! = −X(j) + log  KX i=1 exp X(i) ! 20LoRA Training in the NTK Regime has No Spurious Local Minima with true label Y = j, we have ∇ℓCE (X, Y)j = −1 + exp X(j) PK i=1 exp X(i) = − P i̸=j exp X(Y ) PK i=1 exp X(i) and for k ̸= j, ∇ℓCE (X, Y)k = exp X(k) PK i=1 exp X(Y ) Then we can bound the Euclidean norm of the gradient as follows. ∥∇ℓCE (X, Y)∥2 2 = \u0010P i̸=j exp X(i) \u00112 \u0010PK i=1 exp X(i) \u00112 + P i̸=j exp 2X(k) \u0010PK i=1 exp X(i) \u00112 ≤ 1 + 1 = 2. Hence the gradient of the cross entropy loss is bounded by √ 2 and we may replace G in Theorem C.6 with √ 2 to get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N   2 + r log 1 δ ! . D. Details of experiments Optimizing nuclear norm. Recall that SGD or GD on the loss function with weight decay and with regularization parameter λ is equivalent to minimizing 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , with respect to u and v. In full fine-tuning however, this is equivalent to minimize the following with respect to δ: 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 + λ∥δ∥∗. The problem here is that gradient methods no longer apply since the nuclear norm is non-differentiable. Therefore, we use the proximal gradient method: δt+1 = proxαλ∥·∥∗(δt − α∇ˆL(δt)) where proxαλ∥·∥∗(δ) = argmin δ′ \u0012 λ∥δ ′ ∥∗ + 1 2α∥δ′ − δ∥2 F \u0013 . It is well known that the proximal gradient method on convex objective converges to a global minimum (Polyak, 1987). Hyperparameters on NLP tasks For NLP tasks, we use full batch to perform GD on training. We only train the query (Wq) and value (Wv) weights of the RoBERTa-base model, which was empirically shown to have good performance (Hu et al., 2021). Furthermore, calculating the proximal operator of a nuclear norm is a computational bottleneck during the training of all Wq and Wv matrices. Therefore, we limit our training to only the last layer of Wq and Wv. To ensure a fair comparison, we apply the same approach to the LoRA updates. Additional information is in Table 1. Hyperparameters on image and speech classification tasks Similar to NLP tasks, we train the last attention layers. Further details are in Table 2. 21LoRA Training in the NTK Regime has No Spurious Local Minima Task SST-2,QNLI MR,CR,QQP,Subj Batch size 32 32 Learning rate (Full, LoRA fine tuning) 0.0005 0.001 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0.01 0.01 Table 1.Hyperparameters on experiment in Section 6 (NLP tasks) Task Image classification Speech classification Batch size 16 16 Learning rate (Full, LoRA fine tuning) 0.005 0.005 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0 0.001 Table 2.Hyperparameters on experiment in Section 6 (Image and speech classification tasks) (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 4.Test curves (accuracy vs. epochs) on different NLP tasks. We used the LoRA rank of 16. Test accuracy. For the setting of Section 6 on NLP tasks, we additionally conduct evaluations on a test set of 1000 samples during training and present the results in Figure 4. We observed that in most tasks the performance using LoRA eventually converges a test accuracy that matches that of full fine-tuning, although the rates of convergence sometimes differ. We list the hyperparameters in Table 3 Task SST-2,QQP,MR,CR Subj QNLI Batch size 32 32 24 Learning rate (Full, LoRA fine tuning) 0.0001 0.001 0.0005 Trained layer Wq, Wv (all layers) Wq, Wv (all layers) Wq, Wv (all layers) Weight decay 0.005 0.005 0.005 Table 3.Hyperparameters on experiment in Figure 4 22LoRA Training in the NTK Regime has No Spurious Local Minima For image and speech classification tasks, we also validate the performance of our linearized update to confirm that the accuracy is on par with actual LoRA updates. Accuracies are averaged over 3 runs (See Table 4). Task Image classification Speech classification Accuracy ( actual / linearized) 86.20 / 87.00 74.67 / 73.67 Table 4.Accuaricies of LoRA updates on vision and speech classification tasks 23",
      "meta_data": {
        "arxiv_id": "2402.11867v3",
        "authors": [
          "Uijeong Jang",
          "Jason D. Lee",
          "Ernest K. Ryu"
        ],
        "published_date": "2024-02-19T06:22:09Z",
        "pdf_url": "https://arxiv.org/pdf/2402.11867v3.pdf",
        "github_url": "https://github.com/UijeongJang/LoRA-NTK"
      },
      "llm_extracted_info": {
        "main_contributions": "This research theoretically analyzes LoRA fine-tuning in the Neural Tangent Kernel (NTK) regime. The main contributions are: (i) full fine-tuning (without LoRA) admits low-rank solutions of rank r ≲ √ N; (ii) using LoRA with rank r ≳ √ N eliminates spurious local minima, allowing (stochastic) gradient descent to find the low-rank solutions; and (iii) the low-rank solution found using LoRA generalizes well.",
        "methodology": "The study analyzes LoRA fine-tuning in the Neural Tangent Kernel (NTK) regime, approximating network changes with first-order Taylor expansions. It models LoRA's low-rank parameterization with weight decay as a nuclear-norm regularized optimization problem. Key techniques involve analyzing the optimization landscape by characterizing second-order stationary points (SOSPs) using matrix factorization theory and a perturbed loss function. The absence of spurious local minima is proven using Sard’s theorem and dimension-counting arguments. Generalization bounds are established using Rademacher complexity.",
        "experimental_setup": "The experiments validate the theory by fine-tuning linearized pre-trained models. For NLP tasks, a linearized RoBERTa-base model is fine-tuned on datasets like SST-2, QNLI, MR, CR, Subj, and QQP (N=32, K=2) using prompt-based fine-tuning and cross-entropy loss, comparing LoRA (r ≥ 11) against full fine-tuning. Image classification uses a pre-trained vision transformer on the bean disease dataset (N=48, 3 labels), and speech classification uses a pre-trained wav2vec2 model on the SUPERB dataset (N=64, 4 labels). All experiments monitor training loss curves and test accuracy (for NLP), with image and speech tasks also reporting accuracies. Training focuses on the last layer's Wq and Wv weights.",
        "limitations": "The theoretical analysis primarily relies on the Neural Tangent Kernel (NTK) regime, which is a key assumption that may not hold for all fine-tuning scenarios. The proof for the absence of spurious local minima requires a small, randomly generated perturbation to the loss function, which is not applied in practical LoRA fine-tuning; thus, the theoretical result is interpreted as applying 'generically' to the unperturbed case. The current theory also does not explain or predict the observed differences in convergence rates, particularly the slowdown with lower LoRA ranks, suggesting the loss landscape might still contain unfavorable regions like plateaus or saddles despite lacking spurious local minima. The provided guarantees are upper bounds, indicating an initial step in understanding LoRA dynamics.",
        "future_research_directions": "Future work includes conducting more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, gaining a better understanding of the minimum rank requirement through lower bounds, and analyzing the tradeoff between training rate and LoRA rank, especially considering the observed phenomenon where lower LoRA ranks slow down convergence.",
        "experimental_code": "class LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))  # scaling the initailization by \\frac{1}{sqrt{r}}\n           #torch.nn.init.kaiming_normal_(param)\n      \n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])     \n            \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) \n        self.gradient_dtype = gradient[0].dtype\n\n        # Compute \\langle G(X_i), B*A \\rangle \n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n            \n        def convert_to_buffer(name):\n            if layer_name in name:\n                #logger.info(\"Including {}\".format(name))\n                return False\n            else:\n                return True\n        \n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n  \n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n                \n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n    \n            \n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n            \n        targets.append(label)\n    \n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))\n\n    def finetune(self, train_dataset, eval_dataset):\n        \n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr) # Weight decay will be implented manually\n\n        if self.args.fp16 and _use_apex:\n            if not transformers.is_apex_available():\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            model, optimizer = amp.initialize(self.lora_model, optimizer, opt_level=self.args.fp16_opt_level)\n\n        # Multi-gpu training (should be after apex fp16 initialization)\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n        \n        epoch_count = 0 \n        \n        #Make sure to freeze other parameters\n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False)\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False)\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False)\n         \n        for name, param in self.lora_model.named_parameters():\n            if param.requires_grad:\n                print(f\"{name} is being trained\")\n                \n                \n        writer = SummaryWriter(f\"./finallasttrain/{self.model.model_args.apply_lora}-{self.args.linear_lr}-{self.model.data_args.task_name}-{self.args.seed}-{self.model.model_args.lora_r}\")\n        \n        file_exists = False\n        eval_file_exists = False\n        saved_gradients_eval = []\n        saved_gradients = []\n        for epoch in range(self.args.linear_num_epoch):\n            \n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n            \n            if self.args.eval_during_training:\n                total_loss_eval = 0\n                eval_preds=[]\n                eval_targets_list=[]\n                \n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                    \n                \n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1) \n                    self.gradient_dtype = gradient[0].dtype\n                    \n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)   \n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                ## Weight decay\n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001: #Skip this if there is no weight decay (weight decay = 0)\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)  #Soft-thresholding operator \n                            param = u@s@v\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param\n                \n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:  #Skip this if there is no weight decay (weight decay = 0)\n                            reg += (1/2)*(param ** 2).sum()  \n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            reg += torch.norm(param, p = 'nuc')\n                            \n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item()\n                            \n            file_exists = True\n                          \n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd) \n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n            writer.add_scalar(f\"train_loss_{self.model.data_args.task_name}/epoch\", avg_loss, epoch)\n            \n            # Do evaluation during training if needed.\n            if self.args.eval_during_training: \n                with torch.no_grad():\n                    \n                    for i, inputs_outer in enumerate(tqdm(dataloader_outer_eval, desc=\"Evaluating\")):\n                               \n                        if eval_file_exists:\n                            gradient_eval = saved_gradients_eval[i]\n                            for j in range(len(self.target_layers)):\n                                gradient_eval[j].to(self.args.device)\n                                gradient_eval[j].requires_grad_(False)\n                                \n                        else:\n                            gradient_eval, _  = self.compute_gradient_sharded(inputs_outer)\n                            for j in range(len(self.target_layers)):  \n                                gradient_eval[j].requires_grad_(False)\n                        \n                        \n                        output_eval = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient_eval)\n                        eval_logits, eval_targets = self.compute_model_logits(inputs_outer)\n                        eval_logits = eval_logits.to(self.args.device)\n                            \n                        loss_eval = nn.CrossEntropyLoss(reduction = 'sum')(output_eval + eval_logits , eval_targets) \n                        dist.all_reduce(loss_eval, op=dist.ReduceOp.SUM) \n                        total_loss_eval += loss_eval.item()\n                        \n                        eval_preds.append( eval_logits + output_eval )\n                        eval_targets_list.append(eval_targets)\n                        \n                        saved_gradients_eval_cpu = []\n                        for j in range(len(self.target_layers)):\n                            gradient_eval_layer= gradient_eval[j].detach().cpu()\n                            saved_gradients_eval_cpu.append(gradient_eval_layer)\n                        saved_gradients_eval.append(saved_gradients_eval_cpu)\n                        \n                       \n                eval_file_exists = True\n\n                avg_loss_eval = (total_loss_eval / len(dataloader_outer_eval.dataset) )+ (reg * self.args.linear_wd)\n                logger.info(f\"epoch : {epoch+1} eval_loss : {avg_loss_eval}\")\n                writer.add_scalar(f\"eval_loss_{self.model.data_args.task_name}/epoch\", avg_loss_eval, epoch)\n                \n                eval_preds = torch.cat(eval_preds, dim=0)\n                eval_targets = torch.cat(eval_targets_list, dim=0)\n\n                if self.args.local_rank != -1:\n                    logger.info(\"Starting to gather kernel across GPUs\")\n                    eval_preds = varsize_tensor_all_gather(eval_preds.to(self.args.device), torch.distributed.get_world_size())\n                    eval_targets = varsize_tensor_all_gather(eval_targets.to(self.args.device), torch.distributed.get_world_size())\n                    logger.info(\"Finished gathering kernel across GPUs\")\n\n                # Now calculate the accuarcies\n                metrics = None\n                eval_preds = eval_preds.cpu()\n                eval_targets = eval_targets.cpu()\n                \n                if self.args.binary_classification: # Make sure to compute loss before this transformation!\n                    eval_preds = torch.cat([-eval_preds, eval_preds], dim=-1) # convert back to two logits\n                    eval_targets = ((eval_targets + 1) / 2).long() # convert back from {-1, 1} to {0, 1}\n\n                if self.compute_metrics is not None:\n                    metrics = self.compute_metrics(EvalPrediction(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy()))\n\n                # Prefix all keys with eval_\n                for key in list(metrics.keys()):\n                    if not key.startswith(\"eval_\"):\n                        metrics[f\"eval_{key}\"] = metrics.pop(key)\n                metrics[\"eval_loss\"] = avg_loss\n\n                metrics.update(self.metrics())\n                metrics[\"grad_dim\"] = self.grad_dim\n\n                output = PredictionOutput(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy(), metrics=metrics)\n                metrics = output.metrics\n                objective = default_dev_objective(metrics)\n                logger.info(f\"epoch : {epoch+1}  objective : {objective}\")\n                writer.add_scalar(f\"Eval_acc_{self.model.data_args.task_name}/epoch\", objective, epoch)\n                         \n        writer.flush()\n        writer.close()\n        self.save_model(self.args.output_dir)\n        return avg_loss.item() , epoch_count",
        "experimental_info": "LoRA Configuration:\n- `apply_lora`: True (use LoRA for finetuning)\n- `lora_alpha`: 1 (initialization scale for one of the low-rank matrices in LoRA)\n- `lora_r`: 8 (inner rank for LoRA matrices)\n\nLinearized Training Configuration:\n- `trainer`: \"linearized\" (specifies `LinearizedLoraTrainer`)\n- `linear_freeze_A`: False (do not freeze layer A in LoRA)\n- `linear_lr`: 0.01 (learning rate of linearized LoRA fine-tuning)\n- `linear_num_epoch`: 30 (number of epochs in linearized LoRA training)\n- `linear_wd`: 0 (weight decay in linearized LoRA, 0 implies nuclear-norm regularization using soft-thresholding if `apply_lora` is false, otherwise standard L2 decay on LoRA parameters)\n- `eval_during_training`: False (evaluation during the training process)\n- `train_last_layer`: True (train the last layer only for gradient computation)\n- `binary_classification`: False (if number of classes is 2, convert two softmax logits to single sigmoid logit if needed)"
      }
    },
    {
      "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
      "abstract": "Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.",
      "full_text": "LoRA Training in the NTK Regime has No Spurious Local Minima Uijeong Jang 1 Jason D. Lee 2 Ernest K. Ryu 3 Abstract Low-rank adaptation (LoRA) has become the stan- dard approach for parameter-efficient fine-tuning of large language models (LLM), but our the- oretical understanding of LoRA has been lim- ited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with N data points, showing: (i) full fine- tuning (without LoRA) admits a low-rank solu- tion of rank r ≲ √ N; (ii) using LoRA with rank r ≳ √ N eliminates spurious local minima, allow- ing (stochastic) gradient descent to find the low- rank solutions; (iii) the low-rank solution found using LoRA generalizes well. 1. Introduction The modern methodology of using large language models involves (at least) two phases: self-supervised pre-training on a large corpus followed by supervised fine-tuning to the downstream task. As large language models have grown in scale, pre-training has become out of reach for research groups without access to enormous computational resources. However, supervised fine-tuning remains feasible for such groups. One key strategy facilitating this efficient fine- tuning is Parameter-Efficient Fine-Tuning (PEFT), which freezes most of the pre-trained model’s weights while selec- tively fine-tuning a smaller number of parameters within an adapter module. Among various PEFT methodologies, low- rank adaptation (LoRA) (Hu et al., 2021) has emerged as the standard approach. Given a pre-trained matrixW0 ∈ Rm×n, LoRA trains a low-rank update such that the forward pass evaluates W0x + ∆W x= W0x + BAx 1Department of Mathematical Sciences, Seoul National Uni- versity 2Department of Electrical and Computer Engineering, Princeton University 3Department of Mathematics, University of California, Los Angeles. Correspondence to: Ernest Ryu <eryu@math.ucla.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). where r ≪ min(m, n), A ∈ Rr×n is initialized to be a random Gaussian, and B ∈ Rm×r is initialized to be zero. However, despite the widespread adoption of LoRA, our theoretical understanding of its mechanisms remains limited. One notable prior work is (Zeng & Lee, 2024), which ana- lyzes the expressive power of LoRA, showing that for any given function, there exist weight configurations for LoRA that approximate it. However, their work does not address whether LoRA can efficiently learn such configurations. Additionally, Malladi et al. (2023) experimentally demon- strated that under certain conditions, LoRA fine-tuning is nearly equivalent to a kernel regression, where the A matrix provides random features and is essentially not trained. This regime neglects the possibility of the A matrix learning new features and, consequently, leads to a LoRA rank require- ment of r ≥ Θ(1/ε2), where ε is an approximation toler- ance, originating from the use of the Johnson–Lindenstrauss lemma (Johnson & Lindenstrauss, 1984). Crucially, LoRA’s fundamental nature as a quadratic parameterization has not been considered in the prior analysis of trainability and generalizability. Contribution. In this work, we theoretically analyze LoRA fine-tuning and present results on trainability and generalizability. We consider fine-tuning a deep (trans- former) neural network with K-dimensional outputs using N training (fine-tuning) data points. Assuming that train- ing remains under the NTK regime, which we soon define and justify in Section 2, we show the following. First, full fine-tuning (without LoRA) admits a rank-r solution such that r(r+1) 2 ≤ KN . Second, using LoRA with rank r such that r(r+1) 2 > KN eliminates spurious local minima, al- lowing (stochastic) gradient descent to find the low-rank solutions. Finally, the low-rank solution found using LoRA generalizes well. 1.1. Prior works Theory of neural networks. The question of expressive power addresses whether certain neural networks of interest can approximate a given target function. Starting with the classical universal approximation theorems (Cybenko, 1989; Hornik et al., 1990; Barron, 1993), much research has been conducted in this direction. (Delalleau & Bengio, 2011; 1 arXiv:2402.11867v3  [cs.LG]  28 May 2024LoRA Training in the NTK Regime has No Spurious Local Minima Bengio & Delalleau, 2011; Lu et al., 2017; Duan et al., 2023). These can be thought of as existence results. The question of trainability addresses whether one can com- pute configurations of neural networks that approximate tar- get functions. Ghadimi & Lan (2013); Ge et al. (2015); Du et al. (2017); Jin et al. (2017) studied general convergence results of gradient descent and stochastic gradient descent. Soltanolkotabi et al. (2018); Du & Lee (2018); Allen-Zhu et al. (2019a;b); Du et al. (2019); Zou et al. (2020) stud- ied the loss landscape of neural networks and showed that first-order methods converge to global minima under certain conditions. The question of generalization addresses whether neural net- works trained on finite data can perform well on new unseen data. Classical learning theory (Koltchinskii & Panchenko, 2000; Bartlett et al., 2002; Bousquet & Elisseeff, 2002; Hardt et al., 2016; Bartlett et al., 2017) uses concepts such as uniform stability or the Rademacher complexities to ob- tain generalization bounds. Generalization bounds in the context of modern deep learning often utilize different ap- proaches (Wu et al., 2017; Dinh et al., 2017; Zhang et al., 2021), we use the Rademacher complexity for obtaining our generalization results. Neural tangent kernels. The theory of neural tangent kernel (NTK) concerns the training dynamics of certain infinitely wide neural networks. Jacot et al. (2018) shows that the training of an infinitely wide neural network is equivalent to training a kernel machine. Various studies such as (Arora et al., 2019; Chen et al., 2020) expand the NTK theory to more practical settings. Among these works, Wei et al. (2022a) introduced the concept of empirical NTK (eNTK) and showed that kernel regression with pretrained initialization also performs well on real datasets, providing a background to utilize NTK theory in fine-tuning. Theory of transformers and LLMs. As the transformer architecture (Vaswani et al., 2017) became the state-of-the- art architecture for natural language processing and other modalities, theoretical investigations of transformers have been pursued. Results include that transformers are uni- versal approximators (Yun et al., 2019), that transformers can emulate a certain class of algorithmic instructions (Wei et al., 2022b; Giannou et al., 2023), and that weight matrices in transformers increase their rank during training (Boix- Adsera et al., 2023). Also, (Zhang et al., 2020; Liu et al., 2020) presents improved adaptive optimization methods for transformers. PEFT methods and LoRA. Low-rank adaptation (LoRA) (Hu et al., 2021) has become the standard Parameter- Efficient Fine-Tuning (PEFT) method, and many variants of LoRA have been presented (Fu et al., 2023; Dettmers et al., 2023; Lialin et al., 2023). LoRA has proven to be quite versatile and has been used for convolution layers (Yeh et al., 2024) and for diffusion models (Ryu, 2023; Smith et al., 2023; Choi et al., 2023). Theoretically, Aghajanyan et al. (2021) found an intrinsic low-rank structure is critical for fine-tuning language mod- els, although this finding concerns full fine-tuning, not the setting that uses LoRA. Recently, Zeng & Lee (2024) an- alyzed the expressive power of LoRA. However, we still lack a sufficient theoretical understanding of why LoRA is effective in the sense of optimization and generalization. Matrix factorization. In this work, we utilize techniques developed in prior work on matrix factorization problems. Bach et al. (2008); Haeffele et al. (2014) established the sufficiency of low-rank parameterizations in matrix factor- ization problems, and their techniques have also been used in matrix completion (Ge et al., 2016), matrix sensing (Jin et al., 2023), and semidefinite programming (Bhojanapalli et al., 2018). 1.2. Organization Section 2 introduces the problem setting and reviews rel- evant prior notions and results. Section 3 proves the exis- tence of low-rank solutions. Section 4 proves LoRA has no spurious local minima and, therefore, establishes that (stochastic) gradient descent can find the low-rank global minima. Section 5 shows that the low-rank solution gener- alizes well. Finally, Section 6 presents simple experiments fine-tuning pre-trained models for different modalities. The experimental results validate our theory and provide further experimental insights. 2. Problem setting and preliminaries We primarily consider the setup of pre-trained large lan- guage models fine-tuned with LoRA. However, our theory does generally apply to other setups that utilize pre-training and LoRA fine-tuning, such as diffusion models. Matrix notation. For matrices A and B, let ∥A∥∗ denote the nuclear norm, ∥A∥F the Frobenius norm, and ⟨A, B⟩ = tr(A⊺B) the matrix inner product. We letSn and Sn + for the set of n × n symmetric and positive semi-definite matrices, respectively. Let R(·) and N(·) respectively denote the range and the null-space of a linear operator. Neural network. Let fΘ : X →RK be a neural network (e.g., a transformer-based model) parametrized by Θ, where X is the set of data (e.g., natural language text) and RK is the output (e.g., pre-softmax logits of tokens). K is the output dimension of fΘ, where K = k for k-class classification, K = 1 for binary classification, and K is the 2LoRA Training in the NTK Regime has No Spurious Local Minima dimension of the label Y when using mean square error loss. Assume the model has been pre-trained to Θ = Θ0, i.e., the pre-trained model is fΘ0 . Let W = ( W(1), . . . , W(T)) ⊂ Θ be a subset of the weights (e.g., dense layers in QKV-attention) with size W(i) ∈ Rmi×ni for i = 1, . . . , Tthat we choose to fine- tune. Let W0 = (W(1) 0 , . . . , W(T) 0 ) ⊂ Θ0 be their corre- sponding pre-trained weights. With slight abuse of notation, write fW to denote fΘ, where all parameters ofΘ excluding W are fixed to their corresponding values in Θ0. Fine-tuning loss. Assume we wish to fine-tune the pre- trained model with {(Xi, Yi)}N i=1, where N is the number of (fine-tuning) training data. (In many NLP tasks, it is not uncommon to have N < 100.) Denote δ = (δ(1), . . . , δ(T)) ⊂ Θ to be the change of W after the fine-tuning, i.e., fW0+δ is our fine-tuned model. We use the empirical risk ˆL(δ) = 1 N NX i=1 ℓ(fW0+δ(Xi), Yi), with some loss functionℓ. We assumeℓ(x, y) is convex, non- negative, and twice-differentiable with respect to x for any y. (This assumption holds for the cross-entropy loss and the mean squared error loss.) The empirical risk approximates the true risk L(δ) = E (X,Y )∼P \u0002 ℓ(fW0+δ(X), Y) \u0003 with some data distribution P. NTK regime. Under the NTK regime (also referred to as the lazy-training regime), the change of the network can be approximated by its first-order Taylor expansion fW0+δ(X) ≈ fW0 (X) + ⟨∇fW0 (X), δ⟩ (1) sufficiently well throughout (fine-tuning) training. To clarify, fW0+δ(X) ∈ RK, so the NTK regime requires the first- order Taylor expansion to be accurate for all coordinates: f(j) W0+δ(X) ≈ f(j) W0 (X) + ⟨∇f(j) W0 (X), δ⟩, where f(j) W is the j-th coordinate of fW for j = 1, . . . , K. The NTK regime is a reasonable assumption in fine-tuning if δ is small, and this assertion is supported by the empirical evidence of (Malladi et al., 2023). This prior work provides extensive experiments on various NLP tasks to validate that fine-tuning happens within the NTK regime for many, although not all, NLP tasks. Observation 2.1 (Malladi et al. (2023)) . When prompt- based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) is used, fine-tuning a pre-trained language model stays within the NTK regime. Motivated by this empirical observation, we define lin- earized losses ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001 ≈ ˆL(δ) and L(δ) = E (X,Y )∼P h ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001i ≈ L(δ). LoRA. We use the low-rank parameterization δ(i) = u(i)(v(i))⊺ ∈ Rmi×ni, where u(i) ∈ Rmi×r, v(i) ∈ Rni×r, for i ∈ {1, ··· , T}. Under the NTK regime, the empirical risk can be approxi- mated as ˆL(uv⊺) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 , where u =   u(1) ... u(T)   ∈ Rm×r, v =   v(1) ... v(T)   ∈ Rn×r with m = PT i=1 mi and n = PT i=1 ni, and G(Xi) = diag \u0000 ∇W(1) fW0 (Xi), . . . ,∇W(T) fW0 (Xi) \u0001 is an collection ofK m×n block diagonal matrices. To clar- ify, G(Xi) ∈ RK×m×n, so ⟨G(Xi), uv⊺⟩ ∈RK should be interpreted as K inner products of m ×n matrices where each matrices correspond to each coordinates of f. More specifically, G(j)(Xi) ∈ Rm×n and \u0000 ⟨G(Xi), uv⊺⟩ \u0001 j = ⟨G(j)(Xi), uv⊺⟩ for j = 1, . . . , K. Note that ˆL(uv⊺) under the NTK regime is non-convex in (u, v) so SGD-training does not converge to the global minimizer, in general. Weight decay on LoRA is nuclear norm regularization. The LoRA training of optimizing ˆL is often conducted with weight decay (Hu et al., 2021; Dettmers et al., 2023), which can be interpreted as solving minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , 3LoRA Training in the NTK Regime has No Spurious Local Minima with regularization parameter λ ≥ 0. This problem is equiv- alent to the rank-constrained nuclear-norm regularized prob- lem minimize δ, rankδ≤r ˆLλ(δ) ≜ ˆL(δ) + λ∥δ∥∗. This is due to the following lemma. Lemma 2.2 (Lemma 5.1 of (Recht et al., 2010)). Let r >0. For δ ∈ Rm×n such that rank(δ) ≤ r, ∥δ∥∗ = 1 2 min uv⊺=δ {∥u∥2 F + ∥v∥2 F |u ∈ Rm×r, v ∈ Rn×r}. (The connection between weight decay on Burer–Monteiro style low-rank factorization and nuclear norm regularization has been previously in different contexts not directly related to LoRA (Cabral et al., 2013; Pilanci & Ergen, 2020).) Second-order stationary points. Let ˆL: Rm×n → R be twice-continuously differentiable. We say U ∈ Rm×n is a (first-order) stationary point if ∇ˆL(U) = 0. We say U ∈ Rm×n is a second-order stationary point (SOSP) if ∇ˆL(U) = 0, ∇2 ˆL(U)[V, V] ≥ 0, for any direction V ∈ Rm×n. We say U is strict saddle if U is a first- but not second-order stationary point. Lastly, we say U ∈ Rm×n is a local minimum if there exists an open ball B that contains U and ˆL(U) ≤ ˆL(U′) for any U′ ∈ B. It follows that a local minimum is an SOSP. The following results, roughly speaking, establish that (stochastic) gradient descent only converges to SOSPs when a loss function is twice-continuously differentiable. Theorem 2.3 (Theorem 4.1 of (Lee et al., 2016)). Gradient descent on twice-differentiable function with random ini- tialization, almost surely, does not converge to strict saddle points. I.e., if gradient descent converges, it converges to an SOSP , almost surely. Theorem 2.4 (Informal, Theorem 1 of (Ge et al., 2015)). Stochastic gradient descent with noise on twice- differentiable strict saddle function (i.e., every stationary point is either a local minimum or a strict saddle) does not converge to strict saddle points with high probability. I.e., if stochastic gradient descent with noise converges, it converges to an SOSP with high probability. Therefore, if we can show that all SOSPs are global minima in our setup of interest, then (stochastic) gradient descent will only converge to global minima. 3. Low-rank solution exists In this section, we show that full fine-tuning in the NTK regime admits a low-rank solution of rank r ≲ √ N. The existence of a low-rank solution provides theoretical legit- imacy to using the low-rank parameterization of LoRA, which, of course, can only find low-rank solutions. Theorem 3.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global minimizer (not necessarily unique). Then there is a rank-r solution such that r(r+1) 2 ≤ KN . The assumption that ˆLλ(δ) has a global minimum is very mild; it is automatically satisfied if λ >0. When λ = 0, the assumption holds if ℓ is the mean squared error loss. The inspiration for Theorem 3.1 comes from the classical results of (Barvinok, 1995; Pataki, 1998; 2000) that establish that semi-definite programs (which have symmetric positive semi-definite matrices as optimization variables) admit low- rank solutions. We clarify that Theorem 3.1 does not require δ to be symmetric nor any notion of “semi-definiteness” (δ is not even square). Proof sketch of Theorem 3.1. We quickly outline the key ideas of the proof while deferring the details to Appendix A. We can show that finding δ⋆ λ ∈ argminδ ˆLλ(δ) with rank(δ⋆ λ) = r is equivalent to finding a rank-r global mini- mum of F : S(m+n) + → R where F(Z) = ˆL( ¯Z) + λ 2 tr(Z) and ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of Z such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . (2) Now suppose Z⋆ ∈ S(m+n) + is a global minimizer of F. Define S(Z⋆) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆)} and a linear operator A: S(m+n) → RKN as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Now let rank(Z⋆) = r and assume {0} = S(Z⋆) ∩ N(A). Then by dimension counting, we have the following inequal- 4LoRA Training in the NTK Regime has No Spurious Local Minima Figure 1.Geometric intuition of Theorem 3.1. The three dimensional space describes the space of 2 by 2 matrices \u0014 1 x y z \u0015 . The surface z = xy represents the rank 1 matrices. The blue region on the surface correspond to the region of smaller objective values, and the set of global minima are depicted with purple. (Left) Plot of (a) with N = 1. The set of global minima is a plane, and the intersection with the surface z = xy (curve) is the set of rank-1 global minima. (Middle) Plot of (b) with N = 2. the set of global minima is a line, and the intersection with the surface (two dots) is the set of rank 1 global minima. (Right) Plot of (c) with N = 3. The set of global minima is a line, and there is no intersection with the surface, i.e., there is no global minimum of rank-1 but admits a rank-2 global minima. ity. 0 = dimS(Z⋆) + dimN(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(m+n)) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆) − KN If there exists nonzero Z ∈ S(m+n) such that Z ∈ S(Z⋆) ∩ N(A), then we can show that there exists nonzero t ∈ R such that Z⋆ + tZ is also a global minimizer of F with strictly lower rank. Replace Z⋆ with Z⋆ + tZ and repeat this process until we find a solution Z⋆ with {0} = S(Z⋆) ∩ N(A). Together with the fact that dimS(Z⋆) = r(r+1) 2 , we have the desired result. Illustration of Theorem 3.1. The following toy example illustrates the geometric intuition of Theorem 3.1. Let ℓ be the mean square error loss, K = 1, δ = \u0014w x y z \u0015 , and λ = 0 (no regularization). Then consider the following objective functions each for N = 1, 2, and 3: ˆL0(δ) = (x + y)2 (a) ˆL0(δ) = 1 2(z + 4)2 + 1 2(x + y)2 (b) ˆL0(δ) = 1 3(w − 1)2 + 1 3(z − 4)2 + 1 3( √ 3x + √ 3y)2 (c) The set of low-rank (rank-1) solutions for the three objec- tives are depicted in Figure 1. 4. GD and LoRA finds low-rank solution In this section, we show that the optimization landscape with LoRA in the NTK regime has no spurious local minima if the LoRA parameterization uses rank r ≳ √ N and if we consider an ε-perturbed loss. This implies that optimizers such as stochastic gradient descent only converge to the low-rank global minimizers. Theorem 4.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global min- imizer (not necessarily unique) and r(r+1) 2 > KN. Con- sider the perturbed loss function ˆLλ,P defined as ˆLλ,P (u, v) ≜ ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 ∈ R(m+n)×r and P ∈ S(m+n) + is pos- itive semi-definite. Then, for almost all nonzero P (with respect to the Lebesgue measure on S(m+n) + ⊂ S(m+n) ∼= R (m+n)(m+n+1) 2 ), all SOSPs of ˆLλ,P are global minimizers of ˆLλ,P . To clarify, the conclusion that ‘all SOSPs are global mini- mizers’ holds with probability 1 even if the distribution of P is supported on {P ∈ S(m+n) + : ∥P∥ ≤ε} for arbitrarily small ε >0. In the practical LoRA fine-tuning setup where no perturbation is used and P = 0 is set deterministically, Theorem 4.1 does not apply. However, we can neverthe- less interpret the result of Theorem 4.1 to show that LoRA fine-tuning generically has no spurious local minima. 5LoRA Training in the NTK Regime has No Spurious Local Minima If we do use a randomly generated small perturbation P so that Theorem 4.1 applies, the solution to the perturbed problem with small P does not differ much from that of the unperturbed problem with P = 0 in the following sense. Corollary 4.2. Consider the setup of Theorem 4.1 and let ε > 0. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). Assume P is randomly sampled with a probability distribution supported in {P ∈ S(m+n) + : ∥P∥F < ε} and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Then for any SOSP (ˆu, ˆv) of ˆLλ,P ˆLλ(ˆuˆv⊺) ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗ = min δ ˆLλ(δ) + 2ε∥δ⋆ λ∥∗. I.e., if (ˆu, ˆv) is an SOSP (and thus a global minimizer by Theorem 4.1) of the perturbed loss ˆLλ,P , then it is an ε- approximate minimizer of the unperturbed loss ˆLλ. So if r(r+1) 2 > KN, then Theorem 2.3, Theorem 2.4, and Corollary 4.2 together establish that (stochastic) gradient descent finds a ˆuˆv⊺ such that its unperturbed empirical risk is ε-close to the the minimum unperturbed empirical risk. 4.1. Proof outlines The proof is done by continuing our analysis of global mini- mum of ˆLλ(δ). Given that low-rank solution exists, which we proved in the previous section, recall that LoRA training with weight decay is equivalent to solving argmin u,v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F . In this section, we relate SOSPs with global minimum, which opens the chance to find a global minimum by using gradient-based optimization methods. We start the analysis from the following lemma, which is a prior characterization of SOSPs in the matrix factorization. Lemma 4.3. (Theorem 2 of (Haeffele et al., 2014)) Let G: S(m+n) + → R be a twice differentiable convex function with compact level sets, H : S(m+n) + → R be a proper con- vex lower semi-continuous function, and r > 0. If the function F : U 7→ G(UU ⊺) +H(UU ⊺) defined over matri- ces U ∈ R(m+n)×r has a second order staionary point at a rank-deficient matrix U, then UU ⊺ is a global minimum of G + H. We build our analysis upon Lemma 4.3. However, Lemma 4.3 is not directly applicable to our setting since it requires that the SOSP must be rank-deficient. However, this can be effectively circumvented by employing a per- turbed empirical risk: minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 , and P is a positive semi-definite matrix. Now we get the following lemma by applying Lemma 4.3 to the perturbed empricial risk. Lemma 4.4. Fix λ ≥ 0. Assume ˆLλ(δ) has a global min- imum (not necessarily unique), P ∈ S(m+n) + is nonzero positive semi-definite, and r >0. If ˆQ = \u0014ˆu ˆv \u0015 ∈ R(m+n)×r is a rank deficient SOSP of ˆLλ,P (u, v) = ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, then ˆQ is a global minimum of ˆLλ,P (u, v). Proof. Define G, H: S(m+n) + → R to be G(X) = λ 2 tr(X) + ⟨P, X⟩, H (X) = ˆL( ¯X) where ¯X is the off-diagonal submatrix of X defined in (2). Note that G has compact level set for every λ ≥ 0 since tr(X) ≥ 0 and P, Xare positive semi-definite, concluding that ˆQλ,P is a global minimum of F(Q) ≜ G(QQ⊺) + H(QQ⊺) = ˆLλ,P (u, v). We now give a detailed analysis of the proof of Theorem 4.1. The structure of the proof is inspired by the original work of Pataki (1998) and followed by Burer & Monteiro (2003); Boumal et al. (2016); Du & Lee (2018). The proof uses an application of Sard’s theorem of differential geometry. The argument is captured in Lemma 4.5, and its proof is deferred to Appendix B. Lemma 4.5. Let M be m-dimensional smooth manifold embedded in Rd and V be a linear subspace of Rd with dimension n. If m + n < d, then the set M + V = {p + v : p ∈ M, v∈ V } has Lebesgue measure zero in Rd. Proof of Theorem 4.1. We show that second-order station- ary point ˆQλ,P = \u0014ˆu ˆv \u0015 is rank-deficient for almost all posi- tive semi-definite P, then use Lemma 4.4 to complete the proof. Denote f(j) for the j-th coordinate of f. For simplic- ity of notations, define ˆY (j) i ≜ f(j) W0 (Xi) + ⟨G(j)(Xi), uv⊺⟩, 6LoRA Training in the NTK Regime has No Spurious Local Minima and v(j) i ≜ 1 N ∂ ∂ ˆY (j) i ℓ( ˆYi, Yi) for 1 ≤ i ≤ N and 1 ≤ j ≤ K, which depends on u and v. Then for v = {v(j) i } ∈RKN define S(v) ≜ NX i=1 KX j=1 v(j) i G(j)(Xi) ∈ Rm×n. Then by first-order gradient condition, we have  \u0014 0 S(v) S(v)⊺ 0 \u0015 + λI + P | {z } ≜M ! ˆQλ,P = 0 We observe that the range of ˆQλ,P ∈ R(m+n)×r is in the nullspace of M ∈ S(m+n). We now suppose ˆQλ,P has full rank, i.e., rank( ˆQλ,P ) = r. Hence, we have the following inequality: r = rank( ˆQλ,P ) ≤ dim N(M) ≤ m + n Now for r ≤ s ≤ m + n and s ∈ Z, define As = n P : P = M − λI, M∈ S(m+n), dimN(M) = s o . Then from Proposition 2.1 of (Helmke & Shayman, 1995), As is a smooth manifold embedded in R (m+n)(m+n+1) 2 ∼= S(m+n) with dimension dimAs = (m + n + 1)(m + n) 2 − s(s + 1) 2 . Now by definition of P, we know that P ∈ m+n[ s=r (As + R(S)) where “ + ”is the set-sum (Minkowski sum) and R(S) is the range of S(v) in R (m+n)(m+n+1) 2 for any v ∈ RKN . The dimensions can be bounded by dimAs ≤ (m + n)(m + n + 1) 2 − r(r + 1) 2 for r ≤ s ≤ m + n and dimR(S) ≤ KN. Therefore given that r(r+1) 2 > KN, we have dimAs + dimR(S) < (m + n)(m + n + 1) 2 . Then, by Lemma 4.5, which is effectively an application of Sard’s theorem, we can conclude As + R(S) is a measure- zero set, and the finite union of such measure-zero sets is measure-zero. This implies that every P that makes ˆQλ,P to be of full rank must be chosen from measure-zero subset of S(m+m) + ⊂ S(m+n). Therefore we may conclude that rank( ˆQλ,P ) < rfor almost every nonzero positive semi- definite P. Proof of Corollary 4.2. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). We observe the following chain of inequalities. ˆL(ˆδ) + λ∥ˆδ∥∗ ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F + ⟨P, ˆQ ˆQ⊺⟩ = ˆLλ,P (ˆu, ˆv), where the first inequality of is from Lemma 2.2, the second is from P and ˆQ ˆQ⊺ being positive semi-definite. On the other hand, we can find u⋆ and v⋆ such that δ⋆ λ = u⋆v⋆⊺ and ∥δ⋆ λ∥∗ = 1 2 (∥u⋆∥2 F + ∥v⋆∥2 F ) by using Lemma 2.2. Now take Q⋆ = \u0014u⋆ v⋆ \u0015 , then we get ˆLλ,P (u⋆, v⋆) = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ⟨P, Q⋆Q⋆⊺⟩ ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆Q⋆⊺∥F ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥u⋆∥2 F + ε∥v⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗, where the first inequality is Cauchy–Schwartz inequality, and the second inequality is from sub-multiplicativity of ∥ · ∥F . Moreover by Theorem 4.1, ˆLλ,P (ˆu⋆, ˆv⋆) ≤ ˆLλ,P (u⋆, v⋆), and this happens for almost sure, since we sampled P from a probability distribution which is absolutely continuous with respect to the Lebesgue measure on R (m+n)(m+n+1) 2 ∼= S(m+n). 5. Low-rank LoRA solution generalizes well In this section, we establish a generalization guarantee for the low-rank solution obtained by minimizing the perturbed loss ˆLλ,P of Theorem 4.1. For simplicity, we restrict the following main result to the cross-entropy loss. General- ization guarantees for general convex, non-negative, and twice continuously differentiable losses, are provided as Theorem C.6 in Appendix C. Theorem 5.1. Assume ℓ is cross-entropy loss. Assume the population risk L has a minimizer (not necessarily unique) and denote it as δ⋆ true ∈ argminδ L(δ). Assume δ⋆ true ̸= 0. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely 7LoRA Training in the NTK Regime has No Spurious Local Minima (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 2.Training curves (training loss vs. epochs) on different NLP tasks. with respect to the random data X ∼ P. Let ε >0, η ∈ (0, 1), and λ = 2(2 + ε) √ KR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ). Consider the setup of Corollary 4.2 withP randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺)−L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N \u0012 2 + r log 1 η \u0013 . In the context of fine-tuning, where the target task is closely related to the pre-training task, it is natural to assume that δ⋆ true in Theorem 5.1 is “small”. The proof, deferred to Appendix C, utilizes standard arguments with Rademacher complexity. 6. Experiments In this section, we conduct simple experiments on fine- tuning linearized pre-trained models to validate our theory.1 1Code available at https://github.com/UijeongJang/LoRA-NTK. Experimental setup on NLP tasks. We use prompt-based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) and consider the same architecture and dataset as in (Malladi et al., 2023), which empirically verifies that with prompt- based fine-tuning, the fine-tuning dynamics stay within the NTK regime. We present the results of six NLP tasks that were also considered in (Malladi et al., 2023): sen- timent analysis (SST-2, MR, CR), natural language infer- ence (QNLI), subjectivity (Subj), and paraphrase detection (QQP). We optimize a linearized RoBERTa-base (Liu et al., 2019) model with dataset of size 32 ( N = 32) with two labels (K = 2) using cross entropy loss. With LoRA rank r ≥ 11, our theory guarantees that no spurious local min- ima exist. For a baseline comparison, we also perform full fine-tuning (without LoRA) on the linearized model. The training curves are presented in Figure 2, and additional details are provided in Appendix D. Results showing test accuracy are also presented in Appendix D. Experimental setup on image and speech classification tasks. We use a pre-trained vision transformer (Dosovit- skiy et al., 2021) and fine-tune it on the bean disease dataset (Makerere AI Lab, 2020) to perform an image classification task with 3 labels. We use dataset of size 48 with three labels. Similar to our experiments on NLP tasks, we find that training curves converge to the same loss value, where the rates of convergence differ. For speech classification, we use a pre-trained wav2vec2 (Baevski et al., 2020) model and fine-tune it on a SUPERB dataset (Yang et al., 2021) to perform a speech classification 8LoRA Training in the NTK Regime has No Spurious Local Minima (a) Image classification  (b) Speech classification Figure 3.Training curves (training loss vs. epochs) on image and speech classification tasks. task with 4 labels. We use a dataset of size 64 with four labels. We also find that the training curves converge to the same loss value. The details are the same as with the image classification task. The training curves of both image and speech data are pre- sented in Figure 3, and additional details are provided in Appendix D. Empirical observation. The experiments validate our the- ory as the training curves converge to the same globally optimal loss value. However, we do observe that therates of convergence differ. When the LoRA rank is higher or when full fine-tuning is performed and LoRA is not used, fine-tuning converges faster. Indeed, our theory ensures that spurious local minima do not exist, but it says nothing about how convex or favorable the landscape may or may not be. Our intuitive hypothesis is that using lower LoRA rank creates unfavorable regions of the loss landscape, such as plateaus or saddle points, and they slow down the gradient descent dynamics. If this hypothesis is generally true, we face an interesting tradeoff: lower LoRA rank reduces memory cost and per- iteration computation cost but increases the number of it- erations needed for convergence. Then, using a very low LoRA rank may be suboptimal not due to representation power, presence of spurious local minima, or poor general- ization guarantees, but rather due to unfavorable flat training landscapes slowing down convergence. Exploring this phe- nomenon and designing remedies is an interesting direction for future work. 7. Conclusion In this work, we present theoretical guarantees on the train- ability and generalization capabilities of LoRA fine-tuning of pre-trained models. Together with the work of Zeng & Lee (2024), our results represent a first step in theoretically analyzing the LoRA fine-tuning dynamics of pre-trained models by presenting guarantees (upper bounds). For future work, carrying out further refined analyses under more spe- cific assumptions, relaxing the linearization/NTK regime assumption through a local analysis, better understanding the minimum rank requirement through lower bounds, and, motivated by the observation of Section 6, analyzing the tradeoff between training rate and LoRA rank are exciting directions. Acknowledgments UJ and EKR were supported by the Samsung Science and Technology Foundation (Project Number SSTF-BA2101- 02) and the National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIP) [NRF- 2022R1C1C1010010]. JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. We thank Jungsoo Kang for the discussion on the proof of Lemma 4.5. We also thank Jisun Park for providing valuable feedback. Impact statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrin- sic dimensionality explains the effectiveness of language model fine-tuning. Association for Computational Lin- guistics, 2021. Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. International Conference on Machine Learning, 2019a. Allen-Zhu, Z., Li, Y ., and Song, Z. On the convergence rate of training recurrent neural networks. Neural Information Processing Systems, 2019b. Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely 9LoRA Training in the NTK Regime has No Spurious Local Minima wide neural net. Neural Information Processing Systems, 2019. Bach, F. Learning Theory from First Principles . Draft, 2023. Bach, F., Mairal, J., and Ponce, J. Convex sparse matrix factorizations. arXiv preprint arXiv:0812.1869, 2008. Baevski, A., Zhou, Y ., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Neural Information Processing Systems, 2020. Barron, A. R. Universal approximation bounds for super- positions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930–945, 1993. Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. Bartlett, P. L., Boucheron, S., and Lugosi, G. Model selec- tion and error estimation. Machine Learning, 48:85–113, 2002. Bartlett, P. L., Bousquet, O., and Mendelson, S. Local rademacher complexities. The Annals of Statistics, 33(4): 1497–1537, 2005. Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrally- normalized margin bounds for neural networks. Neural Information Processing Systems, 2017. Barvinok, A. I. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computational Geometry, 13:189–202, 1995. Bengio, Y . and Delalleau, O. On the expressive power of deep architectures. Algorithmic Learning Theory, 2011. Bhojanapalli, S., Boumal, N., Jain, P., and Netrapalli, P. Smoothed analysis for low-rank solutions to semidefi- nite programs in quadratic penalty form. Conference On Learning Theory, 2018. Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J. Transformers learn through gradual rank increase. Neural Information Processing Systems, 2023. Boumal, N., V oroninski, V ., and Bandeira, A. The non-convex Burer–Monteiro approach works on smooth semidefinite programs. Neural Information Processing Systems, 29, 2016. Bousquet, O. and Elisseeff, A. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. Burer, S. and Monteiro, R. D. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329– 357, 2003. Cabral, R., De la Torre, F., Costeira, J. P., and Bernardino, A. Unifying nuclear norm and bilinear factorization ap- proaches for low-rank matrix decomposition. Interna- tional Conference on Computer Vision, 2013. Chen, Z., Cao, Y ., Gu, Q., and Zhang, T. A generalized neu- ral tangent kernel analysis for two-layer neural networks. Neural Information Processing Systems, 2020. Choi, J. Y ., Park, J., Park, I., Cho, J., No, A., and Ryu, E. K. LoRA can replace time and class embeddings in diffusion probabilistic models. NeurIPS 2023 Workshop on Diffusion Models, 2023. Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of Control, Signals and Systems, 2(4):303–314, 1989. Delalleau, O. and Bengio, Y . Shallow vs. deep sum-product networks. Neural Information Processing Systems, 2011. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: efficient finetuning of quantized llms. Neural Information Processing Systems, 2023. Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y . Sharp minima can generalize for deep nets. International Con- ference on Machine Learning, 2017. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. Du, S. and Lee, J. On the power of over-parametrization in neural networks with quadratic activation. International Conference on Machine Learning, 2018. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. International Conference on Machine Learning, 2019. Du, S. S., Jin, C., Lee, J. D., Jordan, M. I., Singh, A., and Poczos, B. Gradient descent can take exponential time to escape saddle points. Neural Information Processing Systems, 2017. Duan, Y ., Ji, G., Cai, Y ., et al. Minimum width of leaky- relu neural networks for uniform universal approximation. International Conference on Machine Learning, 2023. 10LoRA Training in the NTK Regime has No Spurious Local Minima Fu, Z., Yang, H., So, A. M.-C., Lam, W., Bing, L., and Collier, N. On the effectiveness of parameter-efficient fine-tuning. AAAI Conference on Artificial Intelligence, 2023. Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. Association for Computational Linguistics, 2021. Ge, R., Huang, F., Jin, C., and Yuan, Y . Escaping from saddle points—online stochastic gradient for tensor de- composition. Conference on Learning Theory, 2015. Ge, R., Lee, J. D., and Ma, T. Matrix completion has no spurious local minimum. Neural Information Processing Systems, 2016. Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as pro- grammable computers. International Conference on Ma- chine Learning, 2023. Haeffele, B., Young, E., and Vidal, R. Structured low-rank matrix factorization: optimality, algorithm, and applica- tions to image processing. International Conference on Machine Learning, 2014. Hardt, M., Recht, B., and Singer, Y . Train faster, generalize better: stability of stochastic gradient descent. Interna- tional Conference on Machine Learning, 2016. Helmke, U. and Shayman, M. A. Critical points of matrix least squares distance functions. Linear Algebra and its Applications, 215:1–19, 1995. Hornik, K., Stinchcombe, M., and White, H. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551–560, 1990. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., Chen, W., et al. LoRA: low-rank adaptation of large language models. International Conference on Learning Representations, 2021. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker- nel: convergence and generalization in neural networks. Neural Information Processing Systems, 2018. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. How to escape saddle points efficiently. Interna- tional Conference on Machine Learning, 2017. Jin, J., Li, Z., Lyu, K., Du, S. S., and Lee, J. D. Under- standing incremental learning of gradient descent: A fine-grained analysis of matrix sensing. International Conference on Machine Learning, 2023. Johnson, W. and Lindenstrauss, J. Extensions of lipschitz maps into a hilbert space. Contemporary Mathematics, 26:189–206, 1984. Koltchinskii, V . and Panchenko, D. Rademacher processes and bounding the risk of function learning. In Gin ´e, E., Mason, D. M., and Wellner, J. A. (eds.), High Dimen- sional Probability II, pp. 443–457. Springer, 2000. Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B. Gra- dient descent only converges to minimizers. Conference on Learning Theory, 2016. Lialin, V ., Muckatira, S., Shivagunde, N., and Rumshisky, A. ReLoRA: high-rank training through low-rank up- dates. Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Re- source Optimization, 2023. Liu, L., Liu, X., Gao, J., Chen, W., and Han, J. Under- standing the difficulty of training transformers. Empirical Methods in Natural Language Processing, 2020. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . RoBERTa: a robustly optimized BERT pretraining ap- proach. arXiv preprint arXiv:1907.11692, 2019. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres- sive power of neural networks: a view from the width. Neural Information Processing Systems, 2017. Makerere AI Lab. Bean disease dataset, 2020. URLhttps: //github.com/AI-Lab-Makerere/ibean/. Malladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S. A kernel-based view of language model fine-tuning. Inter- national Conference on Machine Learning, 2023. Maurer, A. A vector-contraction inequality for rademacher complexities. Algorithmic Learning Theory, 2016. McDiarmid, C. et al. On the method of bounded differences. Surveys in Combinatorics, 141(1):148–188, 1989. Pataki, G. On the rank of extreme matrices in semidefi- nite programs and the multiplicity of optimal eigenval- ues. Mathematics of Operations Research, 23(2):339– 358, 1998. Pataki, G. The geometry of semidefinite programming. In Wolkowicz, H., Saigal, R., and Vandenberghe, L. (eds.), Handbook of Semidefinite Programming: Theory, Algo- rithms, and Applications, pp. 29–65. Springer, 2000. 11LoRA Training in the NTK Regime has No Spurious Local Minima Pilanci, M. and Ergen, T. Neural networks are convex reg- ularizers: exact polynomial-time convex optimization formulations for two-layer networks. International Con- ference on Machine Learning, 2020. Polyak, B. T. Introduction to Optimization . New York, Optimization Software, 1987. Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010. Ryu, S. Low-rank adaptation for fast text-to-image diffu- sion fine-tuning, 2023. URL https://github.com/ cloneofsimo/lora. Schick, T. and Sch¨utze, H. Exploiting cloze questions for few shot text classification and natural language inference. Association for Computational Linguistics, 2021. Smith, J. S., Hsu, Y .-C., Zhang, L., Hua, T., Kira, Z., Shen, Y ., and Jin, H. Continual diffusion: continual customiza- tion of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. Soltanolkotabi, M., Javanmard, A., and Lee, J. D. Theo- retical insights into the optimization landscape of over- parameterized shallow neural networks. IEEE Transac- tions on Information Theory, 65(2):742–769, 2018. Sridharan, K., Shalev-Shwartz, S., and Srebro, N. Fast rates for regularized objectives.Neural Information Processing Systems, 21, 2008. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention is all you need. Neural Information Processing Systems, 2017. Wei, A., Hu, W., and Steinhardt, J. More than a toy: random matrix models predict how real-world neural representa- tions generalize. International Conference on Machine Learning, 2022a. Wei, C., Chen, Y ., and Ma, T. Statistically meaningful approximation: a case study on approximating turing ma- chines with transformers. Neural Information Processing Systems, 2022b. Wu, L., Zhu, Z., et al. Towards understanding generalization of deep learning: perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Yang, S.-w., Chi, P.-H., Chuang, Y .-S., Lai, C.-I. J., Lakho- tia, K., Lin, Y . Y ., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., et al. Superb: Speech processing universal performance benchmark. Interspeech, 2021. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: from LyCORIS fine-tuning to model evaluation. International Conference on Learning Representations, 2024. Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers universal approximators of sequence-to-sequence functions? International Confer- ence on Learning Representations, 2019. Zeng, Y . and Lee, K. The expressive power of low-rank adaptation. International Conference on Learning Repre- sentations, 2024. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107– 115, 2021. Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. Why are adaptive methods good for attention models? Neural Information Processing Systems, 2020. Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. Machine Learning, 109(3):467–492, 2020. 12LoRA Training in the NTK Regime has No Spurious Local Minima A. Omitted proof of Theorem 3.1 Here, we explain the details in the proof of Theorem 3.1. We first prove the equivalence of minimize δ∈Rm×n ˆL(δ) + λ∥δ∥∗ (P) and minimize Z∈S(m+n) + ˆL( ¯Z) + λ 2 tr(Z) (Q) where ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of X such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . Lemma A.1. The following two statements hold. 1. Fix λ ≥ 0 and suppose (P) has a global minimizer (not necessarily unique). Let δ⋆ λ ∈ Rm×n be a global minimizer of (P). Then there exists an Z⋆ λ ∈ S(m+n) + induced from δ⋆ λ such that Z⋆ λ is a global minimizer of (Q), rank(Z⋆ λ) = rank(δ⋆ λ), and has same objective value. 2. Fix λ ≥ 0 and suppose (Q) has a global minimizer (not necessarily unique). Let Z⋆ λ ∈ S(m+n) + be a global minimum of (Q). Then ¯Z⋆ λ ∈ Rm×n is a global minimizer of (P) such that rank( ¯Z⋆ λ) = min(m, n,rank(Z⋆ λ)) and has same objective value. Proof. We prove the two statements at once. Let δ⋆ λ ∈ Rm×n be a global minimizer of (P) and let r = rank(δ⋆ λ). Then by Lemma 2.2, there exists u ∈ Rm×r and v ∈ Rn×r such that ∥δ⋆ λ∥∗ = 1 2 (∥u∥2 F + ∥v∥2 F ) and uv⊺ = δ⋆ λ. Take Z⋆ λ = \u0014u v \u0015\u0002u⊺ v⊺\u0003 = \u0014uu⊺ uv⊺ vu⊺ vv⊺ \u0015 ∈ S(m+n) + . Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = \r\r\r \u0014u v \u0015\r\r\r 2 F = ∥u∥2 F + ∥v∥2 F = 2∥δ⋆ λ∥∗, (Q) with Z⋆ λ has the same objective value with (P) with δ⋆ λ and rank(δ⋆ λ) = rank(Z⋆ λ) = r. Conversely, let Z⋆ λ ∈ S(m+n) + be a global minimizer of (Q) and let rank(Z⋆ λ) = r. Note that r may be larger than m or n. Then there exists Q = \u0014u v \u0015 ∈ R(m+n)×r such that QQ⊺ = Z⋆ λ. Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = ∥Q∥2 F = ∥u∥2 F + ∥v∥2 F ≥ 2∥uv⊺∥∗ = 2∥ ¯Z⋆ λ∥∗, the objective value of (P) with ¯Z⋆ λ ∈ Rm×n has less than or equal to minimum objective value of (Q) and rank( ¯Z⋆ λ) = min(m, n, r). If there exists m × n matrix whose objective value of (P) is strictly less than the minimum objective value of (Q), then we repeat the same step that was applied on δ⋆ λ to induce a solution of (Q) with strictly less objective value, which is a contradiction. Conversely, if there exists positive semi-definite matrix of size m + n whose objective value of (Q) is strictly less than the minimum objective value of (P), then we repeat the same step applied on Z⋆ λ to induce a solution of (P) with strictly less objective value, which is also a contradiction. Therefore if one of (P) and (Q) has a global minimizer, the other must have a global minimizer with same objective value. Next lemma states that if the rank of the global minimizer of (Q) is sufficiently large, then we can find an another solution with strictly less rank. Lemma A.2. Suppose X ∈ Sn + and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists nonzero t∗ ∈ R such that X + t∗Z is positive semi-definite and rank(X + t∗Z) < rank(X). 13LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let r = rank(X). Suppose Q ∈ Rn×r is a matrix where its columns are basis to R(X). Now suppose µ1(Q⊺(X + tZ)Q) > 0 for all t ∈ R where µ1(·) denotes the smallest eigenvalue (note thatµ1(·) is continuous). Then Q⊺(X +tZ)Q ∈ Sr should be positive definite for all t. For contradiction, take v ∈ R(Z) ⊆ R(X) = R(Q) to be an eigenvector of nonzero eigenvalue of Z. Since v⊺Xv >0 and v⊺Zv ̸= 0, there exists some t such that v⊺(X + tZ)v <0. Now take w ∈ Rr such that Qw = v. Then it follows that w⊺(Q⊺(X + tZ)Q)w <0, which is a contradiction. This implies that there exists t⋆ ̸= 0 such that µ1(Q⊺(X + t∗Z)Q) = 0, Hence we have r >rank(Q⊺(X + t∗Z)Q) = rank(X + t∗Z) and Q⊺(X + t∗Z)Q is positive semi-definite. To show thatX + t∗Z is positive semi-definite, take any x ∈ Rn and consider the decomposition x = Qy + z where y ∈ Rr and z ∈ N(Q) = N(X) ⊆ N(Z). Then, we have y⊺(X + t⋆Z)y = (y⊺Q⊺ + z⊺)(X + t∗Z)(Qy + z) = y⊺Q⊺(X + t∗Z)Qy ≥ 0. Finally, the following lemma and its proof are similar to the previous one, but we state it separately for the sake of clarity. It will be used in the proof of Theorem 3.1. Lemma A.3. Suppose X ∈ Sn + which is nonzero and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists t∗ > 0 such that X ± t∗Z is positive semi-definite. Proof. Let rank(X) = r and {y1, . . . , yr} be orthonormal eigenvectors of nonzero eigenvalues of X. Since y⊺ i Xyi > 0 for all yi, i = 1, ··· , r, there exists an interval (−ai, ai) for ai > 0 such that y⊺ i (X ± tZ)yi ≥ 0 for t ∈ (−ai, ai). Take t∗ = min{a1, . . . , ar}. Then t∗ satisfies the statement of the theorem. Now we provide the complete proof of Theorem 3.1. Proof of Theorem 3.1. Suppose Z⋆ λ ∈ S(m+n) + is a global minimizer of F(Z) = ˆL( ¯Z) + λ 2 tr(Z) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), ¯Z⟩, Yi \u0001 + λ 2 tr(Z) which is induced from δ⋆ λ ∈ Rm×n by Lemma A.1. Suppose there exists nonzero symmetric matrix Z such that Z ∈ S(Z⋆ λ) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆ λ)} and ⟨G(Xi), Z⟩ = 0 for 1 ≤ i ≤ N. In other words, Z ∈ S(Z⋆ λ) ∩ N(A) where A: S(m+n) → RKN is a linear operator defined as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Then there exists t >0 such that Z⋆ λ ± tZ is positive semi-definite by Lemma A.3, since Z⋆ must be nonzero. Therefore tr(Z) = 0, otherwise it will contradict the minimality of Z⋆ λ. Also we know that there exists nonzero t∗ ∈ R such that Z⋆ λ + t∗Z is also positive semi-definite with strictly lower rank by Lemma A.2. Since tr(Z) = 0, Z⋆ λ + t∗Z is also a global minimizer of F. Replace Z⋆ λ with Z⋆ λ + tZ and repeat this process until we find a solution Z⋆ λ with {0} = S(Z⋆ λ) ∩ N(A). Now we let rank(Z⋆ λ) = r. Then by dimension counting, we have the following inequality. 0 = dimS(Z⋆ λ) + dimN(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) − KN + dim(S(m+n)) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆ λ) − KN 14LoRA Training in the NTK Regime has No Spurious Local Minima Now we prove that dimS(Z⋆ λ) = r(r+1) 2 to complete the proof. Consider the diagonalization Z⋆ λ = UΛU⊺ where U is a orthogonal matrix. Since the dimension of the subspace is invariant under orthogonal transformations, we have dimS(Z⋆ λ) = dimS(Λ) = dim{Z ∈ S(m+n) : R(Z) ⊆ R(Λ)} where Λ is diagonal matrix with nontrivial entries in the leading principle minor of size r × r. This restricts the symmetric matrix Z to have nontrivial entries only in the leading r × r block. Hence, dimS(Z⋆ λ) = r(r+1) 2 . B. Omitted proof of Lemma 4.5 We prove Lemma 4.5 in this section. Proof of Lemma 4.5. Let ΠV ⊥ : Rd → V ⊥ be the orthogonal projection onto the orthogonal complement of V in Rd. Then, ΠV ⊥|M : M →V ⊥ is a smooth mapping between manifolds. Since dimV ⊥ = d − n > m= dimM, p is singular for all p ∈ M. Therefore ΠV ⊥(M) has measure zero in Rd−n by Sard’s theorem. Note that M + V ⊆ ΠV ⊥(M) + V and the measure of ΠV ⊥(M) + V in Rd is zero. This concludes that M + V is measure-zero in Rd. As a remark, the prior works of (Boumal et al., 2016; Du & Lee, 2018) also use dimension-counting arguments that would warrant the use of Lemma 4.5, but they do not provide a precise justification. Our Theorem 4.1 makes a similar argument, but does so fully rigorous through Lemma 4.5. C. Generalization guarantee In this section, let ℓ(·, ·) be our loss function which is convex, non-negative, and twice-differentiable on the first argument. Then, our empirical risk is ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 . We start the analysis from this non-regularized risk and expand it to regularized ones. We assume that our model is class of affine predictors X 7→ fW0 (X) + ⟨G(X), δ⟩ for given data X. Now we apply the theory of Rademacher complexity to derive the upper bound of the generalization bound. To begin with, we start with introducing the classical result in probability theory from (McDiarmid et al., 1989) without proof. Lemma C.1. (McDiarmid inequality) Let X1, . . . , XN ∈ Xbe i.i.d N random samples from dataset X. Let g : XN → R be a function satisfying the following property with c >0: |g(X1, . . . , Xi−1, Xi, Xi+1, . . . , XN ) − g(X1, . . . , Xi−1, X′ i, Xi+1, . . . , XN )| ≤c for all X1, . . . , XN , X′ i ∈ X. Then, for all ε >0, P(|g(X1, . . . , XN ) − E[g(X1, . . . , XN )]| ≥ε) ≤ exp \u0012 − 2ε2 Nc2 \u0013 . Now, we define the Rademacher complexity of the class of functions H from X to R: RN (H) = Eε,D   sup h∈H 1 N NX i=1 εih(Xi) ! , where {εi}1≤i≤N are independent Rademacher random variables, and D = {X1, . . . , XN } is N random samples from X. In our analysis, we will focus on class of affine predictors Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ and composition of affine predictors with loss Xi 7→ ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi). Rademacher complexities are closely related to upper bounds on generalization bound due to the following lemma. 15LoRA Training in the NTK Regime has No Spurious Local Minima Lemma C.2. Let RN (H) be the Rademacher complexity of the class of functions H from X to R and X1, . . . , XN are N samples from X. Then the following inequality holds. E \" sup h∈H   1 N NX i=1 h(Xi) − E[h(X)] !# ≤ 2RN (H), E \" sup h∈H   E[h(X)] − 1 N NX i=1 h(Xi) !# ≤ 2RN (H). Proof. The proof is by using standard symmetrization arguments. We defer its proof to Theorem 8 of (Bartlett & Mendelson, 2002), or Section 4.5 of (Bach, 2023). The next lemma uses a contraction property to reduce the Rademacher complexity of losses to linear predictors. These type of results are widely used in Rademacher analysis and we use the following specific version of contraction, which was originally introduced in Corollary 4 of (Maurer, 2016) and adapted to our setting. Write ∥ · ∥2 for Euclidean vector norm. Lemma C.3. Let A be the class of functions a : X →RK. For 1 ≤ i ≤ N, let ℓi : RK → R be G-Lipschitz continuous on A with respect to the Euclidean norm in the sense that the following holds: |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ∈ X. Then we have the following inequality for independent Rademacher random variables {σi}1≤i≤N and {εij}1≤i≤N,1≤j≤K: Eσ,D \" sup a∈A 1 N NX i=1 σiℓi(a(Xi)) # ≤ √ 2G · Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)  , where aj denotes the j-th coordinate of a and D = {(Xi, Yi)}i∈{1,...,N} are i.i.d N random samples sampled from X. Proof. We defer the proof to the Section 5 of (Maurer, 2016). In Lemma C.3, if we sample D from a probability distribution P, we can relax the Lipschitz continuity condition to hold for P- almost surely. In other words, |ℓ(a(X1)) − ℓ(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. The next lemma states that the Rademacher complexity of class of bounded affine predictors decays at most O( 1√ N ) rate. Lemma C.4. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Assume AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D >0. Suppose ∥G(j)(Xi)∥F ≤ R almost surely with respect to the random data Xi ∼ P. Then, Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   ≤ RD √ K√ N where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. 16LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Eε  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 f(j) W0 (Xi) + ⟨G(j)(Xi), δ⟩ \u0011   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩ + 1 N NX i=1 KX j=1 εijf(j) W0 (Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   ≤ Eε   sup ∥δ∥F ≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = Eε  D N sup ∥δ∥F ≤1 NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = D N Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F . The inequality is from the fact that ∥ · ∥F ≤ ∥ · ∥∗, hence {δ : ∥δ∥∗ ≤ D} ⊂ {δ : ∥δ∥F ≤ D}. The last equality is from the fact that ∥ · ∥F is self-dual. Next, we can bound Eε \r\r\rPN i=1 PK j=1 εijG(j)(Xi) \r\r\r F by the following inequalities. Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F ≤ vuuutEε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r 2 F = vuutEε NX i=1 KX j=1 \r\rεijG(j)(Xi) \r\r2 F = vuut NX i=1 KX j=1 \r\rG(j)(Xi) \r\r2 F ≤ R √ NK. a.s. The first inequality is from Jensen’s inequality, the equalities are from i.i.d assumption ofεik. We combine the results and take expectation with respect to D to get Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijak(Xi)   ≤ D N · R √ NK = RD √ K√ N . We then combine the previous results to get the following Lemma. Lemma C.5. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Let ˆL is non-regularized empirical risk defined as ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 17LoRA Training in the NTK Regime has No Spurious Local Minima and AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely with respect to the random data Xi ∼ P. For 1 ≤ i ≤ N, suppose ℓi ≜ ℓ(·, Yi) is G-Lipschitz continuous on A on the first argument (with respect to the Euclidean norm) for almost surely with respect to the random data Xi ⊆ D ∼ P. That is, |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. Then for any ∥δ∥∗ ≤ D, fixed δ0 such that ∥δ0∥∗ ≤ D, and η ∈ (0, 1), the following inequality holds with probability greater than 1 − η: ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . Proof. Take g of Lemma C.1 to be g = sup∥δ∥∗≤D(ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)), which is a function of X1, . . . , XN . Since ∥δ∥∗ ≤ D implies ∥δ∥F ≤ D and by the Lipschitz continuity of ℓ(·, Yi), we have the following for any (Xi, Yi) ∈ D: |ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ0⟩, Yi \u0001 − ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 | ≤G∥⟨δ0 − δ, G(Xi)⟩∥2 ≤ G vuut KX j=1 ∥δ0 − δ∥2 F ∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 ∥δ0 − δ∥2∗∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 4D2 · R2 = 2GRD √ K. Hence if we change only one data point (Xi, Yi) of g to (X ′ i, Y ′ i ), the deviation of ˆL(δ0) − ˆL(δ) is at most 2GRD √ K N . Then by Lemma C.1, we have sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) < E \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # + t √ 2KGRD√ N with probability greater than 1 − e−t2 . The expectation on the right hand side can be reduced to ED \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # = ED \" sup ∥δ∥∗≤D (−ˆL(δ) + L(δ)) + ˆL(δ0) − L(δ0) # = ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # Note that L(δ) − ˆL(δ) = L(δ) − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) = E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi), 18LoRA Training in the NTK Regime has No Spurious Local Minima where the expectation is taken over Xi ∼ P. Now apply Lemma C.2 to get ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # = ED \" sup ∥δ∥∗≤D (E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i# − ED \" 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # ≤ 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # where {σ}1≤i≤N are i.i.d Rademacher variables. Then apply Lemma C.3 to get 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # = 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. Finally, use Lemma C.4 to get 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   ≤ 2 √ 2G · RD √ K√ N . Therefore, we conclude that ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N (2 + t) . for ∥δ∥∗ ≤ D with probability greater than 1 − e−t2 . By reparametrization, we get ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . for ∥δ∥∗ ≤ D with probability greater than 1 − η. Now we can extend this generalization guarantee of constrained optimization to regularized optimization, which aligns with our problem of interest. For notational convenience, let Lλ(δ) = L(δ) + λ∥δ∥∗, ˆLλ(δ) = ˆL(δ) + λ∥δ∥∗ We follow the proof structure of (Bach, 2023), which was motivated by (Bartlett et al., 2005) and (Sridharan et al., 2008). Theorem C.6. Fix ε >0 and let 0 ̸= δ⋆ true ∈ argminδ L(δ) be the true optimum of the population risk and consider the setup of Lemma C.5 with D = (2 + ε)∥δ⋆ true∥∗, which is the upper bound on the nuclear norm of the predictors. Let η ∈ (0, 1) and λ = (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ).Consider the setup of Corollary 4.2 with P randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N \u0012 2 + r log 1 η \u0013 . 19LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let ˜ε = ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ and consider the convex set C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) ≤ λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Then for ∥δ∥∗ = 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, δ /∈ intC since the following inequalities hold. Lλ(δ) − Lλ(δ⋆ true) = L(δ) − L(δ⋆ true) + λ∥δ∥∗ − λ∥δ⋆ true∥∗ ≥ λ∥δ∥∗ − λ∥δ⋆ true∥∗ = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Therefore the boundary ∂C of C should be ∂C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Now suppose ˆuˆv⊺ /∈ C. Then since δ⋆ true ∈ C, there exists δ in the segment [ˆuˆv⊺, δ⋆ true] such that δ ∈ ∂C. By the convexity of ˆLλ, we have ˆLλ(δ) ≤ max \u0010 ˆLλ(δ⋆ true), ˆLλ(ˆuˆv⊺) \u0011 . Then we get ˆLλ(δ⋆ true) − ˆLλ(δ) ≥ −2˜ε∥δ⋆ λ∥∗ by Corollary 4.2. Therefore, ˆL(δ⋆ true) − ˆL(δ) − L(δ⋆ true) + L(δ) = ˆLλ(δ⋆ true) − ˆLλ(δ) − Lλ(δ⋆ true) + Lλ(δ) ≥ Lλ(δ) − Lλ(δ⋆) − 2˜ε∥δ⋆ λ∥∗ (3) = λ∥δ⋆ true∥∗ Note that ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗ < (2 + ε)∥δ⋆ true∥∗ and λ∥δ⋆ true∥∗ = ∥δ⋆ true∥∗ (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Then by Lemma C.5, (3) should happen with probability less than η. Then with probability greater than 1 − η, ˆuˆv⊺ ∈ C. In other words, Lλ(ˆuˆv⊺) − Lλ(δ⋆ true) < λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Hence, L(ˆuˆv⊺) + λ∥ˆuˆv⊺∥∗ < Lλ(δ⋆ true) + λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ = L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ ≤ L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + ελ∥δ⋆ true∥∗. Finally, we get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N   2 + r log 1 δ ! . By using the fact that ℓCE is Lipschitz continuous, we can reduce Theorem C.6 to Theorem 5.1. Note that the loss function ℓ may not be Lipschitz continuous in general. However, Lipschitz continuity is a mild assumption when the domain is restricted to a bounded class of predictors AD of Lemma C.5. Proof of Theorem 5.1. If ℓ(·, Y): RK → R is cross entropy loss defined as ℓ(X, Y) = ℓCE (X, Y) = −log   exp X(j) PK i=1 exp X(i) ! = −X(j) + log  KX i=1 exp X(i) ! 20LoRA Training in the NTK Regime has No Spurious Local Minima with true label Y = j, we have ∇ℓCE (X, Y)j = −1 + exp X(j) PK i=1 exp X(i) = − P i̸=j exp X(Y ) PK i=1 exp X(i) and for k ̸= j, ∇ℓCE (X, Y)k = exp X(k) PK i=1 exp X(Y ) Then we can bound the Euclidean norm of the gradient as follows. ∥∇ℓCE (X, Y)∥2 2 = \u0010P i̸=j exp X(i) \u00112 \u0010PK i=1 exp X(i) \u00112 + P i̸=j exp 2X(k) \u0010PK i=1 exp X(i) \u00112 ≤ 1 + 1 = 2. Hence the gradient of the cross entropy loss is bounded by √ 2 and we may replace G in Theorem C.6 with √ 2 to get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N   2 + r log 1 δ ! . D. Details of experiments Optimizing nuclear norm. Recall that SGD or GD on the loss function with weight decay and with regularization parameter λ is equivalent to minimizing 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , with respect to u and v. In full fine-tuning however, this is equivalent to minimize the following with respect to δ: 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 + λ∥δ∥∗. The problem here is that gradient methods no longer apply since the nuclear norm is non-differentiable. Therefore, we use the proximal gradient method: δt+1 = proxαλ∥·∥∗(δt − α∇ˆL(δt)) where proxαλ∥·∥∗(δ) = argmin δ′ \u0012 λ∥δ ′ ∥∗ + 1 2α∥δ′ − δ∥2 F \u0013 . It is well known that the proximal gradient method on convex objective converges to a global minimum (Polyak, 1987). Hyperparameters on NLP tasks For NLP tasks, we use full batch to perform GD on training. We only train the query (Wq) and value (Wv) weights of the RoBERTa-base model, which was empirically shown to have good performance (Hu et al., 2021). Furthermore, calculating the proximal operator of a nuclear norm is a computational bottleneck during the training of all Wq and Wv matrices. Therefore, we limit our training to only the last layer of Wq and Wv. To ensure a fair comparison, we apply the same approach to the LoRA updates. Additional information is in Table 1. Hyperparameters on image and speech classification tasks Similar to NLP tasks, we train the last attention layers. Further details are in Table 2. 21LoRA Training in the NTK Regime has No Spurious Local Minima Task SST-2,QNLI MR,CR,QQP,Subj Batch size 32 32 Learning rate (Full, LoRA fine tuning) 0.0005 0.001 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0.01 0.01 Table 1.Hyperparameters on experiment in Section 6 (NLP tasks) Task Image classification Speech classification Batch size 16 16 Learning rate (Full, LoRA fine tuning) 0.005 0.005 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0 0.001 Table 2.Hyperparameters on experiment in Section 6 (Image and speech classification tasks) (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 4.Test curves (accuracy vs. epochs) on different NLP tasks. We used the LoRA rank of 16. Test accuracy. For the setting of Section 6 on NLP tasks, we additionally conduct evaluations on a test set of 1000 samples during training and present the results in Figure 4. We observed that in most tasks the performance using LoRA eventually converges a test accuracy that matches that of full fine-tuning, although the rates of convergence sometimes differ. We list the hyperparameters in Table 3 Task SST-2,QQP,MR,CR Subj QNLI Batch size 32 32 24 Learning rate (Full, LoRA fine tuning) 0.0001 0.001 0.0005 Trained layer Wq, Wv (all layers) Wq, Wv (all layers) Wq, Wv (all layers) Weight decay 0.005 0.005 0.005 Table 3.Hyperparameters on experiment in Figure 4 22LoRA Training in the NTK Regime has No Spurious Local Minima For image and speech classification tasks, we also validate the performance of our linearized update to confirm that the accuracy is on par with actual LoRA updates. Accuracies are averaged over 3 runs (See Table 4). Task Image classification Speech classification Accuracy ( actual / linearized) 86.20 / 87.00 74.67 / 73.67 Table 4.Accuaricies of LoRA updates on vision and speech classification tasks 23",
      "meta_data": {
        "arxiv_id": "2402.11867v3",
        "authors": [
          "Uijeong Jang",
          "Jason D. Lee",
          "Ernest K. Ryu"
        ],
        "published_date": "2024-02-19T06:22:09Z",
        "pdf_url": "https://arxiv.org/pdf/2402.11867v3.pdf",
        "github_url": "https://github.com/UijeongJang/LoRA-NTK"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper theoretically analyzes LoRA fine-tuning in the Neural Tangent Kernel (NTK) regime. Its main contributions are proving that full fine-tuning admits low-rank solutions (rank r \rless\rless\rless\rless\rless\rless sqrt(N)), demonstrating that using LoRA with sufficient rank (r \rless\rless\rless\rless\rless\rless sqrt(N)) eliminates spurious local minima, thereby enabling (stochastic) gradient descent to find these low-rank solutions, and showing that these low-rank solutions generalize well.",
        "methodology": "The methodology involves theoretical analysis of LoRA fine-tuning within the Neural Tangent Kernel (NTK) regime, where network changes are approximated by first-order Taylor expansions. LoRA training with weight decay is formulated as a rank-constrained nuclear-norm regularized optimization problem. The existence of low-rank solutions is proven by relating the problem to semi-definite programs. To ensure gradient-based methods find these solutions, the authors demonstrate that, under specific rank conditions and with a small perturbation, all second-order stationary points of the LoRA loss function are global minima, drawing on matrix factorization theory and Sard's theorem. Generalization guarantees are derived using Rademacher complexity and McDiarmid inequality.",
        "experimental_setup": "Experiments were conducted to validate the theory by fine-tuning linearized pre-trained models. For NLP tasks, a linearized RoBERTa-base model was fine-tuned on six datasets (SST-2, QNLI, MR, CR, Subj, QQP) using N=32 data points and K=2 labels. Image classification involved a linearized Vision Transformer on the bean disease dataset (N=48, K=3). Speech classification used a linearized wav2vec2 model on a SUPERB dataset (N=64, K=4). All tasks employed cross-entropy loss, comparing LoRA (with ranks satisfying theoretical guarantees, e.g., r \rless\rless\rless\rless\rless\rless= 11 for NLP) against full fine-tuning. Only query (Wq) and value (Wv) weights of the last layer were fine-tuned. Validation involved observing training loss curves and test accuracy.",
        "limitations": "The primary limitation is the reliance on the Neural Tangent Kernel (NTK) regime assumption, which dictates small network changes during fine-tuning and may not universally hold. The theoretical guarantee for the absence of spurious local minima applies to a 'perturbed' loss function, with an interpretive extension to the unperturbed practical LoRA setting. The theory also requires specific rank conditions (e.g., r(r+1)/2 > KN) for its guarantees. Critically, the theory does not address the rate of convergence; empirical observations indicate that lower LoRA ranks can slow down convergence, despite reaching global optima, possibly due to flatter loss landscapes.",
        "future_research_directions": "Future research directions include conducting more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, better understanding the minimum rank requirements for LoRA through lower bounds, and analyzing the trade-off between training rate and LoRA rank, particularly how lower ranks might create unfavorable, flatter loss landscapes that slow down convergence.",
        "experimental_code": "File: src/linearized.py\n\nclass LogitModelWrapper(nn.Module):\n    def __init__(self, model, binary_classification):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n\n    def forward(self, input_ids, attention_mask, mask_pos):\n        logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0]\n        if self.binary_classification:\n            assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n            logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        return logits\n\nclass LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n\n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))\n\n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])\n\n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n\n    def forward(self, input_ids, attention_mask, mask_pos, gradient):\n\n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0]\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n\n        self.num_labels = gradient[0].size(1)\n        self.gradient_dtype = gradient[0].dtype\n\n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) )\n\n        return output\n\nclass LinearizedLoraTrainer(LinearHeadTrainer):\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n\n        def convert_to_buffer(name):\n            if layer_name in name:\n                return False\n            else:\n                return True\n\n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n\n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n\n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n\n\n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n\n        targets.append(label)\n\n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))\n\n    def compute_gradient_everylayer(self, inputs_outer):\n\n        grads_outer=[]\n        with torch.no_grad():\n            for layer_name in self.target_layers:\n                    grads_outer_layer, targets = self.compute_gradient_perlayer(inputs_outer, layer_name)\n                    grads_outer.append(grads_outer_layer)\n\n        if self.grad_dim is None:\n            self.grad_dim = sum(np.prod(x.shape[2:]) for x in grads_outer)\n\n        return (grads_outer, targets)\n\n    def compute_model_logits(self, inputs_outer):\n\n        self.model_wrapper.eval()\n\n        logits = []\n        targets = []\n        with torch.no_grad():\n            for k, v in inputs_outer.items():\n                if isinstance(v, torch.Tensor):\n                    inputs_outer[k] = v.to(self.args.device)\n            label = inputs_outer.get(\"labels\")\n            if self.args.binary_classification:\n                label = (label * 2 - 1).float() # convert from {0, 1} to {-1, 1}\n            preds = self.model_wrapper(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))\n            logits.append(preds.detach())\n            targets.append(label)\n\n        logits = torch.cat(logits, dim=0)\n        targets = torch.cat(targets, dim=0)\n\n        return logits, targets\n\n    def finetune(self, train_dataset, eval_dataset):\n\n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr)\n\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n\n        epoch_count = 0\n\n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False)\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False)\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False)\n\n        for epoch in range(self.args.linear_num_epoch):\n\n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n\n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n\n\n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1)\n                    self.gradient_dtype = gradient[0].dtype\n\n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)\n\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)\n                            param = u@s@v\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param\n\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:\n                            reg += (1/2)*(param ** 2).sum()\n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            reg += torch.norm(param, p = 'nuc')\n\n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item()\n\n            file_exists = True\n\n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd)\n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n\n            if self.args.eval_during_training:\n                # Evaluation logic as provided in the repository content\n                pass\n\n        return avg_loss.item() , epoch_count\n",
        "experimental_info": "1. **LoRA Specific Arguments (from `ModelArguments` in `run.py`):**\n    - `apply_lora`: Boolean (default `True`), whether to use LoRA for finetuning. If `False`, a full matrix update (`delta`) is used instead of low-rank factors. (Note: LoRA only implemented for RoBERTa models).\n    - `lora_alpha`: Integer (default `1`), initialization scale for one of the low-rank matrices in LoRA.\n    - `lora_r`: Integer (default `8`), inner rank for LoRA matrices (i.e., the `r` in LoRA).\n2. **Linearized Training Specific Arguments (from `DynamicTrainingArguments` in `run.py`):**\n    - `trainer`: String (default `\"linearized\"`), must be set to `\"linearized\"` to use `LinearizedLoraTrainer`.\n    - `linear_freeze_A`: Boolean (default `False`), if `True`, the `Lora_A` matrix of the LoRA layers is frozen during training.\n    - `linear_lr`: Float (default `0.01`), learning rate for the linearized LoRA fine-tuning.\n    - `linear_num_epoch`: Integer (default `30`), number of epochs for training the linearized LoRA model.\n    - `linear_wd`: Float (default `0`), weight decay applied during linearized LoRA training.\n    - `eval_during_training`: Boolean (default `False`), if `True`, evaluation is performed during the training process at each epoch end.\n    - `train_last_layer`: Boolean (default `True`), if `True`, LoRA layers are applied only to `attention.query.weight` and `attention.value.weight` in the last encoder layer (layer 11 for RoBERTa-base). If `False`, LoRA is applied to these weights in all encoder layers.\n    - `binary_classification`: Boolean (default `False`), if `num_classes=2`, converts two softmax logits to a single sigmoid logit for regression-like tasks.\n3. **LoRA Matrix Initialization (from `LinearLoraupdate.__init__` in `src/linearized.py`):**\n    - `Lora_a` matrices are initialized from a normal distribution with `mean = 0` and `std = 1 / (sqrt(lora_r) * target_size[i][1])`.\n    - `Lora_b` matrices are initialized with zeros.\n4. **Weight Decay Implementation (from `LinearizedLoraTrainer.finetune` in `src/linearized.py`):**\n    - If `apply_lora` is `True` and `linear_wd` is greater than a threshold, L2 regularization is applied to `Lora_A` and `Lora_B` matrices: `param = param - linear_wd * linear_lr * param`.\n    - If `apply_lora` is `False` (for full matrix update `delta`) and `linear_wd` is greater than a threshold, a soft-thresholding operator is applied on the singular values of the `delta` matrix for nuclear norm regularization: `s = torch.nn.Threshold(0, 0)(s - linear_lr * linear_wd)`."
      }
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces DP-HyPO, a pioneering framework for adaptive private hyperparameter optimization, effectively bridging the gap between private and non-private HPO. It allows the flexible use of non-DP adaptive HPO methods (e.g., Gaussian process) while avoiding substantial privacy costs due to composition. The framework provides sharp Differential Privacy (DP) guarantees by utilizing the Rényi DP framework, strictly generalizing prior results that required uniform sampling. Empirically, the Gaussian process-based DP-HyPO algorithm demonstrates superior performance compared to its non-adaptive uniform counterpart across diverse real-world datasets and scenarios, and offers practitioners the flexibility to integrate any non-private adaptive HPO methods and allocate privacy budget for adaptivity.",
        "methodology": "The DP-HyPO framework operates by maintaining an adaptive sampling distribution (π) at each iteration, which incorporates accumulated information. A hyperparameter (λ) is randomly drawn from this distribution, and a base DP algorithm (Mλ(D)) is executed. The key innovation lies in adaptively updating π based on previous outputs, while rigorously ensuring that the density of any posterior sampling distribution is bounded (c ≤ π(j+1)(λ) / π(0)(λ) ≤ C) to control privacy loss. The privacy guarantees are quantified using Rényi Differential Privacy (RDP). To privatize any non-private HPO update rules, a projection technique is proposed, which solves a convex functional programming problem to project the sampling distribution into a space of bounded densities (SC,c). The paper provides an instantiation of DP-HyPO using Gaussian Processes, which constructs a surrogate model for performance measures, assigns scores (e.g., UCB) to hyperparameters, and uses a softmax function to convert these scores into a sampling distribution.",
        "experimental_setup": "The DP-HyPO framework, specifically its Gaussian process (GP)-based instantiation, was empirically evaluated against a Uniform DP-HyPO baseline (a non-adaptive special case) in both white-box and black-box privacy settings. In the white-box setting, experiments were conducted on MNIST and CIFAR-10 datasets, training standard CNNs with DP-SGD and optimizing learning rate (η) and clipping norm (R). For MNIST, a semi-real simulation cached mean accuracies of models for discretized hyperparameters, adding Gaussian noise to sampled accuracies for evaluation, with a total privacy budget of ε=15, δ=1e-5. For CIFAR-10, the hyperparameter landscape (mean and standard error of accuracy) was generated using BoTorch, and an oracle returned noisy scores, with a total privacy budget of ε=12, δ=1e-5. In the black-box setting, a real-world Federated Learning task on a proprietary dataset was simulated, optimizing learning rates for the central server (AdaGrad) and individual users (SGD). The loss landscape was also generated by BoTorch, and noisy scores were returned by an oracle. Various C values (1.25, 1.33, 1.5) with c=1/C were explored to analyze the impact of adaptivity on performance.",
        "limitations": "The framework requires that the density of any posterior sampling distribution remains bounded by constants c and C (c ≤ π(j+1)(λ) / π(0)(λ) ≤ C), which may necessitate modifications and projection techniques for existing non-private HPO methods. Practical implementation often requires discretizing the hyperparameter space, which is an approximation inherent in numerical computations. Due to computational resource constraints, the empirical evaluations on MNIST and CIFAR-10, as well as the Federated Learning task, utilized semi-real simulations or pre-generated landscapes from libraries like BoTorch, rather than full training runs for every hyperparameter combination.",
        "future_research_directions": "Two main future research directions are proposed: first, exploring alternative HPO specifications that are more practically favorable and leveraging more advanced HPO methods to further improve empirical performance. Second, establishing a theoretical utility guarantee for DP-HyPO, potentially by adapting and extending proof methodologies similar to those found in prior work on private selection."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces \"Partitioned Neural Networks,\" a novel, efficient, and scalable method for hyperparameter optimization inspired by the marginal likelihood objective. This approach allows for optimizing various hyperparameters (e.g., neural architecture, data augmentation strategies, regularization, dropout rates) in a single training run without requiring a separate validation set. The core idea involves partitioning both the training data into shards and the neural network model into corresponding parameter partitions. Hyperparameters are optimized using an \"out-of-training-sample\" loss on unseen data shards. Key findings include successfully identifying correct models in toy selection tasks, learning effective affine data augmentations that improve test accuracy (especially in low-data regimes), outperforming traditional validation-set-based optimization and prior marginal likelihood approximations (Augerino, Differentiable Laplace) on various datasets like CIFAR10 and TinyImagenet, and demonstrating scalability to larger architectures where previous methods encounter memory issues. Furthermore, the method is shown to be particularly beneficial in federated learning settings by reducing communication overhead and improving model generalization on non-i.i.d. data distributions.",
        "methodology": "The method optimizes an approximation of the marginal likelihood, LML(D,ψ), which serves as a lower bound on the true marginal likelihood and is interpreted as a measure of a model's \"learning speed.\" It operates by partitioning the neural network's weights (w) into C partitions (w1, ..., wC) and the training dataset (D) into C corresponding non-overlapping data shards (D1, ..., DC). For training, a k-th subnetwork, w(k)s, is constructed by concatenating the first k trained partitions (w1, ..., wk) with default values (ˆwk+1, ..., ˆwC) for the remaining partitions, ensuring it's trained only on data shards D1:k. Training involves interleaving parameter and hyperparameter updates: model parameters (wk) for a given partition are updated by optimizing the negative log-likelihood on data from D1:k using the corresponding subnetwork w(k)s. Hyperparameters (ψ) are optimized using stochastic gradients derived from the \"out-of-sample\" loss of a subnetwork w(k-1)s on data from the next shard Dk. The weight partitioning scheme typically involves randomly assigning a fixed proportion of weights in each layer to a given partition before training. For federated learning, clients are assigned to data chunks, and only the updated partitions relevant to their chunk are communicated to the server, significantly reducing upload costs.",
        "experimental_setup": "The research validates Partitioned Neural Networks across a range of tasks and datasets. Tasks include model selection on a synthetic input selection problem (15 informative, 15 spurious features), differentiable input selection using a learnable mask, learning affine image augmentations (shear, translation, scale, rotation), optimizing general feature extractors (first two stages of a Wide ResNet-20), and hyperparameter optimization (augmentations and dropout rates) in federated learning. Datasets utilized are synthetic data, MNIST, RotMNIST, CIFAR10, RotCIFAR10, TinyImagenet, and RotTinyImagenet. For federated learning, non-i.i.d. splits (label-skew with Dirichlet and rotation-skew) of MNIST and CIFAR10 are used. Architectures range from fully-connected MLPs for toy tasks, CNNs for MNIST, Fixup ResNets (8, 14) for CIFAR10, Wide ResNet-20 for feature extraction, to ResNet-50 with GroupNorm(2) for TinyImagenet, and ResNet-9 with GroupNorm for federated CIFAR10, all incorporating learnable dropout where appropriate. Baselines include standard training (no augmentations), Augerino, Differentiable Laplace, Last-layer marginal likelihood, traditional validation set optimization with finetuning, FedAvg, and FedAvg + Augerino. Optimization is performed using Adam or SGD with various learning rates, batch sizes, and weight decay, with 20 augmentation samples for invariance learning. The number of partitions (C) and their data/parameter proportions are varied (e.g., 2-4 chunks, with splits like [80%,10%,10%]). Evaluation metrics include test accuracy, log-likelihood, learned mask evolution, and validation accuracy, along with communication costs for FL.",
        "limitations": "The method inherently requires an additional forward-backward pass for hyperparameter updates, which, while less costly than existing approaches, contributes to computational overhead. Empirically, partitioned networks may also require more training iterations to converge. Furthermore, the act of partitioning the network parameters can constrain the model's capacity, potentially leading to a slight performance loss compared to a fully optimized, non-partitioned network with ideal hyperparameters. A practical limitation is the introduction of the partitioning strategy (e.g., number of chunks, data/parameter proportions) as an additional hyperparameter that may require tuning for optimal performance on specific tasks. The paper also notes a pathological behavior in a compared baseline, Augerino, where its loss function's unbounded regularization term can cause hyperparameters to diverge if not addressed by careful tuning, clipping, or early stopping.",
        "future_research_directions": "Potential future research directions include exploring dynamic partitioning strategies where network parameters are partitioned during training, rather than being fixed beforehand. Investigating methods to alleviate the inherent performance loss associated with network partitioning, such as adjusting training rounds or increasing the initial network capacity, is another promising area. The paper suggests exploring alternative partitioning schemes, like node partitioning, which might enable updating multiple parameter partitions with a single batch, thereby reducing computational overhead. Further work could also focus on optimizing hyperparameter updates through techniques like gradient accumulation from different chunks to achieve lower-variance estimates, or by interleaving hyperparameter updates at less frequent intervals. In the context of federated learning, exploring different sequential update schemes for network partitions on the client side could yield further improvements. Overall, the method aims to contribute to reducing the carbon footprint of AI research by minimizing the need for extensive hyperparameter search through repeated training runs."
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of hyperparameter optimization (HO) for Lasso-type estimators, which suffer from high memory consumption with automatic differentiation or numerical instability with standard implicit differentiation, especially for non-smooth problems. The main contributions include introducing an efficient implicit differentiation algorithm without matrix inversion, tailored for Lasso-type problems, which scales to high-dimensional data by leveraging solution sparsity. The work demonstrates that forward iterative differentiation of block coordinate descent (BCD) converges linearly to the true gradient once the support is identified. A novel algorithm is proposed that decouples Jacobian computation from regression coefficient computation, avoiding the need to solve potentially ill-conditioned linear systems. Experimental results show that the method outperforms state-of-the-art HO techniques for optimizing held-out error or Stein Unbiased Risk Estimator (SURE).",
        "methodology": "Hyperparameter optimization is formulated as a bi-level optimization problem where an outer criterion C(ˆβ(λ)) is minimized with respect to hyperparameters λ, subject to ˆβ(λ) being the solution of an inner Lasso-type problem (e.g., Lasso or weighted Lasso with non-smooth ℓ1 penalties). The regularization parameter is re-parametrized as eλ. The core methodology involves efficiently computing the weak Jacobian ˆJ(λ) (gradient of ˆβ with respect to λ). The proposed \"Implicit Forward Iterative Differentiation\" algorithm (Algorithm 2) works in two main steps: first, solve the inner Lasso-type problem to obtain regression coefficients ˆβ and identify its support ˆS using any state-of-the-art solver. Second, compute the Jacobian by applying forward differentiation recursion steps restricted to the identified support. This method leverages the sparsity of Lasso solutions and the fixed-point iteration induced by BCD solvers to avoid direct matrix inversion of a large, potentially ill-conditioned system, making it memory-efficient and numerically stable. The outer problem then uses standard gradient descent with line-search.",
        "experimental_setup": "The Python code is open-sourced as 'sparse-ho', utilizing Numba for critical BCD loops. For fair comparison, all methods use the same vanilla BCD algorithm (Algorithm 5) for the inner optimization, stopping at a tolerance ϵtol = 10^-5. Gradient-based methods employ a line-search strategy. Lasso initializations are set to λmax - log(10), while weighted Lasso uses a regularized bi-level problem solution for initialization. Competitors include: hypergradient-based methods (Implicit Differentiation, Forward Iterative Differentiation, and the proposed Implicit Forward Iterative Differentiation) and non-hypergradient methods (Grid-search, Random-search, Lattice Hypercube Sampling, and Bayesian Optimization). Experiments evaluate performance on held-out loss (split into train, validation, test) and SURE (Stein Unbiased Risk Estimator) criteria. Real-world datasets used are rcv1 (n=20k, p=20k), 20news (n=11k, p=130k), and finance (n=16k, p=1.6M). Simulated data (n=100, p from 200 to 10k, SNR=3) are used for SURE evaluation, with 50 repetitions. Metrics include objective value convergence, test loss, relative Mean Squared Error (MSE), and computation time.",
        "limitations": "The theoretical guarantees for the proposed method (Propositions 1 and 2) assume smooth loss functions and do not cover non-convex penalty cases (e.g., MCP), although the algorithm shows proper numerical behavior in such settings. The convergence proof for the Jacobian also relies on the uniqueness of the Lasso solution, which, while typically true, might not hold in pathological cases leading to non-continuous solution paths for ˆβ(λ). The objective function L(λ) for hyperparameter optimization is generally non-convex, meaning gradient descent may converge to local minima rather than a global optimum. The SURE criterion requires prior knowledge of the noise variance.",
        "future_research_directions": "Future work primarily involves extending the theoretical framework to cover non-convex Lasso formulations, such as the Minimax Concave Penalty (MCP), which were shown to behave properly numerically but lack theoretical guarantees in the current work.",
        "experimental_code": "class ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            # relative stopping criterion for the computation of the jacobian\n            # and absolute stopping criterion to handle warm start\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    # HACK we only need this for one test, do not rely on it\n    get_only_jac.n_iter = i\n\n    return dbeta\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    \"\"\"\n    Parameters\n    --------------\n    X: array-like, shape (n_samples, n_features)\n        Design matrix.\n    y: ndarray, shape (n_samples,)\n        Observation vector.\n    log_alpha: float or np.array, shape (n_features,)\n        Logarithm of hyperparameter.\n    beta0: ndarray, shape (n_features,)\n        initial value of the regression coefficients\n        beta for warm start\n    dbeta0: ndarray, shape (n_features,)\n        initial value of the jacobian dbeta for warm start\n    max_iter: int\n        number of iterations of the algorithm\n    tol: float\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        primal decrease for optimality and continues until it\n        is smaller than ``tol``\n    compute_jac: bool\n        to compute or not the Jacobian along with the regression\n        coefficients\n    model:  instance of ``sparse_ho.base.BaseModel``\n        A model that follows the sparse_ho API.\n    return_all: bool\n        to store the iterates or not in order to compute the Jacobian in a\n        backward way\n    use_stop_crit: bool\n        use a stopping criterion or do all the iterations\n    gap_freq : int\n        After how many passes on the data the dual gap should be computed\n        to stop the iterations.\n\n    Returns\n    -------\n    mask : ndarray, shape (n_features,)\n        The mask of non-zero coefficients in beta.\n    dense : ndarray, shape (n_nonzeros,)\n        The beta coefficients on the support\n    jac : ndarray, shape (n_nonzeros,) or (n_nonzeros, q)\n        The jacobian restricted to the support. If there are more than\n        one hyperparameter then it has two dimensions.\n    \"\"\"\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    ############################################\n    alpha = np.exp(log_alpha)\n\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    ############################################\n    # warm start for beta\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    ############################################\n    # warm start for dbeta\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    # store the values of the objective\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    ############################################\n    # store the iterates if needed\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if verbose:\n            print(\"%i -st iteration over %i\" % (i, max_iter))\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if i > 1:\n            if verbose:\n                print(\"relative decrease = \", (pobj[-2] - pobj[-1]) / pobj0)\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if verbose:\n                    print(\"dual gap %.2e\" % dual_gap)\n                if verbose:\n                    print(\"gap %.2e\" % dual_gap)\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n        if return_all:\n            list_beta.append(beta.copy())\n        if save_iterates:\n            list_beta.append(beta.copy())\n            list_jac.append(dbeta.copy())\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None",
        "experimental_info": "The hyperparameter optimization framework uses various experimental settings:\n\n**1. Datasets:**\n- Real-world: `rcv1.binary`, `real-sim`, `news20`, `mnist`, `usps`, `sector_scale`, `gina_agnostic`, `leukemia`. Accessed via `libsvmdata.datasets.fetch_libsvm` or custom `sparse_ho.datasets.real.get_data`.\n- Synthetic: `sklearn.datasets.make_classification`, `sklearn.datasets.make_regression`.\n\n**2. Data Splitting:**\n- `train_test_split`: Data is commonly split into training (`idx_train`), validation (`idx_val`), and sometimes test (`idx_test`) sets (e.g., 50% train, 25% val, 25% test or 1/3 train, 1/3 val, 1/3 test).\n- `sklearn.model_selection.KFold`: Used for cross-validation (`cv=5` folds typically).\n\n**3. Inner Problem Solvers (Lasso-type models):**\n- `celer.Lasso`, `celer.ElasticNet`: For Lasso and ElasticNet problems.\n- `sklearn.linear_model.LogisticRegression` (with `penalty='l1'`, `solver='saga'` or `solver='liblinear'`): For sparse logistic regression.\n- `lightning.classification.LinearSVC`: For Support Vector Machines.\n- Common parameters for inner solvers: `fit_intercept=False`, `warm_start=True`, varying `max_iter` (e.g., 50, 100, 1_000, 10_000, 100_000), and `tol` (e.g., 1e-3, 1e-5, 1e-8, 1e-16, 1e-32).\n\n**4. Hyperparameter (`\text{lambda}`) Range and Initialization:**\n- `\text{alpha_max}`: Computed from the data, typically `np.max(np.abs(X.T @ y)) / n_samples` (for Lasso-type objectives).\n- `\text{alpha_min}`: A fraction of `\text{alpha_max}`, e.g., `1e-2 * \text{alpha_max}`, `\text{alpha_max} / 100`, or `\text{alpha_max} / 10_000`.\n- `\text{alpha0}` (starting point for optimization): Commonly `\text{alpha_max} / 10` or `\text{alpha_max} / 100`.\n- Regularization parameters are re-parameterized as `e^{\text{lambda}}`.\n\n**5. Hyperparameter Optimization Methods (Outer Loop):**\n- **Zero-order methods:**\n  - `grid_search`: Evaluates hyperparameters on a logarithmically spaced grid (`np.geomspace`) or linearly spaced grid (`np.linspace`) with `n_alphas` (e.g., 10, 15, 20, 30, 100) or `max_evals` points.\n  - `random_search` (using `hyperopt`): Samples hyperparameters uniformly on a log scale for `max_evals` (e.g., 30, 50, 100).\n  - `bayesian` (using `hyperopt`): Bayesian optimization for `max_evals` (e.g., 30, 50, 100).\n- **First-order methods:**\n  - `grad_search`: Employs gradient descent-based optimizers, using the `ImplicitForward` (or `Forward`, `Implicit`) algorithm for hypergradient computation.\n    - **Optimizers:** `LineSearch`, `GradientDescent`, `Adam`.\n    - **Optimizer Parameters:** `n_outer` (number of outer iterations, e.g., 10, 25, 30, 75, 100), `tol` (outer loop tolerance, e.g., 1e-5, 1e-7, 1e-8), `step_size` (for `GradientDescent`/`Adam`), `p_grad_norm` (for `GradientDescent`, e.g., 1, 1.5, 1.9), `lr` (for `Adam`, e.g., 0.11), `tol_decrease` ('constant', 'geom', 'exponential').\n\n**6. `ImplicitForward` Algorithm Parameters:**\n- `tol_jac`: Tolerance for Jacobian computation (e.g., 1e-3, 1e-8, 1e-11, 1e-32).\n- `n_iter_jac`: Maximum iterations for Jacobian computation (e.g., 100, 1_000, 5_000, 10_000, 100_000).\n- `max_iter`: Maximum iterations for inner solver, can be inherited from the overall `max_iter`.\n- `use_stop_crit`: Boolean, typically `True` to enable stopping criteria.\n\n**7. Objective/Criterion Functions (`C(\text{beta}(\text{lambda}))`):**\n- `HeldOutMSE`: Mean Squared Error on a held-out validation set.\n- `HeldOutLogistic`: Logistic loss on a held-out validation set.\n- `HeldOutSmoothedHinge`: Smoothed Hinge loss on a held-out validation set.\n- `CrossVal`: Cross-validation wrapper for other criteria (e.g., `HeldOutMSE`).\n- `FiniteDiffMonteCarloSure`: Smoothed Stein Unbiased Risk Estimator."
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL), an approach to efficiently tune hyperparameters for deep (reinforcement) learning systems by exploiting the iterative nature of training. Key contributions include an algorithm that optimizes the learning curve through training curve compression (instead of just final performance averages), a method to learn the compression curve from data, and a data augmentation technique for improved sample-efficiency. BOIL demonstrates superior performance in identifying optimal hyperparameters in minimal wall-clock time compared to existing baselines.",
        "methodology": "BOIL frames hyperparameter tuning as a cost-sensitive global optimization problem using Bayesian Optimization (BO). It models the black-box evaluation function over the joint space of hyperparameters (x) and training iterations (t) with a Gaussian Process (GP) using a product kernel. The core innovation is to compress the entire learning curve into a single numeric score using a Sigmoid preference function, whose growth and middle-point parameters are learned by maximizing the GP's log marginal likelihood. To enhance sample efficiency and avoid ill-conditioning of the GP covariance matrix, a selective data augmentation technique is employed, sampling points from the learning curve at maximum GP predictive uncertainty, guided by a condition number threshold.",
        "experimental_setup": "The algorithm's efficiency was demonstrated by tuning hyperparameters for two Deep Reinforcement Learning (DRL) agents and a Convolutional Neural Network (CNN). DRL experiments involved a Dueling DQN agent on the CartPole-v0 environment and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments. CNN tuning was performed on the SVHN and CIFAR10 datasets. All experiments were conducted on an NVIDIA 1080 GTX GPU using the tensorflow-gpu package, with DRL implementations based on OpenAI Baselines. Results were averaged over 20 independent runs. Baselines included Hyperband and Continuous Multi-Task/Fidelity BO (CM-T/F-BO). Square-exponential kernels were used for the GP, and a linear regressor approximated the cost function.",
        "limitations": "The cost function is approximated by a linear regressor, which might be overly simplistic if the actual cost has a more complex dependency on hyperparameters and iterations. While BOIL addresses the issue of GP covariance matrix ill-conditioning through selective data augmentation, this implies that careful management is required to prevent such problems. The approach relies on the assumption that a Sigmoid function is suitable for compressing learning curves, even with learnable parameters. From a broader perspective, the automation facilitated by BOIL could potentially distance humans from the modeling process, making critical failure detection more challenging.",
        "future_research_directions": "The approach is general and can be extended beyond machine learning algorithms to other processes exhibiting iterative structures, such as optimizing manufacturing pipelines. Further research could explore the impact of different acquisition functions and kernel choices within the BOIL framework, as these were not extensively compared in the current study. The paper also implies ongoing work in developing fully automated pipelines for ML model training and deployment.",
        "experimental_code": "from bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import ProductGaussianProcess\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass ProductGaussianProcess(object):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        \n        Euc_dist_t=euclidean_distances(T,T)\n               \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\\\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n\n        return x_max\n\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        \n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    def predict(self,xTest, eval_MSE=True):\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n\n\nclass BOIL(object):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        self.f = func\n    \n        self.X_ori= None\n\n        self.X = None\n        \n        self.Y = None\n               \n        self.Y_ori = None\n        \n        self.T=None\n        self.T_original=None\n        \n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        self.Y_curves=[]\n        \n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        np.random.seed(seed)\n\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))\n\n        self.Y_curves+=y_init_curves\n\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\\\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n\n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1))))\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))))\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:\n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n    def suggest_nextpoint(self):\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        start_opt=time.time()\n\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1:\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
        "experimental_info": "The method uses a Gaussian Process (GP) with a product kernel, specifically an RBF kernel for both hyperparameters (x) and training iterations (t). The GP model incorporates a `noise_delta` of `5e-4`.\n\nThe core innovation involves compressing the learning curve into a single numeric score using a Sigmoid preference function `1.0/(1+np.exp(-growth*(x-midpoint)))`. The parameters of this function (`midpoint` and `growth`), along with the GP's lengthscales (`lengthscale_x` and `lengthscale_t`), are learned by maximizing the GP's log marginal likelihood. This optimization is performed every `2*D` iterations (where `D` is the dimensionality of the search space).\n\nBounds for the optimized parameters are:\n- `lengthscale_x`: `[0.03, 0.3]`\n- `lengthscale_t`: `[0.3, 0.6]`\n- `midpoint`: `[-2, 3]`\n- `growth`: `[0.5, 2]`\n\nFor selecting the next evaluation point, BOIL employs an acquisition function proportional to `log(utility) - log(mean_cost)`. The `utility` is typically Expected Improvement (`ei_mu_max`), where the incumbent is the maximum of the GP mean function. The `mean_cost` is predicted using a `linear_model.LinearRegression` fitted to the observed `(X, T)` pairs and their associated costs.\n\nTo enhance sample efficiency, a selective data augmentation technique is utilized:\n- After each real observation, virtual observations are generated along the training iteration (time) dimension for the newly sampled hyperparameter `x`.\n- Up to `max_n_augmentation=10` virtual points are sampled.\n- These points are chosen by maximizing the GP's predictive uncertainty (using a `pure_exploration` acquisition function) along the time dimension.\n- The augmentation process stops if the logarithm of the GP covariance matrix condition number `log(cond_num)` exceeds a `threshold_cond=15`, or if the predictive variance is extremely low (`< (noise_delta + 1e-3)`).\n- The cost for these virtual observations is interpolated from the observed cost of the full learning curve.\n\nInitial points (`n_init_points=3` by default) are drawn randomly, and virtual observations are also generated for them."
      }
    },
    {
      "title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
      "abstract": "Despite the widespread adoption of prompting, prompt tuning and prefix-tuning\nof transformer models, our theoretical understanding of these fine-tuning\nmethods remains limited. A key question is whether one can arbitrarily modify\nthe behavior of pretrained model by prompting or prefix-tuning it. Formally,\nwhether prompting and prefix-tuning a pretrained model can universally\napproximate sequence-to-sequence functions. This paper answers in the\naffirmative and demonstrates that much smaller pretrained models than\npreviously thought can be universal approximators when prefixed. In fact, the\nattention mechanism is uniquely suited for universal approximation with\nprefix-tuning a single attention head being sufficient to approximate any\ncontinuous function. Moreover, any sequence-to-sequence function can be\napproximated by prefixing a transformer with depth linear in the sequence\nlength. Beyond these density-type results, we also offer Jackson-type bounds on\nthe length of the prefix needed to approximate a function to a desired\nprecision.",
      "full_text": "Prompting a Pretrained Transformer Can Be a Universal Approximator Aleksandar Petrov1 Philip H.S. Torr1 Adel Bibi 1 Abstract Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key ques- tion is whether one can arbitrarily modify the be- havior of pretrained model by prompting or prefix- tuning it. Formally, whether prompting and prefix- tuning a pretrained model can universally approxi- mate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when pre- fixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix- tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approx- imated by prefixing a transformer with depth lin- ear in the sequence length. Beyond these density- type results, we also offer Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision. 1. Introduction The scale of modern transformer architectures (Vaswani et al., 2017) is ever-increasing and training competitive models from scratch, even fine-tuning them, is often pro- hibitively expensive (Lialin et al., 2023). To that end, there has been a proliferation of research aiming at efficient train- ing in general and fine-tuning in particular (Rebuffi et al., 2017; Houlsby et al., 2019; Hu et al., 2021; 2023). Motivated by the success of few- and zero-shot learning (Wei et al., 2021; Kojima et al., 2022), context-based fine- tuning methods do not change the model parameters. In- stead, they modify the way the input is presented. For example, with prompting, one fine-tunes a string of tokens (a prompt) which is prepended to the user input (Shin et al., 2020; Liu et al., 2023). As optimizing over discrete tokens 1Department of Engineering Science, University of Oxford, UK. Correspondence to: A. Petrov <aleks@robots.ox.ac.uk>. is difficult, one can optimize the real-valued embeddings in- stead (soft prompting, prompt tuning, Lester et al. 2021). A generalization to this approach is the optimization over the embeddings of every attention layer (prefix-tuning, Li and Liang 2021). These methods are attractive as they require a few learnable parameters and allow for different prefixes to be used for different samples in the same batch which is not possible with methods that change the model param- eters. As every prompt and soft prompt can be expressed as prefix-tuning (Petrov et al., 2024), in this paper, we will focus primarily on prefix-tuning. While these context-based fine-tuning techniques have seen widespread adoption and are, in some cases, competitive to full fine-tuning (Liu et al., 2022), our understanding of their abilities and restrictions remains limited. How much can the behavior of a model be modified without changing any model parameter? Given a pretrained transformer and an arbitrary target function, how long should the prefix be so that the transformer approximates this function to an arbitrary precision? Differently put, can prefix-tuning of a pretrained transformer be a universal approximator? These are some of the questions we aim to address in this work. It is well-known that fully-connected neural networks with suitable activation functions can approximate any continu- ous function (Cybenko, 1989; Hornik et al., 1989; Barron, 1993; Telgarsky, 2015), while Recurrent Neural Networks (RNNs) can approximate dynamical system. The attention mechanism (Bahdanau et al., 2015) has also been studied in its own right. Deora et al. (2023) derived convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, and Mah- davi et al. (2023) showed that the memorization capacity increases linearly with the number of attention heads. On the other hand, it was shown that attention layers are not expressive enough as they lose rank doubly exponentially with depth if Multi-Layer Perceptrons (MLPs) and residual connections are not present (Dong et al., 2021). However, attention layers, with a hidden size that grows only loga- rithmically in the sequence lengths, were shown to be good approximators for sparse attention patterns (Likhosherstov et al., 2021), except for a few tasks that require a linear scal- ing of the size of the hidden layers in the sequence length (Sanford et al., 2023). 1 arXiv:2402.14753v1  [cs.LG]  22 Feb 2024Prompting a Pretrained Transformer Can Be a Universal Approximator Considering universal approximation using encoder-only transformers, Yun et al. (2019) showed that transformers are universal approximators of sequence-to-sequence func- tions by demonstrating that self-attention layers can com- pute contextual mappings of input sequences. Jiang and Li (2023) demonstrated universality by instead leveraging the Kolmogorov-Albert representation Theorem. Moreover, Al- berti et al. (2023) provided universal approximation results for architectures with non-standard attention mechanisms. Despite this interest in theoretically understanding the ap- proximation properties of the transformer architecture when being trained, much less progress has been made in under- standing context-based fine-tuning methods such as prompt- ing, soft prompting, and prefix-tuning. Petrov et al. (2024) have shown that the presence of a prefix cannot change the relative attention over the context and experimentally demonstrate that one cannot learn completely novel tasks with prefix-tuning. In the realm of in-context learning, where input-target pairs are part of the prompt (Brown et al., 2020), Xie et al. (2021) and Yadlowsky et al. (2023) show that the ability to generalize depends on the choice of pre- training tasks. However, these are not universal approx- imation results. The closest to our objective is the work of Wang et al. (2023). They quantize the input and output spaces allowing them to enumerate all possible sequence-to- sequence functions. All possible functions and inputs can then be hard-coded in a transformer using the constructions by Yun et al. (2019). As this approach relies on memo- rization, the depth of the model depends on the desired approximation precision ϵ. In this work, we demonstrate that prefix-tuning can be a uni- versal approximator much more efficiently than previously assumed. In particular: i. We show that attention heads are especially suited to model functions over hyperspheres, concretely, prefix- tuning a single attention head is sufficient to approx- imate any smooth continuous function on the hyper- sphere Sm to any desired precision ϵ; ii. We give a bound on the required prompt length to approximate a smooth target function to a precision ϵ; iii. We demonstrate how this result can be leveraged to approximate general sequence-to-sequence functions with transformers of depth linear in the sequence length and independent of ϵ; iv. We discuss how prefix-tuning may result in element- wise functions which, when combined with cross- element mixing from the pretrained model, may be able to explain the success behind prefix-tuning and prompting and why it works for some tasks and not others. 2. Background Material 2.1. Transformer Architecture In soft prompting and prefix-tuning, the focus of this work, the sequence fed to a transformer model is split into two parts: a prefix sequence P = ( p1, . . . ,pN ), which is to be learnt or hand-crafted, and an input sequence X = (x1, . . . ,xT ), where xi, pi ∈ Rd. A transformer operat- ing on a sequence consists of alternating attention blocks which operate on the whole sequence and MLPs that op- erate on individual elements. For a sequence of length N + T, an attention head of dimension d is a function ˜u : Rd×(N+T) → Rd×(N+T). Since we will only be in- terested in the output at the positions corresponding to the inputs X, we use u(· ; P) : Rd×(N+T) → Rd×T to denote the output of ˜u at the locations corresponding to the input X when prefixed with P. Therefore, the k-th output of u is defined as: [u(X;P)]k= NX i=1 exp(x⊤kHpi)WVpi+ TX j=1 exp(x⊤kHxj)WVxj NX i=1 exp(x⊤k Hpi) + TX j=1 exp(x⊤k Hxj) , (1) where WV , the value matrix, and H are in Rd×d. H is typically split into two lower-rank matrices H = W⊤ Q WK, query and key matrices. Multiple attention heads can be combined into an attention block but, for simplicity, we will only consider single head attention blocks. A transformer is then constructed by alternating attention heads and MLPs. We consider pretrained transformers but, in the context of this work, these are constructed rather than trained. We refer to the matrices WV , WQ, WK along with the parameters of the MLPs as pretrained parameters, and they are fixed throughout and not learnt. The prefix P is the only variable that can be modified to change the behavior of the model. 2.2. Universal Approximation Let X and Y be normed vector spaces. We consider a fam- ily of target functions which is a subset C of all mappings X→Y , i.e., C ⊆ YX , with C is often referred to as aconcept space. These are the relationships we wish to learn by some simpler candidate functions. Let us denote this set of can- didates by H ⊂ YX , called hypothesis space. The problem of approximation is concerned with how well functions in H approximate functions in C. There are two main ways to measure how well functions in H represent functions in C: density results and approximation rate results (Jiang et al., 2023). Density results show that, given an ϵ, one can find a hypothesis h ∈ Happroximating any f ∈ Cwith error at most ϵ. Approximation rate results, also called Jackson- type, are stronger as they offer a measure of complexity for h to reach a desired precision ϵ. Classically, a Jackson-type 2Prompting a Pretrained Transformer Can Be a Universal Approximator result would provide a minimum width or depth necessary for a neural network to reach a desired precision ϵ. In the context of the present work, the notion of complexity that we care about is the length N of the prefix P. Formally: Definition 1 (Universal Approximation (Density-Type)) . We say that H is a universal approximator for C over a compact set S ⊆ Xif for every f ∈ Cand every ϵ > 0 there exists an h ∈ Hsuch that supx∈S ∥f(x) −h(x)∥ ≤ϵ. One typically says that H is dense in C. Lemma 1 (Transitivity). If A is dense in B and B is dense in C, then A is dense in C. Definition 2 (Approximation Rate (Jackson-Type)). Fix a hypothesis space H. Let {HN : N ∈ N+} be a collection of subsets of H such that HN ⊂ HN+1 and S N∈N+ HN = H. Here, N is a measure of the complexity of the approxima- tion candidates, and HN is the subset of hypotheses with complexity at most N. Then, the approximation rate esti- mate for C over a compact S ⊆ Xis a bound ZH: N ≥ ZH(f, ϵ) =⇒ inf h∈HN sup x∈S ∥f(x)−h(x)∥ ≤ϵ, ∀f∈C. ZH gives an upper bound to the minimum hypothesis com- plexity necessary to reach the target precisionϵ and typically depends on the smoothness of f. Lemma 2. A Jackson bound for {HN | N ∈ N+} with finite ZH for all f ∈ C,ϵ > 0 immediately implies thatS N∈N+ HN = H is dense in C. Hence, Jackson bounds (Definition 2) are stronger than density results (Definition 1). The key hypothesis classes we consider in this work are the set of all prefixed attention heads and the set of pre- fixed transformers. This is very different from the classical universal approximation setting. The hypothesis classes in the classical universal approximation results consist of all possible parameter values of the model itself (Cybenko, 1989; Yun et al., 2019). When studying universal approxi- mation with prefixing, the model parameters are fixed where prefixes are what can be modified. Definition 3 (Prefixed Attention Heads Class) . This is the class of all attention heads as defined in Equation (1) of dimension d, input/output sequence of length T, pre- fix of length at most N, and fixed pretrained components H, WV ∈ Rd×d: HN,T −,d (H, WV ) = \u001a u : Rd×(N′+T)→Rd×T , [u]k as in (1), pi ∈ Rd, N′ ≤ N \u001b . For simplicity, we say that HN,T −,d is dense in C to imply that there exists a pair (H, WV ) such that HN,T −,d (H, WV ) is dense in C. When considering all possible prefix lengths, we drop the N: HT −,d = S N∈N HN,T −,d . Definition 4 (Prefixed Transformers Class). A transformer consists of L layers with each layer l consisting of an at- tention head with Hl and Wl V followed by an MLP con- sisting of kl linear layers, each parameterized as Ll k(x) = Al,kx+ bl,k interspersed with non-linear activation σ. This gives rise to the following hypothesis class when prefixed: HN,T ≡,d \u0012n Hl,Wl V ,{(Al,k,bl,k)}kl k=1 oL l=1 \u0013 = ( LL kL ◦...◦σ ◦ LL 1 ◦hL...◦h2 ◦ L1 k1 ◦...◦σ ◦ L1 1 ◦h1 withhl ∈ HN′,T −,d (Hl,Wl V ), l= 1, . . . , L, N′ ≤ N. ) , applying linear layers L element-wise. Again, we say HN,T ≡,d is dense in C, as a shorthand, to there exists {Hl,WlV,{Al,k,bl,k}kl k=1}Ll=1 such that HN,T ≡,d ({Hl, Wl V , Al,k, bl,k}L l=1) is dense in C. In this paper, we consider several different concept classes. For reasons that will become apparent in the following section, we focus on functions whose domain is a hyper- sphere Sm={y∈Rm+1 | ∥y∥2=1}⊂Rm+1. We consider both scalar and vector-valued functions on the hypersphere. Definition 5 (Scalar Functions on the Hypersphere). Define C(Sm) ⊂ RSm to be the space of all continuous functions defined on Sm with bounded norm, i.e., ∥f∥∞ = sup x∈Sm |f(x)| < ∞, f ∈ C(Sm). (2) This is the concept class Cs,m = C(Sm) ⊂ RSm . Definition 6 (Vector-valued Functions on the Hypersphere). The class of vector-valued functions on the hypersphere is: Cv,m = {f : Sm→Rm+1 | [f]i ∈ C(Sm), i= 1, ..., m+1}. Transformers are typically used to learn mappings over se- quences rather than individual inputs. Hence, we define several sequence-to-sequence concept classes. Definition 7 (General Sequence-to-sequence Functions) . Given a fixed sequence length T ∈ N>0, we define the sequence-to-sequence function class as: CT,m = {f : (Sm)T→(Rm+1)T | f continuous and bounded}. We will also consider the subset of element-wise functions: Definition 8 (Element-wise functions). Element-wise func- tions operate over sequences of inputs but apply the exact same function independently to all inputs: C∥,T,m=    there existsg ∈ Cv,m, such that f ∈ CT,m f(x1, ...,xT )=(g(x1), ..., g(xT )) for all (x1, ...,xT ) ∈ (Sm)T   . 3. Universal Approximation with a Single Attention Head In this section, we will restrict ourselves to the setting when the input sequence is of length T=1, i.e., X=(x). Gen- eral sequence-to-sequence functions will be discussed in 3Prompting a Pretrained Transformer Can Be a Universal Approximator C. Classic attention head A. Core attention head D. Classic attention head Core attention head control points centered at the KernelsControl points    and  B. N=100 N=200 N=1000 λ=100λ=50λ=10 Figure 1. Approximating functions on the hypersphere with a single attention head. A. We simplify the classical attention head into a core attention head. B. The exp(λ⟨x, pα k ⟩)pβ k terms act like kernels when x is restricted to a hypersphere. We can approximate a function f by placing N control points pα 1 , ...,pα N and centering a kernel at each of them. C. Increasing λ results in less smoothing, while increasing N results in more control points and hence better approximation. With large enough λ and N, we can approximate f to any desired accuracy. D. With the normalization term in classical attention close to a constant, and giving x, pα k and pβ k orthogonal subspaces, core attention can be represented as classical attention. Hence, a classical attention head can also approximate f with arbitrary precision. Section 4. We will show that a single attention head can approximate any continuous function on the hypersphere, or that H1 −,m+1 is dense in Cs,m. To do this, we first sim- plify the classical attention head in Equation (1), resulting in what we call a core attention head. Then, we show that each of the terms in the core attention act as a kernel, meaning that it can approximate any function in Cs,m. Finally, we show that any core attention head can be approximated by a classical attention head, hence, H1 −,m+1 is indeed dense in Cs,m. The complete pipeline is illustrated in Figure 1. To illuminate the approximation abilities of the attention head mechanism we relax it a bit. That is, we allow for different values of the prefix positions when computing the attention (the exp terms in Equation (1)) and when computing the value (the right multiplication withWV ). We will also drop the terms depending only on x, set H = λId, λ >0, and WV = Id. We refer to this relaxed version as a split attention head with its corresponding hypothesis class: h ⇋(x) = PN′ k=1 exp(λ⟨x, pα k ⟩)pβ kPN′ k=1 exp(λ⟨x, pα k ⟩) . (3) Definition 9 (Split Attention Head Class). HN⇋,d = n h ⇋as in (3), pα k , pβ k ∈ Rd, N′ ≤ N, λ >0 o . We will later show that a split head can be represented by a classical attention head. For now, let us simplify a bit further: we drop the denominator, resulting in our core attention head: h⊛(x) = PN k=1 exp(λ⟨x, pα k ⟩)pβ k, (4) which gives rise to the hypothesis class: HN ⊛,d = ( x 7→ PN′ k=1 exp(λ⟨x, pα k ⟩)pβ k, where pα k , pβ k ∈ Rd, N′ ≤ N, λ >0 ) . We also have their scalar-valued counterparts: h⊙(x) = PN k=1 exp(λ⟨x, pα k ⟩)pβ k, (5) HN ⊙,d = ( x 7→ PN′ k=1 exp(λ⟨x, pα k ⟩)pβ k, where pα k ∈ Rd, pβ k ∈ R, N′ ≤ N, λ >0 ) . As the dot product is a notion of similarity, one can interpret h⊛ in Equation (4) and h⊙ in Equation (5) as interpolators. The pα i vectors act as control points, while the pβ i vectors designate the output value at the location of the correspond- ing control point. The dot product with the input x controls how much each control point should contribute to the final result, with control points closer to x (larger dot product) contributing more. Unfortunately, it is not generally true that higher dot product means smaller distance, hence the above interpretation fails in Rm+1. To see this, consider two control points pα 1 , pα 2 ∈ Rm+1 such that pα 2 = tpα 1 , with t >1. Then for x = pα 1 4Prompting a Pretrained Transformer Can Be a Universal Approximator 1 1 θ θ = cos−1(⟨x, pα 1 ⟩) = cos−1(⟨x, pα 2 ⟩) x pα 2 pα 1 Figure 2. The dot product is a measure of closeness over the hypersphere. We want large dot product for points with lower distances. That is not the case for general pα 1, pα 2 ∈ Rm+1: above we show larger dot product for points which are further away, i.e., ⟨x, pα 1 ⟩<⟨x, pα 2 ⟩ despite ∥x-pα 1 ∥2<∥x-pα 1 ∥2. However, if we restrict x, pα i , and pα j to the hypersphere Sm, then the dot product measures the cosine between x and pi which is truly a measure of closeness: ⟨x, pα i ⟩<⟨x, pα j ⟩ ⇐⇒ ∥x-pα i ∥2>∥x-pα j ∥2. we would have ⟨x, pα 1 ⟩=∥pα 1 ∥2 2 < ⟨x, pα 2 ⟩=t∥pα 1 ∥2 2; the dot product is smaller forpα 1 , the control point that is closer tox, than for the much further awaypα 2 (see Figure 2). Therefore, the further away control point has a larger contribution than the closer point, which is at odds with the interpolation behaviour we desire. In general, the contribution of control points with larger norms will “dominate” the one of points with smaller norms. This has been observed for the attention mechanism in general by Demeter et al. (2020). Fortunately, the domination of larger norm control points pα i is not an issue if all control points have the same norm. In particular, if x and pα i lie on the unit hypersphere Sm={y∈Rm+1 | ∥y∥2=1} then ⟨x, pα i ⟩ = cos(∠(x, pα i )) and it has the desired property that the closer x is to pα i , the higher their dot product. By doing this, we restrict h⊛ to be a function from the hypersphere Sm to Rm+1. While this might seem artificial, modern transformer architectures do operate over hyperspheres as LayerNorm projects activa- tions to Sm (Brody et al., 2023). The central result of this section is that the functions in the form of Equation (5) can approximate any continu- ous function defined on the hypersphere, i.e., H⊙,m+1 =S∞ N=1 HN ⊙,m+1 is dense in Cs,m (Definition 5) and H⊛,m+1 = S∞ N=1 HN ⊛,m+1 is dense in Cv,m (Definition 6). Furthermore, we offer a Jackson-type approximation rate result which gives us a bound on the necessary prefix length N to achieve a desired approximation quality. Theorem 1 (Jackson-type Bound for Universal Approxima- tion on the Hypersphere). Let f ∈ C(Sm) be a continuous function on Sm, m ≥ 8 with modulus of continuity ω(f;t)= sup{|f(x)-f(y)| |x,y∈Sm,cos-1(⟨x,y⟩) ≤ t} ≤Lt, for some L > 0. Then, for any ϵ > 0, there exist pα 1 , . . . ,pα N ∈ Sm and pβ 1 , . . . , pβ N ∈ R such that sup x∈Sm \f\f\f\f\ff(x) − NX k=1 exp(λ⟨x, pα k ⟩)pβ k \f\f\f\f\f ≤ ϵ, where λ = Λ(ϵ/2) with Λ(σ) = (8LCR+mσ+σ) \u0010 1- σ2 8LCHCR+2σCH \u0011 σ 4LCR+σ σ \u0012 1- \u0010 1- σ2 8LCHCR+2σCH \u0011 2σ 4LCR+σ \u0013 = O \u0010 L3CH σ4 \u0011 , (6)and any N ≥ N(λ, ϵ) with N(λ, ϵ)=Φ(m) \u00103π(L+λ∥f∥∞)cm+1(λ) exp(λ) ϵ \u00112(m+1) =O(ϵ-10-14m-4m2 ), (7) with CH being a constant depending on the smoothness of f (formally defined in the proof), CR being a constant not depending on f or ϵ, Φ(m) = O(m log m) being a function that depends only on the dimension m and cm+1 being a normalization function. Corollary 1. H⊙,m+1 is dense in Cs,m Proof. Theorem 1 holds for all ϵ >0 and Lemma 2. Theorem 1 is a Jackson-type result as Equation (7) gives the number N of control points needed to approximate f with accuracy ϵ. This corresponds to the length of the prefix sequence. Moreover, the smoother the target f is, i.e., the smaller L, CH, the shorter the prefix length N. Thus, our construction uses only as much prefix positions as necessary. The proof of Theorem 1 follows closely (Ng and Kwong, 2022). While they only provide a density result, we offer a Jackson-type bound which is non-trivial and may be of an independent interest. The idea behind the proof is as following. We first approximate f with its convolution with a kernel having the form of the terms in Equation (5): (f ∗ KvMF λ )(x)=R Sm cm+1(λ) exp(λ⟨x,y⟩)f(y) dwm(y). (8) The larger the λ is, the closer f ∗KvMF λ is to f and hence the smaller the approximation error (Menegatto, 1997). Λ(ϵ/2) gives the smallest value forλ such that this error isϵ/2. Equa- tion (8) can then be approximated with sums: we partition Sm into N sets V1, ..., VN small enough that f does not vary too much within each set. Each control point pα k is placed in its corresponding Vk. Then, exp(λ⟨x, y⟩)f(y) can be approximated with exp(λ⟨x, pα k ⟩)f(pα k ) when y is in the k-th set Vk. Hence, Equation (8) can be approximated withPN k=1 exp(λ⟨x, pα k ⟩) Cf (pα k ) for some suitable constant C. By increasing N we can reduce the error of approxi- mating the convolution with the sum. Equation (7) gives us the minimum N needed so that this error is ϵ/2. Hence, we have error of at most ϵ/2 from approximating f with the convolution and ϵ/2 from approximating the convolution with the sum, resulting in our overall error being bounded by ϵ. The full proof is in Appendix B and is illustrated in Figure 4. The theorem can be extended to vector-valued functions in Cv,m with a multiplicative factor 1/√m+1: Corollary 2. Let f : Sm→Rm+1, m≥8 be such that each component fi satisfies the conditions in Theorem 1. De- fine ∥f∥∞= max1≤i≤m+1∥fi∥∞. Then, for any ϵ>0, there 5Prompting a Pretrained Transformer Can Be a Universal Approximator exist pα 1 , ...,pα N ∈ Sm and pβ 1 , ...,pβ N ∈ Rm+1 such that sup x∈Sm \r\r\r\r\rf(x) − NX k=1 exp(λ⟨x, pα k ⟩)pβ k \r\r\r\r\r 2 ≤ ϵ, with λ = Λ(ϵ/2√m+1) for any N ≥ N(λ, ϵ/√m+1). That is, H⊛,m+1 is dense in Cv,m with respect to the ∥ · ∥2 norm. Thanks to Theorem 1 and Corollary 2, we know that func- tions in Cv,m can be approximated by core attention (Equa- tion (4)). We only have to demonstrate that a core attention head can be represented as a classical attention head (Equa- tion (1)). We do this by reversing the simplifications we made when constructing the core attention head. Let’s start by bringing the normalization term back, resulting in HN⇋,d, the split attention head hypothesis (Definition 9). In- tuitively, Pn k=1 exp(λ⟨x, pα k ⟩) is almost constant when the pα k are uniformly distributed over the sphere as the distribu- tion of distances from x to pα k will be similar, regardless of where x lies. We can bound how far Pn k=1 exp(λ⟨x, pα k ⟩) is from being a constant and adjust the approximation error to account for it. Appendix C has the full proof. Theorem 2. Let f : Sm→Rm+1, m≥8 be such that each component fi satisfies the conditions in Theorem 1. Then, for any 0<ϵ<2∥f∥∞, there exist pα 1 , ...,pα N ∈Sm such that sup x∈Sm \r\r\r\r\rf(x) − PN k=1 exp(λ⟨x, pα k ⟩)pβ kPN k=1 exp(λ⟨x, pα k ⟩) \r\r\r\r\r 2 ≤ ϵ, with λ = Λ \u0012 2ϵL 2L + ∥f∥∞ \u0013 pβ k = f(pα k ), ∀k = 1, . . . , N, for any N ≥ N(λ, ϵ/√m+1). That is, H ⇋,m+1 is dense in Cv,m with respect to the ∥ · ∥2 norm. An interesting observation is that adding the normalization term has not affected the asymptotic behavior ofλ and hence also of the prefix length N. Furthermore, notice how the value pβ i at the control point pα i is simply f(pα k ), the target function evaluated at this control point. We ultimately care about the ability of the classical attention head (Definition 3) to approximate functions in Cv,m by prefixing. Hence, we need to bring back the terms depending only on the input x, combine pα k and pβ k parts into a single prefix pk and bring back the H and WV matrices. One can do this by considering an attention head with a hidden dimension 3(m+1) allowing us to place x, pα k and pβ k in different subspaces of the embedding space. To do this, define a pair of embedding and projection operations: Π : Sm → R3(m+1) Π−1 : R3(m+1) → Rm+1 x 7→   Im+1 0m+1 0m+1  x x 7→   Im+1 0m+1 0m+1   ⊤ x. Lemma 3. Π−1 ◦ H1 −,3(m+1) ◦ Π is dense in H ⇋,m+1, with the composition applied to each function in the class. Proof. We can prove something stronger. For all f ∈ H ⇋,m+1 there exists a g ∈ H1 −,3(m+1) such that f = Π−1 ◦ g ◦ Π. If f ∈ H ⇋,m+1, then f(x) = PN k=1 pβ k exp(λ⟨x, pα k ⟩) PN k=1 exp(λ⟨x, pα k ⟩) , ∀x ∈ Sm for some N, λ, pα i , pβ i . Define: pk=   0 λpα k pβ k   ∈ R3(m+1), H=   MI I 0 0 0 0 0 0 0  , WV =   0 0 I 0 0 0 0 0 0  ∈R3(m+1)×3(m+1), With M a negative constant tending to −∞. Then: g(x) = NX i=1 exp(x⊤Hpi)WV pi+ exp(x⊤Hx)WV x NX i=1 exp(x⊤Hpi) + exp(x⊤Hx) , is in H1 −,3(m+1) and f = Π−1 ◦ g ◦ Π. As this holds for all f ∈ H ⇋,m+1, it follows that H ⇋,m+1 ⊂ Π−1 ◦H1 −,3(m+1) ◦ Π. Hence, Π−1 ◦ H1 −,3(m+1) ◦ Π is dense in H ⇋,m+1. Lemma 3 shows that every split attention head can be ex- actly represented as 3 times bigger classical attention head. Note that our choice for H and WV is not unique. Equiva- lent constructions are available by multiplying each compo- nent by an invertible matrix, effectively changing the basis. Finally, the embedding and projection operations can be represented as MLPs and hence can be embedded in a trans- former architecture. Now, we can provide the final result of this section, namely that the standard attention head of a transformer can approximate any vector-valued function on the hypersphere: Theorem 3. Let f : Sm → Rm+1, m ≥ 8 be such that each component fi satisfies the conditions in Theorem 1. Then, for any 0 < ϵ ≤ 2∥f∥∞, there exists an attention head h ∈ HN,1 −,3(m+1) such that supx∈Sm∥f(x) − (Π−1 ◦ h ◦ Π)(x)∥2 ≤ ϵ, (9) 6Prompting a Pretrained Transformer Can Be a Universal Approximator for any N ≥ N(λ, ϵ/√m+1). That is, Π−1 ◦H1 −,3(m+1) ◦Π is dense in Cv,m with respect to the ∥ · ∥2 norm. Proof. The density result follows directly from Theorem 2 and Lemma 3 and transitivity (Lemma 1). The Jackson bound is the same as in Theorem 2 as transforming the split attention head to a classical attention head is exact and does not contribute further error. Therefore, we have shown that a single attention head with a hidden dimension 3(m+1) can approximate any continuous function f : C(Sm)→Rm+1 to an arbitrary accuracy. This is for fixed pretrained components, that is, H and WV are as given in the proof of Lemma 3 and depend neither on the input x nor on the target function f. Therefore, the behavior of the attention head is fully controlled by the prefix. This is a Jackson-type result, with the length N of the prefix given in Theorem 2. To the best of our knowledge, Theorem 3 is the first bound on the necessary prefix length to achieve a desired accuracy of function approximation using an attention head. Most critically, Theorem 3 demonstrates that attention heads are more expressive than commonly thought. A single attention head with a very simple structure can be a universal approximator. 4. Universal Approximation of Sequence-to-Sequence Functions The previous section showed how we can approximate any continuous f : Sm → Rm+1 with a single attention head. Still, one typically uses the transformer architecture for operations over sequences rather than over single inputs (the case with T ≥ 1). We will now show how we can leverage Theorem 3 to model general sequence-to-sequence functions. First, we show the simpler case of functions that apply the exact same mapping to all inputs. We then show how to model general sequence-to-sequence functions using a variant of the Kolmogorov–Arnold theorem. Element-wise functions Theorem 3 can be extended to element-wise functions where the exact same func- tion is applied to each element in the input sequence, i.e., the concept class C∥,T,m from Definition 8. If f ∈ C∥,T,m, then there exists a g ∈ Cv,m such that f(x1, . . . ,xT )=(g(x1), . . . , g(xT )). By Theorem 3, there exists a prefix p1, ...,pN that approximates g. As the con- struction in Lemma 3 prevents interactions between two dif- ferent inputs xi and xj, an attention head hT ∈ HN,T −,3(m+1) for a T-long input (Equation (1)) with the exact same prefix p1, . . . ,pN approximates f: Corollary 3. Π−1 ◦ HT −,3(m+1) ◦Π is dense in C∥,T,m with respect to the ∥ · ∥2 norm applied element-wise. That is, for every ϵ >0, there exists hT ∈ HN,T −,3(m+1) such that: sup {xi}∈(Sm)T max 1≤k≤T \r\r\u0002f({xi}) −(Π−1◦hT◦Π)({xi})\u0003 k \r\r 2 ≤ ϵ, with Π and Π−1 applied element-wise, [·]k selecting the k-th element, and approximate rate bound onN as in Theorem 3. General sequence-to-sequence functions Ultimately, we are interested in modeling arbitrary functions from se- quences of inputs (x1, . . . ,xT ) to sequences of outputs (y1, . . . ,yT ), that is, the CT,m. We will use a version of the Kolmogorov–Arnold representation Theorem. The Theo- rem is typically defined on functions over the unit hypercube [0, 1]m. As there exists a homeomorphism between [0, 1]m and a subset of Sm (Lemma D.1), for simplicity, we will ignore this technical detail. Our construction requires only T + 2 attention layers, each with a single head. The original Kolmogorov-Arnold representation theorem (Kolmogorov, 1957) identifies every continuous function f : [0, 1]d → R with univariate functions gq, ψp,q such that: f(x1, . . . , xd) = P2d q=0 gq \u0010Pd p=1 ψp,q(xp) \u0011 . In other words, multivariate functions can be represented as sums and compositions of univariate functions. As trans- formers are good at summing and attention heads are good at approximating functions, they can approximate functions of this form. However, gq and ψp,q are generally not well- behaved (Girosi and Poggio, 1989), so we will use the con- struction by Schmidt-Hieber (2021) instead. Lemma 4 (Theorem 2 in (Schmidt-Hieber, 2021)). For a fixed d, there exists a monotone functions ψ : [0, 1] → C (the Cantor set) such that for any function f : [0, 1]d → R, we can find a function g : C → R such that i. f(x1, . . . , xd) = g \u0010 3 Pd p=1 3−p ψ(xp) \u0011 , (10) ii. if f is continuous, then g is also continuous, iii. if |f(x) − f(y)| ≤Q∥x − y∥∞, for all x, y ∈ [0, 1]d and some Q, then |g(x) − g(y)| ≤2Q, ∀x, y∈ C. In comparison with the original Kolmogorov–Arnold the- orem, we need a single inner function ψ which does not depend on the target function f and only one outer function g. Furthermore, both ψ and g are Lipschitz. Hence, we can approximate them with our results from Section 3. We need to modify Lemma 4 a bit to make it fit the sequence- to-sequence setting. First, flatten a sequence of T (m + 1)- dimensional vectors into a single vector in [0, 1](m+1)T . Second, define Ψd : [0, 1]d → Rd to be the element-wise application of ψ: Ψd({xi}d i=1) = ( ψ(xi))d i=1. We can 7Prompting a Pretrained Transformer Can Be a Universal Approximator also define Gi : C → Rm+1, i = 1 , . . . , Tand extend Equation (10) for our setting: f(x1, ...,xT ) = (G1(R), . . . , GT (R)), with R = 3 TX i=1 3-(i-1)(m+1) m+1X p=1 3-pψ(xi,p) (11) = 3 TX i=1 3-(i-1)(m+1)   3-1 ... 3-(m+1)   ⊤ Ψm+1(xi). Equation (11) can now be represented with a transformer with T + 2attention layers. Ψm+1 is applied element-wise, hence, all Ψm+1(xi) can be computed in parallel with a single attention head (Corollary 3). The dot product with the \u0002 3-1 ··· 3-(m+1)\u0003 vector can be computed using a single MLP. The product with the 3-(i-1)(m+1) scalar is a bit more challenging as it depends on the position in the sequence. However, if we concatenate position encodings to the input, another MLP can use them to compute this factor and the multiplication. The outer sum over the T inputs and the multiplication by 3 can be achieved with a single attention head. Hence, using only 2 attention layers, we have compressed the whole sequence in a single scalar R. 1 The only thing left is to apply G1, . . . , GT to R to compute each of the T outputs. As each one of these is Lipschitz, we can approximate each with a single attention head using Theorem 3. Each Gi is different and would need its own set of prefixes, requiring T attention heads arranged in T attention layers. Using the positional encodings, each layer can compute the output for its corresponding position and pass the input unmodified for the other positions. The over- all prefix size would be the longest of the prefixes necessary to approximate Ψm+1, G1, . . . , GT . Hence, we have constructed an architecture that can approx- imate any sequence-to-sequence function f ∈ CT,m with only T+2 attention layers. Thus, HT ≡,d is dense in CT,m. 5. Discussion and Conclusions Comparison with prior work Just like us, Wang et al. (2023) show that prefix-tuning can be a universal approxi- mator. Their approach relies on discretizing the input space and the set of sequence-to-sequence functions to a given precision depending on ϵ, resulting in a finite number of pairs of functions and inputs, each having a unique cor- responding output. Then, using the results of Yun et al. (2019), they construct a meta-transformer which maps each of the function-input pairs to their corresponding output. 1Yun et al. (2019) use a similar approach but use discretization to enumerate all possible sequences and require O(ϵ−m) attention layers. In our continuous setting, R is computed with 2 layers. This approach has several limitations: i) the model has ex- ponential depth O(T ϵ−m); ii) reducing the approximation error ϵ requires increasing the model depth; iii) the prefix length is fixed, hence a constant function and a highly non- smooth function would have equal prefix lengths, and iv) it effectively has memorized all possible functions and inputs, explaining the exponential size of their constructions. In contrast, we show that memorization is not needed: atten- tion heads are naturally suited for universal approximation. Section 4 showed that T + 2 layers are enough, we require shorter prefixes for more smooth functions and reducing the approximation error ϵ can be done by increasing the prefix length, without modifying the pretrained model. Petrov et al. (2024) have shown that prefix-tuning cannot change the relative attention patterns over the input tokens and hence cannot learn tasks with new attention patterns. This appears to be a limitation but V on Oswald et al. (2023) and Aky¨urek et al. (2022) proved that there exist attention heads that can learn any linear model, samples of which are given as a prefix. In this work, we showed the existence of a “universal” attention head (H and WV in Lemma 3) that can be used to emulate any new function defined as a prefix. Prefixes have been observed to have larger norms than token embeddings (Bailey et al., 2023). Our results provide an explanation to that. While the control points pα k are in Sm and hence have norm 1, in Lemma 3 we fold λ into them. Recall that the less smooth f is, the higher the concentration parameter λ has to be in order to reduce the influence of one control point on the locations far from it. Hence, the less smooth f is, the larger the norm of the prefixes. Connection to prompting and safety implications While this work focused on prefix-tuning, the results can extend to prompting. Observe that prefix tuning (where we have a distinct prefix) can be reduced to soft prompting (where only the first layer is prefixed) by using an appropri- ate attention mechanism and position embeddings. Hence, if a function f ∈ CT,m requires N prefixes to be approx- imated to precision ϵ with prefix-tuning, it would require O(T N) soft tokens to be approximated with soft prompt- ing. Finally, observe that a soft token can be encoded with a sequence of hard tokens, the number of hard tokens per soft token depends on the required precision and the vo- cabulary size V . Hence, f could be approximated with O(logV (ϵ−1)mT N) hard tokens. Therefore, our univer- sal approximation results may translate to prompting. This raises concerns as to whether it is at all possible to prevent a transformer model from exhibiting undesirable behaviors (Zou et al., 2023; Wolf et al., 2023; Chao et al., 2023). Fur- thermore, this means that transformer-based agents might have the technical possibility to collude in undetectable and uninterpretable manner (de Witt et al., 2023). Still, our results require specific form of the attention and value ma- 8Prompting a Pretrained Transformer Can Be a Universal Approximator trices and, hence, it is not clear whether these risk translate to real-world models. Prefix-Tuning and Prompting a Pretrained Transformer might be Less efficient than Training it Typically, with neural networks one expects that the number of trainable parameters would grow as O(ϵ−m) (Schmidt-Hieber, 2021). Indeed that is the case for universal approximation with a transformer when one learns the key, query and value matrices and the MLP parameters as shown by Yun et al. (2019). However, as Equation (7) shows, our construction results in the trainable parameters (prefix length in our case) growing as O(ϵ−10−14m−4m2 ). That the m2 term indicates worse asymptotic efficiency of prefix-tuning and prompting compared to training a transformer. However, our approach may not be tight. Thus, it remains an open question if a tighter Jackson bound exists or if prefix-tuning and prompt- ing inherently require more trainable parameters to reach the same approximation accuracy as training a transformer. Prefix-tuning and prompting may work by combining prefix-based element-wise maps with pretrained cross- element mixing The construction for general sequence-to- sequence functions in Section 4 is highly unlikely to occur in transformers pretrained on real data as it requires very specific parameter values. While the element-wise setting (Corollary 3) is more plausible, it cannot approximate gen- eral sequence-to-sequence functions. Hence, neither result explains why prefix-tuning works in practice. To this end, we hypothesise that prompting and prefix-tuning, can mod- ify how single tokens are processed (akin to fine-tuning only MLPs), while the cross-token information mixing happens with pretrained attention patterns. Therefore, prompting and prefix-tuning can easily learn novel tasks as long as no new attention patterns are required. Our findings suggest a method for guaranteeing that a pretrained model possesses the capability to act as a token-wise universal approxima- tor. This can be achieved by ensuring each layer of the model includes at least one attention head conforming to the structure in Lemma 3. Limitations. We assume a highly specific pretrained model which is unlikely to occur in practice when pretrain- ing with real-world data. Hence, the question of, given a real-world pretrained transformer, which is the class of functions it can approximate with prefix-tuning is still open. This is an inverse (Bernstein-type, Jiang et al. 2023) bound and is considerably more difficult to derive. Impact Statement This paper presents theoretical understanding about how the approximation abilities of the transformer architecture. Our results show that, under some conditions, prompting and prefix-tuning can arbitrarily modify the behavior of a model. This may have implications on how we design safety and security measures for transformer-based systems. However, whether these theoretical risks could manifest in realistic pretrained models remains an open problem. Acknowledgements We would like to thank Tom Lamb for spotting several mistakes and helping us rectify them. This work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1) and the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (EP/S024050/1). AB has received funding from the Amazon Research Awards. We also thank the Royal Academy of Engineering and FiveAI. References Ekin Aky¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. In International Conference on Learning Representations. Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Ku- tyniok. 2023. Sumformer: Universal approximation for efficient transformers. In Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in Ma- chine Learning (TAG-ML). Donald E Amos. 1974. Computation of modified Bessel functions and their ratios. Mathematics of Computation, 28(125):239–251. Kendall Atkinson and Weimin Han. 2012. Spherical Har- monics and Approximations on the Unit Sphere: An In- troduction. Yogesh J Bagul and Satish K Panchal. 2018. Certain in- equalities of Kober and Lazarevi´c type. Research Group in Mathematical Inequalities and Applications Research Report Collection, 21(8). Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations. Luke Bailey, Gustaf Ahdritz, Anat Kleiman, Siddharth Swa- roop, Finale Doshi-Velez, and Weiwei Pan. 2023. Soft prompting might be a bug, not a feature. In Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning. Alex Barnett. 2021. Lower bounds on the modified Bessel function of the first kind. Mathematics Stack Exchange. Andrew R Barron. 1993. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Trans- actions on Information Theory, 39(3):930–945. 9Prompting a Pretrained Transformer Can Be a Universal Approximator Shaked Brody, Uri Alon, and Eran Yahav. 2023. On the expressivity role of LayerNorm in transformers’ attention. arXiv preprint arXiv:2305.02582. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems. Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. 2023. Jail- breaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419. George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314. Feng Dai and Yuan Xu. 2013. Approximation Theory and Harmonic Analysis on Spheres and Balls. Christian Schroeder de Witt, Samuel Sokota, J. Zico Kolter, Jakob Foerster, and Martin Strohmeier. 2023. Perfectly se- cure steganography using minimum entropy coupling. In International Conference on Learning Representations. David Demeter, Gregory Kimmel, and Doug Downey. 2020. Stolen probability: A structural weakness of neural lan- guage models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. 2023. On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680. Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning. Ricardo Estrada. 2014. On radial functions and distributions and their Fourier transforms. Journal of Fourier Analysis and Applications, 20(2):301–320. Uriel Feige and Gideon Schechtman. 2002. On the opti- mality of the random hyperplane rounding technique for MAX CUT. Random Structures & Algorithms, 20(3):403– 440. Paul Funk. 1915. Beitr¨age zur Theorie der Kugelfunktionen. Mathematische Annalen, 77:136–152. Federico Girosi and Tomaso Poggio. 1989. Representa- tion properties of networks: Kolmogorov’s theorem is irrelevant. Neural Computation, 1(4):465–469. E Hecke. 1917. ¨Uber orthogonal-invariante Integralgle- ichungen. Mathematische Annalen, 78:398–404. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter- efficient transfer learning for NLP. In International Con- ference on Machine Learning. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-rank adaptation of large lan- guage models. In International Conference on Learning Representations. Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An adapter family for parameter- efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933. Haotian Jiang and Qianxiao Li. 2023. Approximation theory of transformer networks for sequence modeling. arXiv preprint arXiv:2305.18475. Haotian Jiang, Qianxiao Li, Zhong Li, and Shida Wang. 2023. A brief survey on the approximation theory for sequence modelling. arXiv preprint arXiv:2302.13752. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural In- formation Processing Systems. Andrei Nikolaevich Kolmogorov. 1957. On the represen- tation of continuous functions of many variables by su- perposition of continuous functions of one variable and addition. In Doklady Akademii Nauk, volume 114, pages 953–956. Russian Academy of Sciences. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing. Shengqiao Li. 2010. Concise formulas for the area and volume of a hyperspherical cap. Asian Journal of Mathe- matics & Statistics, 4(1):66–70. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Opti- mizing continuous prompts for generation. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers). 10Prompting a Pretrained Transformer Can Be a Universal Approximator Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter- efficient fine-tuning. arXiv preprint arXiv:2303.15647. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. 2021. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- roaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. 2023. Memorization capacity of multi-head attention in transformers. arXiv preprint arXiv:2306.02010. Valdir Antˆonio Menegatto. 1997. Approximation by spher- ical convolution. Numerical Functional Analysis and Optimization, 18(9-10):995–1012. Tin Lok James Ng and Kwok-Kun Kwong. 2022. Universal approximation on the hypersphere. Communications in Statistics – Theory and Methods, 51(24):8694–8704. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2024. When do prompting and prefix-tuning work? A theory of capabilities and limitations. In International Conference on Learning Representations. David L Ragozin. 1971. Constructive polynomial approxi- mation on spheres and projective spaces. Transactions of the American Mathematical Society, 162:157–170. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. C. A. Rogers. 1963. Covering a sphere with spheres. Math- ematika, 10(2):157–164. Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2023. Representational strengths and limitations of transform- ers. arXiv preprint arXiv:2306.02896. Johannes Schmidt-Hieber. 2021. The Kolmogorov–Arnold representation theorem revisited. Neural Networks , 137:119–126. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with automatically gen- erated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Matus Telgarsky. 2015. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zh- moginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. 2023. Universality and limitations of prompt tuning. In Advances in Neural Information Processing Systems. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learn- ing Representations. Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. 2023. Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit Bayesian inference. In International Confer- ence on Learning Representations. Steve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni. 2023. Pretraining data mixtures enable narrow model se- lection capabilities in transformer models. arXiv preprint arXiv:2311.00871. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. 2019. Are trans- formers universal approximators of sequence-to-sequence functions? In International Conference on Learning Rep- resentations. Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrik- son. 2023. Universal and transferable adversarial at- tacks on aligned language models. arXiv preprint arXiv:2307.15043. 11Prompting a Pretrained Transformer Can Be a Universal Approximator A. Background on Analysis on the Sphere As mentioned in the main text, the investigation of the properties of attention heads naturally leads to analysing functions over the hypersphere. To this end, our results require some basic facts about the analysis on the hypersphere. We will review them in this appendix. For a comprehensive reference, we recommend (Atkinson and Han, 2012) and (Dai and Xu, 2013). Define Pk(Rm+1) to be the space of polynomials of degree at most k. The restriction of a polynomial p ∈ Pk(Rm+1) to the unit hypersphere Sm = {x ∈ Rm+1 | ∥x∥2 = 1} is called a spherical polynomial. We can thus define the space of spherical polynomials: Pk(Sm) = {p|Sm for p ∈ Pk(Rm+1)}. Define by Hk(Rm+1) the space of polynomials of degree k that are homogeneous: Hk(Rm+1) = span ( (x1, . . . , xm+1) 7→ xα1 1 × ··· ×xαm+1 m+1 | m+1X i=1 αi = k ) . Its restriction to the sphere Hk(Sm) is defined analogously to Pk(Sm). Finally, we can define the space Y(Rm+1) of harmonic homogeneous polynomials: Yk(Rm+1) = \u001a p ∈ Hk(Rm+1) | ∂2 ∂x2 p(x) = 0, ∀x ∈ Rm+1 \u001b . Yk(Sm) which is the restriction of Yk(Rm+1) to Sm is the set of spherical harmonics of degree k. Spherical harmonics are the higher-dimensional extension of Fourier series. Notably, even though Yk(Sm) ⊂ Hk(Sm) ⊂ Pk(Sm), the restriction of any polynomial on Sm is a sum of spherical harmonics: Pk(Sm) = Y0(Sm) ⊕ ··· ⊕Yk(Sm), with ⊕ being the direct sum (Atkinson and Han, 2012, Corollary 2.19). We define C(Sm) to be the space of all continuous functions defined on Sm with the uniform norm ∥f∥∞ = sup x∈Sm |f(x)|, f ∈ C(Sm). (12) Similarly, Lp(Sm), 1 ≤ p <∞ is the space of all functions defined on Sm which are integrable with respect to the standard surface measure dwm. The norm in this space is: ∥f∥p = \u0012 1 wm Z Sm |f(x)|p dwm(x) \u00131/p , f ∈ Lp(Sm), (13) with the surface area being wm = Z Sm dwm = 2π (m+1)/2 Γ((m+1)/2). (14) We will use Vm to denote any of these two spaces and ∥ · ∥m the corresponding norm. A key property of spherical harmonics is that sums of spherical harmonics can uniformly approximate the functions in C(Sm). In other words, the span of S∞ k=0 Yk(Sm) is dense in C(Sm) with respect to the uniform norm ∥ · ∥∞. Hence, any f ∈ C(Sm) can be expressed as a series of spherical harmonics: f(x) = ∞X k=0 Y m k (x), with Y m k ∈ Yk(Sm), ∀k. We will also make a heavy use of the concept of spherical convolutions. Define the space of kernelsL1,m to consist of all measurable functions K on [−1, 1] with norm ∥K∥1,m = wm−1 wm Z 1 −1 |K(t)|(1 − t2)(m−2)/2 dt < ∞. 12Prompting a Pretrained Transformer Can Be a Universal Approximator Log Value 0.0 0.2 0.4 0.6 0.8 1 .0 Figure 3.Plots of the von Mises-Fisher kernel KvMF λ (⟨x, y⟩) for λ = 1, 5, 10 and fixed y in three dimensions (m = 2). The larger λ is, the more concentrated the kernel is around y. Definition A.1 (Spherical convolution). The spherical convolution K ∗ f of a kernel K in L1,m with a function f ∈ Vm is defined by: (K ∗ f)(x) = 1 wm Z Sm K(⟨x, y⟩)f(y) dwm(y), x ∈ Sm. Spherical convolutions map functions f ∈ Vm to functions in Vm. Furthermore, the spherical harmonics are eigenfunctions of the function generated by a kernel in L1,m: Lemma A.1 (Funk and Hecke’s formula (Funk, 1915; Hecke, 1917; Estrada, 2014)). K ∗ Y m k = am k (K)Y m k , when K ∈ L1,m, Ym k ∈ Yk(Sm), k= 0, 1, . . . , where am k (K) are the coefficients in the series expansion in terms of Gegenbauer polynomials associated with the kernel K: am k (K) = wm−1 wm Z 1 −1 K(t) Q(m−1)/2 k (t) Q(m−1)/2 k (1) (1 − t2)(m−2)/2 dt, k= 0, 1, . . . . (15) Here, Q(m−1)/2 k is the Gegenbauer polynomial of degreek. Note also that with a change of variables we have: Z Sm K(⟨x, y⟩) dwm(y) = wm−1 Z 1 −1 K(t)(1 − t2)(m−2)/2 dt. (16) Ideally, we would like a kernel that acts as an identity for the convolution operation. In this case, we would have ∥K ∗ f − f∥∞ = 0, f∈ C(Sm) which would be rather convenient. However, there is no such kernel in the spherical setting (Menegatto, 1997). The next best thing is to construct a sequence of kernels {Kn} ∈ L1,m such that ∥Kn ∗ f − f∥m → 0 as n → ∞for all f ∈ Vm. This sequence of kernels is called an approximate identity. The specific such sequence of kernels we will use is based on the von Mises-Fisher distribution as this gives us the exp(⟨x, y⟩) form that we also observe in the transformer attention mechanism. Definition A.2 (von Mises-Fisher kernels, (Ng and Kwong, 2022)). We define the sequence of von Mises-Fisher kernels as: KvMF λ (t) = cm+1(λ) exp(λt), t∈ [−1, 1], where cm+1(λ) = wmλ m+1 2 −1 (2π) m+1 2 Im+1 2 −1(λ) , with Iv being the modified Bessel function at order v. Note that a von Mises-Fisher kernel can also be expressed in terms of points onSm. In particular, for a fixedy ∈ Sm we have KvMF λ (⟨x, y⟩), x ∈ Sm. The parameter λ is a “peakiness” parameter: the large λ is, the closer KvMF λ (⟨x, y⟩) approximates 13Prompting a Pretrained Transformer Can Be a Universal Approximator A. B. C. D. E. F. Target function Convolution with von Mises-Fisher kernel Partition of    is approximately constant in each cell Approximating dot product with a point in the partition when Sum over the partition Figure 4. Intuition behind the proof of our Jackson-type bound for universal approximation on the hypersphere. A. We want to approximate a function f over the hypersphere Sm. This illustration is in three-dimensional space, so m = 2. B. In order to get the exp(λ⟨·, y⟩) form that we want, we convolve f with the KvMF λ (t) = cm+1(λ) exp(λt) kernel. C. We partition Sm into N cells V1,. . . ,VN . D. Our choice of N is such that f does not vary too much in each cell and hence can be approximated by a function that is constant in each Vk. E. As each cell is small, the dot product of x with any point in the cell Vk can be approximated by the dot product of x with a fixed point bk ∈ Vk. F. This allows us to approximate the integral in the convolution KvMF λ with a finite sum. the delta function centered at y, as can be seen in Figure 3. It is easy to check that ∥KvMF λ ∥1,m = 1, ∀λ >1, m >1 and hence the sequence is in L1,m, meaning they are valid kernels. Ng and Kwong (2022, Lemma 4.2) show that {KvMF λ } is indeed an approximate identity, i.e., ∥KvMF λ ∗ f − f∥m → 0 as λ → ∞for all f ∈ Vm.2 As we want a Jackson-type result however, we will need to upper bound the error ∥KvMF λ ∗ f − f∥m as a function of λ, that is a non-asymptotic result on the quality of the approximation by spherical convolutions with KvMF λ . We do that in Lemma B.5. B. A Jackson-type Bound for Universal Approximation on the Unit Hypersphere The overarching goal in this section is to provide a Jackson-type (Definition 2) bound for approximating functions f : Sm → Rm+1 on the hypersphere Sm = {x ∈ Rm+1 | ∥x∥2 = 1} by functions of the form h(x) = NX k=1 ξk exp(λ⟨x, bk⟩) (17) To this end, we will leverage results from approximation on the hypersphere using spherical convolutions by Menegatto (1997) and recent results on the universal approximation on the hypersphere by Ng and Kwong (2022). While these two works inspire the general proof strategy, they only offer uniform convergence (i.e., density-type results, Definition 1). Instead, we offer a non-asymptotic analysis and develop the first approximation rate results on the sphere for functions of the form of Equation (17), i.e., Jackson-type results (Definition 2). The high-level idea of the proof is to split the goal into approximatingf with the convolutionf ∗KvMF λ and approximating the convolution f ∗KvMF λ with a sum of terms that have theexp(t⟨x, bk⟩) structure resembling the kernel KvMF λ (Definition A.2): sup x∈Sm \r\r\r\r\rf(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \r\r\r\r\r ≤ \r\rf − f ∗ KvMF λ \r\r ∞| {z } Equation (19) / Lemma B.5 + \r\r\r\r\rf ∗ KvMF λ − NX k=1 ξk exp(λ⟨·, bk⟩) \r\r\r\r\r ∞| {z } Lemma B.7 . (18) This is also illustrated in Figure 4. Let’s focus on the first term in Equation (18). It can be further decomposed into three terms by introducing Wq ∈ Pq(Sm), 2The wm term in the normalization constant cm+1(λ) is not in (Ng and Kwong, 2022). However, without it KvMF λ are not an approximate identity. 14Prompting a Pretrained Transformer Can Be a Universal Approximator the best approximation of f with a spherical polynomial of degree q: ∥KvMF λ ∗ f − f∥∞ ≤ ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥∞| {z } Lemma B.2 + ∥KvMF λ ∗ Wq − Wq∥∞| {z } Lemma B.4 + ∥Wq − f∥∞| {z } Lemma B.1 . (19) There are a number of Jackson-type results for how well finite sums of spherical polynomials approximate functionsf ∈ Vm (the last term in Equation (19)). In particular, they are interested in bounding min Wq∈Pq(Sm) ∥f − Wq∥p, 1 ≤ p ≤ ∞. (20) We will use a simple bound by Ragozin (1971): Lemma B.1 (Ragozin bound). For f ∈ C(Sm) and q ∈ N>0 it holds that: min Wq∈Pq(Sm) ∥f − Wq∥∞ ≤ CR ω \u0012 f; 1 q \u0013 , (21) for some constant CR that does not depend on f or q and ω being the first modulus of continuity of f defined as: ω(f; t) = sup{|f(x) − f(y)| |x, y ∈ Sm, cos−1(x⊤y) ≤ t}. We recommend Atkinson and Han (2012, Chapter 4) and Dai and Xu (2013, Chapter 4) for an overview of the various bounds proposed for Equation (20) depending on the continuity properties of f and its derivatives. In particular, the above bound could be improved with a term 1/nk if f has k continuous derivates (Ragozin, 1971). We can upper-bound the first term in Equation (19) by recalling that the norm of the kernel KvMF λ is 1: Lemma B.2. ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥m ≤ ∥f − Wq∥m. Hence: ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥∞ ≤ ∥f − Wq∥∞ ≤ CR ω \u0012 f; 1 q \u0013 . Proof. Convolution is linear so ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥m = ∥KvMF λ ∗ (f − Wq)∥m. Using the H ¨older inequality (Dai and Xu, 2013, Theorem 2.1.2) we get ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥m ≤ ∥KvMF λ ∥1,m∥f − Wq∥m. As ∥KvMF λ ∥1,m = 1 for all λ >0, m >1, we obtain the inequality in the lemma. For the uniform norm, we also use the Ragozin bound from Lemma B.1. Only the second term in Equation (19) is left. However, before we tackle it, we will need a helper lemma that bounds the eigenvalues of the von Mises-Fisher kernel (Equation (15)): Lemma B.3 (Bounds on the eigenvalues am k (KvMF λ )). The eigenvalues am k , as defined in Equation (15), for the sequence of von Mises-Fisher kernels (Definition A.2) are bounded from below and above as: 0 <   λ \u0000m−1 2 + k \u0001 + q λ2 + \u0000m−1 2 + k \u00012   k ≤ am k (KvMF λ ) ≤ 1 Proof. We have am k (KvMF λ ) = wm−1 wm Z 1 −1 KvMF λ (t) Q(m−1)/2 k (t) Q(m−1)/2 k (1) (1 − t2)(m−2)/2dt = wm−1 wm Z 1 −1 cm+1(λ) exp(λt) Q(m−1)/2 k (t) Q(m−1)/2 k (1) (1 − t2)(m−2)/2dt 15Prompting a Pretrained Transformer Can Be a Universal Approximator 1 0 20 30 40 5 0 0.0 0.2 0.4 0.6 0.8 1 .0 True parameter Lower bound Figure 5.The coefficients am k for the von Mises-Fisher kernels KvMF λ for m = 2and k ∈ {1, 2, 3} as well as the lower bound from Lemma B.3. ⋆ = Im−1 2 +k(λ) Im−1 2 (λ) , with ⋆ solved using Mathematica. From here we can see that am 0 (KvMF λ ) = 1 for all m >1, λ >1. Furthermore, for v >1 and λ >0 the modified Bessel function of the first kind Iv(λ) is monotonically decreasing as v increases. Therefore, am k (KvMF λ ) ≤ am 0 (KvMF λ ) = 1, which gives us the upper bound in the lemma. For the lower bound, we will use the following bound on the ratio of modified Bessel functions by Amos (1974, Eq. 9): Iv+1(x) Iv(x) ≥ x (v + 1) + p x2 + (v + 1)2 , v≥ 0, x≥ 0. As mentioned above, 0 ≤ Iv+1(x) Iv(x) ≤ 1. Furthermore, these ratios are decreasing as v increases, i.e., Iv+2(x) Iv+1(x) ≤ Iv+1(x) Iv(x) for all v ≥ 0 and x ≥ 0 (Amos, 1974, Eq. 10). Combining these facts gives us: Iv+k(x) Iv(x) ≥ \u0012 Iv+k(x) Iv+k−1(x) \u0013k =   x (v + k) + p x2 + (v + k)2 !k . (22) We can now give the lower bound foram k (KvMF n ) using Equation (22) : am k (KvMF λ ) = Im−1 2 +k(λ) Im−1 2 (λ) ≥   λ \u0000m−1 2 + k \u0001 + q λ2 + \u0000m−1 2 + k \u00012   k . The lower bound for m = 2 is plotted in Figure 5. We can now provide a bound for the second term in Equation (19): Lemma B.4. Take an f ∈ C(Sm). Furthermore, assume that there exists a constant CH ≥ 0 that upper-bounds the norms of the spherical harmonics of any best polynomial approximation Wq of f: for all q ≥ 1, Wq = qX k=0 Y m k , Ym k ∈ Y(Sm), ∥Y m k ∥∞ ≤ CH, ∀k = 0, . . . , q,where Wq = arg min h∈Pq(Sm) ∥f − h∥∞. Then ∥KvMF λ ∗ Wq − Wq∥∞ ≤ CH q  1 −   λ \u0000m−1 2 + q \u0001 + q λ2 + \u0000m−1 2 + q \u00012   q . 16Prompting a Pretrained Transformer Can Be a Universal Approximator Proof. Using that Wq is a spherical polynomial of degree q and hence can be expressed as a sum of spherical harmonics Wq = Pq k=0 Y m k , we get: ∥KvMF λ ∗ Wq − Wq∥∞ = \r\r\r\r\rKvMF λ ∗ qX k=0 Y m k (x) − qX k=0 Y m k (x) \r\r\r\r\r ∞ = \r\r\r\r\r qX k=0 \u0000 KvMF λ ∗ Y m k (x) − Y m k (x) \u0001 \r\r\r\r\r ∞ ≤ qX k=0 \r\r\u0000 KvMF λ ∗ Y m k (x) − Y m k (x) \u0001\r\r ∞ (Triangle inequality) = qX k=0 \r\r\u0000 am k (KvMF λ )Y m k (x) − Y m k (x) \u0001\r\r ∞ (from Lemma A.1) = qX k=0 \r\r(am k (KvMF λ ) − 1) Y m k (x) \r\r ∞ = qX k=0 |am k (KvMF λ ) − 1|∥Y m k (x)∥∞ ≤ qX k=0 |am k (KvMF λ ) − 1| CH ≤ CH qX k=0  1 −   λ \u0000m−1 2 + k \u0001 + q λ2 + \u0000m−1 2 + k \u00012   k  (from Lemma B.3) ≤ CH qX k=0   1 −   λ \u0000m−1 2 + q \u0001 + q λ2 + \u0000m−1 2 + q \u00012   | {z } B k   (as k ≤ q) = CH   q + 1 − qX k=0 Bk ! ≤ CH (q + 1 − (1 + qBq)) (using that 0 < B <1) ≤ CH q (1 − Bq) . We can finally combine Lemmas B.1, B.2 and B.4 in order to provide an upper bound to Equation (19): Lemma B.5 (Bound on ∥KvMF λ ∗ f − f∥∞). Take an f ∈ Vm with modulus of continuity ω(f; t) ≤ Lt. As in Lemma B.4, assume that there exists a constant CH ≥ 0 that upper-bounds the norms of the spherical harmonics of any best polynomial approximation Wq of f: for all q ≥ 1, Wq = qX k=0 Y m k , Ym k ∈ Y(Sm), ∥Y m k ∥∞ ≤ CH, ∀k = 0, . . . , q,where Wq = arg min h∈Pq(Sm) ∥f − h∥∞. If λ ≥ Λ(ϵ), where Λ(ϵ) = (8LCR + mϵ + ϵ) \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 ϵ 4LCR+ϵ ϵ \u0010 1 − \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 2ϵ 4LCR+ϵ \u0011 = O \u0012L3CHC3 R ϵ4 \u0013 . (23) then ∥f − KvMF λ ∗ f∥∞ ≤ ϵ. 17Prompting a Pretrained Transformer Can Be a Universal Approximator Proof. As we want to upper-bound Equation (19) with ϵ, we will split our ϵ budget over the three terms. For the first and the third terms, using the Ragozin bound from Lemma B.1 we have: ∥f − Wq∥∞ ≤ CR ω \u0012 f; 1 q \u0013 ≤ CRL q , q≥ 1. (24) We want to select an integer q large enough so that ∥f − Wq∥∞ ≤ ϵ/4. That is q = \u00064CRL ϵ \u0007 . This will be how we bound the first and last terms in Equation (19). Let’s focus on the second term. Pick Wq to be the best approximation from the Ragozin bound. From Lemma B.4 we have that ∥KvMF λ ∗ Wq − Wq∥∞ ≤ CH q (1 − Bq) , where B = λ \u0000m−1 2 + q \u0001 + q λ2 + \u0000m−1 2 + q \u00012 . The error budget we want to allocate for the ∥KvMF λ ∗ Wq − Wq∥∞ term is ϵ/2. Hence: B ≥ \u0012 1 − ϵ 2CHq \u00131/q = D =⇒ ∥KvMF n ∗ Wq − Wq∥∞ ≤ ϵ 2. (25) We just need to find the minimum value for λ such that Equation (25) holds. We have: B = λ \u0000m−1 2 + q \u0001 | {z } E + q λ2 + \u0000m−1 2 + q \u00012 = λ E + √ λ2 + E2 . (26) Then, combining Equations (25) and (26) we get: λ E + √ λ2 + E2 ≥ D λ ≥ 2DE 1 − D2 . Finally, replacing D and E with the expressions in Equations (25) and (26), upper-bounding q = \u00064CRL ϵ \u0007 as q ≥ 4CRL ϵ + 1 and simplifying the expression we get our final bound for λ. If λ ≥ Λ(ϵ), with Λ(ϵ) = (8LCR + mϵ + ϵ) \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 ϵ 4LCR+ϵ ϵ \u0010 1 − \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 2ϵ 4LCR+ϵ \u0011 , then ∥KvMF λ ∗ Wq − Wq∥∞ ≤ ϵ/2. Hence, for any λ ≥ Λ(ϵ) we have: ∥KvMF λ ∗ f − f∥∞ ≤ ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥∞ + ∥KvMF λ ∗ Wq − Wq∥∞ + ∥Wq − f∥∞ ≤ ∥f − Wq∥∞ + ϵ 2 + ∥Wq − f∥∞ ≤ ϵ 4 + ϵ 2 + ϵ 4 = ϵ. This concludes our bound on Equation (19). Finally, to give the asymptotic behavior of Λ(ϵ) as ϵ → 0 we observe that the Taylor series expansion of Λ around ϵ = 0 is: Λ(ϵ) = 128L3CHC3 R ϵ4 + 16L2(m − 1)CHC2 R ϵ3 + O \u0012 1 ϵ2 \u0013 , 18Prompting a Pretrained Transformer Can Be a Universal Approximator hence: Λ(ϵ) = O \u0012L3CHC3 R ϵ4 \u0013 . Lemma B.5 is our bound on Equation (19) which is also the first term of Equation (18). Recall that bounding Equation (18) is our ultimate goal. Hence, we are halfway done with our proof. Let’s focus now on the second term in Equation (18), that is, how well we can approximate the convolution of f with the von Mises-Fisher kernel using a finite sum: \r\r\r\r\rf ∗ KvMF λ − NX k=1 ξk exp(λ⟨·, bk⟩) \r\r\r\r\r ∞ . The basic idea behind bounding this term is that we can partition the hypersphere Sm into N sets ({V1, . . . , VN }), each small enough so that, for a fixed x ∈ Sm, the K(⟨x, y⟩)f(y) term in the convolution (K ∗ f)(x) = 1 wm Z Sm K(⟨x, y⟩)f(y) dwm(y) is almost the same for all values y ∈ Vk in that element of the partition. Hence we can approximate the integral over the partition with estimate over a single point bk: Z Vk K(⟨x, y⟩)f(y) dwm(y) ≈ |Vk|K(⟨x, bk⟩)f(bk). The rest of this section will make this formal. First, in order to construct our partition {V1, . . . , VN } of Sm we will first construct a cover of Sm. Then, our partition will be such that each element Vk is a subset of the corresponding element of the cover of Sm. In this way, we can control the maximum size of the elements of the cover. Lemma B.6. Consider a cover {Bδ(b1), . . . , Bδ(bNm δ )} of Sm by Nm δ hyperspherical caps Bδ(x) = {y ∈ Sm | ⟨x, y⟩ ≥ 1 − δ} for 0 < δ <1, centred at b1, . . . ,bNm δ ∈ Sm. By cover we mean that SNm δ i=1 Bδ(bi) = Sm, with Nm δ being the smallest number of hyperspherical caps to cover Sm (its covering number). Then, for m ≥ 8 we have: 2 I(δ(2−δ)) \u0000m 2 , 1 2 \u0001 ≤ Nm δ < Φ(m) (δ(2 − δ)) m+1 2 < Φ(m) δm+1 , with I being the regularized incomplete beta function and Φ(m) = O(m log m) being a function that depends only on the dimension m. Proof. Define ϕ = cos−1(1 − δ): ϕ Bδ(x) x Naturally, the area of the caps needs to be at least as much as the area of the hypersphere for the set of caps to be a cover. This gives us our lower bound. The area of a cap with colatitude angle ϕ as above is (Li, 2010): wϕ m = 1 2wmIsin2 ϕ \u0012m 2 , 1 2 \u0013 . 19Prompting a Pretrained Transformer Can Be a Universal Approximator As sin ϕ = sin \u0000 cos−1(1 − δ) \u0001 = p 1 − (1 − δ)2 = p δ(2 − δ), we have our lower bound: Nm δ ≥ wm wϕ m = 2 Isin2 ϕ \u0000m 2 , 1 2 \u0001 = 2 I(δ(2−δ)) \u0000m 2 , 1 2 \u0001 For the upper bound, we can use the observation that if a unit ball is covered with balls of radius r, then the unit sphere is also covered with caps of radius r. From (Rogers, 1963, intermediate result from the proof of theorem 3) we have that for m ≥ 8 and 1/r ≥ m + 1, a unit ball can be covered by less than e ((m + 1) log(m + 1) + (m + 1) log log(m + 1) + 5(m + 1)) 1 rm+1 = Φ(m) 1 rm+1 balls of radius r. For high m, this is a pretty good approximation since most of the volume of the hypersphere lies near its surface. Our caps Bδ can fit inside balls of radius r = sin ϕ. Hence, we have the upper bound: Nm δ < Φ(m) sinm+1 ϕ = Φ(m) (δ(2 − δ)) m+1 2 . Now we can use a partition resulting from this covering in order to bound the error between the integral and its Riemannian sum approximation: Lemma B.7 (Approximation via Riemann sums) . Let g(x, y) : Sm × Sm → R with m ≥ 8 be a continuous function with modulus of continuity for both arguments ω(g(·; y), t) ≤ Lt, ∀y ∈ Sm and ω(g(x, ·); t) ≤ Lt, ∀x ∈ Sm. Take any 0 < δ < 1. Then, there exists a partition {V1, . . . , VNm δ } of Sm into Nm δ = ⌈Φ(m)/δm+1⌉ subsets, as well as b1, . . . ,bNm δ ∈ Sm such that: max x∈Sm 1 wm \f\f\f\f\f\f Z Sm g(x, y) dwm(y) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f ≤ 3L cos−1(1 − δ). Here, Φ(m) = O(m log m) is a function that depends only on the dimension m. Proof. This proof is a non-asymptotic version of the proof of Lemma 4.3 from Ng and Kwong (2022). First, we can use Lemma B.6 to construct a covering {Bδ(b1), . . . , Bδ(bNm δ )} of Sm. If we have a covering of Sm it is trivial to construct a partition of it {V1, . . . , VNm δ }, SNm δ k=1 Vk = Sm, Vi ∩ Vj = ∅, i̸= j such that Vk ⊆ Bδ(bk), ∀k. This partition can also be selected to be such that all elements of it have the same measure wm(V1) = wm(Vi), ∀i (Feige and Schechtman, 2002, Lemma 21). While this is not necessary for this proof, we will use this equal measure partition in Lemma C.2 and Theorem C.1. We can then use the triangle inequality to split the term we want to bound in three separate terms: \f\f\f\f\f\f Z Sm g(x, y) dwm(y) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f ≤ \f\f\f\f Z Sm g(x, y) dwm(y) − Z Sm g(b⋆, y) dwm(y) \f\f\f\f + \f\f\f\f\f\f Z Sm g(b⋆, y) dwm(y) − Nm δX k=1 g(b⋆, bk) wm(Vk) \f\f\f\f\f\f (27) + \f\f\f\f\f\f Nm δX k=1 g(b⋆, bk) wm(Vk) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f , where b⋆ is the center of one of the caps whose corresponding partition contains x, i.e., b⋆ = bi ⇐⇒ x ∈ Vi. Due to {V1, . . . , VNm δ } being a partition, b⋆ is well-defined as x is in exactly one of the elements of the partition. 20Prompting a Pretrained Transformer Can Be a Universal Approximator Observe also that the modulus of continuity gives us a Lipschitz-like bound, i.e., if ⟨x, y⟩ ≥1 − δ for x, y ∈ Sm and ω(f; t) ≤ Lt, then |f(x) − f(y)| ≤ω(f; cos−1(⟨x, y⟩)) ≤ ω(f; cos−1(1 − δ)) ≤ L cos−1(1 − δ). (28) Let’s start with the first term in Equation (27). Using the fact that we selected b⋆ to be such that ⟨b⋆, x⟩ ≥1 − δ and Equation (28), we have: \f\f\f\f Z Sm g(x, y) dwm(y) − Z Sm g(b⋆, y) dwm(y) \f\f\f\f = \f\f\f\f Z Sm (g(x, y) − g(b⋆, y)) dwm(y) \f\f\f\f ≤ Z Sm |g(x, y) − g(b⋆, y)| dwm(y) ≤ Z Sm L cos−1(1 − δ) dwm(y) = L cos−1(1 − δ) Z Sm dwm(y) = L cos−1(1 − δ) wm. We can similarly upper-bound the second term of Equation (27) using also the fact that {Vk} is a partition of Sm: \f\f\f\f\f\f Z Sm g(b⋆, y) dwm(y) − Nm δX k=1 g(b⋆, bk) wm(Vk) \f\f\f\f\f\f = \f\f\f\f\f\f Nm δX k=1 Z Vk g(b⋆, y) dmw(y) − Nm δX k=1 g(b⋆, bk) wm(Vk) \f\f\f\f\f\f = \f\f\f\f\f\f Nm δX k=1 Z Vk (g(b⋆, y) − g(b⋆, bk)) dmw(y) \f\f\f\f\f\f ≤ Nm δX k=1 Z Vk |g(b⋆, y) − g(b⋆, bk)| dmw(y) ≤ Nm δX k=1 Z Vk L cos−1(1 − δ) dmw(y) = L cos−1(1 − δ) Nm δX k=1 Z Vk dmw(y) = L cos−1(1 − δ)wm. And analogously, for the third term we get: \f\f\f\f\f\f Nm δX k=1 g(b⋆, bk) wm(Vk) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f = \f\f\f\f\f\f Nm δX k=1 (g(b⋆, bk) − g(x, bk)) wm(Vk) \f\f\f\f\f\f ≤ Nm δX k=1 |g(b⋆, bk) − g(x, bk)| wm(Vk) ≤ L cos−1(1 − δ) Nm δX k=1 wm(Vk) = L cos−1(1 − δ)wm. Finally, observing that the above bounds do not depend on the choice of x ∈ Sm and combining the three results we obtain our desired bound. 21Prompting a Pretrained Transformer Can Be a Universal Approximator By observing that we can set g(x, y) = KvMF λ (⟨x, y⟩)f(y), it becomes clear how Lemma B.7 can be used to bound the second term in Equation (18). For that we will also need to know what is the modulus of continuity of the von Mises-Fisher kernels KvMF λ . Lemma B.8 (Modulus of continuity ofKvMF λ ). The von Mises-Fisher kernelsKvMF λ have modulus of continuityω(KvMF λ ; t) ≤ cm+1(λ) exp(λ). Proof. Recall that KvMF λ (u) is defined on u ∈ [−1, 1]. KvMF λ (u) and its derivative are both monotonically increasing in u. Hence: ω(KvMF λ ; t) = sup \b |KvMF λ (⟨z, x⟩) − KvMF λ (⟨z, y⟩) | x, y ∈ Sm, cos−1(⟨x, y⟩) ≤ t \t . Using the mean value theorem we know there exists a d ∈ Sm such that |KvMF λ (⟨z, x⟩) − KvMF λ (⟨z, y⟩)| = |(KvMF λ )′(⟨z, d⟩) (⟨z, x⟩ − ⟨z, y⟩)| = (KvMF λ )′(⟨z, d⟩) |⟨z, x⟩ − ⟨z, y⟩| = λcm+1(λ) exp(λ⟨z, d⟩) |⟨z, x⟩ − ⟨z, y⟩| ≤ λcm+1(λ) exp(λ) |⟨z, x⟩ − ⟨z, y⟩| = λcm+1(λ) exp(λ)∥z∥2∥x − y∥2 cos(angle(z, x − y)) ≤ λcm+1(λ) exp(λ)∥x − y∥2. Using the law of cosines and that the angle between x and y is less than t: ∥x − y∥2 = q ∥x∥2 2 + ∥y∥2 2 − 2∥x∥2 2∥y∥2 2 cos(angle(x, y)) = p 2 − 2 cos(angle(x, y)) ≤ √ 2 − 2 cost ≤ t. Hence: ω(KvMF λ ; t) ≤ λcm+1(λ) exp(λ)t. Our final result, a bound on Equation (18), combines Lemma B.5 and Lemma B.7, each bounding one of the two terms in Equation (18). Theorem B.1 (Jackson-type bound for universal approximation on the hypersphere, Theorem 1 in the main text) . Let f ∈ C(Sm) be a continuous function on Sm with modulus of continuity ω(f; t) ≤ Lt for some L ∈ R>0 and m ≥ 8. Assume that there exists a constant CH ≥ 0 that upper-bounds the norms of the spherical harmonics of any best polynomial approximation Wq of f: for all q ≥ 1, Wq = qX k=0 Y m k , Ym k ∈ Y(Sm), ∥Y m k ∥∞ ≤ CH, ∀k = 0, . . . , q,where Wq = arg min h∈Pq(Sm) ∥f − h∥∞. Then, for any ϵ >0, there exist ξ1, . . . , ξN ∈ R and b1, . . . ,bN ∈ Sm such that sup x∈Sm \f\f\f\f\ff(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ, where λ = Λ(ϵ/2) (Equation (23)) and for any N such that N ≥ N(λ, ϵ) = Φ(m) \u00123π(L + λ∥f∥∞)cm+1(λ) exp(λ) ϵ \u00132(m+1) = O(ϵ−10−14m−4m2 ). (29) 22Prompting a Pretrained Transformer Can Be a Universal Approximator Proof. Recall the decomposition in Equation (18). We will split our error budget ϵ in half. Hence, we first select λ such that approximating f with its convolution with KvMF λ results in an error at most λ/2. Then, using this λ, we find how finely we need to partition Sm in order to be able to approximate the convolution with a sum up to an error ϵ/2. Let’s select how “peaky” we need the kernelKvMF λ to be, that is, how big should λ be. From Lemma B.5 we have that if λ = Λ(ϵ/2), then we would have ∥f − f ∗ KvMF λ ∥∞ ≤ ϵ/2. Now, for the second term in Equation (18), consider Lemma B.7 with g(x, y) = KvMF λ (⟨x, y⟩)f(y). From Lemma B.8 we have that the modulus of continuity of KvMF λ is ω(KvMF λ ; t) ≤ t λ cm+1(λ) exp(λ). Hence, we have modulus of continuity for g(x, y) being bounded as: ω(g; t) ≤ ∥KvMF λ ∥∞ω(f; t) + ∥f∥∞ω(KvMF λ ; t) ≤ KvMF λ (1) Lt + ∥f∥∞λcm+1(λ) exp(λ) t = cm+1(λ) exp(λ) Lt + ∥f∥∞λcm+1(λ) exp(λ) t = cm+1(λ) exp(λ) (L + λ∥f∥∞) t. Take δ = \u0012 2ϵ 6π(L + λ∥f∥∞)cm+1(λ) exp(λ) \u00132 . Then, by Lemma B.7, there exists a partition {V1, . . . , VN } of Sm and b1, . . . ,bN ∈ Sm for N as in the lemma such that: max x∈Sm \f\f\f\f\f 1 wm Z Sm KvMF λ (⟨x,y⟩)f(y) dwm(y) − 1 wm NX k=1 KvMF λ (⟨x,bk⟩)f(bk) wm(Vk) \f\f\f\f\f ≤ 3(L+λ∥f∥∞)cm+1exp(λ) cos−1(1−δ). As (2x/π)2 < 1 − cos(x) (Bagul and Panchal, 2018, Theorem 1), we have: δ = \u0012 2ϵ 6π(L + λ∥f∥∞)cm+1(λ) exp(λ) \u00132 < 1 − cos \u0012 ϵ 6(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013 . (30) Hence: max x∈Sm \f\f\f\f\f 1 wm Z Sm KvMF λ (⟨x, y⟩)f(y) dwm(y) − 1 wm NX k=1 KvMF λ (⟨x, bk⟩)f(bk) wm(Vk) \f\f\f\f\f ≤ 3(L + λ∥f∥∞)cm+1 exp(λ) cos−1(1 − δ) < 3(L + λ∥f∥∞)cm+1 exp(λ) cos−1 \u0012 cos \u0012 ϵ 6(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013\u0013 = ϵ/2. Combining the two results we have: \r\r\r\r\rf- 1 wm NX k=1 KvMF λ (⟨x, bk⟩)f(y) wm(Vk) \r\r\r\r\r ∞ ≤ ∥f-f∗KvMF λ ∥∞ + \r\r\r\r\rf∗KvMF λ - 1 wm NX k=1 KvMF λ (⟨x, bk⟩)f(y) wm(Vk) \r\r\r\r\r ∞ ≤ ϵ/2 + ϵ/2 = ϵ. Now, the only thing left is to show that this expression can be expressed in the form of Equation (17). 1 wm Nm δX k=1 KvMF λ (⟨x, bk⟩) f(bk) wm(Vk) = Nm δX k=1 1 wm cm+1(λ) exp(λ⟨x, bk⟩) f(bk) wm(Vk) = Nm δX k=1 ξk exp(λ⟨x, bk⟩), 23Prompting a Pretrained Transformer Can Be a Universal Approximator with ξk = cm+1(λ)f(bk)wm(Vk) wm . (31) If we have chosen a partition of equal measure this further simplifies to ξk = cm+1(λ) N f(bk). Hence, for this choice of Λ, N and bk and ξk constructed as above, we indeed have sup x∈Sm \f\f\f\f\ff(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ. Finally, let’s study the asymptotic growth ofN as ϵ → 0. We have: N(λ, ϵ) = Φ(m) \u00123π(L + λ∥f∥∞)cm+1(λ) exp(λ) ϵ \u00132(m+1) . Φ(m) is constant in ϵ so we can ignore it. Expanding cm+1 and dropping the terms that do not depend on ϵ gives us: O   (L + λ∥f∥∞)λ m+1 2 −1 exp(λ) ϵ Im+1 2 −1(λ) !2(m+1) . (32) The asymptotics of the modified Bessel function of the first kind are difficult to analyse. However, as we care about an upper bound, we can simplify the expression by lower-bounding Iν(λ) using Equation (22) and that I0(λ) ≥ C exp(λ)/ √ λ for λ >1/2 (Barnett, 2021): Iν(λ) ≥ C  √ ν2 + λ2 − ν λ !ν+1 exp \u0000√ ν2 + λ2\u0001 √ λ , for some constant C. Plugging this in Equation (32), replacing λ with its asymptotic growth ϵ−4 and taking the Taylor series expansion at for ϵ → 0 gives us: O(ϵ−10−14m−4m2 ). We can easily extend Theorem B.1 to vector-valued functions: Corollary B.1 (Corollary 2 in the main text) . Let f : Sm → Rm+1, m ≥ 8 be such that each component fi is in C(Sm), i= 1, . . . , m+ 1 and satisfies the conditions in Theorem B.1. Furthermore, define ∥f∥∞ = max1≤i≤m+1 ∥fi∥∞. Then, for any ϵ >0, there exist ξ1, . . . ,ξN ∈ Rm+1 and b1, . . . ,bN ∈ Sm such that sup x∈Sm \r\r\r\r\rf(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \r\r\r\r\r 2 ≤ ϵ, with λ = Λ(ϵ/2√m+1) for any N ≥ N(λ, ϵ/√m+1). Proof. The proof is the same as for Theorem B.1. As the concentration parameter λ of the kernels KvMF λ depends only on the smoothness properties of the individual components and these are assumed to be the same, the same kernel choice can be used for all components fi. Furthermore, the choice of partition is independent of the function to be approximated and depends only on the concentration parameter of the kernel. Hence, we can also use the same partition for all components fi. We only need to take into account that: ∥x − y∥2 = vuut m+1X i=1 |xi − yi|2 ≤ p (m + 1)ϵ2 = √ m + 1ϵ, ∀x, y∈ Rm+1, |xi − yi| ≤ϵ, i= 1, . . . , m+ 1. which results in the factor of √m + 1. 24Prompting a Pretrained Transformer Can Be a Universal Approximator C. A Jackson-type Bound for Approximation with a Split Attention Head Lemma C.1. Let a, b: Rd → R, c, d: R → R, c(x), d(x) ̸= 0, ∀x ∈ R and ϵ1, ϵ2 ≥ 0 be such that: sup y∈Rd ∥a(y) − b(y)∥2 ≤ ϵ1 sup x∈R |c(x) − d(x)| = |c − d|∞ ≤ ϵ2. Then for all x ∈ R and y ∈ Rd: \r\r\r\r a(y) c(x) − b(y) d(x) \r\r\r\r 2 ≤ ϵ1|c|∞ + ϵ2 supy∈Rd ∥a(y)∥2 |c(x) d(x)| . Proof. For a fixed x ∈ R and y ∈ Rd, using the triangle inequality gives us \r\r\r\r a(y) c(x) − b(y) d(x) \r\r\r\r 2 = \r\r\r\r a(y) d(x) − b(y) c(x) c(x) d(x) \r\r\r\r 2 = ∥a(y) d(x) − b(y) c(x)∥2 |c(x) d(x)| = ∥a(y) d(x) − a(y) c(x) + a(y) c(x) − b(y) c(x)∥2 |c(x) d(x)| ≤ ∥a(y) (d(x) − c(x))∥2 + ∥c(x) (a(y) − b(y))∥2 |c(x) d(x)| ≤ ϵ2∥a(y)∥2 + ϵ1|c(x)| |c(x) d(x)| ≤ ϵ1|c|∞ + ϵ2 supy∈Rd ∥a(y)∥2 |c(x) d(x)| . And as this holds for all x ∈ R and y ∈ Rd, the inequality in the lemma follows. Lemma C.2. Let f : Sm → Rm+1, m ≥ 8 satisfy the requirements in Corollary B.1. Then, given an ϵ >0 and taking λ, N, and b1, . . . ,bN ∈ Sm as prescribed by the Corollary, we have thatPN k=1 exp(λ⟨x, bk⟩) is close to being a constant: sup x∈Sm \f\f\f\f\f1 − cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ 2√m + 1(L + λ∥f∥∞). (33) Proof. We can use Lemma B.7 by taking g(x, y) = KvMF λ (⟨x, y⟩). From Lemma B.8 we have that the modulus of continuity of KvMF λ is ω(KvMF λ ; t) ≤ t cm+1(λ) exp(λ). Observe that using Equation (16) we have Z Sm g(x, y)dwm(y) = Z Sm KvMF λ (⟨x, y⟩)dwm(y) = wm−1 Z 1 −1 KvMF λ (t)(1 − t2)(m−2)/2dt = wm. The value for δ has to be selected as in Corollary B.1 (Equation (30)): δ = \u0012 2ϵ 6π√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) \u00132 < 1 − cos \u0012 ϵ 6√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013 . Now, using the same partition from Lemma B.7, and recalling that we constructed it such that each element of the partition has the same measure wm(V1) = wm(Vi), ∀i, we have: max x∈Sm \f\f\f\f\f 1 wm Z Sm g(x, y)dwm(y) − 1 wm NX k=1 KvMF λ (⟨x, bk⟩)wm(Vk) \f\f\f\f\f 25Prompting a Pretrained Transformer Can Be a Universal Approximator = max x∈Sm \f\f\f\f\f1 − 1 wm NX k=1 cm+1(λ) exp(λ⟨x, bk⟩)wm(Vk) \f\f\f\f\f = max x∈Sm \f\f\f\f\f1 − cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ 3cm+1 exp(λ) cos−1(1 − δ) < 3cm+1 exp(λ) cos−1 \u0012 cos \u0012 ϵ 6√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013\u0013 = 3cm+1 exp(λ)ϵ 6√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) = ϵ 2√m + 1(L + λ∥f∥∞). Theorem C.1. Let f : Sm → Rm+1, m ≥ 8 satisfies the conditions in Corollary B.1. Define∥f∥∞ = max1≤i≤m+1 ∥fi∥∞. For any ϵ >0, there exist b1, . . . ,bN ∈ Sm such that f can be uniformly approximated to an error at most ϵ: sup x∈Sm \r\r\r\r\rf(x) − PN k=1 ξk exp(λ⟨x, bk⟩) PN k=1 exp(λ⟨x, bk⟩) \r\r\r\r\r 2 ≤ ϵ, with: λ = Λ \u0012 2ϵL 2L + ∥f∥∞ \u0013 with Λ from Equation (23), N ≥ Φ(m) \u00123π(L + λ∥f∥∞)√m + 1 cm+1(λ) exp(λ) ϵ \u00132(m+1) , ξk = f(bk), ∀k = 1, . . . , N. Proof. From Corollary B.1 we know that PN k=1 ξk exp(λ⟨x, bk⟩) approximates f(x) and from Lemma C.2 we know that cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) approximates 1. Using Lemma C.1 we can combine the two results to bound how well PN k=1 ξk exp(λ⟨x, bk⟩) cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) approximates f(x)/1 = f(x). The fact that cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) is not identically 1 means that we will need to increase the precision of approximating the numerator by reducing ϵ in order to account for the additional error coming from the denominator. In particular, we have sup x∈Sm \r\r\r\r\rf(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \r\r\r\r\r 2 ≤ ϵ′, sup x∈Sm \f\f\f\f\f1 − cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ′ 2√m + 1(L + λ∥f∥∞), sup x∈Sm ∥f(x)∥2 ≤ √ m + 1∥f∥∞, sup x∈Sm \f\f\f\f\f cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ 1 + ϵ′ 2√m + 1(L + λ∥f∥∞). Hence, applying Lemma C.1 gives us: \r\r\r\r\rf(x) − PN k=1 ξk exp(λ⟨x, bk⟩) cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) \r\r\r\r\r 2 = \r\r\r\r\r f(x) 1 − PN k=1 ξk exp(λ⟨x, bk⟩) cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) \r\r\r\r\r 2 26Prompting a Pretrained Transformer Can Be a Universal Approximator ≤ ϵ′ + ϵ′ 2√m+1(L+λ∥f∥∞) √m + 1∥f∥∞ 1 + ϵ′ 2√m+1(L+λ∥f∥∞) = ϵ′√m + 1(∥f∥∞ + 2L + 2λ∥f∥∞) 2√m + 1(λ∥f∥∞ + L) + ϵ′ ≤ ϵ′ 2 \u0012 ∥f∥∞ λ∥f∥∞ + L + 2λ∥f∥∞ + L λ∥f∥∞ + L \u0013 ≤ ϵ′ 2 \u0012 2 + ∥f∥∞ L \u0013 , where we used that ϵ′, ∥f∥∞L >0, which is the case in realistic scenarios. Therefore, if we want this error to be upper bounded by ϵ, we need to select ϵ′ ≤ 2ϵL 2L + ∥f∥∞ . From Corollary B.1 (Equation (23)) that can be achieved by selecting λ = Λ \u0012 2ϵL 2L + ∥f∥∞ \u0013 and N ≥ Φ(m) \u00123π(L + λ∥f∥∞)√m + 1 cm+1(λ) exp(λ) ϵ \u00132(m+1) . Finally, observe that the cm+1(λ)/N factor can be folded in the ξk terms (Equation (31)): ξk = N cm+1(λ)f(bk) cm+1(λ)wm(Vk) wm = f(bk), with ξk nicely reducing to be the evaluation of f at the corresponding control point bk. D. Additional Results Lemma D.1. Define the stereographic projection and its inverse: Σm : ¯Sm → Rm (x1, . . . , xm+1) 7→ \u0012 x1 1 − xm+1 , . . . ,xm 1 − xm+1 \u0013 Σ−1 m : Rm → Sm (y1, . . . , ym) 7→ \u0012 2y1Pm i=1 y2 i + 1, . . . ,2ymPm i=1 y2 i + 1, Pm i=1 y2 i − 1Pm i=1 y2 i + 1 \u0013 with ¯Sm the part of Sm that gets mapped to [0, 1]m, i.e., ¯Sm = Σ −1 m ([0, 1]m). Σm and Σ−1 m are continuous and inverses of each other and there exist LΣ m and LΣ−1 m such that ω(Σm; t) ≤ LΣ mt and ω(Σ−1 m ; t) ≤ LΣ−1 m t. Furthermore, Σm ◦ H1 H,3(m+1) ◦ Σ−1 m is dense in the set of continuous functions [0, 1]m → Rm. 27",
      "meta_data": {
        "arxiv_id": "2402.14753v1",
        "authors": [
          "Aleksandar Petrov",
          "Philip H. S. Torr",
          "Adel Bibi"
        ],
        "published_date": "2024-02-22T18:12:48Z",
        "pdf_url": "https://arxiv.org/pdf/2402.14753v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that prompting and prefix-tuning can enable a pretrained transformer model to be a universal approximator for sequence-to-sequence functions. It shows that much smaller pretrained models than previously thought can achieve this. Key findings include: a single attention head, when prefix-tuned, is sufficient to approximate any continuous function on a hypersphere; any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length; and Jackson-type bounds are provided for the prefix length needed to achieve a desired approximation precision. The work also explores how prefix-tuning can lead to element-wise functions, offering insights into its practical success.",
        "methodology": "The methodology is primarily theoretical, focusing on mathematical proofs of universal approximation. The core approach involves simplifying the classical attention head into 'core' and 'split' attention heads, demonstrating their ability to approximate continuous functions on hyperspheres (Sm). This restriction is justified by LayerNorm's projection of activations to Sm in modern transformers. Jackson-type bounds are derived for the required prefix length (N) and a concentration parameter (lambda). The paper then shows how a core/split attention head can be represented by a classical attention head through specific constructions of attention (H) and value (WV) matrices and an increase in hidden dimension. For sequence-to-sequence functions, the work extends the single attention head result to element-wise functions and, for general sequence-to-sequence functions, employs a variant of the Kolmogorov-Arnold representation theorem, constructing a transformer with T+2 attention layers to achieve the approximation.",
        "experimental_setup": "The paper is purely theoretical and does not involve empirical experimental setups, datasets, benchmarks, or traditional validation methods. The 'setup' refers to mathematical constructions and proofs for specific idealized transformer architectures and parameter configurations used to demonstrate universal approximation capabilities.",
        "limitations": "The theoretical results assume a highly specific pretrained model architecture and parameter values (e.g., for attention and value matrices H and WV) that are unlikely to occur in real-world pretrained transformers. Consequently, it remains an open question what class of functions a *real-world* pretrained transformer can approximate with prefix-tuning. Additionally, the derived Jackson-type bounds suggest that prefix-tuning and prompting may be asymptotically less efficient than training a transformer from scratch, with trainable parameters growing significantly worse (O(epsilon^-10^-14m^-4m^2)) compared to O(epsilon^-m). The construction for general sequence-to-sequence functions is also considered highly unlikely in practice, meaning the current results do not fully explain why prefix-tuning works for complex tasks requiring cross-token interaction.",
        "future_research_directions": "Future research should focus on deriving tighter Jackson-type bounds for prefix-tuning to determine if its observed asymptotic inefficiency is inherent. A critical direction is to understand the class of functions that *real-world pretrained transformers* can approximate with prefix-tuning, which involves deriving more challenging inverse (Bernstein-type) bounds. Further investigation into the hypothesis that prompting and prefix-tuning work by combining prefix-based element-wise mapping with pretrained cross-element mixing is warranted. Research could also explore methods to guarantee token-wise universal approximation by designing models with specific attention head structures (like those in Lemma 3). Finally, the theoretical implications for designing safety and security measures in transformer-based systems need to be further explored, especially concerning potential undetectable collusion of agents."
      }
    },
    {
      "title": "HyperTuning:  Toward Adapting Large Language Models without Back-propagation",
      "abstract": "Fine-tuning large language models for different tasks can be costly and\ninefficient, and even methods that reduce the number of tuned parameters still\nrequire full gradient-based optimization. We propose HyperTuning, a novel\napproach to model adaptation that uses a hypermodel to generate task-specific\nparameters for a fixed downstream model. We demonstrate a simple setup for\nhypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or\nLoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5\nin two stages: first, hyperpretraining with a modified conditional language\nmodeling objective that trains a hypermodel to generate parameters; second,\nmulti-task fine-tuning (MTF) on a large number of diverse language tasks. We\nevaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and\nshow that it can effectively generate parameters for unseen tasks. Moreover, we\nshow that using hypermodel-generated parameters as initializations for further\nparameter-efficient fine-tuning improves performance. HyperTuning can thus be a\nflexible and efficient way to leverage large language models for diverse\ndownstream applications.",
      "full_text": "HyperTuning: Toward Adapting Large Language Models without Back-propagation Jason Phang1,2∗, Yi Mao3, Pengcheng He3, Weizhu Chen3 1New York University 2EleutherAI 3Microsoft Azure AI Abstract Fine-tuning large language models for different tasks can be costly and inefﬁcient, and even methods that reduce the number of tuned parameters still require full gradient-based optimization. We propose HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-speciﬁc parameters for a ﬁxed downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a T5-based hypermodel that produces soft preﬁxes or LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5 in two stages: ﬁrst, hyperpretraining with a modiﬁed conditional language modeling objective that trains a hypermodel to generate parameters; second, multi-task ﬁne-tuning (MTF) on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and show that it can effectively generate parameters for unseen tasks. Moreover, we show that using hypermodel-generated parameters as initializations for further parameter-efﬁcient ﬁne-tuning improves performance. HyperTuning can thus be a ﬂexible and efﬁcient way to leverage large language models for diverse downstream applications. 1 Introduction While language models (LMs) have achieved remarkable capabilities with increasing model size (Brown et al., 2020; Chowdhery et al., 2022), ﬁne-tuning them on speciﬁc downstream tasks in- troduces signiﬁcant engineering challenges and computational costs. Although large models can perform zero-shot, instruction-prompted, and few-shot learning (Sanh et al., 2022; Wei et al., 2022), they are usually outperformed by fully ﬁne-tuned models when sufﬁcient training data is available. To reduce the computational and memory overhead of ﬁne-tuning LMs, parameter-efﬁcient ﬁne- tuning (PEFT) methods have been proposed, such as adapters (Houlsby et al., 2019), preﬁx tuning (Li and Liang, 2021), and prompt tuning (Lester et al., 2021). These methods update only a small subset of (possibly new) parameters of the LM, and have achieved competitive performance with full ﬁne-tuning (Ding et al., 2022). However, PEFT methods still require full back-propagation through the LM during training, which is computationally expensive and memory intensive. Given that (1) only a small number of parameters need to be updated to adapt an LM to a given task, (2) very large LMs have demonstrated strong in-context learning capabilities on a forward pass, and (3) a forward pass for very large LMs already entails a substantial amount of computation, we hypothesize that it is possible to train a separate model to perform the optimization or adaptation procedure entirely, using only a forward pass. ∗Work done at Microsoft. Correspondence: jasonphang@nyu.edu Preprint. arXiv:2211.12485v1  [cs.CL]  22 Nov 2022To avoid the costly computation of back-propagating through the LM to produce the parameter updates, especially for thousands or millions of iterations during training, we propose a new paradigm of hypertuning: using a hypermodel to adapt a downstream LM to a desired application. As a concrete proof of concept, we explore a simple setup where hypermodels take as input a set of few-shot examples from a given task, and output the PEFT parameters corresponding to that task in a single forward pass. To demonstrate the feasibility of this approach, we trainHyperT5: a set of T5-based hypermodels that output soft preﬁxes (Li and Liang, 2021) or LoRA parameters (Hu et al., 2022), to be incorporated into a frozen downstream T5 LM. To train HyperT5, we introduce a two-stage procedure for training hypermodels: hyperpretraining, where we adapt a pretrained LM to generate PEFT parameters via a modiﬁed language modeling objective, followed by multi-task ﬁne-tuning (MTF) the hypermodel. After training, HyperT5 models can take few-shot examples from unseen tasks and generate the corresponding PEFT parameters, allowing us to adapt a downstream LM without back-propagation. We show in experiments across P3, Super-NaturalInstructions and MetaICL datasets that LMs can be hypertuned using just a small number of examples. Furthermore, we show that when the hypermodel- generated parameters are used as initializations for further parameter-efﬁcient ﬁne-tuning, we can achieve faster training convergence and better overall performance. This work serves as a ﬁrst step toward hypertuning, and we are are aware of certain limitations of this preliminary setup. Because our current formulation of hypermodels can only take a small number of examples as input, its performance cannot compare to full parameter-efﬁcient ﬁne-tuning or full ﬁne-tuning. HyperT5 also generally underperforms T5 explicitly trained for few-shot in-context learning with full attention across examples, although we note that the latter is more computationally expensive to use at inference time. Nevertheless, we believe that our results demonstrate a promising step toward model adaptation without the need for back-propagation. We plan to release the code and model weights for HyperT5, as well as the multi-task ﬁne-tuned versions for the three datasets listed above. 2 Related Work HyperNetworks Several works have explored the concept of \"hypernetworks,\" where an auxiliary network is used to generate parameters for a primary network. This terminology was ﬁrst introduced by Ha et al. (2017) and applied to LSTMs. Among Transformer-based language models, Karimi Ma- habadi et al. (2021) and He et al. (2022) incorporated hypernetworks into T5 models for knowledge sharing during multitask ﬁne-tuning. Peebles et al. (2022) utilized a Transformer with diffusion for generating full model parameters for image-recognition and Cartpole tasks. Similarly, Lester et al. (2022) trained models to generate soft prompts for transferring between downstream models. Our work is closely related to Deb et al. (2022), who also used a hypernetwork to modify downstream model parameters and incorporated Super-NaturalInstuctions (S-NI) in their experimental setting. They found that incorporating instructions via a hypernetwork trained with MAML (Finn et al., 2017) improved downstream performance. Multi-task Training and Transfer A crucial ingredient to hypertuning is the transferrability of task knowledge and generalization to novel tasks. Many past works (Phang et al., 2018; Pruksachatkun et al., 2020; Vu et al., 2020) have explored the effectiveness of single- and multi-task transfer learning. More recent work has shown that large-scale multi-task training tends allows models to generalize to unseen tasks (Sanh et al., 2022; Wei et al., 2022; Wang et al., 2022; Chung et al., 2022). Min et al. (2022) and Chen et al. (2022) show that few-shot learning also beneﬁts from multi-task training. Pfeiffer et al. (2020), Vu et al. (2021) and Gu et al. (2021) have also explored transfer learning among PEFT methods. 3 HyperTuning The impetus for using hypermodels for adapting downstream models derives from two recent developments in natural language processing: 2Figure 1: Overview of HyperTuning. (A) Fine-tuning, where all model parameters are updated (red). (B) Parameter-efﬁcient ﬁne-tuning (PEFT), where all model parameters are frozen (blue) and only a small number of parameters, φ, are updated. (C) HyperTuning, where a hypermodel is used to generate parameters φfor a frozen downstream model. For instance, a hypermodel may take a set of few-shot examples to determine what φto generate. Only the hypermodel’s parameters are updated during training. (D) At inference time, the parameters φonly need to be generated once, and thereafter only need to store φ, with no need to retain the few-shot examples. 1) Large language models can perform in-context learning effectively. Large language models have been shown to be able to learn from the context of a small number of examples or instructions for a task, without any prior training on that task (Brown et al., 2020; Min et al., 2022; Wang et al., 2022). This suggests that models can “understand” what the task is and how to tackle it based on a few samples or descriptions of the task. This capability appears to improve as the models get larger or are trained on more relevant data (Chowdhery et al., 2022; Ouyang et al., 2022; Bai et al., 2022). 2) Large language models can be adapted to downstream tasks by tuning a small set of param- eters. Along with the growth in model sizes, there have been signiﬁcant advances in ﬁne-tuning methods that only modify a small number of parameters (possibly adding some new ones) in a frozen language model to adapt it to a speciﬁc task (Houlsby et al., 2019; Li and Liang, 2021; Lester et al., 2021; Ding et al., 2022). These methods often achieve performance comparable to ﬁne-tuning all parameters in the model. Importantly, the number of parameters that need to be changed is small enough that it is feasible to train a model to generate them (Qin et al., 2021; Lester et al., 2022). Taken together, these ﬁndings suggest that we may be able to use an auxiliary model that can ﬁrst extract some task-relevant knowledge from some input that describes the task (e.g. instruction, few-shot examples), and then generate a small number of adaptive parameters, thereby changing the main model’s behavior to suit the task. This approach, if successful, would enable us to adapt models to downstream applications without using backpropagation, or storing the encoded representations of few-shot examples in memory. In other words, we can delegate the work of model adaptation to a separate model. We call this approach hypertuning, inspired by the work on hypernetworks by Ha et al. (2017). Hy- pertuning uses a hypermodel to adapt a downstream model to a target downstream task or application. This is differs from ﬁne-tuning, which uses backpropagation and a gradient descent algorithm to update model parameters. In this work, we present one possible formulation of hypertuning using few-shot examples and generating a small set of parameters with a single forward pass through the hypermodel. However, this is just one possible way of performing hypertuning, and the idea of adapt- ing models with hypermodels can be generalized to many other cases. For example, hypermodels could also be trained to predict gradients or generate parameter updates based on input-output pairs. This way, hypermodels could work with large training sets, not just a few examples. Ultimately, with sufﬁciently general and well-trained hypermodels, we may be able to replace gradient-descent- based ﬁne-tuning pipelines with hypertuning for many applications, while achieving similar or better performance. 33.1 HyperTuning with Fewshot Examples Let M be a model with parameters θ, initialized at θ0 from pretraining, and L a loss function. Given a dataset of size N with input-output pairs {(x,y)}, standard ﬁne-tuning minimizes the following objective over θ: arg min θ 1 N ∑ {(x,y)} L ( y,M(θ; x) ) (1) In the case of parameter-efﬁcient ﬁne-tuning (PEFT), we ﬁx θ = θ0 and introduce a small set of trainable parameters φ(e.g. adapter parameters, soft prompts) that are injected into M. We optimize only over φ: arg min φ 1 N ∑ {(x,y)} L ( y,M(θ0; x,φ) ) (2) For hypertuning, we further deﬁne a hypermodel Hwith parameters ξthat produces PEFT parameters ˆφbased on its input, which can be a set of few-shot examples or task instructions. For example, if the hypermodel input is a set of few-shot examples {(xi,yi)}K, we have: ˆφ= H ( ξ; {(xi,yi)}K ) (3) One way to train the hypermodel (H,ξ) is to perform PEFT on many tasks and use the resulting φ as targets. However, this is costly in computation, requiring many ﬁne-tuning runs, and does not leverage cross-task knowledge transfer. Instead, we propose to train the hypermodel end-to-end, optimizing through the frozen model (M,θ0). Hence, the hypermodel training objective is: arg min ξ 1 N ∑ {(x,y)},{{(xi,yi)}K} L ( y,M ( θ0; x,H(ξ; {(xi,yi)}K) )) (4) At each training step, we sample a target example (x,y) and non-overlapping few-shot examples {(xi,yi)}K. We generate ˆφfrom the few-shot examples and compute the loss with respect to (x,y) and ˆφ. We then back-propagate the gradients through both M and H to update ξ. Note that since ˆφdoes not depend on x, it can be computed once for a given set of few-shot examples and reused for downstream predictions. At inference time, we can use ˆφdirectly without storing or recomputing the representations for {(x,y)},{(xi,yi)}K, saving memory and computation.2 4 HyperT5: A T5-Based HyperModel 4.1 Architecture and Setup To demonstrate the feasibility of hypertuning, we proposeHyperT5, a hypermodel based on T5, where both the hypermodel and the downstream model share a T5 backbone (Figure 2A). We use a frozen LM-adapted T5 3 as the downstream model. The hypermodel is also initialized with LM-adapted T5 parameters, but with some architectural changes. As deﬁned in Equation 3, the hypermodel encoder takes the few-shot examples (and/or task deﬁnitions, in the case of S-NI) as input. The hypermodel decoder takes a ﬁxed set of newly learned token embeddings as input, and outputs a set of decoder token representations, which are then fed to a set of MLPs to generate the PEFT parameters φfor the downstream model. We also remove the causal masking from the decoder, since the hypermodel does not perform autoregressive generation. 2By construction, few-shot examples occupy at least K times the memory of the target input x. 3This is the model introduced by Lester et al. (2021). We use the T5 v1.1 architecture and initialize all experiments with the LM-adapted parameters, unless stated otherwise. 4Figure 2: Overview of HyperT5. (A) HyperT5 takes as input few-shot examples and outputs PEFT parameters φ. The model is initialized from an LM-adapted T5. (B) In HyperT5-Preﬁx, φare key and value preﬁxes for every attention layer. (C) In HyperT5-LoRA, φare additive low-rank modiﬁcations to the query and value linear maps. We experiment with two PEFT methods: preﬁx tuning (Li and Liang, 2021) and LoRA (Hu et al., 2022). Preﬁx tuning (Figure 2B) prepends a set of learned key and value representations within each attention layer, while LoRA (Figure 2C) learns a low-rank additive modiﬁcation to the query and value linear maps. Both PEFT methods have been shown to achieve good performance across a wide range of tasks (Ding et al., 2022). Chan et al. (2022) also suggest that modifying in-context representations and model weights can lead to different model behaviors, and we seek to demonstrate that hypertuning is applicable to very different PEFT methods. We name the respective hypermodels HyperT5-Preﬁx and HyperT5-LoRA. The number of decoder input tokens and the size of the MLPs depend on the choice of PEFT method and its hyperparameters. For example, for HyperT5-Preﬁx that generates soft preﬁxes corresponding to preﬁx tuning, φwill be of the shape [L,2,2,P,H ], where Lis the number of layers, 2 is for the encoder and decoder, 2 is for the key and value preﬁxes, P is the number of preﬁx tokens, and H is the hidden size. We set the number of decoder input tokens to be 2P. We provide pseudo-code for HyperT5-Preﬁx and HyperT5-LoRA models in the Figure 7 and Figure 8 in the Appendix. 4.2 HyperPretraining To train HyperT5, we ﬁrst undergo an additional stage of pretraining to adapt the hypermodel to generate parameters φfor the downstream model, which we call hyperpretraining. As we show in Section 5.5, hyperpretraining is crucial for good hypermodel performance. We propose a simple scheme for hyperpretraining using a Context-Augmented Conditional Language Modeling (CACLM) objective, which extends the conditional language-modeling (CLM) objective of T5 LM-adaptation. As shown in Figure 3, we sample a 512-token sequence from a pretraining corpus and split it into four consecutive segments A–D. The downstream model receives segment B as input and predicts segment C, following the CLM objective. The hypermodel receives segments A and D as input, which provide additional context from the same document, and outputs PEFT parameters for the downstream model.4 The hypermodel thus compresses contextual information to assist the downstream model in its CLM task. We also make segment B very short (32 tokens) to encourage the downstream model to depend on the hypermodel information for accurate prediction of tokens in C. During hyperpretraining, we freeze the downstream model and only update the hypermodel parame- ters, training for 100K steps on the C4 dataset (Raffel et al., 2020). We perform hyperpretraining separately for HyperT5-Preﬁx and HyperT5-LoRA models. Hyperparameters can be found in Appendix A. 5 Multi-Task Fine-Tuning with HyperT5 5.1 Multitask Fine-Tuning (MTF) After hyperpretraining, we conduct a second stage of training to train the hypermodel to generate task- speciﬁc PEFT parameters based on a small number of examples that we provide as input (Figure 1C). By performing multi-task ﬁne-tuning on a sufﬁciently large number of tasks, we hope to have the 4Segments A and D are marked by sentinel tokens. 5Figure 3: Overview of HyperPretraining using the Context-Augmented Conditional Language Modeling (CACLM) objective to train a hypermodel to predict PEFT parametersφ. (A) Sample a sequence of 512 tokens from a pretraining corpus, and splice into 4 segments A–D. (B) The frozen downstream model takes as input B and predicts continuation C. (C) The hypermodel is trained to encode additional context A and D into PEFT parameters φ, providing additional information to the downstream model to predict C. hypermodel learn to generalize to generate parameters for unseen tasks. We adopt a similar training setup to MetaICL (Min et al., 2022), which uses multi-task ﬁne-tuning (Sanh et al., 2022; Wei et al., 2022) with both a target input example (x) and a set of few-shot input-output pairs {(xi,yi)}K as inputs. The hypermodel takes the few-shot pairs as input, while the downstream model takes the target example as input, as shown in Equation 3. We ﬁne-tune only the hypermodel parameters and keep the downstream model parameters ﬁxed, unless otherwise stated. Appendix A.1 shows how we format the few-shot inputs. We compare our approach with two baselines: multi-task ﬁne-tuning of a T5 model without few-shot inputs, and MetaICL (multi-task ﬁne-tuning with few-shot inputs). In MetaICL, the few-shot pairs are concatenated with the target example as input, both during training and evaluation on new tasks. We also include baselines that use PEFT methods for multi-task ﬁne-tuning, i.e. learning a single set of preﬁx tuning or LoRA parameters. We perform multi-task ﬁne-tuning for 10,000 steps with a batch size of 256. For models that use few-shot inputs (MTF with fewshot, and hypermodels), we use up to 16 examples, and truncate tokens that exceed the maximum input length. Appendix B provides more details on the datasets. 5.2 Datasets To demonstrate the generality of our approach, we conduct experiments on three different multi-task training datasets, each with different held-out tasks and evaluation protocols. Public Pool of Prompts (P3)(Sanh et al., 2022) consists of 62 task datasets, and was used in training the T0 models. The prompt are formatted with 0-shot inference in mind, and often contain instructions or the possible answer options. For training our models, we use the T0-train subset. In order to ﬁt multiple examples into the hypermodel’s context, we further exclude dataset-prompt subsets with average input sequence lengths longer than 320 tokens. The list of included dataset-prompts can be found in Figure 6. Evaluation is performed on a ﬁxed set of held-out tasks, based on multiple-choice scoring with accuracy. We exclude StoryCloze from evaluation as the task is not distributed with training data. MetaICL (Min et al., 2022) introduced a few-shot multi-task training dataset, which is an extension of CrossFit (Ye et al., 2021) with UniﬁedQA (Khashabi et al., 2020) and the addition of training data. For brevity, we will refer to this dataset as MetaICL. Unlike P3 and S-NI, the task inputs are not formatted for 0-shot inference; for instance, the task inputs may give no clue as to the goal of the task, or what the output space is. They provide several different train-task splits for tasks, of which we run our experiments on three (HR→LR, Non-NLI→NLI, Non-Class→Class) to economize on computation costs. Evaluation is performed on held-out tasks, with ROUGE or Macro-F1 on model generations depending on the task. 6Super-NaturalInstructions (S-NI) (Wang et al., 2022) consists of over 1,600 task datasets, each with a task deﬁnition as well as a ﬁxed set of positive and negative demonstrations. Following their ﬁndings, we focus our experiments on two settings: using only the task deﬁnition as the hypermodel input, and using deﬁnitions alongside two ﬁxed positive examples. We only use the English tasks within the dataset. Evaluation is performed on a set of held-out tasks using ROUGE-L on model generations. 5.3 Results 5.3.1 P3 ANLI HSwag CB COPA RTE WiC WSC WGD A VG Full Fine-Tuning T5-MTF 33.4 28.0 63.0 77.9 71.1 50.8 61.0 53.4 54.8 T5-MTF-Few-shot 35.3 27.5 68.6 70.5 75.2 51.7 62.1 52.2 55.4 Parameter-Efﬁcient Fine-Tuning (PEFT) T5-MTF (Preﬁx) 33.1 26.1 53.9 67.8 60.5 49.8 54.7 51.4 49.7 T5-MTF (LoRA) 32.9 26.0 36.0 59.7 49.8 51.2 58.1 50.5 45.5 HyperTuning HyperT5-Preﬁx 33.4 32.3 60.1 73.9 71.5 51.1 63.0 51.1 54.6 HyperT5-LoRA 33.6 33.0 49.5 74.2 67.4 52.0 64.0 52.9 53.3 HyperTuning + Fine-Tuning HyperT5-Preﬁx+ 34.5 32.2 58.1 78.4 76.5 50.4 63.8 54.3 56.0 HyperT5-LoRA+ 33.9 30.7 62.1 75.8 72.3 50.8 64.6 54.5 55.6 Table 1: Results on P3 on held-out tasks (dev) with T5-Large models. T0 results taken from Sanh et al. (2022). ANLI HSwag CB COPA RTE WiC WSC WGD A VG Full Fine-Tuning T5-MTF 39.9 29.4 64.5 88.0 80.8 51.7 60.7 57.9 59.1 T5-MTF-Few-shot 37.9 30.9 67.6 90.5 76.6 51.2 63.3 61.1 59.9 Parameter-Efﬁcient Fine-Tuning (PEFT) T5-MTF (Preﬁx) 38.3 31.2 61.4 82.4 78.6 52.6 57.0 54.3 57.0 T5-MTF (LoRA) 33.9 26.4 47.1 67.2 53.3 50.8 51.5 50.3 47.6 HyperTuning HyperT5-Preﬁx 38.7 33.6 69.6 88.4 79.5 53.1 57.6 56.6 59.6 HyperT5-LoRA 35.3 30.8 66.4 83.3 68.5 50.3 60.0 56.1 56.4 Other results T0 33.4 27.3 45.4 73.1 64.5 50.7 65.0 51.0 51.3 Table 2: Results on P3 on held-out tasks (dev) with T5-XL models. T0 results taken from Sanh et al. (2022). Table 1 and Table 2 show the results of our experiments on the P3 dataset using T5-Large (∼770M parameters) and T5-XL (∼3B parameters), respectively. We compare our HyperT5-Preﬁx and HyperT5-LoRA, which use hypermodels to generate task- speciﬁc PEFT parameters based on few-shot examples, with several baselines: preﬁx tuning, LoRA tuning, T5-MTF, and T5-MTF-Few-shot. T5-MTF is a model that roughly corresponds to the T0 model, and we detail the differences in Appendix B.1. Our results show that both HyperT5-Preﬁx and HyperT5-LoRA signiﬁcantly improve over the preﬁx and LoRA tuning baselines, indicating the effectiveness of using hypermodels to adapt the frozen downstream T5 model to unseen tasks. HyperT5-Preﬁx achieves performance close to T5-MTF, while 7T5-MTF-Few-shot attains the highest scores, in line with the ﬁndings of Min et al. (2022). These patterns are consistent across T5-Large and T5-XL,5 demonstrating the scalability of hypertuning. We emphasize that HyperT5-Preﬁx/LoRA only introduces a very small number of PEFT parameters in the frozen downstream T5 model, whereas all parameters are tuned in the T5-MTF and T5-MTF- Few-shot models. Moreover, the P3 examples are written with prompt templates that are optimized for zero-shot inference, which is the ideal input format for T5-MTF. Furthermore, T5-MTF-Fewshot has full, bidirectional self-attention between the target input xand the few-shot examples, whereas HyperT5-Preﬁx and HyperT5-Lora only incorporate information from the few-shot examples via the respective PEFT parameters. To investigate whether the hypermodel beneﬁts are complementary to updating the downstream model parameters, we conduct an additional set of experiments where we jointly train both the hypermodel and the downstream model (HyperTuning + Fine-Tuning), with results shown at the bottom of Table 1. We observe that both HyperT5-Preﬁx+ and HyperT5-Lora+ slightly surpass T5-MTF-Fewshot, suggesting that the hypermodels can further enhance the performance of ﬁne-tuned downstream models. 5.3.2 MetaICL Table 3 presents the results on three MetaICL task splits. As in the previous experiments, both HyperT5 models surpass the PEFT models and T5-MTF in performance, except for T5-MTF-Few- shot, which outperforms them in all but one case: Non-NLI→NLI, where HyperT5-Preﬁx achieves a higher score. T5-MTF performs poorly in the MetaICL experiments, as it has to handle task examples zero-shot, and the MetaICL inputs are not suitable for zero-shot inference, as explained above. HR →LR Non-NLI →NLI Non-Class →Class Full Fine-Tuning T5-MTF 34.3 48.8 30.3 T5-MTF-Few-shot 41.0 56.7 40.6 Parameter-Efﬁcient Fine-Tuning (PEFT) T5-MTF (Preﬁx) 29.8 42.8 29.6 T5-MTF (LoRA) 31.5 41.3 28.7 HyperTuning HyperT5-Preﬁx 38.0 58.3 38.6 HyperT5-LoRA 35.4 54.2 34.8 Table 3: Results on MetaICL (Test) with T5-Large models. 5.3.3 Super-NaturalInstructions (S-NI) We report the results on the different S-NI settings in Table 4 for T5-Large and Table 5 for T5-XL, using both Def (deﬁnition-only) and Def+2Pos (deﬁnition and two ﬁxed positive examples) settings. The T5-MTF (Def) and T5-MTF (Def+2Pos) models are similar to the corresponding Tk-Instruct variants (Wang et al., 2022), with a slight difference in input formatting (see Appendix A.1). For the hypermodels, we prepend the task deﬁnitions to the few-shot examples and treat them as part of the hypermodel input. On average, the HyperT5 with Def+2Pos outperforms T5-MTF (Def) by a large margin, but still underperforms T5-MTF (Def+2Pos), in line with the above results. 5.4 Discussion Above, we evaluated hypermodels on three multi-task datasets, where they generate task-speciﬁc soft preﬁxes or LoRA parameters from a few examples or instructions. In general, HyperT5 matched or exceeded T5-MTF models, but lagged behind T5-MTF-Fewshot models (or Def+2Pos models, in the case of S-NI). This gap is expected, as T5-MTF-Fewshot uses full self-attention between the 5We note that T0-XL performs much worse than our trained T5-MTF, which is in agreement with other work (Anonymous, 2023; Wu et al., 2022) that have reported similar results in replicating T0. 8A VG Full Fine-Tuning T5-MTF (Def) 40.6 T5-MTF (Def+2Pos) 47.6 HyperTuning HyperT5-Preﬁx (Def) 37.1 HyperT5-Preﬁx (Def+2Pos) 43.5 HyperT5-LoRA (Def) 34.9 HyperT5-LoRA (Def+2Pos) 42.0 Other Results Tk-Instruct (Def+2Pos) 48.0 Table 4: Results on Super-NaturalInstuctions (S-NI; Test) with T5-Large models. T k- Instruct results taken from Wang et al. (2022). A VG Full Fine-Tuning T5-MTF (Def) 46.6 T5-MTF (Def+2Pos) 54.3 HyperTuning HyperT5-Preﬁx (Def) 38.9 HyperT5-Preﬁx (Def+2Pos) 48.6 HyperT5-LoRA (Def) 38.9 HyperT5-LoRA (Def+2Pos) 45.0 Other Results Tk-Instruct (Def+2Pos) 54.0 Table 5: Results on Super-NaturalInstuctions (S-NI; Test) with T5-XL models. Tk-Instruct results taken from Wang et al. (2022). examples and the target input x, while HyperT5 encodes the examples into PEFT parameters that are independent of x. We attribute some of the gap to this limitation. However, this limitation also confers efﬁciency advantages to HyperT5 at inference time compared to T5-MTF-Fewshot. In encoder-decoders such as T5, the full self-attention between the examples and xprevents the separation of their representations: a new forward pass is needed for each new x. In contrast, for hypermodels the examples can be encoded into PEFT parameters once, and reused for all subsequent inputs. Even for decoder-only models (e.g. MetaICL based on GPT-2), where the examples can be cached as key and value representations, the cache size is likely much larger than the PEFT parameters, as the cache stores all the representations for every token in the examples, which are several times longer than the input by deﬁnition. Thus, hypermodels in our setup sacriﬁce some performance for efﬁciency. Regarding T5-MTF, one might wonder what the concrete beneﬁt of HyperT5 is, given their similar performance. After all, unlike T5-MTF-Fewshot, T5-MTF only uses xas the input, requiring no extra computation or memory, and only one set of model weights. Firstly, we stress that the HyperT5 model can only affect the downstream model through a small number of modiﬁed parameters, while in T5-MTF all the parameters that process xare modiﬁed. Although HyperT5 and T5-MTF have roughly the same number of tuned parameters, the parameters modiﬁed in T5-MTF directly interact with the input x, which we expect to help performance. Secondly, we identify two separate, but possibly related, sources of performance improvement: better general task performance of the downstream model (which is usually the goal of MTF training), and adapting the downstream model to a new task based on few-shot examples, using hypermodels in our case. Our aim in this work is to show the feasibility of the latter. We argue that both sources are complementary, and we showed in Section 5.3.1 that when we use hypermodels without freezing the downstream model, thereby acquiring both beneﬁts, performance further improves. More generally, we expect that training a hypermodel against an already multi-task ﬁne-tuned model will lead to better performance than just using the model for zero-shot inference alone, and we plan to explore this in future work. We also observe a consistent trend where HyperT5-Preﬁx outperforms HyperT5-LoRA. We speculate that it is easier for hypermodels to learn to generate soft preﬁxes as compared to LoRA weights, since soft preﬁx are effectively model-internal hidden states, and the generated PEFT parameters are themselves transformations of the hypermodel hidden states. Incidentally, another possible interpretation of the HyperT5-Preﬁx model is that the combination of the hypermodel and the downstream model can be seen as a dual-encoder, single-decoder model with separate encoders for the few-shot examples and the target example. Lastly, the majority of the experiments were conducted with minimal hyperparameter-tuning, and the current results primarily serve as a proof-of-concept of hypertuning being a viable approach to adapt downstream models. We expect that further exploration of hyperpretraining and MTF hyperparameters as well as hypermodel architectures may lead to better results and overcome some of the limitations we identiﬁed. 90 20k 50k 70k 100k HyperPretraining Steps 50 51 52 53 54 55P3 Held-out Performance (a) HyperT5-Preﬁx 0 20k 50k 70k 100k HyperPretraining Steps 50 51 52 53 54 55P3 Held-out Performance  (b) HyperT5-LoRA Figure 4: Performance of HyperT5 models on P3 evaluation with different amounts of hyperpretrain- ing. HyperPretraining is crucial for good performance of the hypermodels. However, hyperpretraining for too many steps can also hurt performance (as see in the case of HyperT5-LoRA). 5.5 Is HyperPretraining Necessary? We demonstrate the beneﬁts of hyperpretraining for the hypermodels in this section. As mentioned in Section 3, we hyperpretrained the hypermodels for 100k steps before multi-task ﬁne-tuning them on P3 tasks. To examine the impact of hyperpretraining, we also multi-task ﬁne-tuned HyperT5-Preﬁx and HyperT5-LoRA from LM-adapted T5 without any hyperpretraining, and from intermediate checkpoints over the course of hyperpretraining. Figure 4 shows the average scores on the held-out tasks for these models. Both HyperT5 models perform very poorly without any hyperpretraining, achieving scores similar to PEFT-only (see Table 1). With hyperpretraining, the performance of both hypermodels signiﬁcantly improves. While HyperT5-Preﬁx appears to consistently improve over the course of 100k steps, we observe that HyperT5-LoRA performance slightly declines after 50k steps. Hypermodels targeting different PEFT methods may beneﬁt from different amounts of hyperpretraining, and we emphasize that our choice of the number of hyperpretraining steps is by no means considered to be optimal.6 We expect that better hyperpretraining conﬁgurations can be explored in future work. 6 HyperModels for Improved Parameter Initialization Thus far, we have discussed hypermodels in the context of generating PEFT parameters in a single forward pass through the hypermodel. We can also consider an alternative use of hypermodels: Instead of randomly initializing new parameters, we can use hypermodels to produce task-speciﬁc PEFT parameters based on a few examples from the task. This can be seen as using task knowledge acquired by the hypermodel during training to provide a ﬁrst approximation of PEFT parameters, and thereafter reﬁning the parmaeters via regular PEFT training. In conventional PEFT, wherever new parameters are introduced into the model, they are either initialized randomly, or with ﬁxed initial values (e.g. the up-projection weights in LoRA are initialized to 0)–for brevity, we will refer to this simply as random initialization. Beyond random initialization, Vu et al. (2021, SPoT) and Gu et al. (2021, PPT) have explored transfer-learning within PEFT, ﬁrst doing PEFT on one or more upstream tasks, and then using the learned PEFT parameters as an initialization for downstream PEFT. This approach has two advantages over conventional PEFT initializations. First, the hypermodel- generated parameters already perform well on the task, as shown in Section 5.3, so PEFT training can reach good performance faster. Second, the hypermodel can automatically transfer relevant knowledge from previous tasks to the new task, similar to SPoT and PPT, except we let the hypermodel determine what previously learned task knowledge is most applicable to the new task. For instance, a major challenge addressed in SPoT was searching for the set of upstream tasks whose PEFT parameters would be the most appropriate initialization for a downstream task–in our case, we can directly provide a hypermodel with few-shot examples to generate our desired initialization. 6We chose 100k steps based on the T5 LM-adaptation procedure (Lester et al., 2021). 100 5000 10000 15000 20000 Training Steps 50 60 70Accuracy Rand Init Shared Init Hyper Init (a) Preﬁx Tuning 0 5000 10000 15000 20000 Training Steps 50 60 70Accuracy Rand Init Shared Init Hyper Init (b) LoRA Figure 5: Average performance on P3 held-out tasks with preﬁx tuning and LoRA, using different parameter initializations. Using hypermodel-generated initializations starts with higher performance and continues to perform better on average over the course of training. To investigate the effectiveness of using hypermodels to generate PEFT initializations, we use the P3-trained models from Section 5.3.1, and perform preﬁx tuning and LoRA tuning on the held-out tasks individually.7 For each method-task pair, we sweep across learning rates {1e−3,1e−4,1e−5} and take the best average result over 3 random seeds. We consider two baselines for initializations: random initialization (Rand Init) and using the multi-task ﬁne-tuned PEFT parameters from Section 5.3.1 as initializations (Shared Init). The hypermodel- generated initialization (Hyper Init) is generated using a randomly sampled set of 16 examples from the respective training sets. We show the results of preﬁx tuning8 and LoRA tuning with different initialization schemes in Table 6. We observe that for both preﬁx tuning and LoRA tuning, shared initialization signiﬁcantly performs random initialization, while using a hypermodel-generated initialization outperforms both on average. We also show the average performance across tasks over the course of tuning in Figure 5. We observe that hypermodel-generated initializations start with much better performance compared to the other two initialization schemes, and continue to outperform them over the course of ﬁne-tuning. Hence, hypermodels can be complementary to a standard PEFT pipeline, providing both performance gains and computational cost savings. ANLI HSwg CB COPA RTE WiC WSC WGD A VG Preﬁx (Rand Init) 54.6 50.5 98.8 79.0 78.8 71.6 63.5 52.2 68.6 Preﬁx (Shared Init) 60.8 51.6 99.4 85.7 84.8 72.4 72.6 65.1 74.0 Preﬁx (Hyper Init) 61.4 51.5 97.6 84.3 87.1 71.2 76.5 71.6 75.2 LoRA (Rand Init) 59.5 51.3 93.5 78.0 82.6 73.5 77.9 65.1 72.7 LoRA (Shared Init) 57.9 51.6 99.4 83.0 83.8 73.1 73.3 67.9 73.7 LoRA (Hyper Init) 57.7 48.4 99.4 87.3 84.1 73.0 83.9 66.2 75.0 Table 6: Preﬁx tuning and LoRA ﬁne-tuning on T5-Large with different initializations on P3 held-out tasks. Using HyperT5-generated parameters as an initialization achieves better performance on average than using shared MTF PEFT parameters or random initialization. 7 Conclusion We introduce the concept of hypertuning, which leverages a hypermodel to adapt a downstream model to a speciﬁc downstream application. We present a basic framework for hypertuning, where a hypermodel is trained to produce parameters for a downstream model from few-shot examples in one forward pass, and we apply this framework to train HyperT5-Preﬁx and HyperT5-LoRA models that can adapt a ﬁxed downstream T5 model. We ﬁnd that a two-stage training procedure of hyperpretraining and multi-task ﬁne-tuning is effective for training hypermodels, and we evaluate the 7We use one speciﬁc prompt format for each task, listed in Appendix B.1. 8Preﬁx tuning is performed via a reparameterization, in line with standard practice. Refer to Appendix D for details. 11HyperT5 models on P3, MetaICL and S-NI datasets, showing that they can generate PEFT parameters that enable the downstream T5 models to perform well on unseen tasks. Furthermore, the parameters generated by hypertuning can also serve as improved parameter initializations for parameter-efﬁcient ﬁne-tuning. We regard these ﬁndings as an initial but encouraging indication of the potential of adapting large language models without back-propagation. 8 Acknowledgements We would like to thank Sam Bowman for their thoughtful feedback and Jonas Pfeiffer for early idea discussion. References Anonymous. 2023. Pretraining One Language Model for All With the Text-to-text Framework Using Model-generated Signals. In Submitted to The Eleventh International Conference on Learning Representations. Under review. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatﬁeld-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc. Stephanie C. Y . Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K. Lampinen, and Felix Hill. 2022. Transformers generalize differently from information stored in context vs in weights. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via Language Model In-context Tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719–730, Dublin, Ireland. Association for Computational Linguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. 2022. Scaling Instruction-ﬁnetuned Language Models. 12Budhaditya Deb, Guoqing Zheng, and Ahmed Hassan Awadallah. 2022. Boosting Natural Language Generation from Instructions with Meta-learning. Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. 8-bit Optimizers via Block- wise Quantization. 9th International Conference on Learning Representations, ICLR. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. 2022. Delta Tuning: A Comprehensive Study of Parameter Efﬁcient Methods for Pre-trained Language Models. Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic Meta-learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126–1135. PMLR. Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. PPT: Pre-trained Prompt Tuning for Few-shot Learning. David Ha, Andrew M. Dai, and Quoc V . Le. 2017. HyperNetworks. InInternational Conference on Learning Representations. Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. 2022. HyperPrompt: Prompt-based task-conditioning of transformers. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8678–8690. PMLR. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efﬁcient Transfer Learning for NLP. CoRR, abs/1902.00751. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021. Parameter-efﬁcient Multi-task Fine-tuning for Transformers via Shared Hypernetworks. In Pro- ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 565–576, Online. Association for Computational Linguistics. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896–1907, Online. Association for Computational Linguistics. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efﬁcient Prompt Tuning. Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah Constant. 2022. Reducing Retraining by Recycling Parameter-Efﬁcient Prompts. Xiang Lisa Li and Percy Liang. 2021. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online. Association for Computational Linguistics. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to Learn In Context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791–2809, Seattle, United States. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. 13William Peebles, Ilija Radosavovic, Tim Brooks, Alexei A. Efros, and Jitendra Malik. 2022. Learning to learn with generative models of neural network checkpoints. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. CoRR, abs/2005.00247. Jason Phang, Thibault Févry, and Samuel R. Bowman. 2018. Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231–5247, Online. Association for Computational Linguistics. Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun, and Jie Zhou. 2021. Exploring Universal Intrinsic Task Subspace via Prompt Tuning. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1–67. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: Memory Optimizations toward Training Trillion Parameter Models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC ’20. IEEE Press. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Repre- sentations. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2021. SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer. CoRR, abs/2110.07904. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella- Micke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and Predicting Transferability across NLP Tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art 14natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online. Association for Computational Linguistics. Zhaofeng Wu, Robert L. Logan IV , Pete Walsh, Akshita Bhagia, Dirk Groeneveld, Sameer Singh, and Iz Beltagy. 2022. Continued Pretraining for Better Zero- and Few-Shot Promptability. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163–7189, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. 15A Training Details All experiments are trained with 1-bit Adam (Dettmers et al., 2022) and batch size of 256, a learning rate of 5e-5, and a linear decay schedule. Training was performed with ZeRO (Rajbhandari et al., 2020) and Transformers (Wolf et al., 2020). For hypermodels, the hypermodel’s max input sequence length is 1024 tokens and the downstream model’s max input sequence length is 384 tokens. Correspondingly, the max input sequence length for all non-few-shot models (e.g. T5-MTF, T5-MTF(Preﬁx)) is 384. The max input sequence length of few-shot models (e.g. T5-MTF-Few-shot) is thus conservatively set at 1024+384=1408 tokens. The max target sequence length is set to 128 for all experiments. A.1 Input Formatting Few-shot examples for hypermodels are formatted in the following manner: <x> Input 1 <y> Target 1 <x> Input 2 <y> Target 2 <x> Input 3 <y> Target 3 where <x> and <y> and special tokens. For S-NI, the task deﬁnitions are treated as just another example: <x> Instruction <x> Input 1 <y>Target 1 <x> Input 2 <y>Target 2 B Dataset-speciﬁc Details B.1 P3 / T0 We highlight some differences our T0 baselines and the T0 setup described in the original paper (Sanh et al., 2022). Besides the different optimizers and batch sizes listed above, we do not use packing to process our training data. Moreover, because our focus is on few-shot learning, we remove a number of tasks formulations with longer inputs from the T0-train dataset, listed in Section 6. For T0, we use an input sequence length of 384 and output length of 128, which matches the input and output lengths of the downstream model in our hypermodel setup. For T5-MTF-Few-shot, we use an input sequence length of 1024+384=1408, which is the combined input lengths of the hypermodel and downstream model. We believe that these changes can meaningfully modify the performance of the T0 models, but provide a fairer baseline to the hypermodel setup. 16adversarial_qa_dbert_answer_the_following_q, adversarial_qa_dbert_based_on, adversarial_qa_dbert_generate_question, adversarial_qa_dbert_question_context_answer, adversarial_qa_dbert_tell_what_it_is, adversarial_qa_dbidaf_answer_the_following_q, adversarial_qa_dbidaf_based_on, adversarial_qa_dbidaf_generate_question, adversarial_qa_dbidaf_question_context_answer, adversarial_qa_dbidaf_tell_what_it_is, adversarial_qa_droberta_answer_the_following_q, adversarial_qa_droberta_based_on, adversarial_qa_droberta_generate_question, adversarial_qa_droberta_question_context_answer, adversarial_qa_droberta_tell_what_it_is, ag_news_classify, ag_news_classify_question_ﬁrst, ag_news_classify_with_choices, ag_news_classify_with_choices_question_ﬁrst, ag_news_recommend, ag_news_which_section, ag_news_which_section_choices, amazon_polarity_Is_this_product_review_positive, amazon_polarity_Is_this_review, amazon_polarity_Is_this_review_negative, amazon_polarity_User_recommend_this_product, amazon_polarity_convey_negative_or_positive_sentiment, amazon_polarity_ﬂattering_or_not, amazon_polarity_negative_or_positive_tone, amazon_polarity_user_satisﬁed, amazon_polarity_would_you_buy, app_reviews_categorize_rating_using_review, app_reviews_convert_to_rating, app_reviews_convert_to_star_rating, app_reviews_generate_review, cnn_dailymail_3.0.0_generate_story, cnn_dailymail_3.0.0_spice_up_story, common_gen_Example_prompt, common_gen_Given_concepts_type_1, common_gen_Given_concepts_type_2, common_gen_Put_together, common_gen_choice_in_concept_centric_sentence_generation, common_gen_random_task_template_prompt, common_gen_sentence_to_concepts, common_gen_topic_to_sentence, common_gen_topics_from_the_sentence, cos_e_v1.11_aligned_with_common_sense, cos_e_v1.11_description_question_option_id, cos_e_v1.11_description_question_option_text, cos_e_v1.11_explain_why_human, cos_e_v1.11_generate_explanation_given_text, cos_e_v1.11_i_think, cos_e_v1.11_question_description_option_id, cos_e_v1.11_question_description_option_text, cos_e_v1.11_question_option_description_id, cos_e_v1.11_question_option_description_text, cos_e_v1.11_rationale, cosmos_qa_context_answer_to_question, cosmos_qa_context_description_question_answer_id, cosmos_qa_context_description_question_answer_text, cosmos_qa_context_description_question_text, cosmos_qa_context_question_description_answer_id, cosmos_qa_context_question_description_answer_text, cosmos_qa_context_question_description_text, cosmos_qa_description_context_question_answer_id, cosmos_qa_description_context_question_answer_text, cosmos_qa_description_context_question_text, cosmos_qa_no_prompt_id, cosmos_qa_no_prompt_text, cosmos_qa_only_question_answer, dbpedia_14_given_a_choice_of_categories_, dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to, dbpedia_14_given_list_what_category_does_the_paragraph_belong_to, dbpedia_14_pick_one_category_for_the_following_text, dream_answer_to_dialogue, dream_baseline, dream_generate_ﬁrst_utterance, dream_generate_last_utterance, dream_read_the_following_conversation_and_answer_the_question, duorc_ParaphraseRC_build_story_around_qa, duorc_SelfRC_build_story_around_qa, gigaword_TLDR, gigaword_ﬁrst_sentence_title, gigaword_generate_summary_for_this, gigaword_in_a_nutshell, gigaword_make_a_title, gigaword_reverse_writing, gigaword_write_a_title_for_this_sentence, gigaword_write_an_article, gigaword_write_its_sentence, glue_mrpc_equivalent, glue_mrpc_generate_paraphrase, glue_mrpc_generate_sentence, glue_mrpc_paraphrase, glue_mrpc_replace, glue_mrpc_same_thing, glue_mrpc_want_to_know, glue_qqp_answer, glue_qqp_duplicate, glue_qqp_duplicate_or_not, glue_qqp_meaning, glue_qqp_quora, glue_qqp_same_thing, imdb_Movie_Expressed_Sentiment, imdb_Movie_Expressed_Sentiment_2, imdb_Negation_template_for_positive_and_negative, imdb_Reviewer_Enjoyment, imdb_Reviewer_Enjoyment_Yes_No, imdb_Reviewer_Expressed_Sentiment, imdb_Reviewer_Opinion_bad_good_choices, imdb_Reviewer_Sentiment_Feeling, imdb_Sentiment_with_choices_, imdb_Text_Expressed_Sentiment, imdb_Writer_Expressed_Sentiment, kilt_tasks_hotpotqa_combining_facts, kilt_tasks_hotpotqa_complex_question, kilt_tasks_hotpotqa_ﬁnal_exam, kilt_tasks_hotpotqa_formulate, kilt_tasks_hotpotqa_straighforward_qa, paws_labeled_ﬁnal_Concatenation, paws_labeled_ﬁnal_Concatenation_no_label, paws_labeled_ﬁnal_Meaning, paws_labeled_ﬁnal_Meaning_no_label, paws_labeled_ﬁnal_PAWS_ANLI_GPT3, paws_labeled_ﬁnal_PAWS_ANLI_GPT3_no_label, paws_labeled_ﬁnal_Rewrite, paws_labeled_ﬁnal_Rewrite_no_label, paws_labeled_ﬁnal_context_question, paws_labeled_ﬁnal_context_question_no_label, paws_labeled_ﬁnal_paraphrase_task, paws_labeled_ﬁnal_task_description_no_label, qasc_is_correct_1, qasc_is_correct_2, qasc_qa_with_combined_facts_1, qasc_qa_with_separated_facts_1, qasc_qa_with_separated_facts_2, qasc_qa_with_separated_facts_3, qasc_qa_with_separated_facts_4, qasc_qa_with_separated_facts_5, quarel_choose_between, quarel_do_not_use, quarel_heres_a_story, quarel_logic_test, quarel_testing_students, quartz_answer_question_based_on, quartz_answer_question_below, quartz_given_the_fact_answer_the_q, quartz_having_read_above_passage, quartz_paragraph_question_plain_concat, quartz_read_passage_below_choose, quartz_use_info_from_paragraph_question, quartz_use_info_from_question_paragraph, ropes_background_new_situation_answer, ropes_background_situation_middle, ropes_given_background_situation, ropes_new_situation_background_answer, ropes_plain_background_situation, ropes_plain_bottom_hint, ropes_plain_no_background, ropes_prompt_beginning, ropes_prompt_bottom_hint_beginning, ropes_prompt_bottom_no_hint, ropes_prompt_mix, ropes_read_background_situation, rotten_tomatoes_Movie_Expressed_Sentiment, rotten_tomatoes_Movie_Expressed_Sentiment_2, rotten_tomatoes_Reviewer_Enjoyment, rotten_tomatoes_Reviewer_Enjoyment_Yes_No, rotten_tomatoes_Reviewer_Expressed_Sentiment, rotten_tomatoes_Reviewer_Opinion_bad_good_choices, rotten_tomatoes_Reviewer_Sentiment_Feeling, rotten_tomatoes_Sentiment_with_choices_, rotten_tomatoes_Text_Expressed_Sentiment, rotten_tomatoes_Writer_Expressed_Sentiment, samsum_Generate_a_summary_for_this_dialogue, samsum_Given_the_above_dialogue_write_a_summary, samsum_Sum_up_the_following_dialogue, samsum_Summarize_, samsum_Summarize_this_dialogue_, samsum_To_sum_up_this_dialog, samsum_Write_a_dialogue_that_match_this_summary, sciq_Direct_Question, sciq_Direct_Question_Closed_Book_, sciq_Multiple_Choice, sciq_Multiple_Choice_Closed_Book_, sciq_Multiple_Choice_Question_First, social_i_qa_Check_if_a_random_answer_is_valid_or_not, social_i_qa_Generate_answer, social_i_qa_Generate_the_question_from_the_answer, social_i_qa_I_was_wondering, social_i_qa_Show_choices_and_generate_answer, social_i_qa_Show_choices_and_generate_index, trec_ﬁne_grained_ABBR, trec_ﬁne_grained_ABBR_context_ﬁrst, trec_ﬁne_grained_DESC, trec_ﬁne_grained_DESC_context_ﬁrst, trec_ﬁne_grained_ENTY , trec_ﬁne_grained_HUM, trec_ﬁne_grained_HUM_context_ﬁrst, trec_ﬁne_grained_LOC, trec_ﬁne_grained_LOC_context_ﬁrst, trec_ﬁne_grained_NUM, trec_ﬁne_grained_NUM_context_ﬁrst, trec_ﬁne_grained_open, trec_ﬁne_grained_open_context_ﬁrst, trec_pick_the_best_descriptor, trec_trec1, trec_trec2, trec_what_category_best_describe, trec_which_category_best_describes, wiki_bio_comprehension, wiki_bio_guess_person, wiki_bio_key_content, wiki_bio_what_content, wiki_bio_who, wiki_qa_Decide_good_answer, wiki_qa_Direct_Answer_to_Question, wiki_qa_Generate_Question_from_Topic, wiki_qa_Is_This_True_, wiki_qa_Jeopardy_style, wiki_qa_Topic_Prediction_Answer_Only, wiki_qa_Topic_Prediction_Question_Only, wiki_qa_Topic_Prediction_Question_and_Answer_Pair, wiki_qa_automatic_system, wiki_qa_exercise, wiki_qa_found_on_google, wiqa_does_the_supposed_perturbation_have_an_effect, wiqa_effect_with_label_answer, wiqa_effect_with_string_answer, wiqa_what_is_the_ﬁnal_step_of_the_following_process, wiqa_what_is_the_missing_ﬁrst_step, wiqa_what_might_be_the_ﬁrst_step_of_the_process, wiqa_what_might_be_the_last_step_of_the_process, wiqa_which_of_the_following_is_the_supposed_perturbation, yelp_review_full_based_on_that, yelp_review_full_format_rating, yelp_review_full_format_score, yelp_review_full_format_star, yelp_review_full_on_a_scale, yelp_review_full_so_i_would, yelp_review_full_this_place Figure 6: List of P3 dataset-prompts used for training. We chose a subset of T0-train with average input lengths shorter than 320 tokens. 17For the hypermodel initialization/PEFT experiments, we do single-task parameter-efﬁcient ﬁne-tuning on each of the following dataset-prompts: 1. anli_GPT_3_style_r1 2. hellaswag_complete_ﬁrst_then 3. super_glue_cb_GPT_3_style 4. super_glue_copa_C1_or_C2_premise_so_because_ 5. super_glue_rte_GPT_3_style 6. super_glue_wic_GPT_3_prompt 7. super_glue_wsc.ﬁxed_GPT_3_Style 8. winogrande_winogrande_debiased_Replace B.2 S-NI / T-KI To standardize the preprocessing across our experiments, we do not use the input formatting provided in the original work (Wang et al., 2022). Instead, we use the format described in Appendix A.1 for all experiments. Given that the same format is used in multi-task ﬁne-tuning and evaluation, this should not unfairly advantage any model. However, because the format deviates from that of the original work, we do not directly evaluate the T-KI models. Additionally, the Super-NaturalInstructions dataset (previously known as NaturalInstructions-v2) has undergone some changes over time. In our experiments, we use the v2.5 version of the dataset. B.3 MetaICL C Model Details D Elaboration on Preﬁx Tuning Comparisons While preﬁx tuning is generally presented as learning a set of prepended key and value representations for each Transformer layer, in practice, the learned preﬁxes are not optimized directly. In the work that introduced preﬁx tuning (Li and Liang, 2021), Section 4.3 explains that directly optimizing the learned preﬁxes leads to unstable training and poorer performance, and instead recommend optimizing a set of learned embeddings and a parameterized MLP to generate the learned preﬁxes. (At inference time, the preﬁxes can be generated from the learned components–this only impacts the training process.) We conﬁrmed in our experiments that directly optimizing preﬁxes leads to poor perfomance, and other works involving preﬁx tuning have similarly used this preﬁx reparamterization Hence, we have two ﬂavors of preﬁx tuning to consider: directly optimizing over preﬁxes (“Preﬁx- Flat\"), and optimizing with reparamterization (“Preﬁx-MLP\"). The T5-MTF (Preﬁx) model uses Preﬁx-MLP, which is the appropriate approach to tuning preﬁxes in that setting. However, because HyperT5-Preﬁx only generates the ﬁnal preﬁxes, only Preﬁx-Flat tuning is possible. Hence, when we perform the preﬁx tuning with different initializations in Section 6, we cannot fairly compare the two methods directly–one which uses a reparameterization during training, and the other which uses direct optimization which we know performs worse in practice. Instead, we compare preﬁx tuning in the two different settings, Preﬁx-Flat and Preﬁx-MLP, completely separately. We describe each individual initialization scheme: Preﬁx-Flat 1. Preﬁx-Flat (Rand): Randomly initialize soft preﬁxes 2. Preﬁx-Flat (Shared): Run a forward pass through the preﬁx reparameterization to obtain the ﬂat preﬁxes, and use them as the initialization 3. Preﬁx-Flat (Hyper): Generate preﬁxes with HyperT5-Preﬁx 18# B = batch_size # T = input_length # P = number of prompt tokens # H = hidden_dim # L = num layers in encoder/decoder # Shape: [B, T] fewshot_input_ids = ... # Shape: [B, T, H] hyper_enc_out = hypermodel.encoder(fewshot_input_ids) # Shape: [B, 2P, H] # Decoder implicitly uses a fixed set of input embeddings of size 2P hyper_dec_out = hypermodel.decoder(hyper_enc_out) # Shape: [B, P, LH] downstream_enc_k_prefix = hypermodel.enc_k_head(hyper_dec_out[:, :P, :]) downstream_enc_v_prefix = hypermodel.enc_v_head(hyper_dec_out[:, :P, :]) downstream_dec_k_prefix = hypermodel.dec_k_head(hyper_dec_out[:, P:, :]) downstream_dec_v_prefix = hypermodel.dec_v_head(hyper_dec_out[:, P:, :]) # Shape: [B, P, L H] downstream_enc_k_prefix = downstream_enc_k_prefix.reshape(B, P, L, H) downstream_enc_v_prefix = downstream_enc_v_prefix.reshape(B, P, L, H) downstream_dec_k_prefix = downstream_dec_k_prefix.reshape(B, P, L, H) downstream_dec_v_prefix = downstream_dec_v_prefix.reshape(B, P, L, H) # These correspond to the per-layer learned prefixes for K and V # where each of the heads is defined (e.g.): hypermode.enc_k_head = nn.Sequential([ nn.LayerNorm(), nn.Linear(H), nn.TanH(), nn.Linear(L*H), ]) Figure 7: Pseudo-code for HyperT5-Preﬁx Preﬁx-MLP 1. Preﬁx-MLP (Rand): Randomly initialize the preﬁx reparameterization embeddings and MLPs (i.e. conventional preﬁx tuning) 2. Preﬁx-MLP (Shared): Directly reuse the preﬁx reparameterization from T5-MTF (Preﬁx) 3. Preﬁx-MLP (Hyper): We train an entirely new HyperT5-Preﬁx-MLP model, where the parameter generation heads directly correspond to the preﬁx tuning reparameterization MLPs. The encoder-decoder in the hypermodel will output the “embeddings\", and we directly reuse the parameter generation heads during tuning. The results for Preﬁx-MLP are presented in the body of the paper in Section 6. We believe that this approach provides the fairest comparison of initializations. Importantly, both Preﬁx-MLP (Shared) and Preﬂix-MLP (Hyper) have been trained on the same number of labeled examples (not including the few-shot examples, which are inputs), but where the Preﬁx-MLP uses a single set of learned embeddings, HyperT5-Preﬁx-MLP generates the embeddings based on few-shot examples. We present the full set of preﬁx tuning results in Table 7, the performance of Preﬁx-Flat Figure 9. 19# B = batch_size # T = input_length # R = LoRA rank # H = hidden_dim # L = num layers in encoder/decoder # Shape: [B, T] fewshot_input_ids = ... # Shape: [B, T, H] hyper_enc_out = hypermodel.encoder(fewshot_input_ids) # Shape: [B, 3L, H] # Decoder implicitly uses a fixed set of input embeddings of size 3L hyper_dec_out = hypermodel.decoder(hyper_enc_out) # Shape: [B, L, H] enc_repr = hyper_dec_out[:, :L, :] dec_repr = hyper_dec_out[:, L:2*L, :] cross_repr = hyper_dec_out[:, 2*L:, :] # Repeat for dec_repr, cross_repr for decoder self- and cross-attention # Shape: [B, L, 2RH] enc_q_repr = hypermodel.enc_q_head(enc_repr) enc_v_repr = hypermodel.enc_v_head(enc_repr) # Shape: [B, L, 2RH] enc_q_repr = enc_q_repr.reshape(B, L, 2, R, H) enc_v_repr = enc_v_repr.reshape(B, L, 2, R, H) # raw_enc_q_gate and raw_enc_v_gate are learned parameters of size [L] # Shape: [1, L, 1, 1, 1] enc_q_gate = torch.tanh(raw_enc_q_gate)[None, :, None, None, None] enc_v_gate = torch.tanh(raw_enc_v_gate)[None, :, None, None, None] # Shape: List of [B, R, H] enc_lora_q_up_list = [enc_q_repr[:, l, 0, :, :] for l in range(L)] enc_lora_q_down_list = [enc_q_repr[:, l, 1, :, :] for l in range(L)] enc_lora_v_up_list = [enc_v_repr[:, l, 0, :, :] for l in range(L)] enc_lora_v_down_list = [enc_v_repr[:, l, 1, :, :] for l in range(L)] # These correspond to up- and down-map deltas in LoRA in Q and V # attention linear maps # where each of the heads is defined (e.g.): hypermode.enc_q_head = nn.Sequential([ nn.LayerNorm(), nn.Linear(H), nn.TanH(), nn.Linear(2*R*H), ]) Figure 8: Pseudo-code for HyperT5-LoRA 20ANLI HSwag CB COPA RTE WiC WSC WGD A VG Preﬁx-Flat (Rand Init) 43.6 36.3 82.7 74.0 72.9 64.4 64.2 53.0 61.4 Preﬁx-Flat (Shared Init) 54.3 40.4 98.8 82.7 83.9 71.0 67.4 57.1 69.4 Preﬁx-Flat (Hyper Init) 56.6 43.5 91.7 84.3 85.3 69.3 73.0 67.6 71.4 Preﬁx-MLP (Rand Init) 54.6 50.5 98.8 79.0 78.8 71.6 63.5 52.2 68.6 Preﬁx-MLP (Shared Init) 60.8 51.6 99.4 85.7 84.8 72.4 72.6 65.1 74.0 Preﬁx-MLP (Hyper Init) 61.4 51.5 97.6 84.3 87.1 71.2 76.5 71.6 75.2 LoRA (Rand Init) 59.5 51.3 93.5 78.0 82.6 73.5 77.9 65.1 72.7 LoRA (Shared Init) 57.9 51.6 99.4 83.0 83.8 73.1 73.3 67.9 73.7 LoRA (Hyper Init) 57.7 48.4 99.4 87.3 84.1 73.0 83.9 66.2 75.0 Table 7: Preﬁx tuning (Flat and MLP) and LoRA ﬁne-tuning on T5-Large with different initializations on P3 held-out tasks. 210 5000 10000 15000 20000 Training Steps 50 60 70Accuracy Rand Init Shared Init Hyper Init Figure 9: Average performance on P3 held-out tasks with preﬁx tuning (ﬂat). 22",
      "meta_data": {
        "arxiv_id": "2211.12485v1",
        "authors": [
          "Jason Phang",
          "Yi Mao",
          "Pengcheng He",
          "Weizhu Chen"
        ],
        "published_date": "2022-11-22T18:52:25Z",
        "pdf_url": "https://arxiv.org/pdf/2211.12485v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces HyperTuning, a novel approach to large language model (LLM) adaptation that uses a hypermodel to generate task-specific parameters for a fixed downstream model, aiming to overcome the cost and inefficiency of gradient-based fine-tuning. The main contribution is demonstrating that a T5-based hypermodel, HyperT5, can effectively generate soft prefixes or LoRA parameters from few-shot examples for unseen tasks in a single forward pass, without back-propagation. Furthermore, hypermodel-generated parameters are shown to improve performance and accelerate convergence when used as initializations for further parameter-efficient fine-tuning (PEFT).",
        "methodology": "HyperTuning involves a hypermodel (H) that takes few-shot examples as input and outputs PEFT parameters (φ̂) for a frozen downstream LLM. The hypermodel is trained end-to-end by optimizing its parameters (ξ) to minimize the loss on the downstream model, where gradients are back-propagated through both the downstream model and the hypermodel. A two-stage training procedure is proposed for HyperT5: 1) HyperPretraining, using a Context-Augmented Conditional Language Modeling (CACLM) objective to teach the hypermodel to generate parameters that assist the downstream model in predicting masked tokens based on provided context. 2) Multi-Task Fine-Tuning (MTF) on diverse language tasks to enable generalization to unseen tasks, where the hypermodel learns to generate task-specific PEFT parameters from few-shot examples.",
        "experimental_setup": "The experiments use HyperT5, a T5-based hypermodel, and a frozen LM-adapted T5 (T5 v1.1 architecture, T5-Large (770M) and T5-XL (3B) models) as the downstream model. Two PEFT methods are explored: prefix tuning (HyperT5-Preﬁx) and LoRA (HyperT5-LoRA). Training is conducted using 1-bit Adam optimizer with a batch size of 256, learning rate of 5e-5, and ZeRO for memory optimization. Max input sequence lengths are 1024 for the hypermodel, 384 for the downstream model, and 1408 for few-shot baselines. Hyperpretraining runs for 100k steps on the C4 dataset. Multi-task fine-tuning is performed for 10,000 steps. Evaluation is conducted on three multi-task datasets: P3 (Public Pool of Prompts) using held-out tasks with multiple-choice accuracy, MetaICL using three train-task splits with ROUGE or Macro-F1, and Super-NaturalInstructions (S-NI v2.5) using definition-only and definition+2positive examples with ROUGE-L. Baselines include T5-MTF, T5-MTF-Few-shot, and PEFT methods (Preﬁx, LoRA) directly tuned. Additional experiments involve joint training of hypermodel and downstream model, and using hypermodel-generated parameters as initializations for PEFT.",
        "limitations": "The current formulation of hypermodels can only take a small number of examples as input, which limits its performance compared to full parameter-efficient fine-tuning or full fine-tuning. HyperT5 generally underperforms T5 models explicitly trained for few-shot in-context learning with full attention across examples (e.g., T5-MTF-Fewshot), which provides higher performance but is computationally more expensive at inference. The choice of hyperpretraining steps might not be optimal, as seen with HyperT5-LoRA's performance decline after 50k steps. The results presented are primarily a proof-of-concept, and extensive hyperparameter tuning was not performed.",
        "future_research_directions": "Future research includes generalizing hypertuning to other scenarios, such as training hypermodels to predict gradients or generate parameter updates for larger training sets. The authors suggest potentially replacing gradient-descent-based fine-tuning pipelines with hypertuning for various applications. They also plan to explore training a hypermodel against an already multi-task fine-tuned model for enhanced performance. Further investigation into hyperpretraining and MTF hyperparameters, as well as alternative hypermodel architectures, is suggested to improve results and address current limitations."
      }
    },
    {
      "title": "Prefix Conditioning Unifies Language and Label Supervision",
      "abstract": "Image-classification datasets have been used to pretrain image recognition\nmodels. Recently, web-scale image-caption datasets have emerged as a source of\npowerful pretraining alternative. Image-caption datasets are more\n``open-domain'', containing a wider variety of scene types and vocabulary words\nthan traditional classification datasets, and models trained on these datasets\nhave demonstrated strong performance on few- and zero-shot recognition tasks.\nWhen naively unifying image-classification and -caption dataset, we show that\nsuch dataset biases negatively affect pre-training by reducing the\ngeneralizability of learned representations and thus jeopardizing zero-shot\nperformance since the unification can tailor the model for the classification\ndataset, making it vulnerable to the distribution shift from the dataset. In\nthis work, we address the problem by disentangling the dataset bias using\nprefix tokens that inform a language encoder of the type of the input dataset\n(e.g., image-classification or caption) at training time. This approach allows\nthe language encoder to share the knowledge from two datasets as well as switch\nthe mode of feature extraction, i.e., image-classification dataset or\nimage-caption dataset tailored mode, where we use image-caption mode in the\nzero-shot evaluation. Our method is generic and can be easily integrated into\nexisting VL pre-training objectives such as CLIP or UniCL. In experiments, we\nshow that this simple technique improves the performance in zero-shot image\nrecognition accuracy and robustness to the image-level distribution shift.",
      "full_text": "Preﬁx Conditioning Uniﬁes Language and Label Supervision Kuniaki Saito1,2*, Kihyuk Sohn 3 , Xiang Zhang 2 , Chun-Liang Li 2 , Chen-Yu Lee2 , Kate Saenko1,4 , Tomas Pﬁster2 {keisaito, saenko}@bu.edu {kihyuks,fancyzhx,chunliang,chenyulee,tpfister}@google.com 1Boston University, 2Google Cloud AI Research, 3Google Research, 4MIT-IBM Watson AI Lab Abstract Pretraining visual models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pretraining on image classiﬁcation data. Image-caption datasets are more “open-domain”, contain- ing broader scene types and vocabulary words, and re- sult in models that have strong performance in few- and zero-shot recognition tasks. However large-scale classi- ﬁcation datasets can provide ﬁne-grained categories with a balanced label distribution. In this work, we study a pretraining strategy that uses both classiﬁcation and cap- tion datasets to unite their complementary beneﬁts. First, we show that naively unifying the datasets results in sub- optimal performance in downstream zero-shot recognition tasks, as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem with novel Preﬁx Condi- tioning, a simple yet effective method that helps disentangle dataset biases from visual concepts. This is done by intro- ducing preﬁx tokens that inform the language encoder of the input data type (e.g., classiﬁcation vs caption) at training time. Our approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Preﬁx conditioning is generic and can be eas- ily integrated into existing VL pretraining objectives, such as CLIP or UniCL. In experiments, we show that it improves zero-shot image recognition and robustness to image-level distribution shift. 1. Introduction Supervised classiﬁcation datasets (e.g., ImageNet [7]) have traditionally been used to pretrain image representa- tions for use in downstream tasks. However, web-scale image-caption datasets have recently emerged as a pow- erful pretraining alternative [13, 20, 31]. Such datasets *Work done during internship at Google Cloud AI Research. Training DatasetIN21KCC12MIN21K + CC12MIN21K + CC12M CLIPUniCL CLIP+Ours UniCL+Ours Accuracy (%) on 11 standard zero-shot benchmarkAccuracy (%) on  11 standard zero-shot benchmark Training Dataset Figure 1. We propose Preﬁx Conditioning to unify image-caption (e.g., CC12M [5]) and image classiﬁcation datasets (e.g., Ima- geNet21K (IN21K) [7]) for training better zero-shot models. Pre- ﬁx conditioning improves zero-shot recognition performance by more than 6% on average when training on ImageNet21K and CC12M. are more “open-domain”, containing a wider variety of scene types and vocabularies than traditional classiﬁcation datasets, which are biased towards speciﬁc categories in their ﬁxed label sets. Consequently, models trained on web- scale image-caption datasets have shown stronger gener- alization in novel tasks [4, 31] and demonstrated remark- able performance on few and zero-shot image classiﬁcation tasks [31]. Nevertheless, classiﬁcation datasets are still use- ful for pre-training as they have a more balanced coverage of categories, including rare and ﬁne-grained categories, and a better focus on the labeled objects in each image. Recent works [43,45] therefore propose to combine cap- tion and classiﬁcation datasets for pre-training. [43] convert classiﬁcation labels to “label-prompts” by inserting the la- bel into a template sentence, e.g., “a photo of a <label>.”1 1We use the term prompt to indicate a template sentence ﬁlled with a class name. arXiv:2206.01125v2  [cs.CV]  15 May 2023Although training on the caption and label-prompt data achieves promising results, it does not fully resolve distribu- tion differences between the open-domain caption data and the classiﬁcation data. In particular, it produces a language embedding entangled with the classiﬁcation dataset “bias”. We note that classiﬁcation datasets tend to be biased in at least two ways: 1) the images mostly contain single ob- jects from restricted domains, and 2) the vocabulary is lim- ited and lacks the linguistic ﬂexibility required for zero-shot learning. Therefore, the class embedding of “a photo of a dog” optimized for ImageNet may really mean a photo of a dog from ImageNet instead, which is biased to ImageNet and does not generalize well to other datasets. We empiri- cally show that such dataset biases negatively affect uniﬁed pretraining by reducing the generalization of learned repre- sentations and thus jeopardizing zero-shot performance. To recognize diverse concepts in the open domain, the language model needs to disentangle the dataset bias from the visual concepts and extract language embeddings gen- eralizable to the open domain, e.g., the language embed- ding representing a photo of a dog from an open-domain dataset, such as image-caption dataset , instead of a photo of a dog from ImageNet. Given this intuition, we propose to learn dataset-speciﬁc language embeddings, while sharing knowledge from both datasets during training. We achieve this by a simple yet effective approach we call Preﬁx Con- ditioning. The idea is to learn a dataset-speciﬁc text token (preﬁx) for each dataset so that the bias of the dataset can be absorbed into this token, and in return the remaining text tokens can focus on learning visual concepts. Speciﬁcally, we prepend a different token for each dataset (e.g., image classiﬁcation or caption dataset) to the text input token se- quence during pre-training. The idea is in part inspired by preﬁx or prompt tun- ing [18, 21, 46], which showed that learnable tokens prepended to the input token sequences of the pre-trained language models are able to learn task-speciﬁc knowledge and thus can be used to solve downstream tasks by com- bining the knowledge of pre-trained large language models and task-speciﬁc preﬁx tokens. Our approach differs from prompt tuning in two ways: 1) the proposed preﬁx condi- tioning is designed to unify image-caption and classiﬁcation datasets by disentangling the dataset bias, which is a unique distinction to prompt-tuning works, 2) our approach is ap- plied for VL pre-training while the standard prompt tuning is used in ﬁne-tuning. In experiments, the proposed simple technique achieves superior performance on zero-shot evaluation if we use the preﬁx of the caption dataset to get the language embedding at test time as shown in Fig. 1. Meanwhile, inserting the pre- ﬁx of the classiﬁcation dataset leads to better performance on classiﬁcation data. We also observe a drastic perfor- mance improvement when combining our preﬁx condition- ing with the UniCL [43] objective because of their comple- mentarity. Our contributions are summarized as follows: • We propose novel Preﬁx Conditioning at pre-training time to unify image-label and image-caption supervision. It is the ﬁrst mechanism to use preﬁxes to condition the source of the dataset during vision language contrastive pre-training, rather than post pre-training. • This simple approach improves zero-shot recognition per- formance by more than 6% on average in experiments on ImageNet21K [7] and CC12M [5]. • Our comprehensive ablation study shows that preﬁx con- ditioning enables the model to switch its approach to ex- tracting language features, e.g., attend to different words. 2. Related Work Vision-Language Contrastive Learning. Zero-shot recognition is conventionally solved by learning the rela- tionship between visual representations and word embed- dings of the class names [1, 9, 12, 26, 38, 40, 41]. Vision- language contrastive learning models, such as CLIP [31], pre-train a model with a large-scale image-caption data (400M) and achieve a remarkable improvement in zero-shot recognition. ALIGN [13] demonstrated the effect of scaling up the size of image-caption data. Various techniques have been proposed to improve the data efﬁciency given a rela- tively small amount of image-caption data (order of 10M). ALBEF [20] employs model distillation and masked lan- guage modeling. DeCLIP [22], SLIP [29] and TCL [42] harness self-supervised contrastive learning. FILIP [44] uses token-to-token contrastive learning rather than the global contrastive learning used in CLIP. BLIP [19] gener- ates pseudo captions to diversify the language modality for each image. Unlike these works that handle only caption- style supervision, we focus on the use of label supervision in vision-language pre-training. Our approach brings or- thogonal improvement to the aforementioned works as they seek to improve training on image-caption data. UniCL [43] and K-Lite [34] unite the image-caption and image-label supervision by converting labels into text with pre-deﬁned template sentences. UniCL leverages a supervised contrastive loss [15] for image-label pairs. K- Lite [34] utilizes external knowledge from WordNet [28] and Wikitionary [27]. The input noun is augmented with the class hierarchy and deﬁnition to enrich the supervision. Our method is complementary to these approaches since both UniCL and K-Lite do not consider the domain shift between datasets. In experiments, we observe a signiﬁcant performance boost when UniCL is combined with the preﬁx conditioning. Learning with Prompts. Prompt tuning is a popu- lar technique to adapt a large language model to a spe- ciﬁc task with few training data and low computationalFigure 2. Left: Preﬁx conditioning at training time. Dataset-speciﬁc token is added to the input tokens with a contrastive learning objective applied. Right: Preﬁx conditioning at test time. Given a class name, we construct a class prompt with pre-deﬁned templates and add a token used to condition real caption during training considering that image-caption dataset covers much wider range of image domains and vocabulary words than image classiﬁcation dataset. cost [10,18,21,23,30]. To avoid tuning all parameters of the model and using hand-crafted prompts, preﬁx embeddings are added to the training input and are the only parame- ters optimized during ﬁne-tuning. The preﬁx embedding can be viewed as the knowledge of the downstream task. In this paper, since the target task is the zero-shot classi- ﬁcation, the bias of the language embedding needs to be from the dataset covering a wide range of domains rather than a speciﬁc domain. Therefore, we choose to use the preﬁx embedding learned for image-caption dataset during test time. This technique is also effective in adapting a pre- trained vision-language model [46, 47] to few-shot classi- ﬁcation by tuning the prompts of the language encoder to adapt to a downstream task. Additionally, prompt-tuning is effective in adapting a pre-trained vision model to a target task [14]. While these works aim to tailor a large pre-trained model to a speciﬁc downstream task with a small amount of data or low computational cost, our goal here is to condi- tion a model with the preﬁx during the pre-training stage by distinguishing between the image label and image caption data. This allows a model to effectively share the knowl- edge obtained from two different types of data sources. Dataset bias in image recognition. A large-scale image recognition dataset such as ImageNet [7] is known to be bi- ased towards a speciﬁc image domain. Therefore, a model trained on such a dataset shows vulnerability to the distri- bution shift, e.g., shift in object pose [3] and style of the images [37]. Nevertheless, [16,39] show that adapting only a linear layer on the pre-trained models can improve per- formance on the downstream tasks with distribution shifts. This indicates the importance of having a good classiﬁer on top of image encoders, such as linear classiﬁers generated by language encoders with preconditioning in our work. [8] propose a method for domain generalization. They condi- tion image recognition models with the domain embedding, which discriminates the input image domains, and demon- strate the importance of the domain-speciﬁc image classi- ﬁer. Our preﬁx conditioning can be seen as an attempt to de-bias the linear classiﬁer to obtain a domain-speciﬁc clas- siﬁer and adapt it from the classiﬁcation to the captioning domain. Also, [2, 17] approach the dataset bias in image classiﬁcation by de-biasing image representations. By con- trast, we tackle the problem in the framework of vision- language learning, disentangle the dataset bias in the lan- guage embedding and utilize the classiﬁer obtained by the caption domain. We note that while captioning datasets can also have data biases, they tend to be more open-domain than existing classiﬁcation datasets. 3. Method In this section, we introduce the Preﬁx Conditioning technique for pretraining a deep learning model on both image-caption and image-label (classiﬁcation) data. In Sec. 3.1, we discuss our problem setting and the back- ground of contrastive learning with image-caption data. In Sec. 3.2, we explain the details of our training approach, and in Sec. 3.3 our inference procedure. 3.1. Preliminaries Setup. Suppose we have access to two datasets: (i) an image label datasetSL = {(xn,tP n ,yn)}NL n=1, where x∈X is the image and tP ∈ Pis a prompt-style language de- scription based on its class label y∈Y, and (ii) a dataset of image-caption pairs SC = {(xn,tC n )}NC n=1, where tC ∈T is a caption. We assume thattis the tokenized language de- scription. For each imagex, an image encoder modelfθpa- rameterized by θextracts a visual representation ˜v∈Rd×1:˜v = fθ(x). For each caption or prompt t∈T , a text en- coder fφ parameterized by φextracts a language represen- tation ˜u∈Rd×1 : ˜u= fφ(t). Contrastive Loss. CLIP [31] is designed to ﬁnd repre- sentations that match an image to its paired caption while separating unpaired ones. For i-th image xi and j-th lan- guage description tj in a batch B, their features are nor- malized using vi = ˜vi ∥˜vi∥ and uj = ˜uj ∥˜uj∥. Finally, CLIP op- timizes the symmetric multi-class N-pair loss [35]: min {θ,φ} Lcon =Lt2i + Li2t, (1) which includes two contrastive terms (a temperature hyper- parameter τ controls the strength of penalties on hard nega- tive samples): Lt2i = −1 |B| ∑ i∈B log exp(τuT i vi)∑ j∈Bexp(τuT i vj), (2) Li2t = −1 |B| ∑ i∈B log exp(τvT i ui)∑ j∈Bexp(τvT i uj). (3) UniCL [43] composes each mini-batch with samples from both SL and SC. Then, for pairs from SL, they regard all samples from the same class as positive pairs while a sam- ple from SC has a unique pair. Except for the number of positive pairs, no special treatment is given to differentiate between the image-caption and image-label data. 3.2. Preﬁx Conditioned Contrastive Learning Fig. 2 describes the overview of our approach. We aim to enable the language encoder to learn embedding strategies conditioned on the type of input dataset. The conditioning can then be used to manipulate the bias at inference time. Preﬁx-tuning [10, 18, 21, 23, 30] shares the intuition that the preﬁx tokens are responsible for switching the context of a language model from the pre-trained task to the down- stream task. These approaches leverage the preﬁx to tailor a model to a single task during ﬁne-tuning and construct dif- ferent preﬁxes for different natural language tasks [18]. In our problem setting, there is no task distinction between the image-caption and image-prompt matching since both are formulated as contrastive learning. However, we focus on the fact that the two datasets have different biases in the im- age distributions and vocabulary words. The label-prompt sentences are embedded closer to the image classiﬁcation data, even though we may want to use them to match a new label to an image from the open-domain image distribution during zero-shot classiﬁcation. To solve this problem, we propose to inform the model of the type of dataset at the input level to switch the fea- ture extraction. Speciﬁcally, to make the model aware of the dataset type, preﬁx-conditioning prepends a preﬁx to- ken to an input sentence to obtain ¯tP = [ PREFIXP ; tP ], ¯tC = [ PREFIXC; tC]. The brackets indicate the con- catenation of two lists of discrete tokens; PREFIX P and PREFIXC denote a prompt-style and caption-style token re- spectively. In this way, we prepend the token to learn the dataset-speciﬁc bias, which enables us to disentangle the bias in language representations and utilize the embedding learned on the image-caption dataset at test time even with- out an input caption. In prompt-tuning, the number of preﬁx tokens can af- fect the performance of the model [18, 21, 46]. However, we do not see the performance difference by the number of preﬁx tokens. This is probably because adding one token is enough to distinguish the domain of input sentences. To avoid signiﬁcantly increasing the training cost, we set the number of preﬁxes to one in all experiments. Then, the lan- guage representations for each data source are extracted as ˜uP ,˜uC = fφ(¯tP ),fφ(¯tC). This input design is indepen- dent from the training objectives, and therefore we can eas- ily apply the technique to optimize Eq. 1 or UniCL’s loss. Data Sampling. [6] argue that the data sampling matters when learning from multiple data sources in a contrastive learning framework, as the model may learn to distinguish the samples by exploiting the dataset bias. As such, we need to take data sampling into consideration in our problem set- ting as we learn from two different data sources. One op- tion is a debiased sampling [6], which constructs each mini batch to contain samples from a single data source. Alterna- tively, as done in UniCL [43], we can compose each mini- batch with samples from both data sources (image-caption and image-label) with equal probability. In experiments, we choose the debiased sampling, but ﬁnd that the choice of sampling does not signiﬁcantly affect the performance. 3.3. Inference with Preﬁx Conditioning During inference (the right side of Fig. 2), an input im- age is classiﬁed as one of K classes by embedding the cor- responding label-prompts and choosing the one most sim- ilar to the image embedding. Following [31], we obtain class prompts by ﬁlling the default prompt templates with class names, and add a preﬁx. Considering the wider cov- erage of domains in the image-caption dataset, the caption- style preﬁx conditioning may work better to classify novel downstream data. In our experiments, we empirically ﬁnd that the caption-style preﬁx indeed outperforms the prompt- style preﬁx with a large margin in zero-shot recognition while prompt preﬁx performs better on the image classiﬁca- tion dataset used to train the model. We provide a detailed analysis of different conditioning in Section 4.3. 4. Experiments The goal of experiments is twofold: comparing our ap- proach with baselines in zero-shot recognition, and analyz- ing the behavior of preﬁx conditioning. We describe the experimental setup in Sec. 4.1, show the main results inTraining Data Objective Preﬁx Conditioning Metric Classiﬁcation Caption Size IN-1K Zero-shot 11 datasets – CC-3M 3M CLIP 18.1 28.7 – CC-12M 12M CLIP 33.4 41.2 ImageNet-1K – 1M CLIP 72.1 20.2 ImageNet-21K – 12M CLIP 47.1 39.6 ImageNet-1K CC-12M 13M CLIP 68.7 43.3 ImageNet-1K CC-12M 13M CLIP ✓ 71.5 45.5 ImageNet-1K CC-12M 13M UniCL 68.8 43.1 ImageNet-1K CC-12M 13M UniCL ✓ 71.7 44.5 ImageNet-21K CC-12M 25M CLIP 56.8 49.5 ImageNet-21K CC-12M 25M CLIP ✓ 67.3 57.8 ImageNet-21K CC-12M 25M UniCL 58.2 51.7 ImageNet-21K CC-12M 25M UniCL ✓ 66.5 58.4 ImageNet-21K w/o IN-1K CC-12M 24M CLIP 29.1 46.9 ImageNet-21K w/o IN-1K CC-12M 24M CLIP ✓ 47.8 56.4 Table 1. Performance comparison among different training datasets and training objectives. Note that we use caption preﬁx to obtain these results. The proposed preﬁx conditioning shows improved zero-shot recognition accuracy across models trained with different combinations of image-classiﬁcation and image-caption datasets and training objectives. Sec. 4.2, and analyze the properties of preﬁx-conditioning in Sec. 4.3. 4.1. Setup Training Datasets. We conduct experiments on the set- ting where we have a large source of image-caption and image-label datasets. Following UniCL [43], we utilize CC3M [33] and CC12M [5] as image-caption data. For the image classiﬁcation dataset, we utilize ImageNet21K and ImageNet1K [7]. While ImageNet1k contains 1,000 classes, ImageNet21K has more than 20,000 categories that include ﬁne-grained and general objects. To observe the behavior in diverse image classiﬁcation data, we also run experiments on ImageNet21K while excluding the classes of ImageNet1K. Details are explained in each section. Training. We use the same prompt strategy and 80 prompt templates as used in CLIP [31]. During training, we randomly sample one prompt template and ﬁll it with the class names, followed by a tokenization step before feeding into the text encoder. We average language embeddings ex- tracted from all 80 templates in validation. We use the same language encoder as CLIP [31] and Swin-Tiny transformer [24] as the vision encoder following UniCL [43]. All mod- els are optimized with AdamW [25] where the learning rate is set to 0.001, and weight decay to 0.1. All models are trained with a batch size of 1024. Considering the amount of training data, we train the models for 15 and 50 epochs in the experiments on ImageNet21K and ImageNet1K re- spectively.2 For all training, we used a cosine learning rate 2When training a model on two different datasets, e.g., IN21K and schedule with a warm-up of 10,000 iterations. Baselines. We train CLIP [31] and UniCL [43] as our baselines. For comparison, we present results on CLIP trained only on image-caption or image classiﬁcation data, as well as CLIP and UniCL trained on both image-caption and IN21K data. Unless otherwise stated, CLIP and UniCL are trained with equal sampling (ES) strategy as in [43], while our preﬁx conditioning model is trained with debiased sampling (DS) [6]. We provide an analysis of the sampling in Sec. 4.2 and ﬁnd that DS itself does not have a noticeable advantage over ES. Evaluation. We evaluate the learned representations on supervised and zero-shot image classiﬁcation on Ima- geNet1K3 and on 11 datasets chosen from the ones used in CLIP [31] including object classiﬁcation (e.g., CIFAR10, CIFAR100), ﬁne-grained classiﬁcation (e.g., Oxford-IIIT Pets, Oxford Flowers 102, and Food-101), and aerial images (e.g., EuroSAT and Resisc45). Although our main focus is at the zero-shot generalization, we also provide an analysis of a linear-probe evaluation of the image encoder. CC12M, we count the epochs based on how many samples are used from the image classiﬁcation dataset. For instance, in UniCL, each mini-batch consists of approximately the same number of samples from IN21K and CC12M. Then, to train a model for 15 epochs, we train for N/1024 ×2 ×15 iterations, where N indicates the number of samples in IN21K. 3While we follow the same zero-shot evaluation protocol when evalu- ating on ImageNet1K, we note that it is zero-shot only where we explicitly exclude ImageNet1K from the training, last two rows of Table 1Train Preﬁx Sampling IN-1K Cal CF100 CF10 ESTAT Food Flower Pets Patch R45 VOC DTD A VG ES 56.8 70.2 55.0 79.4 21.1 46.0 60.3 57.2 51.2 24.8 57.7 21.4 49.5 ✓ ES 65.4 81.2 62.6 88.9 30.4 51.7 61.8 71.9 50.0 28.2 78.1 27.7 57.5 DS 58.7 65.9 55.0 85.7 22.8 40.8 55.7 60.2 50.0 20.6 45.2 23.8 47.8 ✓ DS 67.3 79.7 63.8 87.9 31.5 53.4 58.8 69.6 50.6 31.5 80.5 28.4 57.8 Table 2. Ablation study for sampling in IN21K + CC12M. Equal sampling (ES) composes a mini-batch with roughly equal number of samples from two datasets. Debiased sampling (DS) samples a mini-batch of either IN21K or CC12M with equal probability. Train Data Preﬁx ConditioningIN-1K Cifar10 Cifar100 Caltech Food Pet Patch VOC DTD ImageNet-21K 71.5 94.3 79.1 83.5 79.1 86.3 82.3 88.9 61.3 ImageNet-21K + CC12M 69.2 93.0 76.4 82.4 78.4 82.2 81.4 88.7 61.4 ImageNet-21K + CC12M ✓ 69.4 93.5 77.3 83.2 78.8 83.6 82.0 88.8 62.5 Table 3. Linear evaluation accuracy on models trained with and without preﬁx conditioning. Preﬁx conditioning slightly improves the performance upon a model without it (second row vs. last row). 4.2. Main Results We describe our main results in Table 1, followed by the analysis of preﬁx conditioning in Sec. 4.3. There are three observations. First, the improvements upon a model trained only with image-caption or image- label data are obvious in almost all cases. As the previ- ous work indicates [43], the effectiveness of combining two types of supervision is clear from these results. Second, in all cases, our preﬁx conditioning signiﬁcantly improves performance on both ImageNet-1K (supervised recognition) and 11 zero-shot recognition tasks. When training on ImageNet-21K, the conditioning improves the baseline by more than 8% in ImageNet-1K and more than 6% in zero-shot recognition on average. In training with ImageNet-1K, the margin from the baseline is smaller than training with ImageNet-21K, probably because the size of ImageNet-1K (1M) is much smaller than that of ImageNet- 21K (12M). Also, preﬁx conditioning is effective in both UniCL and CLIP objectives. Due to its simplicity, our ap- proach can be easily integrated with various objectives. Finally, our method is less affected by ablating a part of categories. The classes of ImageNet-1K are excluded from ImageNet-21K in the last two rows of Table 1. There- fore, both approaches signiﬁcantly drop performance on ImageNet-1K, whose task now becomes true zero-shot recognition, compared to other settings. Even in this set- ting, preﬁx conditioning maintains high accuracy and out- performs a CLIP baseline model by a large margin. Sampling Method. We analyze the data sampling scheme to construct a mini-batch in Table 2. We apply debiased sampling (DS) in our method, namely, sampling one data source with equal probability and getting a mini- batch of it. The other option is mixing two data sources with equal probability (ES). The table indicates that pre- ﬁx conditioning works well with ES sampling and the sam- pling strategy itself is not advantageous. Ablating preﬁx conditioning during training clearly drops the performance in both sampling strategies, and the performance is worse than ES on average in zero-shot results (49.5 vs. 47.8). ES sampling should allow the model to differentiate sen- tences by using the prepended preﬁx. Interestingly, this re- sult implies that differentiating sentences by preﬁx infor- mation does not much degrade the performance. The dis- tinguished sentences enable the model to associate images from different datasets. Since images of two datasets are different with respect to the categories and the locations of objects in images, distinguishing the two kinds of images may not harm generalizability of the representations. Linear-probe Evaluation. We evaluate the linear-probe performance in Table 3 to see the quality of learned image representations. Although the accuracy is better than the model trained without preﬁx conditioning (second line), the improvements are not substantial. This result indicates that the zero-shot performance gain obtained by our method is not due to the image representations. We investigate the learned language and image features in the next subsection. 4.3. Analysis of Preﬁx-Conditioning We present a detailed analysis of preﬁx conditioning. We ﬁrst study how different preﬁxes impact the zero-shot recognition performance and analyze their behaviors by looking into the attention weights of the language trans- former encoder. We also demonstrate improved robustness with respect to the image-level domain shift. Unless other- wise stated, we employ a model trained with CLIP objec- tive on ImageNet-21K and CC12M in this analysis. Finally, this section concludes that preﬁx conditioning enables the language encoder to switch its role during training, which eases learning from different types of datasets, e.g., image classiﬁcation and image caption dataset.Data Test-time Preﬁx IN-1K Cal C100 C10 ESTAT Food Flower Pets Patch R45 VOC DTD A VG IN-1K + CC12M N/A 68.7 68.7 38.4 69.5 24.4 31.9 13.3 66.6 50.2 25.4 65.6 22.3 43.3 Prompt 75.4 71.7 35.5 63.9 24.2 20.0 8.1 72.2 50.4 24.2 61.1 15.3 40.6 Caption 71.5 75.1 39.4 70.5 26.7 33.9 13.9 72.3 50.5 25.8 67.8 25.4 45.5 IN-21K + CC12M N/A 56.8 70.2 55.0 79.4 21.1 46.0 60.3 57.2 51.2 24.8 57.7 21.4 49.5 Prompt 71.4 76.5 59.0 86.0 20.1 45.7 62.3 69.1 52.4 26.3 76.8 21.4 54.1 Caption 67.3 79.7 63.8 87.9 31.5 53.4 58.8 69.6 50.6 31.5 80.5 28.4 57.8 IN-21K w/o 1K + CC12M N/A 29.1 67.4 45.9 80.0 28.6 40.8 56.9 39.2 50.2 21.9 64.9 19.8 46.9 Prompt 40.8 74.9 61.0 84.6 31.2 48.1 58.7 45.2 51.2 23.5 67.5 21.4 51.6 Caption 47.8 81.9 63.3 87.3 32.4 52.9 62.8 57.0 50.6 25.6 80.1 26.2 56.4 Table 4. Ablation study for test-time preﬁx conditioning. Note that the difference between two results come from the preﬁx used in test time and we use the same model for this evaluation. A model trained without conditioning is shown at the top of each block. Caption PrefixPromptPrefix Unconditional Model Figure 3. An example of attention weights for an end token. Best viewed in color. The sentence shown here is one of class prompts in the VOC 2007 dataset. Different rows show the weights of different transformer layers. With a prompt preﬁx (leftmost), the model focuses on a class name (airplane) while caption preﬁx (middle) allows a model to pay attention to another noun, sculpture. By preﬁx conditioning, the attention of the model changes as intended. Test Time Preﬁx. We analyze the role of the preﬁx to- ken in Table 4, where the table describes the comparison in the choice of test time preﬁx conditioning. As explained in Sec. 3, the choice of preﬁx during test time should change the behavior of the model since the preﬁx should tailor the language encoder for classiﬁcation-style or caption-style feature extraction. Except for the IN-1K results of a model trained with the entire IN21K or IN-1K, conditioning with the caption preﬁx shows much better results. The superi- ority of the caption preﬁx is noticeable in several datasets. This means caption preﬁx works better if the target comes from outside the image classiﬁcation data, indicating that the class-prompt preﬁx conditioning makes the model tai- lored for the image classiﬁcation dataset. Class-prompts preﬁx works better to categorize IN-1K data because the preﬁx is trained to specialize in classifying it. Note that caption-style preﬁx performs better than prompt-style preﬁx in IN-1K for a model trained with IN21K excluding IN1k classes. This indicates that the caption-style preﬁx works better when the vocabulary of the class name comes from outside the image classiﬁcation data since the caption data covers much more diverse words. Preﬁx controls attention. Fig. 3 visualizes the atten- tion weights for an end token in different preﬁx conditions and models. The input sentence, a sculpture of an airplane, is one of the class-prompts. When a prompt preﬁx (left- most) is employed, the language model pays attention to the class name at the ﬁrst layer, it does not focus on the noun in other layers. The only noun the encoder focuses on is airplane. By contrast, the model attends to both sculpture and airplane in the case of the caption preﬁx and uncon- ditional model. Note that this behavior does not mean that the prompt-preﬁx performs better in zero-shot recognition as shown in experiments due to the effect of the bias in im- age classiﬁcation dataset. While we visualize only one example in the main text due to the space limit and defer more examples to the ap- pendix, this highlights a general trend that the prompt preﬁx guides the language encoder to focus on a single word (e.g., class name), whereas the caption preﬁx makes the model at- tend to multiple words. In other words, preﬁx conditioning allows the language encoder to “switch gears” to represent sentences from different datasets (i.e., image-classiﬁcation vs image-caption). On the other hand, the baseline model without preﬁx conditioning attends to multiple words (e.g., Fig. 3 rightmost) even though the input sentence is a class prompt. This indicates that it is hard to switch the gears without explicitly informing of the type of dataset. Language Feature Visualization. Fig. 4 visualizes ex- tracted language features conditioned with different pre-Train Data Preﬁx Conditioning Test-Time Preﬁx IN IN-V2 IN-R IN-S ImageNet-1K N/A 72.1 59.3 19.9 17.8 ImageNet-1K + CC12M N/A 68.7 57.4 27.7 27.8 ImageNet-1K + CC12M ✓ Caption 71.5 60.2 31.8 30.7 ImageNet-1K + CC12M ✓ Prompt 75.4 63.3 29.2 27.9 ImageNet-21K N/A 47.1 41.1 20.1 16.1 ImageNet-21K + CC12M N/A 56.8 48.6 29.4 30.6 ImageNet-21K + CC12M ✓ Caption 67.3 57.5 35.2 34.6 ImageNet-21K + CC12M ✓ Prompt 71.4 61.1 32.1 32.2 Table 5. Evaluation on the robustness to the image-level domain shift. Preﬁx conditioned training achieves better robustness, and caption- preﬁx outperforms prompt-preﬁx in the images distinct from those used in training (IN-R and IN-S). (a) Different conditions  (b) Prompt condition (c) Caption condition  (d) No condition Figure 4. T-SNE [36] visualization of the class-prompt features of 20 classes of VOC 2007 with different preﬁx conditions. (a): Language embeddings with prompt (red) and caption (blue) pre- ﬁxes, respectively. (b)(c)(d): Different colors indicate language embeddings of different classes. ﬁxes. As seen in Fig. 4a, language features extracted with caption-preﬁx (blue) and prompt-preﬁx (red) are clearly separated. In addition, prompt-preﬁx (Fig. 4b) has lower intra-class and higher inter-class variance, whereas caption- preﬁx (Fig. 4c) shows higher intra-class variance across prompts. Interestingly, results in Table 4 suggest that the caption-preﬁx conditioned language features result in a bet- ter zero-shot recognition performance than those condi- tioned on the prompt-preﬁx. Although the prompt-preﬁx mode extracts discriminative language embeddings, the em- beddings do not perform well on the zero-shot recogni- tion because the embeddings contain signiﬁcant bias from image-classiﬁcation dataset. Robustness in image domain shift. Test samples can be unseen with respect to image classiﬁcation data in two ways (or combinations of two): 1) The image is similar to the training distribution, but the class name is different from the seen image classiﬁcation labels. 2) Although the class label is the same, the image data comes from a different distribution. Datasets evaluated in the zero-shot recognition include both two cases since the vocabularies and image are from different domains. To understand them, we analyze the test-time preﬁx by using ImageNet-1K and evaluate the performance on image-level domain shift using variants of ImageNet, i.e., ImageNet-V2 [32], ImageNet-R [11], and ImageNet-S [37]. Table 5 describes the results of ablat- ing preﬁx-conditioned training and the test-time preﬁx. The preﬁx-conditioned training outperforms all baselines. This reveals that the preﬁx-conditioned training achieves class embeddings that are generalizable across image domains. The prompt-style preﬁx performs the best in IN, IN-V2, both of which have image styles similar to ImageNet. By contrast, the caption-style preﬁx performs the best in IN-R and IN-S, which has art-style and sketch-style images re- spectively. Thus, the caption-style preﬁx generates more generalizable class embeddings for the domain dissimilar from the ImageNet training data. This observation is con- sistent with the results in the paragraph Test time Preﬁx. 5. Conclusion In this paper, we explore a simple yet effective mecha- nism for uniﬁed pre-training on image-caption and image classiﬁcation data. We propose to learn preﬁx tokens at training time to condition the language encoder to switch the input source. Specifying the preﬁx allows the model to switch the manner of feature extraction and can control which visual domain the embedding is projected to. This approach boosts the performance of zero-shot recognition accuracy of the contrastive learning models. Our analysis suggests that the trained language encoder provides robust- ness to the image-level domain shift. Although we limit our scope to unifying image-caption and image-label supervi- sion, incorporating other supervision such as object detec- tion or semantic segmentation is an interesting next step. Acknowledgment. We thank Zizhao Zhang for their help- ful feedback on the manuscript. This work was supported in part by DARPA LwLL.Abbreviation Dataset #Concepts Train size Test size Source link Food Food-101 102 75,750 25,250 Tensorﬂow CF10 CIFAR-10 10 50,000 10,000 Tensorﬂow CF100 CIFAR-100 100 50,000 10,000 Tensorﬂow VOC VOC2007 classiﬁcation 20 5,011 4,952 Tensorﬂow DTD Describable Textures 47 3,760 1,880 Tensorﬂow Pets Oxford-IIIT Pets 37 3,680 3,669 Tensorﬂow Cal Caltech-101 102 3,060 6084 Tensorﬂow Flower Oxford Flowers 102 102 1,020 6,149 Tensorﬂow Patch PatchCamelyon 2 294,912 32,768 Tensorﬂow ESTAT EuroSAT 10 N/A 27,000 Tensorﬂow R45 Resisc45 45 N/A 31,500 Tensorﬂow Table 6. Statistics of datasets used in zero-shot and linear probe. A. Experimental Details Dataset. Table 6 describes the statistics of dataset used for evaluation. We pick the test datasets based on UniCL [43] and availability in Tensorﬂow dataset. We use the test set to evaluate zero-shot recognition and linear probe while the train set is used to train a linear classiﬁer. Note that since EuroSAT and Resisc45 utilize the training split for evaluation, we exclude the two datasets from linear probe evaluation. Also, since Oxford Flowers do not have many training samples (10 samples per class), we exclude the dataset from the evaluation too. Data Augmentation. Following UniCL [43], only ran- dom cropping is applied to train all models for a fair com- parison. Computation. We use 32 Nvidia Tesla V100 GPUs to train all models. 4 nodes, where each node has 8 GPUs, are used to run experiments. B. Additional Results Attention Visualization. Fig. 5 visualizes attention weights for the class forest area , where a prompt tem- plate, a tatto of , is employed. The model focuses on a word, forest when prompt preﬁx is employed. In other two cases, the model also pays much attention to tatoo proba- bly because the word should provide useful information to distinguish a sentence from others for image-caption con- trastive learning. Fig. 6 represents attention for a real cap- tion from CC3M. While the model conditioned with cap- tion preﬁx and unconditional model attend to several words through many layers, the model conditioned with prompt preﬁx shows clear attention only in the ﬁrst layer. Since the prompt-conditioned model has never seen the real caption during training, it fails in attending to discriminative words. Class Name Shift. Test samples can be unseen with re- spect to image classiﬁcation data in two ways (or combina- tions of two): 1) The image is similar to training distribu- tion, but the class name used for testing is different from the image classiﬁcation label. 2) Although the class label is the same, the image data comes from the different distri- butions. Datasets evaluated in the zeros-shot recognition in- clude both two cases since class names and images are from different domains. 2) is analyzed in Subsection 4.3 of the main paper, Robustness in image domain shift. We analyze 1) by evaluating the recognition performance of ImageNet- 1K by changing its class name from the one used during training. We ﬁnd a synonym for each class with Word- Net [28], where we exclude synonyms substantially simi- lar to the original class name and obtain synonyms for 525 classes. Then, we use the synonym to classify images dur- ing evaluation. Since the input image distribution does not vary, we can evaluate the performance on the class name shift. If the model is robust to the change in the class name, the degrade in the performance should be small. The ﬁrst 6 rows of Table 7 describe the models trained with the original class names and evaluated on both original ones and synonyms, and the last two rows represent a model trained with synonyms, where the original class names are replaced with synonyms. Prompt preﬁx outperforms cap- tion preﬁx with a large margin in testing with class names used in training time. Generally, caption preﬁx performs better when tested with the class names different from the ones used during training. Prompt preﬁx is tailored to han- dle class names employed during training time while cap- tion preﬁx enables the language encoder to extract more general representations. Interestingly, the choice of class names seems to signif- icantly change the generalization as shown in the compar- ison between a model trained with synonyms and original class names. The original model decreases the accuracy more than 30% by changing the class name while the model trained with synonym decreases less than 20%. Image-Caption Retrieval. In Table 8, we evaluate the performance of image-caption retrieval using the sub-Caption PrefixPrompt Prefix Unconditional Model Figure 5. Attention visualization for a class prompt. Note that the attention weights are for and end token. Best viewed in color. The class name shown here is one of class prompts in the EUROSAT dataset. Different rows show the weights of different transformer layers. With a prompt preﬁx (leftmost), the model focuses on a class name ( forest area) while caption preﬁx (middle) allows a model to pay attention to another noun, tattoo. By preﬁx conditioning, the attention of the model changes as intended. Caption PrefixPrompt Prefix Unconditional Model Figure 6. Attention visualization for a real caption. Note that the attention weights are for and end token. Best viewed in color. The sentence shown here is from CC3M. Different rows show the weights of different transformer layers. Caption preﬁx conditioning helps to attend to many words while prompt conditioning fails to do that. set of CC3M (12288 pairs of image and caption) and COCO validation set (5000 pairs of image and caption), where all models are trained with CC12M and ImageNet- 21K. First, our model (last row) slightly performs better than the model without conditioning (ﬁrst row). Second, prompt preﬁx conditioning (second row) signiﬁcantly per- forms worse than caption preﬁx conditioning (last row). Since the prompt preﬁx conditioning specializes a model for the class name prompts of ImageNet21K, the condition- ing does not generalize well to real captions. Larger Batch-size and Training Epochs. We exam- ine the effect of increasing batch-size and training epochs in Table 9. In CLIP, increasing the batch-size and train- ing epochs improves the performance of both ImageNet-1K and zero-shot recognition. On the other hand, the zero-shot performance of UniCL is not beneﬁted from training with longer epochs (compare last and second to last row). UniCL attempts to ensure the invariance of images from the same classes by supervised contrastive loss while CLIP does not consider it. However, such invariance is not necessarily re- quired in zero-shot recognition, which leads to the degraded performance. Comparison to Reported UniCL’s Results. In the main paper, we provide our reproduced results of UniCL,Train Data Train on Synonym Preﬁx Training Test-Time Preﬁx Original Synonym top-1 top-5 top-1 top-5 IN1K + CC12M N/A 69.3 89.3 31.2 49.5 IN1K + CC12M ✓ Prompt 75.0 92.9 38.3 54.8 IN1K + CC12M ✓ Caption 71.4 91.6 36.6 56.7 IN21K + CC12M N/A 54.5 83.2 23.1 43.9 IN21K + CC12M ✓ Prompt 69.9 92.4 32.1 53.7 IN21K + CC12M ✓ Caption 65.3 90.6 33.5 56.9 IN21K + CC12M ✓ ✓ Prompt 54.4 78.6 70.8 92.8 IN21K + CC12M ✓ ✓ Caption 54.5 82.6 59.0 86.1 Table 7. Evaluation on the robustness to the class name shift using ImageNet-1K. Original refers to the subset of ImageNet-1K classes while synonym refers to their synonyms taken from Wordnet. The last two rows indicate the models trained with the synonyms, thus showing superior performance on synonym whereas degrading performance on Original. Preﬁx Training Test-time Preﬁx CC3M COCO I2T@1 I2T@5 T2I@1 T2I@5 I2T@1 I2T@5 T2I@1 T2I@5 N/A 21.8 47.4 21.0 45.7 23.9 49.5 18.7 43.2 ✓ Prompt 13.1 31.3 8.1 21.8 17.2 38.1 16.8 37.7 ✓ Caption 22.6 47.5 21.6 46.1 24.7 49.7 19.7 43.9 Table 8. Image-text retrieval results on CC3M and COCO. The performance is evaluated on the subset of CC3M and validation set of COCO. All models are trained on CC12M and ImageNet-21K. Caption conditioning (last row) slightly improves retrieval performance compared to the unconditional model (ﬁrst row). Since prompt conditioning (middle) tailors a model for class-prompt, it fails to extract discriminative information from real captions. which is based on our implementation, since the authors have not released the code and did not report the numer- ical accuracy of each zero-shot recognition. In this para- graph, we compare our approach and the reported perfor- mance of UniCL [43] and K-Lite [34] by aligning sev- eral hyper-parameters, e.g., batch-size and training epochs, using ImageNet-1K. When using ImageNet-22K and CC- 15M for training, our method (batch-size:4096, training epochs: 30) shows 73.9 while UniCL (batch-size:4096, training epochs 32) reports 71.5. When using ImageNet- 21K excluding ImageNet-1K and CC-15M, our method (batch-size:1024, training epochs 30) shows 49.7 whereas UniCL (batch-size: 4096, training epochs: 32) and K- Lite (batch-size: 4096, training epochs: 32) perform 46.6 and 48.7 respectively according to K-Lite results (See last two rows of Table 3 in [34]). These results suggest that our method performs better than the reported numbers of UniCL and K-Lite in ImageNet-1K. Also, the knowledge augmentation technique proposed by K-Lite can be comple- mentary to our approach, thus combining two approaches is an interesting research direction. T-SNE visualization for language features. Fig. 7 vi- sualizes extracted language features (ImageNet-1K) condi- tioned with different preﬁxes. The prompt-preﬁx (left) has lower intra-class and higher inter-class variance, whereas caption-preﬁx (right) shows higher intra-class variance across prompts. T-SNE visualization for image features. Fig. 8 visu- alizes image features from ImageNet-1K (blue) and CC3M (red). Since ImageNet-1K is object-centered while CC3M covers more diverse scenes, the distributions are separated. This is consistent across baseline (w/o conditioning) and our method (with conditioning). Comparison between unconditioned and conditioned model by language features. Fig. 9 visualizes language features of ImageNet-1K class prompts (Blue) and CC3M captions (Red) for unconditioned (left) and conditioned (right) respectively. Note that the conditioned model uti- lizes prompt preﬁx for class prompts and caption preﬁx for real captions respectively. As seen from the visualiza- tion, unconditioned model cannot distinguish some prompts from captions of CC3M. This is probably because some captions are similar to class prompts of ImageNet. By contrast, the conditioned model differentiate class prompts from captions better than unconditioned model due to the preﬁx conditioning. References [1] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for ﬁne-grained image classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2927–2936, 2015. 2Training Data Objective Batch-size Epochs Metric Classiﬁcation Caption IN-1K Zero-shot 11 datasets ImageNet-21K CC-12M CLIP 1024 15 67.3 57.8 ImageNet-21K CC-12M CLIP 1024 30 69.1 58.3 ImageNet-22K CC-15M CLIP 1024 15 69.3 58.5 ImageNet-22K CC-15M CLIP 4096 15 71.1 59.5 ImageNet-22K CC-15M CLIP 4096 30 72.2 59.8 ImageNet-22K CC-15M UniCL 1024 15 69.7 58.5 ImageNet-22K CC-15M UniCL 4096 15 70.3 60.4 ImageNet-22K CC-15M UniCL 4096 30 73.9 58.9 Table 9. Performance comparison among different batch-size and training epochs. ImageNet-22K denotes the combination of ImageNet- 21K and ImageNet-1K, CC-15M indicates that of CC-12M and CC-3M. (a) Prompt conditioned (b) Caption conditioned Figure 7. T-SNE [36] visualization of the class-prompt features of ImageNet-1K with different preﬁx conditions. Different colors indicate language embeddings of different classes. Prompt con- ditioning extracts more class discriminative representations than caption conditioning. (a) W/O conditioning (b) With conditioning Figure 8. T-SNE [36] visualization of the image features of ImageNet-1K (blue) and CC3M (red). Since ImageNet-1K is object-centered while CC3M covers more diverse scenes, the dis- tributions are separated. This is consistent across baseline (w/o conditioning) and our method (with conditioning).(a) Unconditioned model (b) Conditioned model Figure 9. T-SNE [36] visualization of language features of ImageNet-1K class prompts (Blue) and CC3M captions (Red) for unconditioned (left) and conditioned (right) respectively. Our pro- posed condition better differentiates prompts from real captions. [2] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Ma- chine Learning, pages 528–539. PMLR, 2020. 3 [3] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Ad- vances in neural information processing systems , 32, 2019. 3 [4] Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay Mc- Clelland, and Felix Hill. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022. 1 [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 1, 2, 5 [6] Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu, and Osamu Yoshie. Zerovl: A strong baseline for align- ing vision-language representations with limited resources. arXiv preprint arXiv:2112.09331, 2021. 4, 5 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 1, 2, 3, 5 [8] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world do- main generalization. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 14340–14349, 2021. 3 [9] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. De- vise: A deep visual-semantic embedding model. Advances in neural information processing systems, 26, 2013. 2 [10] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre- trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. 3, 4 [11] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada- vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robust- ness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340–8349, 2021. 8 [12] Dinesh Jayaraman and Kristen Grauman. Zero-shot recogni- tion with unreliable attributes. Advances in neural informa- tion processing systems, 27, 2014. 2 [13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representa- tion learning with noisy text supervision. In International Conference on Machine Learning, pages 4904–4916. PMLR, 2021. 1, 2 [14] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- sual prompt tuning. arXiv preprint arXiv:2203.12119, 2022. 3 [15] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020. 2 [16] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained fea- tures and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022. 3 [17] Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo. Learning debiased representation via disentan- gled feature augmentation. Advances in Neural Information Processing Systems, 34:25123–25133, 2021. 3 [18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning.arXiv preprint arXiv:2104.08691, 2021. 2, 3, 4 [19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni- ﬁed vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022. 2 [20] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn- ing with momentum distillation. Advances in Neural Infor- mation Processing Systems, 34, 2021. 1, 2 [21] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimiz- ing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 2, 3, 4 [22] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su- pervision exists everywhere: A data efﬁcient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021. 2 [23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- roaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in nat- ural language processing. arXiv preprint arXiv:2107.13586, 2021. 3, 4 [24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations, 2019. 5 [26] Thomas Mensink, Efstratios Gavves, and Cees GM Snoek. Costa: Co-occurrence statistics for zero-shot classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2441–2448, 2014. 2 [27] Christian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography. na, 2012. 2 [28] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995. 2, 9 [29] Norman Mu, Alexander Kirillov, David Wagner, and Sain- ing Xie. Slip: Self-supervision meets language-image pre- training. arXiv preprint arXiv:2112.12750, 2021. 2 [30] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. 3, 4 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning , pages 8748–8763. PMLR, 2021. 1, 2, 4, 5 [32] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize to im- agenet? In International Conference on Machine Learning, pages 5389–5400. PMLR, 2019. 8 [33] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, im- age alt-text dataset for automatic image captioning. In Pro- ceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2556–2565, 2018. 5 [34] Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jian- wei Yang, Pengchuan Zhang, Anna Rohrbach, Zhe Gan, Lijuan Wang, Lu Yuan, et al. K-lite: Learning transfer- able visual models with external knowledge. arXiv preprint arXiv:2204.09222, 2022. 2, 11 [35] Kihyuk Sohn. Improved deep metric learning with multi- class n-pair loss objective. Advances in neural information processing systems, 29, 2016. 4 [36] Laurens Van der Maaten and Geoffrey Hinton. Visualiz- ing data using t-sne. Journal of machine learning research, 9(11), 2008. 8, 12, 13 [37] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506–10518, 2019. 3, 8 [38] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6857–6866, 2018. 2 [39] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon- tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust ﬁne-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959–7971, 2022. 3 [40] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latent embed- dings for zero-shot classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 69–77, 2016. 2 [41] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. InProceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4582–4591, 2017. 2 [42] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. arXiv preprint arXiv:2202.10401, 2022. 2 [43] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Uniﬁed contrastive learning in image-text-label space. arXiv preprint arXiv:2204.03610, 2022. 1, 2, 4, 5, 6, 9, 11 [44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021. 2 [45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo- jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 1 [46] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021. 2, 3, 4 [47] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi- wei Liu. Conditional prompt learning for vision-language models. arXiv preprint arXiv:2203.05557, 2022. 3",
      "meta_data": {
        "arxiv_id": "2206.01125v2",
        "authors": [
          "Kuniaki Saito",
          "Kihyuk Sohn",
          "Xiang Zhang",
          "Chun-Liang Li",
          "Chen-Yu Lee",
          "Kate Saenko",
          "Tomas Pfister"
        ],
        "published_date": "2022-06-02T16:12:26Z",
        "pdf_url": "https://arxiv.org/pdf/2206.01125v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of unifying image classification and image-caption datasets for visual model pretraining, which often leads to suboptimal zero-shot recognition due to dataset bias. It proposes 'Prefix Conditioning', a novel pre-training strategy that introduces dataset-specific prefix tokens to inform the language encoder of the input data type. This method disentangles dataset biases from visual concepts, allowing the language encoder to learn from both data types while tailoring feature extraction. The approach significantly improves zero-shot recognition performance (over 6% on average) and enhances robustness to image-level distribution shift.",
        "methodology": "The core methodology is 'Prefix Conditioning'. During pre-training, a dataset-specific prefix token (e.g., PREFIXP for classification data, PREFIXC for caption data) is prepended to the text input token sequence. This prefix token absorbs dataset-specific biases, allowing the main text tokens to focus on learning generalizable visual concepts. The language encoder then extracts dataset-conditioned representations. This input design is independent of the training objectives and can be easily integrated with existing vision-language contrastive learning frameworks like CLIP's symmetric multi-class N-pair loss or UniCL's objective. The number of prefix tokens is set to one. Data sampling can be debiased or equal, with debiased sampling chosen in experiments, though sampling strategy itself did not show significant advantage.",
        "experimental_setup": "Experiments utilized image-caption datasets (CC3M, CC12M) and image classification datasets (ImageNet21K, ImageNet1K, and ImageNet21K excluding ImageNet1K classes). The vision encoder was a Swin-Tiny transformer and the language encoder was the same as CLIP. Prompting followed CLIP's strategy with 80 templates. Models were optimized with AdamW (learning rate 0.001, weight decay 0.1, batch size 1024) for 15 or 50 epochs with a cosine learning rate schedule. Baselines included CLIP and UniCL trained on single or combined datasets. Evaluation was conducted on zero-shot image classification across ImageNet1K and 11 other diverse datasets, linear-probe evaluation of image encoders, robustness to image-level domain shifts (ImageNet-V2, ImageNet-R, ImageNet-S), robustness to class name shifts (using WordNet synonyms), and image-caption retrieval on CC3M and COCO validation sets. All training used 32 Nvidia Tesla V100 GPUs.",
        "limitations": "The current scope of the research is limited to unifying image-caption and image-label supervision. Although prefix conditioning helps disentangle dataset biases in language embeddings, the zero-shot performance gain is primarily attributed to improved language representations rather than direct improvements in image representations. While captioning datasets are generally more open-domain, they can still have data biases.",
        "future_research_directions": "Future research directions include incorporating other forms of supervision beyond image-caption and image-label, such as object detection or semantic segmentation. Another promising direction is to explore combinations of prefix conditioning with knowledge augmentation techniques, like those proposed by K-Lite, as they are considered complementary."
      }
    },
    {
      "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
      "abstract": "Recent works have demonstrated great success in pre-training large-scale\nautoregressive language models on massive GPUs. To reduce the wall-clock\ntraining time, a common practice is to increase the batch size and learning\nrate. However, such practice is often brittle and leads to a so-called\nstability-efficiency dilemma: increasing the batch sizes and learning rates\nleads to better training efficiency but can also result in training\ninstability, leading to poor generalization accuracy or failed runs. To better\nunderstand this phenomenon, we conduct an in-depth analysis on large-scale\npre-training experiments replicating the GPT-2 model. We find that there is a\nstrong correlation between training instability and extreme values of gradient\nvariance, and that samples with long sequence lengths contribute to these\nextreme gradient variance values, especially at the beginning of the training,\nindicating that long sequence length can be a main source of training\ninstability. Based on the analysis, we present a Sequence Length Warmup method\nthat aims to solve the training stability-efficiency dilemma. Experiments\nreplicating GPT-2 models show that our approach enables stable training with 8x\nlarger batch size and 4x larger learning rate, whereas the baseline approach\nstruggles with training instability. To achieve the same or better zero-shot\nevaluation results, our method reduces the required number of training tokens\nand wall clock time by up to 2.2x and 3.7x, respectively. Experiments\nreplicating GPT-3 model (125M) show that our approach enables stable training\nwith 8x larger batch size and 40x larger learning rate, and retains 99% of the\nzero-shot accuracy on 11 tasks using 10x less data and 17x less time compared\nto the original GPT-3 training recipe, while the baseline diverges under the\nsame settings and only retain 95% of accuracy under lower learning rate.",
      "full_text": "The Stability-Efﬁciency Dilemma: Investigating Sequence Length Warmup for Training GPT Models Conglong Li Microsoft conglong.li@microsoft.com Minjia Zhang Microsoft minjiaz@microsoft.com Yuxiong He Microsoft yuxhe@microsoft.com Abstract Recent works have demonstrated great success in pre-training large-scale autore- gressive language models (e.g., GPT-3) on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efﬁciency dilemma: increasing the batch sizes and learning rates leads to better training efﬁciency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model with public dataset. We ﬁnd that there is a strong correlation between training instability and extreme values of gradient variance. We further identify that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a simple yet effective Sequence Length Warmup method that aims to solve the training stability-efﬁciency dilemma by avoiding extreme gradient variance values. Moreover, we present a lightweight tuning strategy that allows us to tune our method with just a small portion of the expensive full training. Experiments replicating GPT-2 models (117M and 1.5B) show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate. 1 Introduction Large-scale Transformer-based language models have achieved great success in many natural lan- guage processing tasks [46, 11]. Among them, large-scale autoregressive models, such as GPT-3 [6], have attracted lots of attention due to their superior performance on zero-shot generalization, i.e., they can perform a wide range of tasks that they are not explicitly trained on. However, pre-training GPT models raises huge challenges on training efﬁciency and less-discussed training instability issues. On the efﬁciency side, as the model size continues to grow from a few hundreds of millions (e.g., GPT [31]), to billion-scale parameters (1.5B GPT-2 [33]), and to more recent hundreds of billions of parameters (175B GPT-3 [6]), the training cost also increases exponentially: it requires 9.2 days This paper was previously titled “Curriculum Learning: A Regularization Method for Efﬁcient and Stable Billion-Scale GPT Model Pre-Training” in an early arxiv preprint version[22]. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2108.06084v4  [cs.LG]  16 Oct 2022on 512 V100 GPUs to train a 8.3B GPT-2 [40], and 47.8 days on 2240 A100 GPUs to train a 530B GPT-3-style model [26]. Such a long training time makes it almost infeasible for most research labs and practitioners to reproduce these models. Various solutions have been proposed to reduce the training wall clock time of these large models [40, 25, 35]. However, many solutions require using more GPUs or sophisticated system techniques. In this work, we investigate speeding up the pre-training of GPT-style models via exploiting data efﬁciency, not at the cost of excessive hardware resources. In particular, in a distributed training environment, increasing the batch sizes and/or using more aggressive learning rates can make the model converge faster [41]. However, it has been observed that larger batch sizes and learning rates can make large-scale GPT model training more difﬁcult, e.g., causing training instability that leads to divergence or slow convergence [49, 9]. To investigate this training instability issue, we conduct a thorough study of the GPT-2 pre-training task [33, 40] with different models sizes under various batch sizes, learning rates, and sequence lengths. We ﬁnd a stability-efﬁciency dilemma: • A larger batch size (and larger learning rate) increases the per-iteration computational efﬁciency but with increasing likelihood of training instability and even divergence. • A smaller batch size makes the training more stable but decreases the per-step computation efﬁciency signiﬁcantly. We ﬁnd it difﬁcult to overcome this dilemma by existing techniques such as extra gradient clipping. More recent proposed techniques such as batch size warmup proposed in [6] does not provide stability beneﬁt in our evaluations. Recently, Shortformer[30] shows that by adding an additional ﬁrst training stage with a shorter sequence length, language models can achieve the same dev. set perplexity with shorter total training time. However, (1) its main focus is to purely improve training efﬁciency instead of solving the efﬁciency-stability dilemma, and our evaluations show that Shortformer’s 2-stage approach is insufﬁcient for overcoming the training instability issue for large models (Section 5.1), (2) it is primarily evaluated on small scale transformer models (247M) on WikiText datasets (103M tokens) without considering large-scale generative models like GPT with hundreds or even billions of parameters, and (3) it does not discuss how to choose some of the important hyperparameters, which is very expensive to ﬁgure out for large-scale model training. Inspired by Shortformer, we investigate the importance of sequence length in training GPT models and ﬁnd that it plays an important role in both training stability and efﬁciency. Based on our investigation, we propose a simple yet effective method called Sequence Length Warmup (SLW), which starts training with short sequences and gradually increases the length. We observe that our approach enables stable and efﬁcient training with much larger batch sizes and learning rates than baseline approaches. Speciﬁcally, we make the following contributions: (1) We conduct an extensive study of the GPT-2 pre-training task, which provides detailed insights about the training stability-efﬁciency dilemma, the correlation between instability and gradient variance outliers, and how sequence length plays a critical role (Section 3). (2) Based on the study, we present a simple yet effective sequence length warmup method for GPT-style model (and autoregressive model in general) that enables stable training with improved training efﬁciency. We also identify a lightweight hyperparameter tuning strategy for the approach, which identiﬁes promising hyperparameters by only incurring a small fraction of the expensive total pre-training cost (Section 4). The implementation of our approach as well as the necessary changes to the GPT-2/3 pre-training framework has been open sourced in a deep learning optimization library called DeepSpeed1. (3) We conduct large-scale experiments to demonstrate the proposed work’s ability to provide superior training stability and efﬁciency at the same time (Section 5). Our empirical results show that: • SLW enables stable and efﬁcient training with 8x larger batch size and 4x larger learning rate on GPT-2 (117M and 1.5B) models with public datasets, while the baseline and related works struggle with instability under the same settings. To achieve the same or better zero-shot WikiText- 103/LAMBADA evaluation results at the end of training, SLW reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. • On GPT-3 model (125M) pre-training we study an even more aggressive training scenario where only 10% of data can be used. Our method, with 8x larger batch size and 40x larger learning rate than the original GPT-3 training recipe, is able to maintain the training stability, retaining 99% of the zero-shot accuracy on 11 evaluation tasks, and use 10x less data and 17x less time. Without our 1https://github.com/microsoft/DeepSpeed, https://www.deepspeed.ai/ 2method, the baseline has unrecoverable divergence under the same settings, and can only retain 95% of the zero-shot accuracy after lowering learning rate to 30x. 2 Background and Related Work Language Model Pre-training: The accuracy of transformer-based language models grows substan- tially with its model size [31, 33, 6]. Today, a large language model such as GPT-3 [6] contains up to 175B parameters, and recent studies show that model accuracy can continue to improve with even larger model sizes [16]. However, training these large models often incurs excessively long training time and training difﬁculties [6]. Therefore, there are a lot of demands of performing efﬁcient and stable training for these models. To have the pre-training ﬁnished in a reasonable amount of time, the most common way is to leverage data parallelism to train models on multiple GPUs. However, the speedup gains often saturate beyond a few tens of GPUs, because communication becomes the major bottleneck, i.e., the workers will spend more time communicating gradients than computing them, as the number of GPUs increases. To mitigate this bottleneck, recent works such as 1-bit Adam [43] have studied gradient compression and demonstrate their effectiveness against auto-encoding models such as BERT [11]. An alternative approach to alleviate these overheads is to use large batch sizes. For example, LAMB [52] and 1-bit LAMB [21] enable stable and efﬁcient distributed BERT pre-training with batch size up to 64K/32K (for sequence length 128/512, i.e., 8M/16M tokens per batch) while maintaining the sample-wise convergence speed. For encoder-decoder models, T5 [34] uses batch size up to 2K (for sequence length 512, i.e., 1M tokens per batch). For autoregressive models such as the GPT family [ 31, 33, 6], existing works use batch size up to 1.6K (for sequence length 2K, i.e, 3.2M tokens per batch). Despite the beneﬁt of reduced communication overhead, large-batch training is sensitive to hyperparameters and often leads to issues such as slow convergence, training instabilities, and model divergence. For example, recently a research project shared that they are dealing with challenging training instability issues when pre-training a 104B GPT-style model with batch size 2K [49], and another work on a 540B model with batch size 2K observed spikes in the loss roughly 20 times during training, despite the fact that gradient clipping was enabled [9]. Curriculum Learning: Our method can be viewed as a kind of curriculum learning (CL) [12, 39, 2], which presents easier/simpler examples earlier during training and gradually increases the sample difﬁculties2. Comparing with traditional CL works which focus on solely improving the convergence speed under the same batch size, learning rate and other hyperparameters, our work is motivated by the stability-efﬁciency dilemma and we aim to achieve both efﬁcient convergence and better stability by enabling stable training with more aggressive hyperparameters. To our knowledge, we are the ﬁrst to investigate and conﬁrm that certain curriculum learning method can provide a dual stability-efﬁciency beneﬁt. In the NLP area, most of the curriculum learning works focus on small-scale one-stage tasks and downstream ﬁne-tuning tasks, such as neural machine translation (NMT) [ 18, 5, 56, 29, 57] and natural language understanding (NLU) [36, 37, 44, 50]. There are also a few works explore curriculum learning for language model pre-training [ 30, 55, 7]. These works show that curriculum learning can improve convergence speed, reduce training time, and improve accuracy under the same training hyperparameters as baseline. In these works, the curriculum difﬁculty metrics for each training sample are usually deﬁned as the sentence length, vocabulary frequency, the inference loss on smaller/cheaper models, or based on self-paced learning [19]. For the pacing function (i.e., to decide the curriculum difﬁculty range when sampling next training data batch), these works usually use ﬁxed predeﬁned functions (e.g., gradually increase difﬁculty upper bound by linear, root, and exponential functions), bucketing heuristics (group data with similar difﬁculties, and sample from a subset of buckets every time), or based on self-paced learning. 3 GPT-2 Pre-training Stability-Efﬁciency Analysis In this section we perform an in-depth analysis of pre-training tasks (without our method) replicating the GPT-2 models with public data. We follow the training pipeline from the NVIDIA Megatron-LM work [40]3. All of the experiments are performed on 128 NVIDIA V100 GPUs (32GB memory). There are 16 nodes and 8 GPUs per node. GPUs inside the same node are connected by NVLink 2.0, and nodes are connected by a 100 Gigabit InﬁniBand EDR inter-node network. We evaluate 2The shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. 3https://github.com/NVIDIA/Megatron-LM 30 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 1: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. In Appendix A.1 we plot the same ﬁgure but zoom in the ﬁrst 30B tokens. Table 1: Measuring training instability by the ratio between the current step training loss and the minimum loss among all previ- ous steps. Larger ratios (esp. those greatly larger than 1.0) indicate larger training insta- bility/divergence. Proposed work (SLW) and related works (last two rows) are discussed in Section 5. #steps with max Pre-training loss ratio > 1.2 loss Case parameters (% of total steps) ratio 117M: 1: Baseline bsz512-seqlen1K 0 (0.0%) 1.05 2: SLW 60K bsz512-seqlen1K 0 (0.0%) 1.06 3: Baseline bsz4K-seqlen1K 22 (0.06%) 1.42 4: SLW 20K bsz4K-seqlen1K 0 (0.0%) 1.02 5: Baseline bsz512-seqlen2K 0 (0.0%) 1.04 6: SLW 110K bsz512-seqlen2K 0 (0.0%) 1.04 1.5B: 7: Baseline bsz512-seqlen1K 114 (0.04%) 2.10 8: SLW 270K bsz512-seqlen1K 0 (0.0%) 1.06 9: Baseline bsz4K-seqlen1K 1381 (3.68%) 5.65 10: SLW 45K bsz4K-seqlen1K 0 (0.0%) 1.02 11: Shortformer bsz4K-seqlen1K 219 (0.4%) 2.86 12: Bsz Warmup bsz4K-seqlen1K 1179 (2.01%) 4.32 two GPT-2 model sizes from the original GPT-2 work [33]: 117M parameters (12 layers, 768 hidden size, 12 attention heads) and 1.5B parameters (48 lay- ers, 1600 hidden size, 25 attention heads). For training data, we collect and use the same dataset blend as the Megatron-LM work: Wikipedia [11], CC-Stories [45], RealNews [54], and OpenWebtext [32]. We evaluate two sets of training parameters. The ﬁrst set follows the Megatron-LM work: batch size 512, 300K total training steps (157B tokens), and learning rate 1.5 ×10−4 with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 297K steps (1 ×10−5 min. learning rate). The second parameter set tests a more aggressive training strategy: batch size 4K (8×larger), 37.5K total training steps (157B tokens4), and learning rate6×10−4 (4×larger) with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 34.5K steps (same min. learning rate). For sequence length/context size, we mainly use 1K which is the default for GPT-2. But we also test 2K (on the 117M model with batch size 512 and 157B tokens) which is the default for GPT-3. All experiments are performed with mixed precision/FP16 training, Adam optimizer (β1 = 0.9, β2 = 0.999, ϵ= 1×10−8) [17], 0.01 weight decay, same random seed, and gradient clipping at 1.0. For both batch sizes we use the same number of gpus (128). It is true that under fewer nodes, smaller batch sizes can also achieve good computation efﬁciency. However, in practice, the goal of a training task is usually \"given a ﬁxed number of hardwares, how to train the model in the fastest wall clock time\". And given the increasing model sizes, pre-training on hundreds of GPUs is not uncommon. Thus we believe that using the same hardware resources is a fair comparison. The stability-efﬁciency dilemma: Figure 1(a) and 1(b) present the training loss curves of 5 baseline cases under different model sizes, batch sizes (and LR), and sequence lengths. At 117M, the baseline has a few training loss spikes at batch size 4K. At 1.5B, the baseline has many loss spikes when training with either batch size 512 or 4K. As an indicative measurement to quantitatively study training instability, we deﬁne “loss ratio” which measures the ratio between the current step training loss and the minimum loss among all previous steps. A ratio larger than 1.0 means that current step’s loss is larger than the previous minimum loss, thus larger ratio indicates a larger loss spike and training instability. Table 1 summarizes the number of steps with loss ratio larger than 1.2, and the 4For pre-training it is common to keep the number of training tokens the same for fair comparison. 4Table 2: Zero-shot evaluation of the trained models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case 2 to 9 are compared with case 1, and case 11 to 17 are compared with case 10. Proposed work (SLW) and related works (16, 17) are discussed in Section 5. Pre-training Training Training Training WikiText LAMBADA Case parameters steps tokens time PPL ↓ accuracy ↑ 117M: 1: Baseline bsz512-seqlen1K 300K 157B 37Hr 27.78 33.19% 2: SLW 60K bsz512-seqlen1K 200K 89B (1.8x) 20Hr (1.9x) 27.74 34.78% 3: SLW 60K bsz512-seqlen1K 330K 157B (1x) 33Hr (1.1x) 27.01 34.41% 4: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 16Hr (2.3x) 28.09 32.54% 5: SLW 30K bsz4K-seqlen1K 37K 92B (1.7x) 10Hr ( 3.7x) 27.77 33.40% 6: SLW 30K bsz4K-seqlen1K 52.5K 157B (1x) 16Hr (2.3x) 27.15 34.16% 7: Baseline bsz512-seqlen2K 150K 157B (1x) 32Hr (1.2x) 28.19 32.99% 8: SLW 110K bsz512-seqlen2K 122.5K 71B ( 2.2x) 15Hr (2.5x) 27.06 33.24% 9: SLW 110K bsz512-seqlen2K 205K 157B (1x) 31Hr (1.2x) 26.03 34.58% 1.5B: 10: Baseline bsz512-seqlen1K 300K 157B 341Hr 13.89 57.29% 11: SLW 270K bsz512-seqlen1K 360K 122B (1.3x) 286Hr (1.2x) 13.89 57.38% 12: SLW 270K bsz512-seqlen1K 428K 157B (1x) 364Hr (0.9x) 13.88 57.89% 13: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 151Hr (2.3x) 14.76 55.06% 14: SLW 45K bsz4K-seqlen1K 50K 121B (1.3x) 121Hr (2.8x) 13.88 58.20% 15: SLW 45K bsz4K-seqlen1K 58.8K 157B (1x) 155Hr (2.2x) 13.72 58.47% 16: Shortformer bsz4K-seqlen1K 55K 157B (1x) 162Hr (2.1x) 14.14 57.23% 17: Bsz Warmup bsz4K-seqlen1K 58.8K 157B (1x) 165Hr (2.1x) 14.21 56.36% Reference 18: Original GPT-2 117M [33], different data 37.50 45.99% works: 19: Original GPT-2 1.5B [33], different data 17.48 63.24% 20: Megatron-LM GPT-2 355M [33], same data 19.31 45.18% 21: Megatron-LM GPT-2 2.5B [33], same data 12.76 61.73% maximum loss ratio during the training. At 117M model size only the baseline with batch size 4K has high loss ratios up to 1.421. At 1.5B model size the baseline with both batch size 512 and 4K has much more steps with large loss ratios, and with the maximum loss ratio as high as 5.65. Baseline with batch size 4K is less stable than baseline with batch size 512, indicating that larger batch sizes (combined with larger learning rates) could lead to more training instability risks. In Appendix A.3.1 we show that larger learning rates under the same batch size could also increase training instability. Training instability are undesirable because (1) it could lead to divergence that never recover as in [49] and our GPT-3 experiments (Section 5.2); (2) in our GPT-2 case it leads to worse convergence, validation loss, and zero-shot downstream task accuracy. Table 2 summarizes the zero-shot WikiText- 103/LAMBADA evaluation results. For both 117M and 1.5B models, increasing baseline’s batch size (and LR) or sequence length leads to training instabilities and loss spikes, and it requires a nontrivial number of training steps/tokens to recover the training loss back to a normal level (e.g., Figure 1(b)). These training “detours” slow down the learning and eventually lead to worse evaluation results (e.g., case 13 vs case 10 in Table 2). On the other hand, increasing batch size (and LR) or sequence length improves training efﬁciency, reducing the training time by up to 2.3x under the same number of training tokens (case 1, 4, 10, 13). Overall, the above observations demonstrate the stability-efﬁciency dilemma for baseline pre-training: the training is more stable and can achieve better ﬁnal generalization, but presumably with poorer training efﬁciency under smaller batch size/learning rate/sequence length; increasing them leads to better training efﬁciency, but with lower stability and worse generalization. The correlation between instability and gradient variance outliers: For stochastic gradient opti- mization, when the gradient variance is large, the algorithm might spend much time bouncing around, leading to slower convergence and potential divergence [48]. Previous studies show that variance reduction methods improve training stability in areas such as reinforcement learning [ 24, 8, 1]. Figure 1(c), 1(d), 1(e) and 1(f) plot the l1 norm and max element of Adam’s variance state (√vt, where vt = β2vt−1 + (1−β2)(gt)2)5. When baseline’s batch size increases, the max variance norm decreases but the max element increases. Comparing GPT-2 117M and 1.5B cases, larger model size leads to larger variance norm and max element. When sequence length increases for the GPT-2 117M case, the variance norm stays the same but the max element increases. To further study the link between instability and gradient variance, Figure 1(g) and 1(h) plot the loss ratio (deﬁned earlier in this section) and gradient variance norm/max element (all normalized by max value) for the most unstable 1.5B baseline with 4K batch size. Results show that when training loss spike happens and loss ratio increases, the gradient variance norm/max also increase (especially the max outliers). Table 3 presents the Pearson correlation coefﬁcient calculations, which demonstrate 5We use l1 norm to avoid outlier ampliﬁcation. 5Table 3: Pearson correlation coefﬁcient (with range (-1, 1)) between loss ratio and gradient variance norm/max. Low p-value indicates that the correlation is statistically signiﬁcant. Pearson correlation coefﬁcient p-value Loss ratio vs Gradient variance norm 0.23 0.0 Loss ratio vs Gradient variance max 0.26 0.0 0 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Steps 2 4 6 8 10 12 14Training loss Seqlen1K, baseline Seqlen128 Seqlen128(90%)+1K(10%) Figure 2: Step-wise training loss during GPT-2 1.5B pre-training (ﬁrst 10K steps only) with batch size 4K, comparing seqlen 1K (baseline), seqlen 128, and mixed seqlen of 128+1K (1K seqlen used at the cyan areas). a statistically signiﬁcant positive correlation between loss ratio and gradient variance norm/max. Overall, our analysis shows that training instability has a strong correlation with gradient variance norm and (especially) max element outliers. Length of early data sequences is critical to training stability: Aiming to solve the stability- efﬁciency dilemma we ﬁrst tried traditional methods such as increasing gradient clipping, but it does not fully resolve the instability issue (Appendix A.3.2). Seeing that in Figure 1 the training instability mostly happens at the ﬁrst half of training, we then explored whether we can solve the issue by gradually increasing any of the batch size/learning rate/sequence length during training. We already employed the same learning rate warmup mechanism used by existing GPT-2 and GPT-3 works [33, 40, 6]. We tried the batch size warmup method proposed in GPT-3 work [ 6], but the instability issue still appears when increasing the batch size (Section 5.1). Our investigation on the sequence length leads to interesting ﬁndings, where we ﬁnd that sequence lengths play a critical role in training instability. Figure 2 presents the training loss curve of the most unstable GPT-2 1.5B pre-training with batch size 4K and seqlen 1K, together with another two artiﬁcial settings: one with seqlen 128, the other with mixed seqlen where we feed 900 steps of seqlen 128 then 100 steps of seqlen 1K in every 1K steps. The seqlen 128 case has no instability issue, even with large model size/batch size/learning rate. The mixed seqlen case has instability issues, and (1) they mostly happen when we switch to seqlen 1K (e.g., at step 900, 1900, 2900...); (2) they mostly happen during the ﬁrst 5K steps, and after that it becomes more stable than the seqlen 1K case. These observations indicate that training instability is strongly correlated with early long sequence lengths, which motivates us to explore the sequence length warmup method described in the next section, and evaluations in Section 5 will demonstrate how this method provides a gradient variance reduction effect and solves the stability-efﬁciency dilemma in our experiments. 4 The Sequence Length Warmup Method The analysis in last section about training instability and sequence lengths motivates us to explore sequence length warmup methods: the model needs to start learning with short sequence length for more stable training, then gradually increase the length when training becomes more stable so that the model can still learn from longer contextual information to achieve better ﬁnal model accuracy. The sequence length warmup strategy depends on two factors: how to support variable sequence length during training and how to adaptively decide the sequence length for each iteration (the pacing function). For the ﬁrst component, we develop an efﬁcient truncation-based implementation: For the baseline GPT-2 pre-training, the raw text inputs are indexed into sequences with the same length before training, so that the model can efﬁciently retrieve a batch of ﬁxed-length sequences regardless of the actual sentence boundaries. It’s possible to index the text inputs based on all possible sequence lengths, but that adds signiﬁcant amount of overhead due to the massive pre-training data. To avoid the large indexing overhead, we take a lightweight approach: our implementation still lets the dataloader index the raw text into only the full sequence length. At each training step, our method uses pacing function to determine the sequence length and then truncates the full-length sequences to obtain a modiﬁed version of the mini-batch for training. It is true that this truncation-based implementation will drop some data in the current step. However, with some implementation changes, it’s possible to record the index of dropped data and use them in future steps. We deﬁne the pacing function as a step-wise linear function with the following properties: Given a starting sequence length seqlens, an ending sequence length seqlene (full sequence length), and a total duration T (number of steps), the sequence length used for the training batch at step tis seqlent = seqlens + (seqlene −seqlens) ×min( t T ,1). Besides step-wise linear, we also explored 3 other pacing functions: i) We tried a discrete 2-stage pacing function from [ 30], but it leads 6to unstable training and worse convergence (Section 5.1). ii) We tried a step-wise root function (seqlent = seqlens + (seqlene −seqlens) ×min(( t T )r,1), where r is the root degree), which performs similar to linear but requires one extra hyperparameter. iii) We tried an adaptive pacing function based on training/validation losses, which also performs similar and requires extra tuning. 0 2K 4K 6K 8K 10K Steps 0 250 500 750 1000 1250 1500 1750 2000Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (a) Step-wise validation per- plexity (beginning of training) 0 50B 100B 150B Tokens 20.0 20.2 20.4 20.6 20.8 21.0Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (b) Token-wise validation per- plexity (end of training) Figure 3: Validation perplexity during GPT-2 117M seqlen 1K pre-training with batch size 512 and different duration T. (“SLW 20K” means proposed approach withT=20K steps). Pacing function analysis and tuning strategy: To study the impact of our approach’s pacing function, we set the starting sequence length (seqlens) ﬁxed at 8 and perform a grid search for the pacing function duration ( T) on the GPT-2 117M case full training (de- tails in Appendix A.4). After full train- ings we perform evaluation on pretrain data test set and WikiText/LAMBADA zero-shot tasks to conﬁrm which du- ration T provides the best accuracy performance. All the cases have quite comparable evaluation results, indicat- ing that the performance is not very sensitive to the duration T within a reasonable range. This grid search sheds light on a low-cost tuning strategy: we ﬁnd that (for GPT-2 117M training with batch size 512 and 4K) the best duration T is the longest duration that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst 10K steps (i.e., a few multiples of the LR warmup steps). In on our study, this “signiﬁcant ﬂuctuation” can be deﬁned as “whether the perplexity value becomes larger than 1.3x of the previous best perplexity”. In Figure 3(a) the SLW 60K is the longest duration we tested that does not have signiﬁcant validation ﬂuctuation during the ﬁrst 10K steps. In Figure 3(b) and Appendix A.4 SLW 60K does provide the best ﬁnal validation perplexity, best ﬁnal test perplexity, and second best eval results. Since it does not require training the model until full convergence, this heuristic greatly reduces the hyperparameter tuning cost of our approach. Another grid search on the starting sequence length seqlens shows that it’s generally better to set it as small as possible, to maximize the stability and convergence speedup beneﬁt. However, if the validation perplexity has signiﬁcant ﬂuctuation near the starting sequence length, increasing seqlens would lead to better convergence. Overall, the low-cost tuning strategy can be summarized as: (1) Start with seqlens = 8and T = a few multiples of LR warmup steps. (2) Increase seqlens until the validation perplexity no longer has signiﬁcant ﬂuctuation at the very beginning. (3) Perform a binary search to ﬁnd the largest T that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst few multiples of LR warmup steps. This tuning strategy relies only on the validation set and does not require test set or downstream task evaluation. For the GPT-2 1.5B and GPT-3 125M models, we used this strategy to tune T and seqlens for the pacing function, and results show that this low-cost tuning strategy could provide similar stability-efﬁciency beneﬁt as grid search on full training runs (GPT-2 117M case). 5 Evaluation 5.1 GPT-2 experiments For GPT-2 model, dataset, and hardware, we follow the same methodology in Section 3. For proposed work’s pacing function conﬁgurations (Section 4), we useseqlens = 8/64 (for 117M/1.5B model based on tuning) and seqlene = 1K/2K(full sequence length). To fully utilize the NVIDIA Tensor Core acceleration, we add a seqlent = seqlent −(seqlent mod 8) postprocessing to make sure the sequence length is always a multiple of 8. For the total duration T, we tune this parameter (grid search for 117M and low-cost tuning for 1.5B) for each case. For the training parameters, for our approach we use the same shared parameters as the baseline except two parameters: 1) Because during sequence length warmup the number of tokens in a data batch is smaller, we modify the training termination condition so that all cases stop when reaching the same 157B training tokens. 2) Because of 1), proposed approach now has more training steps, which make it necessary to modify the learning rate decay schedule to have a fair comparison with the baseline. We change the learning rate decay to token-wise over the 157B tokens (still cosine decay) instead of step-wise over the total number of steps. We describe the underlying rationale in Appendix A.2. 70 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (a) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2500 5000 7500 10000 12500 15000 17500Variance norm Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (e) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2000 4000 6000 8000 10000 12000 14000Variance norm Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (h) Step-wise Adam vari- ance max element Figure 4: Validation perplexity and Adam variance norm/max element during GPT-2 1.5B seqlen 1K pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR. Also compare with related works (“Shortformer” [30] and “Bsz Warmup” [6]) at 2nd row. Each row of subﬁgures share the same legend (“SLW 45K” means our work withT=45K steps). Based on the following observations, we demonstrate that our approach resolves the dilemma and simultaneously improves the stability and efﬁciency. We will mainly present the GPT-2 1.5B results and leave some GPT-2 117M results in Appendix. Signiﬁcant stability gain: In Section 3 Table 1 we discussed how we measure the training instability based on the “loss ratio” metric, which shows that the baseline becomes less stable under larger model size/batch size/learning rate/sequence length. Comparing with baseline and proposed work in this table shows that our work reduces this instability measurement to zero in all cases, together with max ratio close to 1.0 (no spike). This demonstrates the signiﬁcant stability gain by our method. Faster token-wise and time-wise convergence: Figure 4(a) and 4(b) present the validation perplex- ity curves during GPT-2 1.5B seqlen 1K pre-training, comparing baseline and our approach. When the batch size increases from 512 to 4K for baseline, the time-wise convergence becomes faster but the token-wise convergence becomes slower and poorer. On the other hand, our approach at batch size 4K provides faster and better convergence both token-wise and time-wise comparing with the best baseline curve in each case. The shape of SLW’s curves is different (worse than baseline in early stage) because SLW cases initially only learn from shorter sequences, which limit the validation perplexity it can reach (since validation data is always full-length). On the other hand, when SLW cases start to learn from longer sequences, the validation perplexity drops faster than baseline (and related works) and eventually surpasses them. Our approach with batch size 512 provides smaller convergence speedup because (1) Baseline with batch size 512 has less instability issue, limiting the gain from the proposed approach; (2) At batch size 512 the communication overhead is very high, and our approach takes more steps (i.e., communication rounds) than baseline to reach the same 157B training tokens. This extra communication cost “cancelled” part of the time saving from our approach. For GPT-2 117M, our approach provides similar token-wise and time-wise convergence speedup (Appendix A.4). Advancing cost-quality Pareto curve: In Section 3 Table 2 we discussed about baseline’s zero-shot evaluation results. For proposed work eval results in this table, we present them in two ways: one evaluated at the earliest checkpoint that provides better eval results than baseline (batch size 512 and seqlen 1K); the other one evaluated at the end of full training. Results show that our approach is able to advance the cost-quality Pareto curve in two ways: (1) To reach the same eval result quality as baseline, our approach reduces the required number of pre-training tokens and wall clock time by up to 2.2x and 3.7x, respectively; (2) Under the same 157B training tokens, our approach can further improve the eval result quality. In (1) the time-wise saving is higher than the token-wise because (a) For each Transformer block, the self-attention and intermediate layers have time complexity of O(B×L2 ×H) and O(B×L×H2), respectively6. The proposed method uses shorter sequences at 6B, L, Hrepresent batch size, sequence length, hidden size. 8the beginning, reducing the time complexity quadratically for the self-attention sub-layer and linearly for the intermediate sub-layer of Transformer blocks; (b) By enabling stable training at larger batch size, our approach achieves additional time-wise saving by reducing the communication overhead. Variance reduction helps stabilize training: In Section 3 we discussed the strong correlation between training instability and gradient variance norm/max. Figure 4(c) and 4(d) demonstrate that proposed approach stabilizes training and reduces both the Adam variance norm and the variance max element. Importantly, it avoids all the spikes of the variance max element, which all happen to be where the baseline has training loss spikes. One may wonder why gradient clipping cannot help avoid these extreme gradient variance outliers. Although gradient clipping can avoid large gradient at every single step, it cannot avoid the gradient variance getting accumulated from multiple steps (Appendix A.3.2). Comparing with related works: We now compare the proposed work with two related works on the most challenging “1.5B model + batch size 4K” case. The ﬁrst work is the Shortformer where the ﬁrst stage uses shorter sequences and the second stage uses full-length sequences [30]. Following the grid search in the paper, we use seqlen 128 for the ﬁrst stage and set its duration at about half of the baseline duration (20K steps). The second work is the batch size warmup technique used by GPT-3 [6], where we set the starting batch size at 128 and then gradually increase it to 4K, and set the warmup duration same as the proposed work. Other training hyperparameters are unchanged. Figure 4(e) to 4(h) present the results. Both related works provide convergence speedup but it is less than our work. More importantly, they still have training instability issues. The Shortformer has an obvious training divergence at step 20K when the sequence length switches from 128 to 1K (the spike at 20K in Figure 4(h)). This shows when staying at the same shorter sequence length for too long, the model becomes heavily overﬁtted for that length which leads to divergence risk when/after switching to full length. Although both batch size warmup and our method reduce the number of tokens per batch in a similar fashion, batch size warmup does not provide any training stability beneﬁt compared to the baseline. This indicates that providing the same number of shorter (simpler) sequences leads to better training stability than providing fewer number of same length (same difﬁculty) sequences. In addition, batch size warmup has a limitation that the batch size must be multiple of data-parallel size. On the other hand, for our method the sequence length only needs to be multiple of 8 to enable Tensor Core acceleration. These two limitations are different: for the proposed SLW method, the “multiple of 8” limitation is ﬁxed and unrelated to data-parallel size. For batch size warmup it’s a dynamic \"multiple of data-parallel size\" limitation: since nowadays pre-training tasks are performed on up to thousands of GPUs, the data-parallel size can easily go beyond 100, prohibiting ﬂexible conﬁguration of the method (or requires reducing the number of GPUs when using smaller batches, increasing the training clock time). Last but not least, both related works provide non-zero “loss ratio” in Table 1 and worse zero-shot evaluation results in Table 2. 5.2 GPT-3 experiments For experiments replicating the GPT-3 125M model [6] using the Pile public dataset [13], ﬁrst we reproduce the original GPT-3 training recipe: 300B training tokens, seqlen 2K, batch size 256 with batch size warmup (start with 16 then gradually increase to 256 in ﬁrst 4B tokens), learning rate 6 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (6 ×10−5 min. learning rate) 7. Then we explore an aggressive training scenario where only 30B tokens (10%) are allowed. This is because (1) GPT-3 paper admits that it has poor training sample efﬁciency and it sees much more text during pre-training than a human sees in the their lifetime [6, 23]. (2) There could exist cases where the total amount of data/computation resource is limited. We adjust several hyperparameters in this 30B-token training: 8x batch size (2K) for better training efﬁciency, learning rate decay reduced to 30B tokens (based on study that LR schedule should match total training tokens [14], warmup stays at 375M), min. learning rate reduced to 0 (based on recent study on GPT-3 [51]). For baseline we keep using 4B-token batch size warmup, but when our method is used (seqlens = 72, T = 11.5Kbased on tuning) we disable it since both methods reduce tokens per batch. And for both cases we tune the learning rate and use the highest one that provides stable training, which is 30x (1.8×10−2) for baseline and 40x (2.4×10−2) for our method. All experiments are performed on 128 V100 GPUs. Figure 5 and 6 present the training loss and gradient variance max for the GPT-3 pre-training experiments. When applying 40x learning rate to the baseline (batch size warmup), it quickly 7Different from GPT-2, GPT-3 uses token-based learning rate schedule and we follow it. 9Table 4: Zero-shot evaluation of the trained GPT-3 125M models on 11 tasks used by the original GPT-3 work [6]. Per- task eval results in Appendix A.5. Batch Training Training Average Case size tokens time accuracy ↑ 1: Original [6] 256 300B 33.6 2: Baseline repro 256 300B (1x) 61Hr 31.4 3: Baseline 30x LR 2K 30B (10x) 7Hr (9x) 29.8 (95%) 4: SLW 40x LR 2K 30B (10x) 3.5Hr (17x) 31.1 (99%) 0 250M 500M 750M 1B Tokens 2 3 4 5 6 7 8 9 10Training loss Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 5: Training loss during GPT-3 125M pre-training (ﬁrst 1B to- kens). 0 250M 500M 750M 1B Tokens 0.00 0.05 0.10 0.15 0.20 0.25Variance max Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 6: Gradient vari- ance max during GPT-3 125M pre-training (ﬁrst 1B tokens). diverges and cannot continue to train due to NaN losses. The corresponding gradient variance max element becomes a ﬂat line after divergence because the gradients on all dimensions are so large that all gradients get clipped including the max element. After lowering the learning rate to 30x, the baseline is able to ﬁnish the whole training, but it can only retain 95% of average zero-shot accuracy on 11 tasks (HellaSwag [53], LAMBADA [28], TriviaQA [15], WebQs [3], Winogrande [38], PIQA [4], ARC Challenge/Easy [10], ANLI R1/R2/R3 [27]) compared with the case that reproduces the original GPT-3 training recipe8 as shown in Table 4. In contrast, our approach enables stable training with 40x learning rate, demonstrates lower gradient variance max outliers than baseline with 30x learning rate, retains 99% of the original training recipe’s average zero-shot accuracy, and achieves 10x data saving and 17x time saving.9 This demonstrates that the proposed method not only solves the stability-efﬁciency dilemma, but also opens a promising direction of signiﬁcantly reducing total training cost in a different data efﬁciency dimension. Finally, in Appendix A.6 we evaluate the proposed method on a larger GPT-3 1.3B model, including not only zero-shot but also few-shot evaluation. Results show that under the same 300B training tokens the proposed SLW method provides better average accuracy than the baseline for both zero- shot (from 41.6 to 41.9) and few-shot (from 44.8 to 45.3) tasks, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) can provide better accuracy performance. Similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup and proposed SLW method. 6 Limitation and Future Work Although our paper discovers the correlation between long sequences and training instability (and proposes a method to alleviate the issue), the root cause of this causal relationship is not fully deciphered and would be an interesting future work direction. One assumption we have is that the shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. We think encoding shorter sequences (especially at the early training stage when weight is still relatively random) will generate less \"noisy\" gradients thus leading to higher stability. This work presents correlation analysis on an empirical connection between training instability and gradient variance norm/max element, but it is not sufﬁcient to prove a causal relationship, and training instability could be caused by other factors. Overall, our work is one preliminary attempt to study the topic of training instability in large-scale model training, and we hope it could inspire future works to further study this important issue in both theory and practice. 7 Conclusion This paper presents the Sequence Length Warmup method, which solves a stability-efﬁciency dilemma inside GPT-style model pre-training, an critical issue that hinders efﬁcient language model pre-training as explained in our in-depth analysis. By enabling stable training on more aggressive training recipe, this method also motivates a new dimension of training cost reduction by improving the data efﬁciency, as demonstrated by the 10x data and 17x time saving in our GPT-3 experiments. We believe that the effectiveness, simplicity, and easy-to-use/tune make the proposed method a must-try for deep learning practitioners, and we hope this work could motivate more studies on improving training data efﬁciency. 8Our reproduced GPT-3 has 2.2 point lower average accuracy than the original GPT-3, which is because of the different training data and OpenAI employed many data processing techniques [6] 9We want to emphasize here that the SLW method “only retains 99% of accuracy” because this experiment is an extreme case: only 10% of original training data is used during training. 10References [1] Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. In International Conference on Machine Learning, pages 176–185. PMLR, 2017. [2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48, 2009. [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artiﬁcial intelligence, pages 7432–7439, 2020. [5] Ondˇrej Bojar, Jindˇrich Helcl, Tom Kocmi, Jindˇrich Libovick`y, and Tomáš Musil. Results of the wmt17 neural mt training task. In Proceedings of the second conference on machine translation, pages 525–533, 2017. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020. [7] Daniel Campos. Curriculum learning for language modeling. arXiv preprint arXiv:2108.02170, 2021. [8] Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick. Control regularization for reduced variance reinforcement learning. In International Conference on Machine Learning, pages 1141–1150. PMLR, 2019. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. [12] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99, 1993. [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. [17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. 11[18] Tom Kocmi and Ondˇrej Bojar. Curriculum learning and minibatch bucketing in neural machine translation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 379–386, 2017. [19] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS, volume 1, page 2, 2010. [20] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [21] Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit LAMB: Communication Efﬁcient Large-Scale Large-Batch Training with LAMB’s Convergence Speed. arXiv preprint arXiv:2104.06069, 2021. [22] Conglong Li, Minjia Zhang, and Yuxiong He. Curriculum learning: A regularization method for efﬁcient and stable billion-scale gpt model pre-training. arXiv preprint arXiv:2108.06084, 2021. [23] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020. [24] Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh. Variance reduction for reinforcement learning in input-driven environments. In International Conference on Learning Representations, 2018. [25] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. [26] Microsoft and Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to- train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful- generative-language-model/, 2021. [27] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019. [28] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [29] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, and Tom M Mitchell. Competence-based curriculum learning for neural machine translation. In NAACL- HLT, 2019. [30] Oﬁr Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832, 2020. [31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [32] Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, and Ilya Sutskever. Better language models and their implications. OpenAI Blog, 2019. [33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza- tions toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020. [36] Mrinmaya Sachan and Eric Xing. Easy questions ﬁrst? a case study on curriculum learning for question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 453–463, 2016. 12[37] Mrinmaya Sachan and Eric Xing. Self-training for jointly learning to ask and answer questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 629–640, 2018. [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 8732–8740, 2020. [39] Terence D Sanger. Neural network learning control of robot manipulators using gradually increasing task difﬁculty. IEEE transactions on Robotics and Automation, 10(3):323–333, 1994. [40] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [41] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V . Le. Don’t decay the learning rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [42] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. [43] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit Adam: Communication Efﬁcient Large-Scale Training with Adam’s Convergence Speed. InProceedings of the 38th International Conference on Machine Learning, pages 10118–10129, 2021. [44] Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C Phan, Xingdi Yuan, Jinfeng Rao, Siu Cheung Hui, and Aston Zhang. Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4922–4931, 2019. [45] Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information processing systems, pages 5998–6008, 2017. [47] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019. [48] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. In NIPS, 2013. [49] Thomas Wolf. The engineering group in @bigsciencew ﬁghting training instabilities over +100b parameters. https://twitter.com/Thom_Wolf/status/1447565680384032776, 2021. [50] Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Curriculum learning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6095–6104, 2020. [51] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. [52] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. [53] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019. 13[54] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9054–9065, 2019. [55] Wei Zhang, Wei Wei, Wen Wang, Lingling Jin, and Zheng Cao. Reducing bert computation by padding removal and curriculum learning. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 90–92. IEEE, 2021. [56] Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. An empirical exploration of curriculum learning for neural machine translation. arXiv preprint arXiv:1811.00739, 2018. [57] Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. Curriculum learning for domain adaptation in neural machine translation. In NAACL-HLT, 2019. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] We have a section describing limitation and future work. (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed 10, and Section 3 and 5 include all instructions needed to reproduce the experiments. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Section 3 and 5 include all the training details. (c) Did you report error bars (e.g., with respect to the random seed after running ex- periments multiple times)? [N/A] Our results are based on a single seed given the pre-training is expensive. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Section 3 and 5 include the information. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 10https://github.com/microsoft/DeepSpeed 14(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 150 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 10B 20B 30B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 10B 20B 30B Tokens 0.00 0.02 0.04 0.06 0.08 0.10Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 7: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. A Appendix A.1 Zoom in of Figure 1 Figure 7 zoom in the ﬁrst 30B token in main paper Figure 1, where the training is the most unstable. A.2 Learning rate decay for proposed approach As discussed in main paper Section 5.1 GPT-2 experiments, proposed approach needs more training steps than baseline in order to reach the same 157B training tokens. This makes it necessary to modify the learning rate decay schedule for proposed approach. We ﬁrst tried to increase the number of learning rate decay steps by half of the proposed approach’s pacing function durationT (since the proposed approach roughly needs T/2 additional steps to reach 157B tokens). However, we ﬁnd that simply increasing decay steps still leads to faster learning rate decay than baseline. At last we change the learning rate decay to token-wise (same cosine decay over the 157B tokens) instead of step-wise. This is because for the proposed approach there are fewer tokens per step at the beginning. So even if we increase the LR decay steps, it still cannot avoid decaying faster token-wise at the beginning compared to baseline. As shown in Figure 8, the proposed approach with step-wise LR decay (with T/2 extra decay steps) has faster LR decay token-wise compared to baseline, which leads to a worse validation perplexity curve. On the other hand, the same proposed approach case with token-wise LR decay has the same token-wise LR decay schedule as baseline, which leads to better convergence. A.3 Additional analysis about training hyperparameters In main paper Section 4 we demonstrate that proposed approach’s two hyperparameters can be tuned with very low cost only running the very beginning of the training (the third hyperparameter, ending sequence length, does not require tuning since it will always be the full length). To understand more about how proposed approach affects the choice and tuning of normal training hyperparameters, this section provides additional analysis about learning rates and gradient clipping. Results demonstrate that (a) Compared to baseline, proposed approach requires less tuning effort on these hyperparameters to provide a stable training; (b) By enabling stable training on larger learning rates, proposed approach could provide better training efﬁciency and convergence (as demonstrated in main paper Section 5); (c) Tuning gradient clipping for baseline could not provide the same training stability as proposed approach. A.3.1 Learning rate In Section 5.1 we demonstrate that proposed approach can provide stable and more efﬁcient training at larger batch size and learning rate, where baseline suffers from training instability. We increased both batch size and learning rate at the same time because (a) Large-batch training is more efﬁcient 160 40B 80B 120B 160B Tokens 11.5 12.0 12.5 13.0 13.5 14.0Validation PPL Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (a) Token-wise validation perplexity 0 100K 200K 300K 400K Steps 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (b) Step-wise learning rate 0 50B 100B 150B Tokens 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (c) Token-wise learning rate Figure 8: Validation perplexity and learning rate during GPT-2 1.5B seqlen 1K pre-training with batch size 512, comparing the baseline and proposed approach under different learning rate decay schedules. “SLW 270K” means proposed approach withT=270K steps. 0 1K 2K 3K Steps 3 4 5 6 7 8 9 10 11 12Training loss Baseline LR=6e-4 Baseline LR=12e-4 SLW 8K LR=6e-4 SLW 8K LR=12e-4 Figure 9: Step-wise training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, seed 1236, and different learning rates for baseline and proposed approach (“SLW 8K” means proposed approach with T=8K steps). for large-scale distributed training, so larger batch was necessary in our study (b) In order to maintain the same convergence speed, it is necessary to simultaneously increase the learning rate under larger batch size. A well-known rule of thumb is that the learning rate should at least increase by the square root of the batch size’s increase ratio. As a controlled experiment, here we perform additional studies about what if we keep the batch size the same and only tune learning rate for baseline and proposed approach. We do not consider the case of “same learning rate, different batch sizes” due to the reason (b) above. Table 5 presents the number of steps with training loss ratios (deﬁned in main paper Section 3 as an indicative measurement of training instability) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K11, 5 different seeds, and different learning rates for baseline and proposed approach. And Figure 9 illustrates some of the cases with seed 1236 to show how the loss spikes look 11Batch size 2K is used here because this analysis was performed at an early stage of this work, and we do not have enough resource to rerun the same analysis with batch size 4K. 17Table 5: Number of steps with training loss ratios (deﬁned in Section 3) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, 5 different seeds, and different learning rates for baseline and proposed approach (SLW). Left/right number in each cell is for baseline/SLW, respectively. Baseline/SLW LR = LR = LR = LR = #loss ratio > 1.5 1.5 × 10−4 3 × 10−4 6 × 10−4 12 × 10−4 Seed 1234 0/0 296/0 359/0 179/74 Seed 1235 0/0 302/0 408/0 555/459 Seed 1236 0/0 0/0 569/0 626/414 Seed 1237 7/0 0/0 548/0 614/139 Seed 1238 0/0 0/0 121/0 394/29 Total 7/0 598/0 2005/0 2368/1115 like. Results show that proposed approach provides stable training during this ﬁrst 3K steps for all ﬁve seeds at learning rates up to 6 ×10−4, while baseline with seed 1237 still has 7 large loss ratios at learning rate as low as 1.5 ×10−4. At learning rate 12 ×10−4 both cases have large loss ratios, but proposed approach reduces the frequency by 2.1x. This demonstrates that (a) Larger learning rates lead to higher training instability risk for both cases. (b) With the same amount of tuning effort, proposed approach has a higher probability of providing a stable training because of the wider range of learning rates it enables; (c) Since proposed approach enables stable training at larger learning rate, it could provide better and faster training convergence as shown in main paper Section 5. A.3.2 Gradient clipping In main paper Section 5 we used gradient clipping at 1.0 (global gradient l2 norm is clipped to 1.0) following the previous work [40]. Here we perform additional studies about what if we apply more gradient clipping to baseline. Figure 10(a) presents the training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K (the same hyperparameters as the second set in Section 3), comparing the baseline and proposed approach under different gradient clipping levels12. Results show that when applying more gradient clipping to baseline, the training has less and smaller loss spikes. And the Adam varaince norm is also reduced as shown in Figure 10(c). However, more gradient clipping does not fully resolve the training instability issue. Even baseline with the lowest gradient clipping norm cannot avoid all training loss spikes, while proposed approach with default gradient clipping has no loss spike. As described in main paper, we believe that this is a limitation of common gradient clipping technique: Although gradient clipping can avoid too large gradient at every single step, it cannot avoid the gradient variance getting accumulated at certain dimensions (as shown in Figure 10(d)), especially for large batch sizes. Another concern about applying more gradient clipping is that the momentum norm is also reduced due to more clipping (Figure 10(b)). This indicates that when later the training reaches a more stable stage, more gradient clipping could hurt the convergence speed. On the other hand, proposed approach will not affect the convergence speed after the full sequence length is reached. Another thing to note is that proposed approach relies less on gradient clipping: at gradient clipping norm 1.0, baseline has 798 clippings in the ﬁrst 5K steps while proposed approach has 628 clippings (21% less). Overall, this analysis demonstrates that proposed approach requires less or no tuning on gradient clipping, while baseline still has training stability issue with more gradient clipping. It is possible that more complex and adaptive gradient/variance/activation clipping techniques could potentially achieve the same level of training stability as proposed approach. However, inventing and applying such techniques would require an effort no lower than the proposed approach, which is both easy to integrate and low-cost to tune. A.4 GPT-2 117M evaluation results Figure 11 presents the validation perplexity and Adam variance norm/max element during GPT-2 117M pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. Table 6 presents the zero-shot evaluation of the trained 117M models on 12We also tried less than 0.25 gradient clipping, which triggered a silent crash without error messages after around 100 steps. We did not have enough time to ﬁnd the root cause, but it could be caused by the too extreme gradient clipping. 180 1K 2K 3K 4K 5K Steps 2 4 6 8 10 12 14 16 18Training loss Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (a) Step-wise training loss 0 1K 2K 3K 4K 5K Steps 0 500 1000 1500 2000 2500 3000Momentum norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (b) Step-wise Adam momentum l1 norm 0 1K 2K 3K 4K 5K Steps 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (c) Step-wise Adam variance l1 norm 0 1K 2K 3K 4K 5K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06Variance max Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (d) Step-wise Adam variance max element Figure 10: Training loss, Adam momentum l1 norm, and Adam variance l1 norm/max element during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K, comparing the baseline and proposed approach under different gradient clipping levels. Grad clip 1.0 indicates that the global gradient l2 norm is clipped to 1.0. ‘SLW 45K” means proposed approach withT=45K steps. 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (a) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (e) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (h) Step-wise Adam vari- ance max element Figure 11: Validation perplexity and Adam variance norm/max element during GPT-2 117M pre- training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. “SLW 60K” means proposed work withT=60K steps. the WikiText-103 and LAMBADA datasets for baseline and proposed work with different pacing function duration. A.5 GPT-3 125M evaluation results Table 7 presents the zero-shot evaluation of the trained GPT-3 125M models on the 11 tasks used by the original GPT-3 work [6]. 19Table 6: Zero-shot evaluation of the GPT-2 117M models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case Pre-training Pre-training Pre-training WikiText-103 LAMBADA parameters steps, tokens, time test perplexity ↓ perplexity ↓ accuracy ↑ 1: Baseline bsz512-seqlen1K 300K, 157B, 37Hr 20.75 27.78 33.19% 2: SLW 20K bsz512-seqlen1K 310K, 157B, 30Hr 20.49 27.43 34.60% 3: SLW 60K bsz512-seqlen1K 330K, 157B, 33Hr 20.11 27.01 34.41% 4: SLW 100K bsz512-seqlen1K 350K, 157B, 35Hr 20.16 26.91 34.21% 5: SLW 140K bsz512-seqlen1K 370K, 157B, 35Hr 20.17 27.17 33.92% 6: Baseline bsz4K-seqlen1K 37.5K, 157B, 16Hr 20.99 28.09 32.54% 7: SLW 10K bsz4K-seqlen1K 42.5K, 157B, 16Hr 20.34 27.22 33.98% 8: SLW 20K bsz4K-seqlen1K 47.5K, 157B, 16Hr 20.25 27.13 34.54% 9: SLW 30K bsz4K-seqlen1K 52.5K, 157B, 16Hr 20.22 27.15 34.16% 10: SLW 40K bsz4K-seqlen1K 57.5K, 157B, 16Hr 20.26 27.11 33.53% 13: Baseline bsz512-seqlen2K 150K, 157B, 32Hr 20.87 28.19 32.99% 15: SLW 70K bsz512-seqlen2K 185K, 157B, 31Hr 19.82 26.04 33.46% 17: SLW 110K bsz512-seqlen2K 205K, 157B, 31Hr 19.64 26.03 34.58% 18: SLW 150K bsz512-seqlen2K 215K, 157B, 32Hr 19.64 25.99 33.32% 15: SLW 190K bsz512-seqlen2K 245K, 157B, 33Hr 19.64 26.09 33.09% Table 7: GPT-3 125M zero-shot evaluation results Baseline Baseline SLW Case Original [6] repro 30x LR 40x LR Model size 125M 125M 125M 125M Train tokens 300B 300B 30B 30B Batch size 256 256 2K 2K Bsz warmup 4B 4B 4B N/A LR 6e-4 6e-4 1.8e-2 2.4e-2 min LR 6e-5 6e-5 0 0 LR warmup 375M 375M 375M 375M LR decay 260B 260B 30B 30B decay style cosine cosine cosine cosine SLW N/A N/A N/A 11.5K steps Avg. accuracy 33.6 31.4 29.8 31.1 (0) HellaSwag 33.7 30.4 28.2 28.9 (1) LAMBADA 42.7 39.3 30.4 34.2 (2) TriviaQA 4.15 1.72 0.76 1.45 (3) WebQs 1.77 0.197 0 0.394 (4) Winogrande 52.0 49.3 50.9 51.9 (5) PIQA 64.6 61.9 59.8 62.7 (6) ARC Challenge 26.6 23.3 21.7 22.3 (7) ARC Easy 43.6 39.9 36.0 39.1 (8) ANLI R1 33.4 32.8 33.1 33.4 (9) ANLI R2 33.2 33.3 33.3 33.6 (10) ANLI R3 33.6 33.3 33.2 34.7 A.6 GPT-3 1.3B evaluation results In this section we evaluate the proposed SLW method on the larger GPT-3 1.3B model. Compared to the GPT-3 125M evaluation in main paper section 5.2 there are two differences on the setup: (1) The GPT-3 125M evaluation aims to explore whether the proposed method can retain the accuracy performance while greatly reducing the training tokens, while this GPT-3 1.3B evaluation aims to explore that, under same amount of training tokens, does proposed method provides better training stability and better accuracy performance. (2) To improve the training data quality, for GPT-3 1.3B pre-training we added two additional sub-datasets (CC-Stories [45] and RealNews [54]), together with additional data cleaning on all data following the process in [42]. Similar to previous experiments, we test two set of hyperparameters on both baseline and proposed method: The ﬁrst set follows the original GPT-3 setup: 300B training tokens, seqlen 2K, batch size 512 (baseline case includes batch size warmup that starts with 16 then gradually increase to 512 in ﬁrst 8B tokens), learning rate 2 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (2 ×10−5 min. learning rate). The second set changes the batch size to 4K (8x) and learning rate to 8 ×10−4 (4x). 20Table 8: GPT-3 1.3B zero-shot evaluation results Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 44.4 41.6 41.9 (0) LAMBADA 63.6 63.7 65.0 (1) TriviaQA 19.7 10.1 11.3 (2) WebQs 4.63 3.25 2.36 (3) PIQA 75.1 73.4 73.8 (4) RACE-h 40.9 35.6 37.1 (5) BoolQ 62.4 63.4 61.8 The baseline case only enables stable training on the ﬁrst set of hyperparameters. Under larger batch size and learning rate, a training divergence (similar to main paper Figure 5 blue line) happened and the training cannot continue. On the other hand, the proposed SLW method is able to provide stable training under 8x larger batch size and 4x larger learning rate. Under the same number of training tokens, the 8x larger batch size leads to better training efﬁciency and 2x training time speedup, similar to what we obserbe in GPT-2 pre-training (main paper Table 2 case 10 vs. 15). This demonstrate the stability-efﬁciency beneﬁt of the proposed method. In addition, Table 8 and 9 present the zero-shot and few-shot evaluations of the trained GPT-3 1.3B models on 6 tasks used by the original GPT-3 work [6]: LAMBADA [28], TriviaQA [15], WebQs [3], PIQA [4], RACE-h [20], BoolQ [47]. Results show that similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup (from 41.6 to 44.8) and proposed SLW method (from 41.9 to 45.3).13 The change on each task also follows the same pattern: TriviaQA and WebQs accuracy improve a lot under few-shot; PIQA, RACE-h, and BoolQ have similar accuracy under zero and few-shot; LAMBADA accuracy becomes worse under few-shot. More importantly, under the same 300B training tokens the proposed SLW method provides better average accuracy (zero-shot from 41.6 to 41.9, few-shot from 44.8 to 45.3) than the baseline, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) is able to provide better accuracy performance. 13Similar to main paper section 5.2, our reproduced GPT-3 baseline has 2.9/3.3 point lower average zero/few- shot accuracy than the original GPT-3, which is because of the different training data and OpenAI employed special data processing techniques [6] 21Table 9: GPT-3 1.3B few-shot evaluation results. k denotes the number of shots following the original GPT-3 work [6]. Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 48.1 44.8 45.3 (0) LAMBADA (k=15) 57.0 58.8 59.7 (1) TriviaQA (k=64) 32.1 19.2 19.0 (2) WebQs (k=64) 19.6 18.4 19.4 (3) PIQA (k=50) 74.3 74.2 72.8 (4) RACE-h (k=10) 41.4 35.0 37.6 (5) BoolQ (k=32) 64.1 63.2 63.2 22",
      "meta_data": {
        "arxiv_id": "2108.06084v4",
        "authors": [
          "Conglong Li",
          "Minjia Zhang",
          "Yuxiong He"
        ],
        "published_date": "2021-08-13T06:32:53Z",
        "pdf_url": "https://arxiv.org/pdf/2108.06084v4.pdf",
        "github_url": "https://github.com/NVIDIA/Megatron-LM"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the stability-efficiency dilemma in pre-training large-scale GPT models, where increasing batch sizes and learning rates improves efficiency but causes training instability (divergence or slow convergence). It identifies a strong correlation between training instability and extreme gradient variance values, particularly linked to long sequence lengths at the beginning of training. The main contribution is the proposed Sequence Length Warmup (SLW) method, which starts training with short sequences and gradually increases the length to mitigate this instability. SLW enables stable training with significantly larger batch sizes (8x) and learning rates (up to 40x) compared to baselines, leading to substantial reductions in training tokens (up to 2.2x) and wall-clock time (up to 17x) while maintaining or improving accuracy on GPT-2 and GPT-3 models. A lightweight hyperparameter tuning strategy for SLW is also presented.",
        "methodology": "The methodology involves an in-depth analysis of GPT-2 pre-training tasks using the NVIDIA Megatron-LM pipeline on 128 NVIDIA V100 GPUs. Training instability is quantitatively measured using a 'loss ratio' metric (current loss / minimum previous loss) and correlated with the l1 norm and max element of Adam's variance state. Based on this analysis, the Sequence Length Warmup (SLW) method is proposed. SLW is implemented using a truncation-based approach where a linear pacing function determines the sequence length for each training batch, starting from a short `seqlens` and gradually increasing to the full `seqlene` over a duration `T`. A low-cost hyperparameter tuning strategy for `seqlens` and `T` is developed, relying on validation perplexity fluctuations during the initial training steps. The learning rate decay schedule is modified to be token-wise for fair comparisons.",
        "experimental_setup": "Experiments are conducted on GPT-2 (117M and 1.5B parameters) and GPT-3 (125M and 1.3B parameters) models. The GPT-2 experiments use a blend of Wikipedia, CC-Stories, RealNews, and OpenWebtext datasets, while GPT-3 experiments primarily use the Pile dataset, with additional CC-Stories and RealNews for the 1.3B model. All experiments utilize 128 NVIDIA V100 GPUs with mixed precision/FP16 training, Adam optimizer, 0.01 weight decay, and gradient clipping at 1.0. Baselines include standard GPT-2/3 training recipes and comparisons with Shortformer and Batch Size Warmup. Evaluation metrics include training loss curves, Adam variance norm/max element, validation perplexity (token-wise and time-wise), and zero-shot/few-shot accuracy on various downstream tasks such as WikiText-103, LAMBADA, HellaSwag, TriviaQA, WebQs, Winogrande, PIQA, ARC Challenge/Easy, ANLI R1/R2/R3, RACE-h, and BoolQ. The implementation is open-sourced in DeepSpeed.",
        "limitations": "The paper acknowledges that the root cause of the causal relationship between long sequences and training instability is not fully deciphered, making the assumption that shorter sequences are 'simpler' and lead to less 'noisy' gradients. The correlation analysis between instability and gradient variance outliers is empirical and not sufficient to prove a causal relationship, as other factors could also contribute to instability. Due to the high computational cost of pre-training, experiments are based on a single random seed, thus error bars are not reported.",
        "future_research_directions": "Future research directions include further deciphering the root cause of the causal relationship between long sequences and training instability. The authors also encourage more theoretical and practical studies into the important issue of training instability in large-scale model training. Exploring more complex and adaptive gradient/variance/activation clipping techniques is also suggested as a potential area for future work.",
        "experimental_code": "@dataclass\nclass GPTDatasetConfig(BlendedMegatronDatasetConfig):\n    random_seed: int\n    sequence_length: int\n\n    blend: Optional[Tuple[List[str], Optional[List[float]]]] = None\n\n    blend_per_split: Optional[List[Optional[Tuple[List[str], Optional[List[float]]]]]] = None\n\n    multiple_validation_sets: Optional[bool] = None\n\n    full_validation: Optional[bool] = None\n\n    split: Optional[str] = None\n\n    split_matrix: Optional[List[Tuple[float, float]]] = field(init=False, default=None)\n\n    num_dataset_builder_threads: int = 1\n\n    path_to_cache: Optional[str] = None\n\n    mmap_bin_files: bool = True\n\n    mock: bool = field(init=False, default=False)\n\n    tokenizer: Optional[MegatronTokenizerBase] = None\n\n    mid_level_dataset_surplus: float = 0.005\n\n    allow_ambiguous_pad_tokens: Optional[bool] = False\n\n    reset_position_ids: Optional[bool] = None\n\n    reset_attention_mask: Optional[bool] = None\n\n    eod_mask_loss: Optional[bool] = None\n\n    create_attention_mask: bool = True\n\n    drop_last_partial_validation_sequence: bool = True\n\n    add_extra_token_to_sequence: bool = True\n\n    object_storage_cache_path: Optional[str] = None\n\n\nclass GPTDataset(MegatronDataset):\n    def __getitem__(self, idx: Optional[int]) -> Dict[str, torch.Tensor]:\n        if idx is None:\n            text, _ = self._query_document_sample_shuffle_indices(0)\n        else:\n            text, _ = self._query_document_sample_shuffle_indices(idx)\n\n        text = torch.from_numpy(text).long()\n        if self.config.add_extra_token_to_sequence:\n            tokens = text[:-1].contiguous()\n            labels = text[1:].contiguous()\n        else:\n            tokens = text\n            labels = torch.roll(text, shifts=-1, dims=0)\n            labels[-1] = self._pad_token_id\n\n        attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(\n            tokens,\n            self.config.tokenizer.eod,\n            self.config.reset_position_ids,\n            self.config.reset_attention_mask,\n            self.config.eod_mask_loss,\n            self.config.create_attention_mask,\n        )\n\n        loss_mask[labels == self._pad_token_id] = 0.0\n        tokens[tokens == self._pad_token_id] = 0\n        labels[labels == self._pad_token_id] = 0\n\n        if idx is None:\n            loss_mask = torch.zeros_like(loss_mask)\n\n        if self.config.create_attention_mask:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"attention_mask\": attention_mask,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\n        else:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\n\ndef train_valid_test_datasets_provider(train_val_test_num_samples):\n    args = get_args()\n\n    print_rank_0('> building train, validation, and test datasets '\n                 'for GPT ...')\n    train_ds, _, test_ds = BlendedMegatronDatasetBuilder(\n        GPTDataset,\n        train_val_test_num_samples,\n        lambda: True,\n        GPTDatasetConfig(\n            blend=get_blend_from_list(args.data_path),\n            split=args.split,\n            random_seed=args.seed,\n            sequence_length=args.seq_length,\n            path_to_cache=args.data_cache_path,\n            return_document_ids=False,\n            mid_level_dataset_surplus=args.mid_level_dataset_surplus,\n        )\n    ).build()\n    print_rank_0(\"> finished creating finetuning GPT datasets ...\")\n\n    _, valid_ds, _ = BlendedMegatronDatasetBuilder(\n        GPTDataset,\n        train_val_test_num_samples,\n        lambda: True,\n        GPTDatasetConfig(\n            blend=get_blend_from_list(args.data_path2),\n            split=\"98,2,0\",\n            random_seed=1234,\n            sequence_length=2048,\n            path_to_cache=args.data_cache_path,\n            return_document_ids=False,\n            mid_level_dataset_surplus=args.mid_level_dataset_surplus,\n        )\n    ).build()\n    print_rank_0(\"> finished creating pretrained GPT datasets ...\")\n\n    return train_ds, valid_ds, test_ds\n\ndef get_batch(data_iterator):\n    args = get_args()\n    tokenizer = get_tokenizer()\n\n    keys = ['text']\n    datatype = torch.int64\n\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    data_b = mpu.broadcast_data(keys, data, datatype)\n\n    tokens_ = data_b['text'].long()\n    labels = tokens_[:, 1:].contiguous()\n    tokens = tokens_[:, :-1].contiguous()\n\n    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(\n        tokens,\n        tokenizer.eod,\n        args.reset_position_ids,\n        args.reset_attention_mask,\n        args.eod_mask_loss)\n\n    return tokens, labels, loss_mask, attention_mask, position_ids\n\ndef forward_step(data_iterator, model):\n    timers = get_timers()\n\n    timers('batch-generator').start()\n    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(\n        data_iterator)\n    timers('batch-generator').stop()\n\n    output_tensor = model(tokens, position_ids, attention_mask,\n                          labels=labels)\n\n    return output_tensor, partial(loss_func, loss_mask)",
        "experimental_info": "The methodology involves an in-depth analysis of GPT-2 pre-training tasks using the NVIDIA Megatron-LM pipeline. The provided repository content outlines a standard GPT training framework within Megatron-LM, particularly in 'examples/academic_paper_scripts/detxoify_lm/finetune_gpt.py' and 'megatron/core/datasets/gpt_dataset.py'.\n\nExperimental settings that are **directly related** to the method description:\n-   **GPT-2 Pre-training Tasks**: The code from 'finetune_gpt.py' demonstrates setting up a GPT model for pre-training/fine-tuning by defining a `model_provider` and utilizing the `pretrain` function, which is central to the Megatron-LM pipeline. This implies the general context of pre-training GPT-like models.\n-   **NVIDIA Megatron-LM Pipeline**: The entire extracted code uses Megatron-LM's `GPTModel`, `get_args`, `get_tokenizer`, `mpu`, `BlendedMegatronDatasetBuilder`, and `pretrain` components, confirming the use of the specified pipeline.\n\nSpecific implementations of the following experimental details were **not directly found** in the provided repository content:\n-   **Sequence Length Warmup (SLW) Method**: The method describes a 'truncation-based approach where a linear pacing function determines the sequence length for each training batch'. While `GPTDatasetConfig` includes `sequence_length` (passed via `args.seq_length` in `train_valid_test_datasets_provider`) and `get_batch` dynamically truncates input to a fixed length (`tokens_[:, :-1]`), there is no explicit linear pacing function or dynamic adjustment of this `sequence_length` per batch during a training run visible in the provided code. The code currently reflects a static `sequence_length` configuration.\n-   **Training Instability Measurement**: The 'loss ratio' metric (current loss / minimum previous loss) and its correlation with the l1 norm and max element of Adam's variance state are not explicitly implemented or measured in the provided code snippets.\n-   **Low-cost Hyperparameter Tuning Strategy**: The hyperparameter tuning for `seqlens` and `T` based on validation perplexity fluctuations during initial training steps is not present.\n-   **Token-wise Learning Rate Decay Schedule**: A modified learning rate decay schedule to be token-wise is not explicitly shown."
      }
    },
    {
      "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
      "abstract": "Recent works have demonstrated great success in pre-training large-scale\nautoregressive language models on massive GPUs. To reduce the wall-clock\ntraining time, a common practice is to increase the batch size and learning\nrate. However, such practice is often brittle and leads to a so-called\nstability-efficiency dilemma: increasing the batch sizes and learning rates\nleads to better training efficiency but can also result in training\ninstability, leading to poor generalization accuracy or failed runs. To better\nunderstand this phenomenon, we conduct an in-depth analysis on large-scale\npre-training experiments replicating the GPT-2 model. We find that there is a\nstrong correlation between training instability and extreme values of gradient\nvariance, and that samples with long sequence lengths contribute to these\nextreme gradient variance values, especially at the beginning of the training,\nindicating that long sequence length can be a main source of training\ninstability. Based on the analysis, we present a Sequence Length Warmup method\nthat aims to solve the training stability-efficiency dilemma. Experiments\nreplicating GPT-2 models show that our approach enables stable training with 8x\nlarger batch size and 4x larger learning rate, whereas the baseline approach\nstruggles with training instability. To achieve the same or better zero-shot\nevaluation results, our method reduces the required number of training tokens\nand wall clock time by up to 2.2x and 3.7x, respectively. Experiments\nreplicating GPT-3 model (125M) show that our approach enables stable training\nwith 8x larger batch size and 40x larger learning rate, and retains 99% of the\nzero-shot accuracy on 11 tasks using 10x less data and 17x less time compared\nto the original GPT-3 training recipe, while the baseline diverges under the\nsame settings and only retain 95% of accuracy under lower learning rate.",
      "full_text": "The Stability-Efﬁciency Dilemma: Investigating Sequence Length Warmup for Training GPT Models Conglong Li Microsoft conglong.li@microsoft.com Minjia Zhang Microsoft minjiaz@microsoft.com Yuxiong He Microsoft yuxhe@microsoft.com Abstract Recent works have demonstrated great success in pre-training large-scale autore- gressive language models (e.g., GPT-3) on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efﬁciency dilemma: increasing the batch sizes and learning rates leads to better training efﬁciency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model with public dataset. We ﬁnd that there is a strong correlation between training instability and extreme values of gradient variance. We further identify that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a simple yet effective Sequence Length Warmup method that aims to solve the training stability-efﬁciency dilemma by avoiding extreme gradient variance values. Moreover, we present a lightweight tuning strategy that allows us to tune our method with just a small portion of the expensive full training. Experiments replicating GPT-2 models (117M and 1.5B) show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate. 1 Introduction Large-scale Transformer-based language models have achieved great success in many natural lan- guage processing tasks [46, 11]. Among them, large-scale autoregressive models, such as GPT-3 [6], have attracted lots of attention due to their superior performance on zero-shot generalization, i.e., they can perform a wide range of tasks that they are not explicitly trained on. However, pre-training GPT models raises huge challenges on training efﬁciency and less-discussed training instability issues. On the efﬁciency side, as the model size continues to grow from a few hundreds of millions (e.g., GPT [31]), to billion-scale parameters (1.5B GPT-2 [33]), and to more recent hundreds of billions of parameters (175B GPT-3 [6]), the training cost also increases exponentially: it requires 9.2 days This paper was previously titled “Curriculum Learning: A Regularization Method for Efﬁcient and Stable Billion-Scale GPT Model Pre-Training” in an early arxiv preprint version[22]. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2108.06084v4  [cs.LG]  16 Oct 2022on 512 V100 GPUs to train a 8.3B GPT-2 [40], and 47.8 days on 2240 A100 GPUs to train a 530B GPT-3-style model [26]. Such a long training time makes it almost infeasible for most research labs and practitioners to reproduce these models. Various solutions have been proposed to reduce the training wall clock time of these large models [40, 25, 35]. However, many solutions require using more GPUs or sophisticated system techniques. In this work, we investigate speeding up the pre-training of GPT-style models via exploiting data efﬁciency, not at the cost of excessive hardware resources. In particular, in a distributed training environment, increasing the batch sizes and/or using more aggressive learning rates can make the model converge faster [41]. However, it has been observed that larger batch sizes and learning rates can make large-scale GPT model training more difﬁcult, e.g., causing training instability that leads to divergence or slow convergence [49, 9]. To investigate this training instability issue, we conduct a thorough study of the GPT-2 pre-training task [33, 40] with different models sizes under various batch sizes, learning rates, and sequence lengths. We ﬁnd a stability-efﬁciency dilemma: • A larger batch size (and larger learning rate) increases the per-iteration computational efﬁciency but with increasing likelihood of training instability and even divergence. • A smaller batch size makes the training more stable but decreases the per-step computation efﬁciency signiﬁcantly. We ﬁnd it difﬁcult to overcome this dilemma by existing techniques such as extra gradient clipping. More recent proposed techniques such as batch size warmup proposed in [6] does not provide stability beneﬁt in our evaluations. Recently, Shortformer[30] shows that by adding an additional ﬁrst training stage with a shorter sequence length, language models can achieve the same dev. set perplexity with shorter total training time. However, (1) its main focus is to purely improve training efﬁciency instead of solving the efﬁciency-stability dilemma, and our evaluations show that Shortformer’s 2-stage approach is insufﬁcient for overcoming the training instability issue for large models (Section 5.1), (2) it is primarily evaluated on small scale transformer models (247M) on WikiText datasets (103M tokens) without considering large-scale generative models like GPT with hundreds or even billions of parameters, and (3) it does not discuss how to choose some of the important hyperparameters, which is very expensive to ﬁgure out for large-scale model training. Inspired by Shortformer, we investigate the importance of sequence length in training GPT models and ﬁnd that it plays an important role in both training stability and efﬁciency. Based on our investigation, we propose a simple yet effective method called Sequence Length Warmup (SLW), which starts training with short sequences and gradually increases the length. We observe that our approach enables stable and efﬁcient training with much larger batch sizes and learning rates than baseline approaches. Speciﬁcally, we make the following contributions: (1) We conduct an extensive study of the GPT-2 pre-training task, which provides detailed insights about the training stability-efﬁciency dilemma, the correlation between instability and gradient variance outliers, and how sequence length plays a critical role (Section 3). (2) Based on the study, we present a simple yet effective sequence length warmup method for GPT-style model (and autoregressive model in general) that enables stable training with improved training efﬁciency. We also identify a lightweight hyperparameter tuning strategy for the approach, which identiﬁes promising hyperparameters by only incurring a small fraction of the expensive total pre-training cost (Section 4). The implementation of our approach as well as the necessary changes to the GPT-2/3 pre-training framework has been open sourced in a deep learning optimization library called DeepSpeed1. (3) We conduct large-scale experiments to demonstrate the proposed work’s ability to provide superior training stability and efﬁciency at the same time (Section 5). Our empirical results show that: • SLW enables stable and efﬁcient training with 8x larger batch size and 4x larger learning rate on GPT-2 (117M and 1.5B) models with public datasets, while the baseline and related works struggle with instability under the same settings. To achieve the same or better zero-shot WikiText- 103/LAMBADA evaluation results at the end of training, SLW reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. • On GPT-3 model (125M) pre-training we study an even more aggressive training scenario where only 10% of data can be used. Our method, with 8x larger batch size and 40x larger learning rate than the original GPT-3 training recipe, is able to maintain the training stability, retaining 99% of the zero-shot accuracy on 11 evaluation tasks, and use 10x less data and 17x less time. Without our 1https://github.com/microsoft/DeepSpeed, https://www.deepspeed.ai/ 2method, the baseline has unrecoverable divergence under the same settings, and can only retain 95% of the zero-shot accuracy after lowering learning rate to 30x. 2 Background and Related Work Language Model Pre-training: The accuracy of transformer-based language models grows substan- tially with its model size [31, 33, 6]. Today, a large language model such as GPT-3 [6] contains up to 175B parameters, and recent studies show that model accuracy can continue to improve with even larger model sizes [16]. However, training these large models often incurs excessively long training time and training difﬁculties [6]. Therefore, there are a lot of demands of performing efﬁcient and stable training for these models. To have the pre-training ﬁnished in a reasonable amount of time, the most common way is to leverage data parallelism to train models on multiple GPUs. However, the speedup gains often saturate beyond a few tens of GPUs, because communication becomes the major bottleneck, i.e., the workers will spend more time communicating gradients than computing them, as the number of GPUs increases. To mitigate this bottleneck, recent works such as 1-bit Adam [43] have studied gradient compression and demonstrate their effectiveness against auto-encoding models such as BERT [11]. An alternative approach to alleviate these overheads is to use large batch sizes. For example, LAMB [52] and 1-bit LAMB [21] enable stable and efﬁcient distributed BERT pre-training with batch size up to 64K/32K (for sequence length 128/512, i.e., 8M/16M tokens per batch) while maintaining the sample-wise convergence speed. For encoder-decoder models, T5 [34] uses batch size up to 2K (for sequence length 512, i.e., 1M tokens per batch). For autoregressive models such as the GPT family [ 31, 33, 6], existing works use batch size up to 1.6K (for sequence length 2K, i.e, 3.2M tokens per batch). Despite the beneﬁt of reduced communication overhead, large-batch training is sensitive to hyperparameters and often leads to issues such as slow convergence, training instabilities, and model divergence. For example, recently a research project shared that they are dealing with challenging training instability issues when pre-training a 104B GPT-style model with batch size 2K [49], and another work on a 540B model with batch size 2K observed spikes in the loss roughly 20 times during training, despite the fact that gradient clipping was enabled [9]. Curriculum Learning: Our method can be viewed as a kind of curriculum learning (CL) [12, 39, 2], which presents easier/simpler examples earlier during training and gradually increases the sample difﬁculties2. Comparing with traditional CL works which focus on solely improving the convergence speed under the same batch size, learning rate and other hyperparameters, our work is motivated by the stability-efﬁciency dilemma and we aim to achieve both efﬁcient convergence and better stability by enabling stable training with more aggressive hyperparameters. To our knowledge, we are the ﬁrst to investigate and conﬁrm that certain curriculum learning method can provide a dual stability-efﬁciency beneﬁt. In the NLP area, most of the curriculum learning works focus on small-scale one-stage tasks and downstream ﬁne-tuning tasks, such as neural machine translation (NMT) [ 18, 5, 56, 29, 57] and natural language understanding (NLU) [36, 37, 44, 50]. There are also a few works explore curriculum learning for language model pre-training [ 30, 55, 7]. These works show that curriculum learning can improve convergence speed, reduce training time, and improve accuracy under the same training hyperparameters as baseline. In these works, the curriculum difﬁculty metrics for each training sample are usually deﬁned as the sentence length, vocabulary frequency, the inference loss on smaller/cheaper models, or based on self-paced learning [19]. For the pacing function (i.e., to decide the curriculum difﬁculty range when sampling next training data batch), these works usually use ﬁxed predeﬁned functions (e.g., gradually increase difﬁculty upper bound by linear, root, and exponential functions), bucketing heuristics (group data with similar difﬁculties, and sample from a subset of buckets every time), or based on self-paced learning. 3 GPT-2 Pre-training Stability-Efﬁciency Analysis In this section we perform an in-depth analysis of pre-training tasks (without our method) replicating the GPT-2 models with public data. We follow the training pipeline from the NVIDIA Megatron-LM work [40]3. All of the experiments are performed on 128 NVIDIA V100 GPUs (32GB memory). There are 16 nodes and 8 GPUs per node. GPUs inside the same node are connected by NVLink 2.0, and nodes are connected by a 100 Gigabit InﬁniBand EDR inter-node network. We evaluate 2The shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. 3https://github.com/NVIDIA/Megatron-LM 30 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 1: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. In Appendix A.1 we plot the same ﬁgure but zoom in the ﬁrst 30B tokens. Table 1: Measuring training instability by the ratio between the current step training loss and the minimum loss among all previ- ous steps. Larger ratios (esp. those greatly larger than 1.0) indicate larger training insta- bility/divergence. Proposed work (SLW) and related works (last two rows) are discussed in Section 5. #steps with max Pre-training loss ratio > 1.2 loss Case parameters (% of total steps) ratio 117M: 1: Baseline bsz512-seqlen1K 0 (0.0%) 1.05 2: SLW 60K bsz512-seqlen1K 0 (0.0%) 1.06 3: Baseline bsz4K-seqlen1K 22 (0.06%) 1.42 4: SLW 20K bsz4K-seqlen1K 0 (0.0%) 1.02 5: Baseline bsz512-seqlen2K 0 (0.0%) 1.04 6: SLW 110K bsz512-seqlen2K 0 (0.0%) 1.04 1.5B: 7: Baseline bsz512-seqlen1K 114 (0.04%) 2.10 8: SLW 270K bsz512-seqlen1K 0 (0.0%) 1.06 9: Baseline bsz4K-seqlen1K 1381 (3.68%) 5.65 10: SLW 45K bsz4K-seqlen1K 0 (0.0%) 1.02 11: Shortformer bsz4K-seqlen1K 219 (0.4%) 2.86 12: Bsz Warmup bsz4K-seqlen1K 1179 (2.01%) 4.32 two GPT-2 model sizes from the original GPT-2 work [33]: 117M parameters (12 layers, 768 hidden size, 12 attention heads) and 1.5B parameters (48 lay- ers, 1600 hidden size, 25 attention heads). For training data, we collect and use the same dataset blend as the Megatron-LM work: Wikipedia [11], CC-Stories [45], RealNews [54], and OpenWebtext [32]. We evaluate two sets of training parameters. The ﬁrst set follows the Megatron-LM work: batch size 512, 300K total training steps (157B tokens), and learning rate 1.5 ×10−4 with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 297K steps (1 ×10−5 min. learning rate). The second parameter set tests a more aggressive training strategy: batch size 4K (8×larger), 37.5K total training steps (157B tokens4), and learning rate6×10−4 (4×larger) with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 34.5K steps (same min. learning rate). For sequence length/context size, we mainly use 1K which is the default for GPT-2. But we also test 2K (on the 117M model with batch size 512 and 157B tokens) which is the default for GPT-3. All experiments are performed with mixed precision/FP16 training, Adam optimizer (β1 = 0.9, β2 = 0.999, ϵ= 1×10−8) [17], 0.01 weight decay, same random seed, and gradient clipping at 1.0. For both batch sizes we use the same number of gpus (128). It is true that under fewer nodes, smaller batch sizes can also achieve good computation efﬁciency. However, in practice, the goal of a training task is usually \"given a ﬁxed number of hardwares, how to train the model in the fastest wall clock time\". And given the increasing model sizes, pre-training on hundreds of GPUs is not uncommon. Thus we believe that using the same hardware resources is a fair comparison. The stability-efﬁciency dilemma: Figure 1(a) and 1(b) present the training loss curves of 5 baseline cases under different model sizes, batch sizes (and LR), and sequence lengths. At 117M, the baseline has a few training loss spikes at batch size 4K. At 1.5B, the baseline has many loss spikes when training with either batch size 512 or 4K. As an indicative measurement to quantitatively study training instability, we deﬁne “loss ratio” which measures the ratio between the current step training loss and the minimum loss among all previous steps. A ratio larger than 1.0 means that current step’s loss is larger than the previous minimum loss, thus larger ratio indicates a larger loss spike and training instability. Table 1 summarizes the number of steps with loss ratio larger than 1.2, and the 4For pre-training it is common to keep the number of training tokens the same for fair comparison. 4Table 2: Zero-shot evaluation of the trained models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case 2 to 9 are compared with case 1, and case 11 to 17 are compared with case 10. Proposed work (SLW) and related works (16, 17) are discussed in Section 5. Pre-training Training Training Training WikiText LAMBADA Case parameters steps tokens time PPL ↓ accuracy ↑ 117M: 1: Baseline bsz512-seqlen1K 300K 157B 37Hr 27.78 33.19% 2: SLW 60K bsz512-seqlen1K 200K 89B (1.8x) 20Hr (1.9x) 27.74 34.78% 3: SLW 60K bsz512-seqlen1K 330K 157B (1x) 33Hr (1.1x) 27.01 34.41% 4: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 16Hr (2.3x) 28.09 32.54% 5: SLW 30K bsz4K-seqlen1K 37K 92B (1.7x) 10Hr ( 3.7x) 27.77 33.40% 6: SLW 30K bsz4K-seqlen1K 52.5K 157B (1x) 16Hr (2.3x) 27.15 34.16% 7: Baseline bsz512-seqlen2K 150K 157B (1x) 32Hr (1.2x) 28.19 32.99% 8: SLW 110K bsz512-seqlen2K 122.5K 71B ( 2.2x) 15Hr (2.5x) 27.06 33.24% 9: SLW 110K bsz512-seqlen2K 205K 157B (1x) 31Hr (1.2x) 26.03 34.58% 1.5B: 10: Baseline bsz512-seqlen1K 300K 157B 341Hr 13.89 57.29% 11: SLW 270K bsz512-seqlen1K 360K 122B (1.3x) 286Hr (1.2x) 13.89 57.38% 12: SLW 270K bsz512-seqlen1K 428K 157B (1x) 364Hr (0.9x) 13.88 57.89% 13: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 151Hr (2.3x) 14.76 55.06% 14: SLW 45K bsz4K-seqlen1K 50K 121B (1.3x) 121Hr (2.8x) 13.88 58.20% 15: SLW 45K bsz4K-seqlen1K 58.8K 157B (1x) 155Hr (2.2x) 13.72 58.47% 16: Shortformer bsz4K-seqlen1K 55K 157B (1x) 162Hr (2.1x) 14.14 57.23% 17: Bsz Warmup bsz4K-seqlen1K 58.8K 157B (1x) 165Hr (2.1x) 14.21 56.36% Reference 18: Original GPT-2 117M [33], different data 37.50 45.99% works: 19: Original GPT-2 1.5B [33], different data 17.48 63.24% 20: Megatron-LM GPT-2 355M [33], same data 19.31 45.18% 21: Megatron-LM GPT-2 2.5B [33], same data 12.76 61.73% maximum loss ratio during the training. At 117M model size only the baseline with batch size 4K has high loss ratios up to 1.421. At 1.5B model size the baseline with both batch size 512 and 4K has much more steps with large loss ratios, and with the maximum loss ratio as high as 5.65. Baseline with batch size 4K is less stable than baseline with batch size 512, indicating that larger batch sizes (combined with larger learning rates) could lead to more training instability risks. In Appendix A.3.1 we show that larger learning rates under the same batch size could also increase training instability. Training instability are undesirable because (1) it could lead to divergence that never recover as in [49] and our GPT-3 experiments (Section 5.2); (2) in our GPT-2 case it leads to worse convergence, validation loss, and zero-shot downstream task accuracy. Table 2 summarizes the zero-shot WikiText- 103/LAMBADA evaluation results. For both 117M and 1.5B models, increasing baseline’s batch size (and LR) or sequence length leads to training instabilities and loss spikes, and it requires a nontrivial number of training steps/tokens to recover the training loss back to a normal level (e.g., Figure 1(b)). These training “detours” slow down the learning and eventually lead to worse evaluation results (e.g., case 13 vs case 10 in Table 2). On the other hand, increasing batch size (and LR) or sequence length improves training efﬁciency, reducing the training time by up to 2.3x under the same number of training tokens (case 1, 4, 10, 13). Overall, the above observations demonstrate the stability-efﬁciency dilemma for baseline pre-training: the training is more stable and can achieve better ﬁnal generalization, but presumably with poorer training efﬁciency under smaller batch size/learning rate/sequence length; increasing them leads to better training efﬁciency, but with lower stability and worse generalization. The correlation between instability and gradient variance outliers: For stochastic gradient opti- mization, when the gradient variance is large, the algorithm might spend much time bouncing around, leading to slower convergence and potential divergence [48]. Previous studies show that variance reduction methods improve training stability in areas such as reinforcement learning [ 24, 8, 1]. Figure 1(c), 1(d), 1(e) and 1(f) plot the l1 norm and max element of Adam’s variance state (√vt, where vt = β2vt−1 + (1−β2)(gt)2)5. When baseline’s batch size increases, the max variance norm decreases but the max element increases. Comparing GPT-2 117M and 1.5B cases, larger model size leads to larger variance norm and max element. When sequence length increases for the GPT-2 117M case, the variance norm stays the same but the max element increases. To further study the link between instability and gradient variance, Figure 1(g) and 1(h) plot the loss ratio (deﬁned earlier in this section) and gradient variance norm/max element (all normalized by max value) for the most unstable 1.5B baseline with 4K batch size. Results show that when training loss spike happens and loss ratio increases, the gradient variance norm/max also increase (especially the max outliers). Table 3 presents the Pearson correlation coefﬁcient calculations, which demonstrate 5We use l1 norm to avoid outlier ampliﬁcation. 5Table 3: Pearson correlation coefﬁcient (with range (-1, 1)) between loss ratio and gradient variance norm/max. Low p-value indicates that the correlation is statistically signiﬁcant. Pearson correlation coefﬁcient p-value Loss ratio vs Gradient variance norm 0.23 0.0 Loss ratio vs Gradient variance max 0.26 0.0 0 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Steps 2 4 6 8 10 12 14Training loss Seqlen1K, baseline Seqlen128 Seqlen128(90%)+1K(10%) Figure 2: Step-wise training loss during GPT-2 1.5B pre-training (ﬁrst 10K steps only) with batch size 4K, comparing seqlen 1K (baseline), seqlen 128, and mixed seqlen of 128+1K (1K seqlen used at the cyan areas). a statistically signiﬁcant positive correlation between loss ratio and gradient variance norm/max. Overall, our analysis shows that training instability has a strong correlation with gradient variance norm and (especially) max element outliers. Length of early data sequences is critical to training stability: Aiming to solve the stability- efﬁciency dilemma we ﬁrst tried traditional methods such as increasing gradient clipping, but it does not fully resolve the instability issue (Appendix A.3.2). Seeing that in Figure 1 the training instability mostly happens at the ﬁrst half of training, we then explored whether we can solve the issue by gradually increasing any of the batch size/learning rate/sequence length during training. We already employed the same learning rate warmup mechanism used by existing GPT-2 and GPT-3 works [33, 40, 6]. We tried the batch size warmup method proposed in GPT-3 work [ 6], but the instability issue still appears when increasing the batch size (Section 5.1). Our investigation on the sequence length leads to interesting ﬁndings, where we ﬁnd that sequence lengths play a critical role in training instability. Figure 2 presents the training loss curve of the most unstable GPT-2 1.5B pre-training with batch size 4K and seqlen 1K, together with another two artiﬁcial settings: one with seqlen 128, the other with mixed seqlen where we feed 900 steps of seqlen 128 then 100 steps of seqlen 1K in every 1K steps. The seqlen 128 case has no instability issue, even with large model size/batch size/learning rate. The mixed seqlen case has instability issues, and (1) they mostly happen when we switch to seqlen 1K (e.g., at step 900, 1900, 2900...); (2) they mostly happen during the ﬁrst 5K steps, and after that it becomes more stable than the seqlen 1K case. These observations indicate that training instability is strongly correlated with early long sequence lengths, which motivates us to explore the sequence length warmup method described in the next section, and evaluations in Section 5 will demonstrate how this method provides a gradient variance reduction effect and solves the stability-efﬁciency dilemma in our experiments. 4 The Sequence Length Warmup Method The analysis in last section about training instability and sequence lengths motivates us to explore sequence length warmup methods: the model needs to start learning with short sequence length for more stable training, then gradually increase the length when training becomes more stable so that the model can still learn from longer contextual information to achieve better ﬁnal model accuracy. The sequence length warmup strategy depends on two factors: how to support variable sequence length during training and how to adaptively decide the sequence length for each iteration (the pacing function). For the ﬁrst component, we develop an efﬁcient truncation-based implementation: For the baseline GPT-2 pre-training, the raw text inputs are indexed into sequences with the same length before training, so that the model can efﬁciently retrieve a batch of ﬁxed-length sequences regardless of the actual sentence boundaries. It’s possible to index the text inputs based on all possible sequence lengths, but that adds signiﬁcant amount of overhead due to the massive pre-training data. To avoid the large indexing overhead, we take a lightweight approach: our implementation still lets the dataloader index the raw text into only the full sequence length. At each training step, our method uses pacing function to determine the sequence length and then truncates the full-length sequences to obtain a modiﬁed version of the mini-batch for training. It is true that this truncation-based implementation will drop some data in the current step. However, with some implementation changes, it’s possible to record the index of dropped data and use them in future steps. We deﬁne the pacing function as a step-wise linear function with the following properties: Given a starting sequence length seqlens, an ending sequence length seqlene (full sequence length), and a total duration T (number of steps), the sequence length used for the training batch at step tis seqlent = seqlens + (seqlene −seqlens) ×min( t T ,1). Besides step-wise linear, we also explored 3 other pacing functions: i) We tried a discrete 2-stage pacing function from [ 30], but it leads 6to unstable training and worse convergence (Section 5.1). ii) We tried a step-wise root function (seqlent = seqlens + (seqlene −seqlens) ×min(( t T )r,1), where r is the root degree), which performs similar to linear but requires one extra hyperparameter. iii) We tried an adaptive pacing function based on training/validation losses, which also performs similar and requires extra tuning. 0 2K 4K 6K 8K 10K Steps 0 250 500 750 1000 1250 1500 1750 2000Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (a) Step-wise validation per- plexity (beginning of training) 0 50B 100B 150B Tokens 20.0 20.2 20.4 20.6 20.8 21.0Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (b) Token-wise validation per- plexity (end of training) Figure 3: Validation perplexity during GPT-2 117M seqlen 1K pre-training with batch size 512 and different duration T. (“SLW 20K” means proposed approach withT=20K steps). Pacing function analysis and tuning strategy: To study the impact of our approach’s pacing function, we set the starting sequence length (seqlens) ﬁxed at 8 and perform a grid search for the pacing function duration ( T) on the GPT-2 117M case full training (de- tails in Appendix A.4). After full train- ings we perform evaluation on pretrain data test set and WikiText/LAMBADA zero-shot tasks to conﬁrm which du- ration T provides the best accuracy performance. All the cases have quite comparable evaluation results, indicat- ing that the performance is not very sensitive to the duration T within a reasonable range. This grid search sheds light on a low-cost tuning strategy: we ﬁnd that (for GPT-2 117M training with batch size 512 and 4K) the best duration T is the longest duration that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst 10K steps (i.e., a few multiples of the LR warmup steps). In on our study, this “signiﬁcant ﬂuctuation” can be deﬁned as “whether the perplexity value becomes larger than 1.3x of the previous best perplexity”. In Figure 3(a) the SLW 60K is the longest duration we tested that does not have signiﬁcant validation ﬂuctuation during the ﬁrst 10K steps. In Figure 3(b) and Appendix A.4 SLW 60K does provide the best ﬁnal validation perplexity, best ﬁnal test perplexity, and second best eval results. Since it does not require training the model until full convergence, this heuristic greatly reduces the hyperparameter tuning cost of our approach. Another grid search on the starting sequence length seqlens shows that it’s generally better to set it as small as possible, to maximize the stability and convergence speedup beneﬁt. However, if the validation perplexity has signiﬁcant ﬂuctuation near the starting sequence length, increasing seqlens would lead to better convergence. Overall, the low-cost tuning strategy can be summarized as: (1) Start with seqlens = 8and T = a few multiples of LR warmup steps. (2) Increase seqlens until the validation perplexity no longer has signiﬁcant ﬂuctuation at the very beginning. (3) Perform a binary search to ﬁnd the largest T that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst few multiples of LR warmup steps. This tuning strategy relies only on the validation set and does not require test set or downstream task evaluation. For the GPT-2 1.5B and GPT-3 125M models, we used this strategy to tune T and seqlens for the pacing function, and results show that this low-cost tuning strategy could provide similar stability-efﬁciency beneﬁt as grid search on full training runs (GPT-2 117M case). 5 Evaluation 5.1 GPT-2 experiments For GPT-2 model, dataset, and hardware, we follow the same methodology in Section 3. For proposed work’s pacing function conﬁgurations (Section 4), we useseqlens = 8/64 (for 117M/1.5B model based on tuning) and seqlene = 1K/2K(full sequence length). To fully utilize the NVIDIA Tensor Core acceleration, we add a seqlent = seqlent −(seqlent mod 8) postprocessing to make sure the sequence length is always a multiple of 8. For the total duration T, we tune this parameter (grid search for 117M and low-cost tuning for 1.5B) for each case. For the training parameters, for our approach we use the same shared parameters as the baseline except two parameters: 1) Because during sequence length warmup the number of tokens in a data batch is smaller, we modify the training termination condition so that all cases stop when reaching the same 157B training tokens. 2) Because of 1), proposed approach now has more training steps, which make it necessary to modify the learning rate decay schedule to have a fair comparison with the baseline. We change the learning rate decay to token-wise over the 157B tokens (still cosine decay) instead of step-wise over the total number of steps. We describe the underlying rationale in Appendix A.2. 70 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (a) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2500 5000 7500 10000 12500 15000 17500Variance norm Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (e) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2000 4000 6000 8000 10000 12000 14000Variance norm Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (h) Step-wise Adam vari- ance max element Figure 4: Validation perplexity and Adam variance norm/max element during GPT-2 1.5B seqlen 1K pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR. Also compare with related works (“Shortformer” [30] and “Bsz Warmup” [6]) at 2nd row. Each row of subﬁgures share the same legend (“SLW 45K” means our work withT=45K steps). Based on the following observations, we demonstrate that our approach resolves the dilemma and simultaneously improves the stability and efﬁciency. We will mainly present the GPT-2 1.5B results and leave some GPT-2 117M results in Appendix. Signiﬁcant stability gain: In Section 3 Table 1 we discussed how we measure the training instability based on the “loss ratio” metric, which shows that the baseline becomes less stable under larger model size/batch size/learning rate/sequence length. Comparing with baseline and proposed work in this table shows that our work reduces this instability measurement to zero in all cases, together with max ratio close to 1.0 (no spike). This demonstrates the signiﬁcant stability gain by our method. Faster token-wise and time-wise convergence: Figure 4(a) and 4(b) present the validation perplex- ity curves during GPT-2 1.5B seqlen 1K pre-training, comparing baseline and our approach. When the batch size increases from 512 to 4K for baseline, the time-wise convergence becomes faster but the token-wise convergence becomes slower and poorer. On the other hand, our approach at batch size 4K provides faster and better convergence both token-wise and time-wise comparing with the best baseline curve in each case. The shape of SLW’s curves is different (worse than baseline in early stage) because SLW cases initially only learn from shorter sequences, which limit the validation perplexity it can reach (since validation data is always full-length). On the other hand, when SLW cases start to learn from longer sequences, the validation perplexity drops faster than baseline (and related works) and eventually surpasses them. Our approach with batch size 512 provides smaller convergence speedup because (1) Baseline with batch size 512 has less instability issue, limiting the gain from the proposed approach; (2) At batch size 512 the communication overhead is very high, and our approach takes more steps (i.e., communication rounds) than baseline to reach the same 157B training tokens. This extra communication cost “cancelled” part of the time saving from our approach. For GPT-2 117M, our approach provides similar token-wise and time-wise convergence speedup (Appendix A.4). Advancing cost-quality Pareto curve: In Section 3 Table 2 we discussed about baseline’s zero-shot evaluation results. For proposed work eval results in this table, we present them in two ways: one evaluated at the earliest checkpoint that provides better eval results than baseline (batch size 512 and seqlen 1K); the other one evaluated at the end of full training. Results show that our approach is able to advance the cost-quality Pareto curve in two ways: (1) To reach the same eval result quality as baseline, our approach reduces the required number of pre-training tokens and wall clock time by up to 2.2x and 3.7x, respectively; (2) Under the same 157B training tokens, our approach can further improve the eval result quality. In (1) the time-wise saving is higher than the token-wise because (a) For each Transformer block, the self-attention and intermediate layers have time complexity of O(B×L2 ×H) and O(B×L×H2), respectively6. The proposed method uses shorter sequences at 6B, L, Hrepresent batch size, sequence length, hidden size. 8the beginning, reducing the time complexity quadratically for the self-attention sub-layer and linearly for the intermediate sub-layer of Transformer blocks; (b) By enabling stable training at larger batch size, our approach achieves additional time-wise saving by reducing the communication overhead. Variance reduction helps stabilize training: In Section 3 we discussed the strong correlation between training instability and gradient variance norm/max. Figure 4(c) and 4(d) demonstrate that proposed approach stabilizes training and reduces both the Adam variance norm and the variance max element. Importantly, it avoids all the spikes of the variance max element, which all happen to be where the baseline has training loss spikes. One may wonder why gradient clipping cannot help avoid these extreme gradient variance outliers. Although gradient clipping can avoid large gradient at every single step, it cannot avoid the gradient variance getting accumulated from multiple steps (Appendix A.3.2). Comparing with related works: We now compare the proposed work with two related works on the most challenging “1.5B model + batch size 4K” case. The ﬁrst work is the Shortformer where the ﬁrst stage uses shorter sequences and the second stage uses full-length sequences [30]. Following the grid search in the paper, we use seqlen 128 for the ﬁrst stage and set its duration at about half of the baseline duration (20K steps). The second work is the batch size warmup technique used by GPT-3 [6], where we set the starting batch size at 128 and then gradually increase it to 4K, and set the warmup duration same as the proposed work. Other training hyperparameters are unchanged. Figure 4(e) to 4(h) present the results. Both related works provide convergence speedup but it is less than our work. More importantly, they still have training instability issues. The Shortformer has an obvious training divergence at step 20K when the sequence length switches from 128 to 1K (the spike at 20K in Figure 4(h)). This shows when staying at the same shorter sequence length for too long, the model becomes heavily overﬁtted for that length which leads to divergence risk when/after switching to full length. Although both batch size warmup and our method reduce the number of tokens per batch in a similar fashion, batch size warmup does not provide any training stability beneﬁt compared to the baseline. This indicates that providing the same number of shorter (simpler) sequences leads to better training stability than providing fewer number of same length (same difﬁculty) sequences. In addition, batch size warmup has a limitation that the batch size must be multiple of data-parallel size. On the other hand, for our method the sequence length only needs to be multiple of 8 to enable Tensor Core acceleration. These two limitations are different: for the proposed SLW method, the “multiple of 8” limitation is ﬁxed and unrelated to data-parallel size. For batch size warmup it’s a dynamic \"multiple of data-parallel size\" limitation: since nowadays pre-training tasks are performed on up to thousands of GPUs, the data-parallel size can easily go beyond 100, prohibiting ﬂexible conﬁguration of the method (or requires reducing the number of GPUs when using smaller batches, increasing the training clock time). Last but not least, both related works provide non-zero “loss ratio” in Table 1 and worse zero-shot evaluation results in Table 2. 5.2 GPT-3 experiments For experiments replicating the GPT-3 125M model [6] using the Pile public dataset [13], ﬁrst we reproduce the original GPT-3 training recipe: 300B training tokens, seqlen 2K, batch size 256 with batch size warmup (start with 16 then gradually increase to 256 in ﬁrst 4B tokens), learning rate 6 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (6 ×10−5 min. learning rate) 7. Then we explore an aggressive training scenario where only 30B tokens (10%) are allowed. This is because (1) GPT-3 paper admits that it has poor training sample efﬁciency and it sees much more text during pre-training than a human sees in the their lifetime [6, 23]. (2) There could exist cases where the total amount of data/computation resource is limited. We adjust several hyperparameters in this 30B-token training: 8x batch size (2K) for better training efﬁciency, learning rate decay reduced to 30B tokens (based on study that LR schedule should match total training tokens [14], warmup stays at 375M), min. learning rate reduced to 0 (based on recent study on GPT-3 [51]). For baseline we keep using 4B-token batch size warmup, but when our method is used (seqlens = 72, T = 11.5Kbased on tuning) we disable it since both methods reduce tokens per batch. And for both cases we tune the learning rate and use the highest one that provides stable training, which is 30x (1.8×10−2) for baseline and 40x (2.4×10−2) for our method. All experiments are performed on 128 V100 GPUs. Figure 5 and 6 present the training loss and gradient variance max for the GPT-3 pre-training experiments. When applying 40x learning rate to the baseline (batch size warmup), it quickly 7Different from GPT-2, GPT-3 uses token-based learning rate schedule and we follow it. 9Table 4: Zero-shot evaluation of the trained GPT-3 125M models on 11 tasks used by the original GPT-3 work [6]. Per- task eval results in Appendix A.5. Batch Training Training Average Case size tokens time accuracy ↑ 1: Original [6] 256 300B 33.6 2: Baseline repro 256 300B (1x) 61Hr 31.4 3: Baseline 30x LR 2K 30B (10x) 7Hr (9x) 29.8 (95%) 4: SLW 40x LR 2K 30B (10x) 3.5Hr (17x) 31.1 (99%) 0 250M 500M 750M 1B Tokens 2 3 4 5 6 7 8 9 10Training loss Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 5: Training loss during GPT-3 125M pre-training (ﬁrst 1B to- kens). 0 250M 500M 750M 1B Tokens 0.00 0.05 0.10 0.15 0.20 0.25Variance max Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 6: Gradient vari- ance max during GPT-3 125M pre-training (ﬁrst 1B tokens). diverges and cannot continue to train due to NaN losses. The corresponding gradient variance max element becomes a ﬂat line after divergence because the gradients on all dimensions are so large that all gradients get clipped including the max element. After lowering the learning rate to 30x, the baseline is able to ﬁnish the whole training, but it can only retain 95% of average zero-shot accuracy on 11 tasks (HellaSwag [53], LAMBADA [28], TriviaQA [15], WebQs [3], Winogrande [38], PIQA [4], ARC Challenge/Easy [10], ANLI R1/R2/R3 [27]) compared with the case that reproduces the original GPT-3 training recipe8 as shown in Table 4. In contrast, our approach enables stable training with 40x learning rate, demonstrates lower gradient variance max outliers than baseline with 30x learning rate, retains 99% of the original training recipe’s average zero-shot accuracy, and achieves 10x data saving and 17x time saving.9 This demonstrates that the proposed method not only solves the stability-efﬁciency dilemma, but also opens a promising direction of signiﬁcantly reducing total training cost in a different data efﬁciency dimension. Finally, in Appendix A.6 we evaluate the proposed method on a larger GPT-3 1.3B model, including not only zero-shot but also few-shot evaluation. Results show that under the same 300B training tokens the proposed SLW method provides better average accuracy than the baseline for both zero- shot (from 41.6 to 41.9) and few-shot (from 44.8 to 45.3) tasks, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) can provide better accuracy performance. Similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup and proposed SLW method. 6 Limitation and Future Work Although our paper discovers the correlation between long sequences and training instability (and proposes a method to alleviate the issue), the root cause of this causal relationship is not fully deciphered and would be an interesting future work direction. One assumption we have is that the shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. We think encoding shorter sequences (especially at the early training stage when weight is still relatively random) will generate less \"noisy\" gradients thus leading to higher stability. This work presents correlation analysis on an empirical connection between training instability and gradient variance norm/max element, but it is not sufﬁcient to prove a causal relationship, and training instability could be caused by other factors. Overall, our work is one preliminary attempt to study the topic of training instability in large-scale model training, and we hope it could inspire future works to further study this important issue in both theory and practice. 7 Conclusion This paper presents the Sequence Length Warmup method, which solves a stability-efﬁciency dilemma inside GPT-style model pre-training, an critical issue that hinders efﬁcient language model pre-training as explained in our in-depth analysis. By enabling stable training on more aggressive training recipe, this method also motivates a new dimension of training cost reduction by improving the data efﬁciency, as demonstrated by the 10x data and 17x time saving in our GPT-3 experiments. We believe that the effectiveness, simplicity, and easy-to-use/tune make the proposed method a must-try for deep learning practitioners, and we hope this work could motivate more studies on improving training data efﬁciency. 8Our reproduced GPT-3 has 2.2 point lower average accuracy than the original GPT-3, which is because of the different training data and OpenAI employed many data processing techniques [6] 9We want to emphasize here that the SLW method “only retains 99% of accuracy” because this experiment is an extreme case: only 10% of original training data is used during training. 10References [1] Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. In International Conference on Machine Learning, pages 176–185. PMLR, 2017. [2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48, 2009. [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artiﬁcial intelligence, pages 7432–7439, 2020. [5] Ondˇrej Bojar, Jindˇrich Helcl, Tom Kocmi, Jindˇrich Libovick`y, and Tomáš Musil. Results of the wmt17 neural mt training task. In Proceedings of the second conference on machine translation, pages 525–533, 2017. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020. [7] Daniel Campos. Curriculum learning for language modeling. arXiv preprint arXiv:2108.02170, 2021. [8] Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick. Control regularization for reduced variance reinforcement learning. In International Conference on Machine Learning, pages 1141–1150. PMLR, 2019. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. [12] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99, 1993. [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. [17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. 11[18] Tom Kocmi and Ondˇrej Bojar. Curriculum learning and minibatch bucketing in neural machine translation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 379–386, 2017. [19] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS, volume 1, page 2, 2010. [20] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [21] Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit LAMB: Communication Efﬁcient Large-Scale Large-Batch Training with LAMB’s Convergence Speed. arXiv preprint arXiv:2104.06069, 2021. [22] Conglong Li, Minjia Zhang, and Yuxiong He. Curriculum learning: A regularization method for efﬁcient and stable billion-scale gpt model pre-training. arXiv preprint arXiv:2108.06084, 2021. [23] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020. [24] Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh. Variance reduction for reinforcement learning in input-driven environments. In International Conference on Learning Representations, 2018. [25] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. [26] Microsoft and Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to- train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful- generative-language-model/, 2021. [27] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019. [28] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [29] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, and Tom M Mitchell. Competence-based curriculum learning for neural machine translation. In NAACL- HLT, 2019. [30] Oﬁr Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832, 2020. [31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [32] Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, and Ilya Sutskever. Better language models and their implications. OpenAI Blog, 2019. [33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza- tions toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020. [36] Mrinmaya Sachan and Eric Xing. Easy questions ﬁrst? a case study on curriculum learning for question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 453–463, 2016. 12[37] Mrinmaya Sachan and Eric Xing. Self-training for jointly learning to ask and answer questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 629–640, 2018. [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 8732–8740, 2020. [39] Terence D Sanger. Neural network learning control of robot manipulators using gradually increasing task difﬁculty. IEEE transactions on Robotics and Automation, 10(3):323–333, 1994. [40] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [41] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V . Le. Don’t decay the learning rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [42] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. [43] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit Adam: Communication Efﬁcient Large-Scale Training with Adam’s Convergence Speed. InProceedings of the 38th International Conference on Machine Learning, pages 10118–10129, 2021. [44] Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C Phan, Xingdi Yuan, Jinfeng Rao, Siu Cheung Hui, and Aston Zhang. Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4922–4931, 2019. [45] Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information processing systems, pages 5998–6008, 2017. [47] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019. [48] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. In NIPS, 2013. [49] Thomas Wolf. The engineering group in @bigsciencew ﬁghting training instabilities over +100b parameters. https://twitter.com/Thom_Wolf/status/1447565680384032776, 2021. [50] Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Curriculum learning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6095–6104, 2020. [51] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. [52] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. [53] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019. 13[54] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9054–9065, 2019. [55] Wei Zhang, Wei Wei, Wen Wang, Lingling Jin, and Zheng Cao. Reducing bert computation by padding removal and curriculum learning. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 90–92. IEEE, 2021. [56] Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. An empirical exploration of curriculum learning for neural machine translation. arXiv preprint arXiv:1811.00739, 2018. [57] Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. Curriculum learning for domain adaptation in neural machine translation. In NAACL-HLT, 2019. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] We have a section describing limitation and future work. (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed 10, and Section 3 and 5 include all instructions needed to reproduce the experiments. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Section 3 and 5 include all the training details. (c) Did you report error bars (e.g., with respect to the random seed after running ex- periments multiple times)? [N/A] Our results are based on a single seed given the pre-training is expensive. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Section 3 and 5 include the information. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 10https://github.com/microsoft/DeepSpeed 14(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 150 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 10B 20B 30B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 10B 20B 30B Tokens 0.00 0.02 0.04 0.06 0.08 0.10Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 7: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. A Appendix A.1 Zoom in of Figure 1 Figure 7 zoom in the ﬁrst 30B token in main paper Figure 1, where the training is the most unstable. A.2 Learning rate decay for proposed approach As discussed in main paper Section 5.1 GPT-2 experiments, proposed approach needs more training steps than baseline in order to reach the same 157B training tokens. This makes it necessary to modify the learning rate decay schedule for proposed approach. We ﬁrst tried to increase the number of learning rate decay steps by half of the proposed approach’s pacing function durationT (since the proposed approach roughly needs T/2 additional steps to reach 157B tokens). However, we ﬁnd that simply increasing decay steps still leads to faster learning rate decay than baseline. At last we change the learning rate decay to token-wise (same cosine decay over the 157B tokens) instead of step-wise. This is because for the proposed approach there are fewer tokens per step at the beginning. So even if we increase the LR decay steps, it still cannot avoid decaying faster token-wise at the beginning compared to baseline. As shown in Figure 8, the proposed approach with step-wise LR decay (with T/2 extra decay steps) has faster LR decay token-wise compared to baseline, which leads to a worse validation perplexity curve. On the other hand, the same proposed approach case with token-wise LR decay has the same token-wise LR decay schedule as baseline, which leads to better convergence. A.3 Additional analysis about training hyperparameters In main paper Section 4 we demonstrate that proposed approach’s two hyperparameters can be tuned with very low cost only running the very beginning of the training (the third hyperparameter, ending sequence length, does not require tuning since it will always be the full length). To understand more about how proposed approach affects the choice and tuning of normal training hyperparameters, this section provides additional analysis about learning rates and gradient clipping. Results demonstrate that (a) Compared to baseline, proposed approach requires less tuning effort on these hyperparameters to provide a stable training; (b) By enabling stable training on larger learning rates, proposed approach could provide better training efﬁciency and convergence (as demonstrated in main paper Section 5); (c) Tuning gradient clipping for baseline could not provide the same training stability as proposed approach. A.3.1 Learning rate In Section 5.1 we demonstrate that proposed approach can provide stable and more efﬁcient training at larger batch size and learning rate, where baseline suffers from training instability. We increased both batch size and learning rate at the same time because (a) Large-batch training is more efﬁcient 160 40B 80B 120B 160B Tokens 11.5 12.0 12.5 13.0 13.5 14.0Validation PPL Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (a) Token-wise validation perplexity 0 100K 200K 300K 400K Steps 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (b) Step-wise learning rate 0 50B 100B 150B Tokens 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (c) Token-wise learning rate Figure 8: Validation perplexity and learning rate during GPT-2 1.5B seqlen 1K pre-training with batch size 512, comparing the baseline and proposed approach under different learning rate decay schedules. “SLW 270K” means proposed approach withT=270K steps. 0 1K 2K 3K Steps 3 4 5 6 7 8 9 10 11 12Training loss Baseline LR=6e-4 Baseline LR=12e-4 SLW 8K LR=6e-4 SLW 8K LR=12e-4 Figure 9: Step-wise training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, seed 1236, and different learning rates for baseline and proposed approach (“SLW 8K” means proposed approach with T=8K steps). for large-scale distributed training, so larger batch was necessary in our study (b) In order to maintain the same convergence speed, it is necessary to simultaneously increase the learning rate under larger batch size. A well-known rule of thumb is that the learning rate should at least increase by the square root of the batch size’s increase ratio. As a controlled experiment, here we perform additional studies about what if we keep the batch size the same and only tune learning rate for baseline and proposed approach. We do not consider the case of “same learning rate, different batch sizes” due to the reason (b) above. Table 5 presents the number of steps with training loss ratios (deﬁned in main paper Section 3 as an indicative measurement of training instability) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K11, 5 different seeds, and different learning rates for baseline and proposed approach. And Figure 9 illustrates some of the cases with seed 1236 to show how the loss spikes look 11Batch size 2K is used here because this analysis was performed at an early stage of this work, and we do not have enough resource to rerun the same analysis with batch size 4K. 17Table 5: Number of steps with training loss ratios (deﬁned in Section 3) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, 5 different seeds, and different learning rates for baseline and proposed approach (SLW). Left/right number in each cell is for baseline/SLW, respectively. Baseline/SLW LR = LR = LR = LR = #loss ratio > 1.5 1.5 × 10−4 3 × 10−4 6 × 10−4 12 × 10−4 Seed 1234 0/0 296/0 359/0 179/74 Seed 1235 0/0 302/0 408/0 555/459 Seed 1236 0/0 0/0 569/0 626/414 Seed 1237 7/0 0/0 548/0 614/139 Seed 1238 0/0 0/0 121/0 394/29 Total 7/0 598/0 2005/0 2368/1115 like. Results show that proposed approach provides stable training during this ﬁrst 3K steps for all ﬁve seeds at learning rates up to 6 ×10−4, while baseline with seed 1237 still has 7 large loss ratios at learning rate as low as 1.5 ×10−4. At learning rate 12 ×10−4 both cases have large loss ratios, but proposed approach reduces the frequency by 2.1x. This demonstrates that (a) Larger learning rates lead to higher training instability risk for both cases. (b) With the same amount of tuning effort, proposed approach has a higher probability of providing a stable training because of the wider range of learning rates it enables; (c) Since proposed approach enables stable training at larger learning rate, it could provide better and faster training convergence as shown in main paper Section 5. A.3.2 Gradient clipping In main paper Section 5 we used gradient clipping at 1.0 (global gradient l2 norm is clipped to 1.0) following the previous work [40]. Here we perform additional studies about what if we apply more gradient clipping to baseline. Figure 10(a) presents the training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K (the same hyperparameters as the second set in Section 3), comparing the baseline and proposed approach under different gradient clipping levels12. Results show that when applying more gradient clipping to baseline, the training has less and smaller loss spikes. And the Adam varaince norm is also reduced as shown in Figure 10(c). However, more gradient clipping does not fully resolve the training instability issue. Even baseline with the lowest gradient clipping norm cannot avoid all training loss spikes, while proposed approach with default gradient clipping has no loss spike. As described in main paper, we believe that this is a limitation of common gradient clipping technique: Although gradient clipping can avoid too large gradient at every single step, it cannot avoid the gradient variance getting accumulated at certain dimensions (as shown in Figure 10(d)), especially for large batch sizes. Another concern about applying more gradient clipping is that the momentum norm is also reduced due to more clipping (Figure 10(b)). This indicates that when later the training reaches a more stable stage, more gradient clipping could hurt the convergence speed. On the other hand, proposed approach will not affect the convergence speed after the full sequence length is reached. Another thing to note is that proposed approach relies less on gradient clipping: at gradient clipping norm 1.0, baseline has 798 clippings in the ﬁrst 5K steps while proposed approach has 628 clippings (21% less). Overall, this analysis demonstrates that proposed approach requires less or no tuning on gradient clipping, while baseline still has training stability issue with more gradient clipping. It is possible that more complex and adaptive gradient/variance/activation clipping techniques could potentially achieve the same level of training stability as proposed approach. However, inventing and applying such techniques would require an effort no lower than the proposed approach, which is both easy to integrate and low-cost to tune. A.4 GPT-2 117M evaluation results Figure 11 presents the validation perplexity and Adam variance norm/max element during GPT-2 117M pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. Table 6 presents the zero-shot evaluation of the trained 117M models on 12We also tried less than 0.25 gradient clipping, which triggered a silent crash without error messages after around 100 steps. We did not have enough time to ﬁnd the root cause, but it could be caused by the too extreme gradient clipping. 180 1K 2K 3K 4K 5K Steps 2 4 6 8 10 12 14 16 18Training loss Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (a) Step-wise training loss 0 1K 2K 3K 4K 5K Steps 0 500 1000 1500 2000 2500 3000Momentum norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (b) Step-wise Adam momentum l1 norm 0 1K 2K 3K 4K 5K Steps 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (c) Step-wise Adam variance l1 norm 0 1K 2K 3K 4K 5K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06Variance max Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (d) Step-wise Adam variance max element Figure 10: Training loss, Adam momentum l1 norm, and Adam variance l1 norm/max element during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K, comparing the baseline and proposed approach under different gradient clipping levels. Grad clip 1.0 indicates that the global gradient l2 norm is clipped to 1.0. ‘SLW 45K” means proposed approach withT=45K steps. 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (a) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (e) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (h) Step-wise Adam vari- ance max element Figure 11: Validation perplexity and Adam variance norm/max element during GPT-2 117M pre- training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. “SLW 60K” means proposed work withT=60K steps. the WikiText-103 and LAMBADA datasets for baseline and proposed work with different pacing function duration. A.5 GPT-3 125M evaluation results Table 7 presents the zero-shot evaluation of the trained GPT-3 125M models on the 11 tasks used by the original GPT-3 work [6]. 19Table 6: Zero-shot evaluation of the GPT-2 117M models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case Pre-training Pre-training Pre-training WikiText-103 LAMBADA parameters steps, tokens, time test perplexity ↓ perplexity ↓ accuracy ↑ 1: Baseline bsz512-seqlen1K 300K, 157B, 37Hr 20.75 27.78 33.19% 2: SLW 20K bsz512-seqlen1K 310K, 157B, 30Hr 20.49 27.43 34.60% 3: SLW 60K bsz512-seqlen1K 330K, 157B, 33Hr 20.11 27.01 34.41% 4: SLW 100K bsz512-seqlen1K 350K, 157B, 35Hr 20.16 26.91 34.21% 5: SLW 140K bsz512-seqlen1K 370K, 157B, 35Hr 20.17 27.17 33.92% 6: Baseline bsz4K-seqlen1K 37.5K, 157B, 16Hr 20.99 28.09 32.54% 7: SLW 10K bsz4K-seqlen1K 42.5K, 157B, 16Hr 20.34 27.22 33.98% 8: SLW 20K bsz4K-seqlen1K 47.5K, 157B, 16Hr 20.25 27.13 34.54% 9: SLW 30K bsz4K-seqlen1K 52.5K, 157B, 16Hr 20.22 27.15 34.16% 10: SLW 40K bsz4K-seqlen1K 57.5K, 157B, 16Hr 20.26 27.11 33.53% 13: Baseline bsz512-seqlen2K 150K, 157B, 32Hr 20.87 28.19 32.99% 15: SLW 70K bsz512-seqlen2K 185K, 157B, 31Hr 19.82 26.04 33.46% 17: SLW 110K bsz512-seqlen2K 205K, 157B, 31Hr 19.64 26.03 34.58% 18: SLW 150K bsz512-seqlen2K 215K, 157B, 32Hr 19.64 25.99 33.32% 15: SLW 190K bsz512-seqlen2K 245K, 157B, 33Hr 19.64 26.09 33.09% Table 7: GPT-3 125M zero-shot evaluation results Baseline Baseline SLW Case Original [6] repro 30x LR 40x LR Model size 125M 125M 125M 125M Train tokens 300B 300B 30B 30B Batch size 256 256 2K 2K Bsz warmup 4B 4B 4B N/A LR 6e-4 6e-4 1.8e-2 2.4e-2 min LR 6e-5 6e-5 0 0 LR warmup 375M 375M 375M 375M LR decay 260B 260B 30B 30B decay style cosine cosine cosine cosine SLW N/A N/A N/A 11.5K steps Avg. accuracy 33.6 31.4 29.8 31.1 (0) HellaSwag 33.7 30.4 28.2 28.9 (1) LAMBADA 42.7 39.3 30.4 34.2 (2) TriviaQA 4.15 1.72 0.76 1.45 (3) WebQs 1.77 0.197 0 0.394 (4) Winogrande 52.0 49.3 50.9 51.9 (5) PIQA 64.6 61.9 59.8 62.7 (6) ARC Challenge 26.6 23.3 21.7 22.3 (7) ARC Easy 43.6 39.9 36.0 39.1 (8) ANLI R1 33.4 32.8 33.1 33.4 (9) ANLI R2 33.2 33.3 33.3 33.6 (10) ANLI R3 33.6 33.3 33.2 34.7 A.6 GPT-3 1.3B evaluation results In this section we evaluate the proposed SLW method on the larger GPT-3 1.3B model. Compared to the GPT-3 125M evaluation in main paper section 5.2 there are two differences on the setup: (1) The GPT-3 125M evaluation aims to explore whether the proposed method can retain the accuracy performance while greatly reducing the training tokens, while this GPT-3 1.3B evaluation aims to explore that, under same amount of training tokens, does proposed method provides better training stability and better accuracy performance. (2) To improve the training data quality, for GPT-3 1.3B pre-training we added two additional sub-datasets (CC-Stories [45] and RealNews [54]), together with additional data cleaning on all data following the process in [42]. Similar to previous experiments, we test two set of hyperparameters on both baseline and proposed method: The ﬁrst set follows the original GPT-3 setup: 300B training tokens, seqlen 2K, batch size 512 (baseline case includes batch size warmup that starts with 16 then gradually increase to 512 in ﬁrst 8B tokens), learning rate 2 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (2 ×10−5 min. learning rate). The second set changes the batch size to 4K (8x) and learning rate to 8 ×10−4 (4x). 20Table 8: GPT-3 1.3B zero-shot evaluation results Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 44.4 41.6 41.9 (0) LAMBADA 63.6 63.7 65.0 (1) TriviaQA 19.7 10.1 11.3 (2) WebQs 4.63 3.25 2.36 (3) PIQA 75.1 73.4 73.8 (4) RACE-h 40.9 35.6 37.1 (5) BoolQ 62.4 63.4 61.8 The baseline case only enables stable training on the ﬁrst set of hyperparameters. Under larger batch size and learning rate, a training divergence (similar to main paper Figure 5 blue line) happened and the training cannot continue. On the other hand, the proposed SLW method is able to provide stable training under 8x larger batch size and 4x larger learning rate. Under the same number of training tokens, the 8x larger batch size leads to better training efﬁciency and 2x training time speedup, similar to what we obserbe in GPT-2 pre-training (main paper Table 2 case 10 vs. 15). This demonstrate the stability-efﬁciency beneﬁt of the proposed method. In addition, Table 8 and 9 present the zero-shot and few-shot evaluations of the trained GPT-3 1.3B models on 6 tasks used by the original GPT-3 work [6]: LAMBADA [28], TriviaQA [15], WebQs [3], PIQA [4], RACE-h [20], BoolQ [47]. Results show that similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup (from 41.6 to 44.8) and proposed SLW method (from 41.9 to 45.3).13 The change on each task also follows the same pattern: TriviaQA and WebQs accuracy improve a lot under few-shot; PIQA, RACE-h, and BoolQ have similar accuracy under zero and few-shot; LAMBADA accuracy becomes worse under few-shot. More importantly, under the same 300B training tokens the proposed SLW method provides better average accuracy (zero-shot from 41.6 to 41.9, few-shot from 44.8 to 45.3) than the baseline, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) is able to provide better accuracy performance. 13Similar to main paper section 5.2, our reproduced GPT-3 baseline has 2.9/3.3 point lower average zero/few- shot accuracy than the original GPT-3, which is because of the different training data and OpenAI employed special data processing techniques [6] 21Table 9: GPT-3 1.3B few-shot evaluation results. k denotes the number of shots following the original GPT-3 work [6]. Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 48.1 44.8 45.3 (0) LAMBADA (k=15) 57.0 58.8 59.7 (1) TriviaQA (k=64) 32.1 19.2 19.0 (2) WebQs (k=64) 19.6 18.4 19.4 (3) PIQA (k=50) 74.3 74.2 72.8 (4) RACE-h (k=10) 41.4 35.0 37.6 (5) BoolQ (k=32) 64.1 63.2 63.2 22",
      "meta_data": {
        "arxiv_id": "2108.06084v4",
        "authors": [
          "Conglong Li",
          "Minjia Zhang",
          "Yuxiong He"
        ],
        "published_date": "2021-08-13T06:32:53Z",
        "pdf_url": "https://arxiv.org/pdf/2108.06084v4.pdf",
        "github_url": "https://github.com/NVIDIA/Megatron-LM"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the stability-efficiency dilemma in pre-training large-scale autoregressive language models like GPT, where increasing batch sizes and learning rates improves efficiency but causes instability. The main contributions include an in-depth analysis correlating training instability with extreme gradient variance values, particularly linked to long sequence lengths at the beginning of training. Based on this, the authors propose Sequence Length Warmup (SLW), a method that starts training with short sequences and gradually increases length, effectively resolving the stability-efficiency dilemma. SLW is shown to enable stable training with significantly larger batch sizes (8x) and learning rates (4x for GPT-2, 40x for GPT-3), leading to substantial reductions in required training tokens (up to 2.2x) and wall-clock time (up to 3.7x for GPT-2; 10x data, 17x time for GPT-3 125M while retaining 99% accuracy). A lightweight hyperparameter tuning strategy for SLW is also presented, and the method's implementation is open-sourced in DeepSpeed.",
        "methodology": "The methodology involves an in-depth analysis of GPT-2 pre-training to understand instability, measuring training loss and Adam optimizer's variance norm/max element, and correlating them with a 'loss ratio' metric. The proposed Sequence Length Warmup (SLW) method truncates full-length input sequences to shorter lengths determined by a step-wise linear pacing function: `seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`. `seqlens` is the starting length, `seqlene` is the full length, `T` is the total duration, and `t` is the current step. A low-cost tuning strategy is developed for `seqlens` and `T`, involving validation perplexity monitoring during early training steps. Training used mixed precision (FP16), Adam optimizer, 0.01 weight decay, and gradient clipping at 1.0. Learning rate decay schedules were adjusted to be token-wise for fair comparisons.",
        "experimental_setup": "Experiments replicated GPT-2 models (117M and 1.5B parameters) and GPT-3 models (125M and 1.3B parameters). Hardware consisted of 128 NVIDIA V100 GPUs across 16 nodes, interconnected with NVLink 2.0 and 100 Gigabit InfiniBand. Datasets included Wikipedia, CC-Stories, RealNews, and OpenWebtext for GPT-2, and the Pile public dataset (with additional CC-Stories and RealNews for GPT-3 1.3B) for GPT-3. Baseline training parameters varied, with batch sizes up to 4K and learning rates up to 6e-4 (GPT-2), or 256 and 6e-4 (GPT-3). Aggressive scenarios for GPT-3 125M used 8x batch size (2K) and up to 40x learning rate. Evaluation metrics included training loss, validation perplexity (PPL), and zero-shot accuracy on WikiText-103, LAMBADA, and 11 GPT-3 tasks (e.g., HellaSwag, TriviaQA, PIQA). Few-shot evaluation was performed for GPT-3 1.3B. Efficiency was measured by required training tokens and wall-clock time.",
        "limitations": "The paper identifies that the root cause of the causal relationship between long sequences and training instability is not fully deciphered, with an assumption that shorter sequences are simpler and generate less noisy gradients. The work provides correlation analysis between instability and gradient variance outliers but acknowledges it is insufficient to prove a causal relationship, as other factors could contribute to instability. Due to the high cost of pre-training, most experimental results are based on a single random seed.",
        "future_research_directions": "Future research directions include further deciphering the root cause of the causal relationship between long sequences and training instability. The authors also hope their work inspires more studies into understanding other factors contributing to training instability in large-scale model training, both in theory and practice. Investigating more complex and adaptive gradient/variance/activation clipping techniques is also implied as a potential alternative, although the proposed method offers a simpler solution.",
        "experimental_code": "def gpt_builder(args, pre_process, post_process, vp_stage=None, config=None):\n    print_rank_0('building GPT model ...')\n    if config is None:\n        if args.yaml_cfg is not None:\n            config = core_transformer_config_from_yaml(args, \"language_model\")\n        else:\n            config = core_transformer_config_from_args(args)\n    if args.use_legacy_models:\n        model = megatron.legacy.model.GPTModel(\n            config,\n            num_tokentypes=0,\n            parallel_output=True,\n            pre_process=pre_process,\n            post_process=post_process,\n        )\n    else:  # using core models\n        if args.spec is not None:\n            transformer_layer_spec = import_module(args.spec)\n        else:\n            use_te = args.transformer_impl == \"transformer_engine\"\n\n            if args.num_experts:\n                # Define the decoder block spec\n                transformer_layer_spec = get_gpt_decoder_block_spec(\n                    config,\n                    use_transformer_engine=use_te,\n                    normalization=args.normalization,\n                    qk_l2_norm=args.qk_l2_norm,\n                    vp_stage=vp_stage,\n                )\n            elif args.heterogeneous_layers_config_path is not None:\n                transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)\n            else:\n                # Define the decoder layer spec\n                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)\n        mtp_block_spec = None\n        if args.mtp_num_layers is not None:\n            if (\n                hasattr(transformer_layer_spec, 'layer_specs')\n                and len(transformer_layer_spec.layer_specs) == 0\n            ):\n                # Get the decoder layer spec explicitly if no decoder layer in the last stage,\n                # Only happens with block spec (TransformerBlockSubmodules) when using MoE.\n                transformer_layer_spec_for_mtp = _get_transformer_layer_spec(use_te, config)\n            else:\n                transformer_layer_spec_for_mtp = transformer_layer_spec\n            mtp_block_spec = get_gpt_mtp_block_spec(\n                config,\n                transformer_layer_spec_for_mtp,\n                use_transformer_engine=use_te,\n                vp_stage=vp_stage,\n            )\n\n        model = GPTModel(\n            config=config,\n            transformer_layer_spec=transformer_layer_spec,\n            vocab_size=args.padded_vocab_size,\n            max_sequence_length=args.max_position_embeddings,\n            pre_process=pre_process,\n            post_process=post_process,\n            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n            parallel_output=True,\n            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n            position_embedding_type=args.position_embedding_type,\n            rotary_percent=args.rotary_percent,\n            rotary_base=args.rotary_base,\n            rope_scaling=args.use_rope_scaling,\n            mtp_block_spec=mtp_block_spec,\n            vp_stage=vp_stage,\n        )\n\n    return model\n\ndef _get_transformer_layer_spec(use_te, config):\n    \"\"\"Get transformer layer specification based on configuration.\n\n    Args:\n        use_te (bool): Whether to use Transformer Engine\n        args: Training arguments\n        config: Model configuration\n\n    Returns:\n        transformer_layer_spec: The transformer layer specification\n    \"\"\"\n    args = get_args()\n    if use_te:\n        return get_gpt_layer_with_transformer_engine_spec(\n            args.num_experts,\n            args.moe_grouped_gemm,\n            args.qk_layernorm,\n            args.multi_latent_attention,\n            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,\n            qk_l2_norm=args.qk_l2_norm,\n            use_kitchen=config.use_kitchen,\n        )\n    else:\n        return get_gpt_layer_local_spec(\n            args.num_experts,\n            args.moe_grouped_gemm,\n            args.qk_layernorm,\n            args.multi_latent_attention,\n            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,\n            normalization=args.normalization,\n            use_kitchen=config.use_kitchen,\n        )",
        "experimental_info": "The provided repository content does not contain a direct implementation of the proposed Sequence Length Warmup (SLW) method (`seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`) or the instability analysis/loss ratio metric. However, the repository supports general GPT model training, which is the context for the method.\n\nKey experimental settings related to GPT training that are configurable via command-line arguments (`args`) in the Megatron framework (as reflected in `gpt_builders.py` and `finetune_gpt.py`) include:\n-   **Mixed Precision (FP16)**: Configurable through arguments like `args.fp16_lm_cross_entropy` during model building, allowing for FP16 training.\n-   **Adam Optimizer**: Explicitly used in some training loops (e.g., `examples/run_simple_mcore_train_loop.py`) and implicitly configured for general Megatron `pretrain` function usage (as seen in `examples/academic_paper_scripts/detxoify_lm/finetune_gpt.py`).\n-   **Weight Decay (0.01)**: A common optimizer hyperparameter, typically set via command-line arguments such as `args.weight_decay`.\n-   **Gradient Clipping (1.0)**: A common training stability technique, typically set via command-line arguments such as `args.gradient_clip_val`.\n-   **Learning Rate Decay Schedules**: The method mentions these being adjusted to be 'token-wise', which implies configuration through relevant `args` (e.g., `args.lr_decay_tokens`).\n-   **Sequence Length**: While a `sequence_length` is a configurable parameter (e.g., `args.seq_length`, `args.max_position_embeddings`), the dynamic adjustment logic of SLW is not present in the provided code snippets."
      }
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "Spectral-AdapterA/R re-uses exactly the top-r singular directions of every weight matrix and scales them by independent coefficients.  • Every adapted direction is treated as equally important although, even inside the top sub-space, some directions contribute far more to the downstream task than others.  • A larger rank r partly solves this but linearly increases memory/latency, defeating the PEFT goal.  We need a way to let the adapter automatically re-allocate its very small parameter budget among the r directions without increasing r or adding new matrices.",
        "method": "Weighted-Spectral Adapter (WSA) 1. Keep the original Spectral-Adapter decomposition W≈U diag(S) Vᵀ and the trainable top-r columns. 2. Replace the element-wise scale vector spectral_C∈R^r with a *simplex-constrained* weight vector w∈R^r:  w = r·softmax(ζ) where ζ∈R^r are unconstrained parameters initialised at 0.  The adapted weight is Ŵ = U  diag(S ⊙ w)  Vᵀ.  ∑ w_i = r keeps the same total scale budget as Spectral-AdapterA while letting the model emphasise whichever singular directions matter most. 3. Add a tiny entropy regulariser Ω(ζ)=λ·Σw_i log w_i (λ≈10⁻³) that encourages sparsity so only a subset of directions receives most of the budget. 4. All other Spectral-Adapter mechanics (zero-init additive update or Cayley rotation) stay unchanged.  This modification costs only r extra parameters and one softmax per forward pass.",
        "experimental_setup": "Base method: Spectral-AdapterA with rank r=8 on the GLUE benchmark using DeBERTa-v3-base (same hyper-parameters as the original paper). Proposed: identical setup but replace spectral_C with the simplex-constrained weights and add the entropy term to the loss.  Train for 3 epochs, batch-size = 32, learning-rate = 2e-4.  Evaluate on GLUE dev sets; report average accuracy (primary metric) and per-task scores.  Ablation: rank 8 with and without WSA; rank 4 WSA to show better allocation beats simply lowering r.",
        "primary_metric": "glue_avg_accuracy",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass WeightedSpectralAdapter(nn.Module):\n    def __init__(self, U,S,V, rank=8, entropic_lambda=1e-3):\n        super().__init__()\n        self.register_buffer('U', U)\n        self.register_buffer('S', S)\n        self.register_buffer('V', V)\n        self.rank = rank\n        self.zeta = nn.Parameter(torch.zeros(rank))  # unconstrained\n        self.entropy_lambda = entropic_lambda\n    def weights(self):\n        w = self.rank * F.softmax(self.zeta, dim=0)  # simplex constraint\n        return w\n    def forward(self, x):\n        w = self.weights()\n        S_hat = self.S.clone()\n        S_hat[:self.rank] = S_hat[:self.rank] * w  # re-allocate scale budget\n        W_hat = (self.U * S_hat) @ self.V.T\n        return F.linear(x, W_hat)\n    def extra_loss(self):\n        w = self.weights()\n        return self.entropy_lambda * (w * torch.log(w + 1e-10)).sum()",
        "expected_result": "• GLUE average accuracy of baseline Spectral-AdapterA-r8: ≈ 88.1%.  • WSA-r8 expected: ≈ 89.0 – 89.3 % (+0.9 – 1.2).  • WSA-r4 expected: ≈ 88.3 %, matching or slightly surpassing baseline r=8 while using half the parameters.  Entropy regularisation keeps the effective number of active directions ≈ 3-4, confirmed by weight histograms.",
        "expected_conclusion": "A one-line replacement of the per-direction scale vector with a simplex-constrained, entropy-regularised weight vector lets the adapter *redistribute* its fixed parameter budget towards the most useful spectral directions.  This yields consistent accuracy gains and can offset a 2× reduction in rank, improving parameter-efficiency with negligible compute overhead.  The idea is generic and can be dropped into any Spectral-Adapter or LoRA variant that scales singular values."
      },
      "evaluation": {
        "novelty_reason": "None of the cited works let a PEFT method dynamically redistribute the fixed *singular-value* budget *within* the top-r sub-space while keeping rank and parameter count unchanged.  • Spectral-AdapterA/B scale each of the r directions with an independent, *unconstrained* parameter, effectively giving every direction the same a-priori weight; they do not impose a global budget or encourage sparsity.  • AdaLoRA and related budget-allocation methods operate at the layer or matrix level, not at the level of individual singular directions.  • SVFT, DoRA, LoRA, ReFT etc. all learn free scaling coefficients (or low-rank updates) but none tie them together through a simplex constraint or a sparsity-promoting entropy regulariser.  • The proposed “Weighted-Spectral Adapter” therefore introduces a new idea: a simplex-constrained weight vector (∑w_i = const) plus a tiny entropy term so that a *subset* of the top-r directions can be emphasised, allowing automatic re-allocation without raising rank or adding extra matrices.  This precise combination of (i) fixed-sum re-allocation, (ii) entropy sparsification, and (iii) keeping the spectral factorisation intact is absent from prior literature.",
        "novelty_score": 8,
        "significance_reason": "If the mechanism works as expected it offers a very cheap (≈ r parameters, one soft-max) drop-in improvement for all Spectral-Adapter or LoRA-SVD variants.  That could: 1) give ≈1 pp GLUE gain at the same rank, 2) allow cutting the rank in half with little loss, directly improving parameter-efficiency, and 3) be implemented with two lines of code in existing PEFT libraries.  Academically it refines our understanding of how singular directions contribute unequally to downstream tasks and provides a principled way (simplex + entropy) to exploit that fact.  Societally the impact is modest—better fine-tuning of language models reduces compute/energy and may lower entry barriers, but the gains are incremental and tested only on medium-size models/tasks so far.  Therefore the advance is practically useful and scientifically interesting, yet not transformative.",
        "significance_score": 6
      }
    },
    {
      "hypothesis": {
        "open_problems": "Spectral-AdapterA/B allocate an equal, fixed number of trainable coefficients (rank r) to every layer and every singular direction, ignoring that: 1) some layers matter far more than others for a given downstream task; 2) within an important layer only a few singular directions carry most task-relevant signal.  Increasing r everywhere wastes memory/latency while lowering r hurts accuracy.  We need a mechanism that can (a) globally redistribute a *single* tiny parameter budget across *all* adapted directions of *all* layers, (b) automatically prune unused directions so that the *runtime* rank drops, and (c) do so with negligible overhead and without extra matrices.",
        "method": "Adaptive Rank-Sparse Spectral Adapter (ARSSA)\n1.  For every weight matrix W we keep the fixed SVD factors U,S,V and expose the top-R (R≫r, e.g. 16) singular directions.\n2.  All trainable scale coefficients c_{ℓ,i} (layer ℓ, direction i) are concatenated into one vector ζ∈ℝ^{L·R}.  We map ζ→w by a *global simplex* transform\n     w = B·softmax(ζ) with B = L·r  (total budget equals \"r per layer\" of Spectral-AdapterA).  Thus the entire model competes for a shared pool of B importance weights.\n3.  We add a sparsity-promoting regulariser Ω(ζ)=λ₁·Σw_j log w_j  (entropic) + λ₂·||w||₁.  During training many w_j shrink toward zero.\n4.  At each forward pass we scale the corresponding singular values: Ŵ_ℓ = U_ℓ·diag(S_ℓ ⊙ w_{ℓ,:})·V_ℓᵀ.  This costs one softmax per *model*, not per layer.\n5.  After training we *prune* directions with w_{ℓ,i}<τ.  The remaining ones are fused into standard low-rank matrices; all zeroed directions are dropped, reducing inference rank and FLOPs automatically.\n6.  The procedure is a drop-in replacement for Spectral-AdapterA; code change ≈20 LoC.",
        "experimental_setup": "Model: DeBERTa-v3-base on GLUE.\nBaselines: (a) Spectral-AdapterA rank = 8 per layer, (b) AdaLoRA (layer-level budget).\nProposed: ARSSA with global budget B=8·L (L=12).  Set R=16, λ₁=1e-3, λ₂=5e-4, τ=0.05.\nOptimiser & schedule identical to baseline (3 epochs, lr = 2e-4, bs = 32).\nMetrics:\n  • glue_avg_accuracy (primary)\n  • effective inference rank = Σ_{ℓ} non-pruned directions\n  • GPU memory & latency during evaluation.\nAblations: remove entropy term, remove L1 term, compare different B (4·L, 8·L, 12·L).",
        "primary_metric": "glue_avg_accuracy",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass ARSSALayer(nn.Module):\n    def __init__(self, U,S,V, dir_cap=16, start_idx=0):\n        super().__init__(); self.register_buffer('U',U); self.register_buffer('S',S); self.register_buffer('V',V)\n        self.dir_cap=dir_cap; self.start=start_idx  # slice into global weight vector\n    def forward(self,x,global_w):\n        w = global_w[self.start:self.start+self.dir_cap]  # simplex-constrained outside\n        S_hat = self.S.clone(); S_hat[:self.dir_cap] = S_hat[:self.dir_cap]*w\n        W_hat = (self.U * S_hat) @ self.V.T\n        return F.linear(x,W_hat)\nclass ARSSAModule(nn.Module):\n    def __init__(self, svd_mats):\n        super().__init__(); L=len(svd_mats); R=svd_mats[0][1].numel(); self.B=L*8  # budget 8 per layer\n        self.zeta = nn.Parameter(torch.zeros(L*R))\n        offs=0; self.layers=nn.ModuleList()\n        for U,S,V in svd_mats:\n            self.layers.append(ARSSALayer(U,S,V,R,offs)); offs+=R\n        self.entropy=1e-3; self.l1=5e-4\n    def _simplex_w(self):\n        w = self.B*F.softmax(self.zeta,0); return w\n    def extra_loss(self):\n        w=self._simplex_w(); return self.entropy*(w*torch.log(w+1e-10)).sum()+self.l1*w.abs().sum()\n    def forward(self,x):\n        w=self._simplex_w(); outs=[layer(x,w) for layer in self.layers]; return outs",
        "expected_result": "• GLUE avg accuracy ≥ 89.2 % (≈ +1.1 pp over Spectral-AdapterA-r8).\n• Post-pruning effective rank ≈ 3 – 4 per layer (≤ 40 % of baseline parameters) with <0.2 pp accuracy drop.\n• Inference memory & latency reduced by ≥35 % relative to Spectral-AdapterA-r8.",
        "expected_conclusion": "A *single* simplex-constrained weight vector with sparsity regularisation can jointly allocate and prune singular-value budgets across *all* layers.  This achieves higher accuracy than fixed-rank Spectral-Adapters, while automatically compressing the adapter to a smaller effective rank for inference—closing the stability-efficiency gap without extra matrices or tuning effort.  The technique generalises AdaLoRA’s layer-wise budget to the finer granularity of singular directions and yields state-of-the-art parameter efficiency in Spectral-Space fine-tuning."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT methods cover three main axes: (1) fixed-rank spectral adaptation (Spectral-AdapterA/B) which allots the same rank to every layer and every singular direction; (2) layer-wise adaptive rank allocation (AdaLoRA) that redistributes a global budget across layers but still treats each adapted direction inside a layer uniformly; (3) direction-level sparsity inside one weight matrix (e.g. SVFT variants) but with an independent parameter set per layer.  The proposed ARSSA is the first to unify these ideas by:\n• introducing a single simplex-constrained parameter vector that simultaneously governs *all* singular directions in *all* layers;  \n• allowing that vector to be optimised with entropy + L1 regularisers so that both inter-layer and intra-layer budgets emerge automatically;  \n• collapsing unused directions to zero and therefore shrinking *runtime* rank without extra fusion steps or auxiliary matrices;  \n• keeping the computational overhead of the allocation mechanism constant (one softmax for the whole model) instead of linear in #layers.  \nNo related work provides global, direction-level budget competition with automatic sparsity and post-training rank collapse, hence the idea is genuinely new.",
        "novelty_score": 8,
        "significance_reason": "If the mechanism works as hypothesised, it would (a) deliver higher task accuracy than fixed-rank spectral adapters by shifting capacity to truly salient directions, (b) cut inference FLOPs and memory because pruned directions disappear, and (c) do so with almost zero extra implementation/latency cost.  This directly advances the practical goal of making large-scale language models cheaper to fine-tune *and* cheaper to deploy – an important concern for both academia and industry.  Academically, it refines the granularity of adaptive rank methods from layer level (AdaLoRA) to singular-direction level, potentially opening a new line of theoretical work on global importance allocation in linear algebraic parameterisations.  The impact is therefore solid but still needs empirical validation beyond the planned GLUE study and may be limited to methods that already rely on SVD, giving it high but not maximal significance.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "Even the proposed ARSSA still relies on (i) continuous soft weights that must be *post-thresholded* to obtain the final low–rank structure and (ii) a manually chosen threshold τ.  This makes the realised rank non-differentiable w.r.t. the training objective, complicates hyper-parameter tuning and provides no guarantee that the budget B is exactly met at inference.  A method that learns *discrete* on/off decisions for every singular direction – while keeping a single global rank budget – would: 1) remove the need for threshold heuristics, 2) guarantee the desired memory/FLOP footprint, and 3) enable gradient-based exploration of *combinatorial* rank patterns, something no current PEFT method can do.",
        "method": "Budgeted Hard-Concrete Spectral Pruning (BHSP)\n1.  As in Spectral-Adapter, expose the top-R singular directions of every weight matrix and collect their trainable logits α∈ℝ^{L·R} into one vector.\n2.  Convert each logit into a *stochastic binary gate* g_j∈{0,1} via the Hard-Concrete relaxation:\n       \n      u∼U(0,1),   s=σ((log u−log (1−u)+α_j)/β),   ĝ = clamp(s·(ζ−γ)+γ,0,1)\n      \n      During training we use ĝ∈(0,1); at inference we set g_j=1[s>0].  Typical β=2/3, ζ=1.1, γ=−0.1.\n3.  Impose a *soft global budget* with a differentiable proxy:\n      \n      ℒ_budget = λ·(Σ_j ĝ_j − B)^2\n      \n      which drives the expected number of active directions toward B (e.g. B=L·r with r=8).\n4.  The adapted weight becomes Ŵ_ℓ = U_ℓ·diag(S_ℓ⊙ĝ_{ℓ,:}·α_scale)·V_ℓᵀ where α_scale are optional per-direction magnitudes.\n5.  Training loss = task_loss + ℒ_budget.  No entropy/L1 terms, no thresholds, one global λ.\n6.  At inference g_j are strictly 0/1, so the runtime rank is exactly B and the adapter collapses to standard low-rank matrices.  Implementation adds ~30 LOC and one logistic sampling per direction only during training.",
        "experimental_setup": "Model & Data: DeBERTa-v3-base on GLUE.\nBaselines: (a) Spectral-AdapterA r=8, (b) ARSSA (softmax+entropy) with same budget.\nProposed: BHSP with R=16, B=8·L, β=2/3, λ=0.1.\nTraining: 3 epochs, lr 2e-4, batch 32; identical optimiser/schedule for all systems.\nMetrics:  • glue_avg_accuracy (primary)\n          • achieved rank (Σ g_j) – should equal B exactly\n          • inference latency / memory.\nAblations: vary λ, compare soft vs. hard gates, study convergence of budget term.",
        "primary_metric": "glue_avg_accuracy",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass HardConcreteGates(nn.Module):\n    def __init__(self, n, beta=2/3, gamma=-.1, zeta=1.1):\n        super().__init__(); self.alpha=nn.Parameter(torch.zeros(n)); self.beta=beta; self.gamma=gamma; self.zeta=zeta\n    def _sample_gates(self):\n        u=torch.rand_like(self.alpha); s=torch.sigmoid((torch.log(u)-torch.log1p(-u)+self.alpha)/self.beta)\n        y=s*(self.zeta-self.gamma)+self.gamma; return torch.clamp(y,0,1)\n    def forward(self, training=True):\n        if training: return self._sample_gates()\n        else: return (self.alpha>0).float()\nclass BHSPAdapter(nn.Module):\n    def __init__(self, svd_mats, R=16, r=8, beta=2/3):\n        super().__init__(); L=len(svd_mats); self.B=L*r; self.gates=HardConcreteGates(L*R,beta)\n        self.slices=[slice(i*R,(i+1)*R) for i in range(L)]; self.USV=svd_mats\n    def extra_loss(self):\n        g=self.gates(False if self.training else True); return 0.1*(g.sum()-self.B).pow(2)\n    def forward(self,x):\n        g=self.gates(self.training)\n        outs=[]\n        for (U,S,V), sl in zip(self.USV,self.slices):\n            s=S.clone(); s[:len(g[sl])]*=g[sl]\n            W=(U*s)@V.T; outs.append(F.linear(x,W))\n        return outs",
        "expected_result": "• GLUE avg ≥ 89.2 % (match or exceed ARSSA).\n• Runtime rank equals exactly B with no accuracy drop.\n• >35 % memory/latency reduction over fixed-rank baseline, identical to ARSSA but without threshold tuning.",
        "expected_conclusion": "Replacing continuous softmax weights with Hard-Concrete binary gates plus a simple budget penalty yields *discrete, budget-exact* allocation of singular directions.  This removes heuristic pruning, guarantees the deployment footprint and maintains (or improves) accuracy – a practical step toward truly plug-and-play, auto-compressing spectral fine-tuning."
      },
      "evaluation": {
        "novelty_reason": "Among published PEFT methods the rank (or number of spectral directions) is almost always controlled by (i) fixing it a-prio­ri (LoRA, Spectral-AdapterA/R, ReFT, SVFT, MEFT, etc.) or (ii) using continuous importance scores that are later thresholded or annealed (AdaLoRA, ARSSA).  None of the cited works learns a truly discrete 0/1 decision for every singular direction while enforcing a single global rank budget during training.  The proposed BHSP borrows the Hard-Concrete gate from L0-regularisation literature but applies it *inside the spectral space of adapters* and couples all gates with a differentiable quadratic budget penalty so that the deployed rank is guaranteed to equal B without any post-processing.  This combination – hard-concrete per-direction gating + global soft constraint + spectral adapters – is not found in the related papers, giving the hypothesis clear algorithmic novelty.",
        "novelty_score": 7,
        "significance_reason": "Academically, BHSP tackles a long-standing practical weakness of adaptive PEFT: the realised low-rank structure is non-differentiable and only approximately meets memory/FLOP targets.  By making rank selection part of the gradient path and guaranteeing the budget, the method could open the door to principled study of discrete architectural search inside LLM fine-tuning.  Practically, it removes threshold heuristics and deployment-time surprises, which matters for on-device or multi-tenant serving where strict parameter / latency envelopes are required.  However, the improvement is incremental (mainly engineering convenience and exactness) rather than a step-change in capability or a new theoretical insight, so its broader scientific and societal impact, while useful, is moderate.",
        "significance_score": 6
      }
    },
    {
      "hypothesis": {
        "open_problems": "All existing PEFT methods – including the recently-proposed BHSP – decide once-for-all which spectral directions to keep.  This makes the compute / memory footprint •static across inputs and •identical during training and inference, even though many tokens, sentences or tasks require far less capacity than the worst-case budget B.  We lack a PEFT mechanism that (a) learns discrete on/off decisions at the level of individual singular directions, (b) conditions these decisions on the *current input* so that easy examples run with fewer FLOPs, and (c) still obeys a *guaranteed average* rank budget, hence a predictable cost envelope – something neither LoRA, AdaLoRA, ARSSA nor BHSP can provide.",
        "method": "Conditional Budgeted Spectral Pruning (CBSP) – an input-adaptive extension of BHSP\n1.  For every weight matrix we expose its top-R singular directions as in Spectral-Adapter.  Their logits α_{ℓ,i}(x) are now produced by a *tiny controller* f_θ that sees the layer’s averaged hidden state h̄_ℓ(x):      α_{ℓ,i}(x)= w_{ℓ,i}ᵀ·ReLU(W_ℓ h̄_ℓ)+b_{ℓ,i}.     (≈2·R·L·d extra params; negligible.)\n2.  Each logit is converted to a *stochastic binary gate*  ĝ_{ℓ,i}(x)∈(0,1) with the Hard-Concrete trick (same β,ζ,γ as BHSP).  At inference we set g_{ℓ,i}=1[s>0].\n3.  Enforce an *expected global budget* with a differentiable penalty   ℒ_budget = λ·( E_x Σ_{ℓ,i} ĝ_{ℓ,i}(x) – B )²   computed over the minibatch.  This guarantees the *average* rank equals B, while individual inputs may use less or more.\n4.  The adapted weight is   Ŵ_ℓ(x)=U_ℓ diag(S_ℓ⊙ĝ_{ℓ}(x)·α_scale) V_ℓᵀ.  Forward pass therefore spends FLOPs proportional to Σ ĝ_{ℓ,i}(x).\n5.  Training loss = task_loss + ℒ_budget.  No thresholds, one hyper-parameter λ.\n6.  A straight-through estimator back-propagates through g_{ℓ,i}.  Implementation adds ~50 LOC plus one 2-layer MLP per transformer block.",
        "experimental_setup": "Model/Data: DeBERTa-v3-base on GLUE + the BoolQ subset of SuperGLUE (mix of easy & hard examples).\nBaselines: (a) Fixed-rank LoRA r=8, (b) BHSP (static) with B=8·L.\nProposed: CBSP with R=16, B=8·L *average*, controller hidden size 128, λ=0.1.\nMetrics:\n • glue_avg_accuracy (primary)\n • mean realised rank  E[Σ g] (should ≈B)\n • 90th-percentile rank (should stay ≤B)\n • average inference latency & energy per sample (NVIDIA-SMI power).\nAblations: remove controller (reduces to BHSP), vary λ, visualise rank vs. input length.",
        "primary_metric": "Average per-sample energy (J) at equal or better GLUE accuracy.",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass HCGate(nn.Module):\n    def __init__(self,beta=2/3,gamma=-.1,zeta=1.1):\n        super().__init__(); self.beta=beta; self.gamma=gamma; self.zeta=zeta\n    def forward(self,logits,train=True):\n        if train:\n            u=torch.rand_like(logits); s=torch.sigmoid((torch.log(u)-torch.log1p(-u)+logits)/self.beta)\n            y=s*(self.zeta-self.gamma)+self.gamma; return torch.clamp(y,0,1)\n        else:\n            return (logits>0).float()\nclass CBSPController(nn.Module):\n    def __init__(self,hidden,R):\n        super().__init__(); self.fc=nn.Sequential(nn.Linear(hidden,hidden),nn.ReLU(),nn.Linear(hidden,R))\n    def forward(self,h):\n        return self.fc(h)\nclass CBSPAdapter(nn.Module):\n    def __init__(self,svd_mats,hidden=768,R=16,r_avg=8):\n        super().__init__(); self.L=len(svd_mats); self.R=R; self.B=self.L*r_avg\n        self.ctrl=nn.ModuleList([CBSPController(hidden,R) for _ in range(self.L)])\n        self.gate=HCGate(); self.svd=svd_mats\n    def forward(self,hs,train=True):\n        ranks=[]; outs=[]\n        for (U,S,V),h,c in zip(self.svd,hs,self.ctrl):\n            logits=c(h.mean(0))  # (R,)\n            g=self.gate(logits,train)\n            ranks.append(g.sum())\n            S_adj=S.clone(); S_adj[:self.R]*=g\n            W=(U*S_adj)@V.T\n            outs.append(F.linear(h,W))\n        return outs, torch.stack(ranks).sum()\n    def budget_loss(self,ranks):\n        return 0.1*(ranks.mean()-self.B)**2",
        "expected_result": "• GLUE avg ≥ 89.2 % (parity with BHSP).\n• Mean rank ≈ B, but median rank ≈ 0.6 B → 25-35 % energy saving.\n• Latency reduces proportionally on short inputs; no degradation on long ones.",
        "expected_conclusion": "Learning *input-conditioned* discrete spectral gates under a global budget turns PEFT into an energy-adaptive mechanism: easy examples run faster and greener while tough cases still get full capacity.  CBSP thus reconciles accuracy with sustainability and sets a new direction for conditional compression in language-model fine-tuning."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT methods fall into two families: (1) static low-rank updates whose rank is decided once during training (LoRA, Spectral-Adapter, SVFT, BHSP, AdaLoRA after its rank-annealing phase) and therefore incur identical FLOPs for every input; (2) conditional / routing schemes that switch among whole adapters or experts (MHR, CA-MTL, hyper-tuning, Mixture-of-Experts) but do not vary the *intrinsic rank* of a single adapter and usually have no hard cost guarantee.  The proposed CBSP is the first to learn *binary gates on individual singular directions* that are (a) conditioned on the current hidden state, (b) optimised with a hard-concrete trick so they collapse to true 0/1 at inference, and (c) regulated by a differentiable global budget that guarantees the *expected* rank equals a user-specified B.  This yields per-example adaptive compute inside one spectral adapter – a design that is absent from all related works.  Combining stochastic direction pruning with a spectral parameterisation is also orthogonal to previously explored adaptive-rank (AdaLoRA) or adaptive-expert (MHR) techniques.",
        "novelty_score": 8,
        "significance_reason": "Academically, CBSP extends the PEFT design space from static or task-conditioned adapters to fine-grained, input-conditioned compute, linking ideas from neural sparsification (hard-concrete gating) with spectral fine-tuning.  It provides the first theoretical cost envelope (expected rank = B) for conditional PEFT, a property desirable for deployment yet missing from MoE-style conditional compute.  Practically, PEFT is widely used on edge devices where energy and latency matter; allowing 25-35 % average energy savings at equal accuracy would have immediate sustainability impact and lower inference costs.  The method is simple to implement (≈50 LOC), compatible with existing SVD-based adapters, and introduces only one new hyper-parameter, which increases the chance of real-world adoption.  Its significance is limited, however, to scenarios where spectral adapters are already effective; it does not address other fine-tuning challenges such as catastrophic forgetting or cross-modality generalisation, so the overall impact, while solid, is not breakthrough-level.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "Even the input-adaptive BHSP / CBSP keeps *one set of spectral gates per layer* that is shared by every token of the sequence.  This ignores the large intra-sequence variability that characterises language: (a) only a handful of “hard” tokens (rare words, entity spans, long-range dependencies) need the full adapter capacity while the majority can be served with rank 0–2; (b) sentences differ in length, so a fixed *average* budget B either over-allocates on short inputs or under-allocates on long ones, breaking the promised latency envelope.  We lack a PEFT mechanism that •decides the rank at *token granularity*, •keeps the *sequence-level* compute strictly below a user-specified FLOP cap regardless of length, and •does so with negligible controller overhead.",
        "method": "Token–Aware Budgeted Spectral Pruning (TABS)\n1.  For each transformer layer ℓ we expose its top-R singular directions as in Spectral-Adapter.  A *token controller* g_θ maps every hidden state h_{ℓ,t}∈ℝ^d to R Bernoulli gates:      s_{ℓ,t}=W²_ℓ ReLU(W¹_ℓ h_{ℓ,t})  →  ĝ_{ℓ,t}=HardConcrete(s_{ℓ,t}).  The two 1×R MLPs add 2·R·d params per layer (≪1 %).\n2.  Token-wise adapter output is   ŷ_{ℓ,t}=U_ℓ diag(S_ℓ ⊙ ĝ_{ℓ,t}·α) V_ℓᵀ h_{ℓ,t}.  Multiplications with deactivated directions are skipped via sparse matmul.\n3.  Enforce a *hard sequence budget* B_max by projecting the expected token ranks onto the simplex:  after each forward pass we compute r_seq=Σ_{t,i} ĝ_{ℓ,t,i}.  If r_seq>B_max we down-scale the top-k logits s_{ℓ,t,i} with largest gradients until the constraint is met (differentiable “soft top-k” projection).\n4.  Training objective = task_loss + λ·Σ_ℓ⟨max(0,r_seq−B_max)⟩².  No extra hyper-parameters besides λ and B_max.\n5.  At inference HardConcrete collapses to {0,1}; projection guarantees FLOPs ≤ B_max·d per layer independent of sentence length.",
        "experimental_setup": "Model/Data: DeBERTa-v3-base on (i) GLUE, (ii) Long Range Arena (for length stress-test).\nBaselines: LoRA-r8, BHSP (rank budget 8·L), CBSP (avg budget 8·L).\nProposed: TABS with R=16, B_max=8·T where T is input length (tokens), λ=0.05.\nMetrics: • glue_avg, • LRA accuracy, • mean & 99-th percentile latency, • realised FLOPs vs. length, • per-token rank histogram.\nAblations: remove projection (violates budget), share gates across tokens (reverts to CBSP).",
        "primary_metric": "99-th-percentile latency at equal or better GLUE accuracy.",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass HC(nn.Module):\n    def __init__(self,beta=2/3,gamma=-.1,zeta=1.1):\n        super().__init__(); self.beta=beta; self.gamma=gamma; self.zeta=zeta\n    def forward(self,logits,train=True):\n        if train:\n            u=torch.rand_like(logits); s=torch.sigmoid((torch.log(u)-torch.log1p(-u)+logits)/self.beta)\n            y=s*(self.zeta-self.gamma)+self.gamma; return torch.clamp(y,0,1)\n        else:\n            return (logits>0).float()\nclass TokenController(nn.Module):\n    def __init__(self,d,R):\n        super().__init__(); self.mlp=nn.Sequential(nn.Linear(d,d//4),nn.ReLU(),nn.Linear(d//4,R))\n        self.gate=HC()\n    def forward(self,h,train=True):\n        return self.gate(self.mlp(h),train)\nclass TABSLayer(nn.Module):\n    def __init__(self,U,S,V,R,B_max,d):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.ctrl=TokenController(d,R); self.B=B_max\n    def forward(self,h,train=True):\n        g=self.ctrl(h,train)                          # (T,R)\n        #   simplex projection per sequence to meet B_max\n        if train and g.sum()>self.B:\n            excess=g.sum()-self.B; flat=g.view(-1); topk=torch.topk(flat,flat.numel())[1]\n            flat[topk[:int(excess)]]*=0  # soft drop\n            g=flat.view_as(g)\n        S_adj=self.S[:self.R]; out=[]\n        for tok_g,ht in zip(g,h):\n            W=(self.U[:,:self.R]* (S_adj*tok_g)).matmul(self.V[:self.R])\n            out.append(ht @ W.T)\n        return torch.stack(out)",
        "expected_result": "TABS matches CBSP on GLUE (≈89.2 %) but cuts 99-th latency by ≥40 % and average FLOPs by 55 %. On 4k-token LRA tasks it stays within budget whereas CBSP exceeds it by 2-3×.",
        "expected_conclusion": "Fine-grained token-aware spectral gating under a strict sequence budget reconciles worst-case latency guarantees with parameter-efficient accuracy, pushing PEFT towards *real-time adaptive* language models suitable for latency-critical applications like dialogue or on-device NLP."
      },
      "evaluation": {
        "novelty_reason": "The related papers that work in the spectral/low–rank PEFT design space (Spectral-Adapter, SVFT, AdaLoRA, SHiRA, ReFT, etc.) either (i) choose a single global rank per layer, (ii) at most adapt this rank across layers (AdaLoRA / CBSP), or (iii) gate the whole layer uniformly for the full token sequence (BHSP / CBSP). None of them (a) decide the active spectral directions on a token-by-token basis, (b) couple this decision to a hard, sentence-level FLOP budget that is enforced by a differentiable projection, or (c) show how to do both with near-zero controller cost via HardConcrete gates and sparse matmuls. Hence the proposal to expose top-R singular vectors and let a light MLP + HardConcrete gate select a variable per-token rank while globally projecting the expected token-ranks onto a simplex constitutes a new control-granularity (token-level) and a new optimisation constraint (strict worst-case compute cap) that are absent from current spectral or budgeted PEFT methods.",
        "novelty_score": 8,
        "significance_reason": "If successful, TABS would let practitioners keep strict latency / energy envelopes independent of input length while still reaping the accuracy benefits of high-rank adapters on only the “hard” tokens. This directly tackles a pressing deployment barrier for on-device or real-time NLP that existing PEFT methods cannot handle (they violate worst-case latency when the sentence is long or token complexity spikes). Academically, it opens a fresh axis (token-wise adaptive capacity under global resource constraints) that could influence future work in dynamic inference and fine-tuning. Societally, reducing tail latency and FLOPs on edge devices lowers energy consumption and enables broader access. The downside is added implementation complexity and reliance on sparse kernels which may limit immediate adoption, but the potential impact on both efficiency research and practical deployment is substantial.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length × layers × R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.",
        "method": "Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).",
        "experimental_setup": "Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6× TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.",
        "primary_metric": "Mean Joules per sample at equal or better Rouge-L.",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,λ,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=λ*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc",
        "expected_result": "HEST matches TABS on Rouge-L (≈44.3) while cutting mean energy by ≥40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8× on the longest sequences.",
        "expected_conclusion": "A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget—rather than FLOPs alone—yields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP."
      },
      "evaluation": {
        "novelty_reason": "Among existing PEFT papers, several explore (1) spectral–space adapters (Spectral Adapter, SVFT), (2) adaptive rank/budget allocation (AdaLoRA, TABS), (3) Int8 inference (occasionally for deployment), and (4) token- or layer-wise gating (TABS, MHR).  None of them jointly\n• treat *energy* (FLOPs × precision cost) as the primary, hard-constrained budget;\n• allow each spectral direction to be executed in {skip, INT8, FP16} under a single global Lagrangian;\n• couple token-specific and layer-specific gating through a rank-1 factorisation that shrinks controller parameters from O(T·L·R) to O(T+L);\n• keep worst-case energy strictly bounded while re-using native INT8 kernels.\nTABS addresses per-example FLOPs but not precision, has per-layer independent gates and linear controller cost. AdaLoRA reallocates rank but neither adapts precision nor gives hard energy guarantees. No listed work combines hierarchical (depth × spectral) allocation with sub-linear controller cost and energy-aware precision selection.  Therefore the hypothesis introduces a genuinely new combination of ideas and a new optimisation objective (energy envelope).",
        "novelty_score": 8,
        "significance_reason": "If HEST fulfils its claims—40 % mean energy and 35 % P99 latency reduction on 4 k-token inputs while preserving accuracy, with a provable sequence-level energy cap—it directly tackles the growing carbon and cost footprint of long-context LLM fine-tuning, something only peripherally discussed in prior PEFT work.  Academically it offers: (i) a practical method to integrate discrete precision choices into differentiable budgeted optimisation; (ii) a controller architecture whose parameter cost is independent of sequence-length scaling, relevant for 8–32 k token contexts; (iii) evidence that FLOPs are an insufficient proxy and that energy-aware adaptation yields better real-world efficiency.  Societally, enabling edge / low-power deployment of adapted LLMs without accuracy loss is valuable for privacy-preserving and green AI scenarios.  Hence the potential impact on both research directions (budgeted adaptive tuning, sustainable ML) and practice (mobile inference) is high.",
        "significance_score": 8
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "Token-wise spectral gating (TABS) solves the per-example FLOP cap but still presents three unresolved issues: 1) every layer decides gates independently, so easy tokens may still be processed in deep layers although they were already solved by earlier ones; 2) the per-token controllers scale linearly with sequence length × layers × R and therefore dominate the adapter parameter count on long inputs; 3) the budget is expressed only in FLOPs, disregarding the large energy gap between low-precision and fp16 kernels. We lack a PEFT mechanism that (a) allocates capacity jointly across *layer depth* and *spectral directions*, (b) keeps the controller cost sub-linear in sequence length, (c) trades off *rank* and *precision* under one universal, user-specified *energy* envelope.",
      "method": "Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).",
      "experimental_setup": "Model/Data: Llama-2-7B-chat with sequence length up to 4k on (i) CNN/DailyMail summarisation, (ii) Long Range Arena text benchmarks.\nBaselines: LoRA-r8, BHSP (token-agnostic), TABS (token-wise, fp16 only).\nProposed: HEST with k=32, R=16, E_max set to 0.6× TABS average energy.\nMetrics: Rouge-L / accuracy, realised Joules/sample (NVML), 99-th latency, controller-param count, layer-depth utilisation heat-map.\nAblations: no precision choice (fp16 only), no dual variable (soft penalty), unfactorised controller.",
      "primary_metric": "Mean Joules per sample at equal or better Rouge-L.",
      "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass FactorisedController(nn.Module):\n    def __init__(self,d,L,k):\n        super().__init__(); self.W_tok=nn.Linear(d,k,bias=False); self.W_layer=nn.Parameter(torch.zeros(L,k))\n    def forward(self,h,layer_id):          # h:(T,d)\n        s_tok=torch.sigmoid(self.W_tok(h))  # (T,k)\n        s_layer=torch.sigmoid(self.W_layer[layer_id])  # (k,)\n        return (s_tok*s_layer).mean(-1)     # (T,)\nclass HESTLayer(nn.Module):\n    def __init__(self,U,S,V,energy_fp16,energy_int8,R,k):\n        super().__init__(); self.U,self.S,self.V=U,S,V; self.R=R; self.e16=energy_fp16; self.e8=energy_int8\n        self.ctrl=FactorisedController(U.size(0),1,k)  # layer dim inserted later\n    def forward(self,h,λ,E_acc):\n        p=self.ctrl(h,0).unsqueeze(-1).repeat(1,self.R)             # (T,R)\n        logits=torch.log(p+1e-6).unsqueeze(-1).repeat(1,1,3)        # off,int8,fp16\n        g=F.gumbel_softmax(logits,tau=2/3,hard=False)               # (T,R,3)\n        E_batch=(g[:,:,1]*self.e8 + g[:,:,2]*self.e16).sum()\n        E_acc+=E_batch; penalty=λ*E_batch\n        state=g.argmax(-1)                                          # hard for matmul\n        S_adj=self.S[:self.R]\n        outs=[]\n        for st,ht in zip(state,h):\n            mask_fp16=(st==2).float(); mask_int8=(st==1).float()\n            W=(self.U[:,:self.R]*S_adj).matmul(self.V[:self.R])\n            if mask_int8.any():\n                ht_int8=ht.half(); out_i8=(ht_int8@W.T).half(); out_i8*=mask_int8\n            else: out_i8=0\n            out_fp16=(ht@W.T)*mask_fp16\n            outs.append(out_i8+out_fp16)\n        return torch.stack(outs), penalty,E_acc",
      "expected_result": "HEST matches TABS on Rouge-L (≈44.3) while cutting mean energy by ≥40 %, 99-th latency by 35 %, and reducing controller parameters by 90 %. On 4 k-token inputs it never exceeds E_max whereas TABS overshoots by 1.8× on the longest sequences.",
      "expected_conclusion": "A hierarchical, factorised controller plus joint rank/precision gating turns spectral adapters into an energy-adaptive mechanism that scales to long sequences and edge devices. HEST demonstrates that enforcing a single global *energy* budget—rather than FLOPs alone—yields substantial real-world savings without accuracy loss, pushing PEFT towards sustainable, green NLP."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Hierarchical Energy-constrained Spectral Tuning (HEST)\n1. Factorised controller  g(h_{ℓ,t}) = σ( W_tok h_{ℓ,t})·σ( w_ℓ )  produces a rank-1 outer product of token-score and layer-score which is broadcast to the R directions.  Two tiny matrices W_tok∈ℝ^{d×k} and W_ℓ∈ℝ^{L×k} with k≪d remove the O(T·L·R) cost (now O((T+L)·k)).\n2. Each direction i can be chosen in {off, int8, fp16}.  We encode this with a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² whose expected energy cost is E_{state}[E_dir(i,state)].\n3. One global Lagrangian λ enforces  Σ_{ℓ,t,i}E_dir(g_{ℓ,t,i}) ≤  E_max  via the differentiable loss  ℒ = task_loss + λ·(Ē−E_max).  λ is updated every batch with a single dual ascent step (no projection).\n4. At convergence the gates are hardened; int8 choices are fused into INT8 MatMuls, others stay fp16.  The worst-case energy per sequence is now strictly bounded by E_max and wall-time improves because many late-layer MatMuls run in INT8 or are skipped.\n5. All additions fit in <200 lines of PyTorch and reuse native Int8 GEMM kernels (torch.cuda.matmul.allow_tf32/int8).",
        "experimental_design": {
          "experiment_summary": "We will fine-tune a pretrained Llama-2-7B-chat model on the CNN/DailyMail summarisation task using the proposed Hierarchical Energy-constrained Spectral Tuning (HEST) adapter. During training we jointly optimise task loss and an energy-budget Lagrangian so that, for every input sequence, the total expected energy of all spectral directions stays below a user-specified envelope. The controller is factorised across tokens and layers (rank-1 outer product) and produces a 3-way Gumbel-Softmax gate per spectral direction that selects one of {off, INT8, FP16}. After convergence we harden the gates and fuse INT8 choices into integer GEMM kernels; de-activated directions are skipped. We compare HEST against the strongest prior energy-oblivious spectral adapter, Token-wise Adaptive Basis Selection (TABS), under the same backbone, data split, and optimisation schedule. We report summarisation quality, energy consumed per sample (read with NVML), tail latency, and parameter-efficiency. A small grid search over learning-rate, controller width k, and the energy budget E_max is performed on the validation set. Experiments run on a single NVIDIA A100 80 GB GPU with plenty of system RAM.",
          "evaluation_metrics": [
            "Mean Joules per sample at equal or better Rouge-L",
            "ROUGE-L",
            "Accuracy",
            "99th percentile latency",
            "Controller parameter count",
            "Mean Joules per sample at equal or better Rouge-L."
          ],
          "proposed_method": "Hierarchical Energy-constrained Spectral Tuning (HEST) adds fewer than 200 lines of PyTorch to a standard low-rank spectral adapter.\n1. Factorised controller: g(h_{ℓ,t}) = σ(W_tok h_{ℓ,t}) · σ(w_ℓ) with W_tok∈ℝ^{d×k} and w_ℓ∈ℝ^{k}. This reduces controller complexity from O(T·L·R) to O((T+L)·k).\n2. Precision/state gating: For every retained spectral direction i we learn a 3-way Gumbel-Softmax gate g_{ℓ,t,i}∈Δ² that selects {off, INT8, FP16}. Its expected energy is E_dir(i,state) and is differentiable via the Gumbel trick (τ=2⁄3).\n3. Single global energy budget: A dual variable λ is updated each batch (λ←[λ+α(Ē−E_max)]₊) so the loss becomes 𝓛 = task_loss + λ·(Ē−E_max). This enforces Σ_{ℓ,t,i}E_dir ≤ E_max per sequence.\n4. Inference hardening: At the end of training we sample argmax over gates, eliminate “off” directions, fuse INT8 directions into torch.int8 GEMMs, and leave the remainder in FP16. The bound E_max now holds exactly in worst-case.\n5. Implementation: relies only on standard PyTorch, torch.autocast for INT8, and NVML for energy readings; compatible with any transformer block.",
          "comparative_methods": [
            "Token-wise Adaptive Basis Selection (TABS)"
          ],
          "models_to_use": [
            "Llama-2-7B-chat (7B)"
          ],
          "datasets_to_use": [
            "CNN/DailyMail summarisation"
          ],
          "hyperparameters_to_search": {
            "learning_rate": "1e-5-5e-4",
            "controller_width_k": "16,32,64",
            "energy_budget_E_max_ratio": "0.4-0.8",
            "gumbel_temperature_tau": "0.4-1.0",
            "dual_step_size_alpha": "0.01-0.2"
          },
          "external_resources": {
            "hugging_face": {
              "models": [
                {
                  "id": "Qwen/Qwen2.5-7B-Instruct",
                  "author": "Qwen",
                  "sha": "a09a35458c702b33eeacc393d103063234e8bc28",
                  "created_at": "2024-09-16T11:55:40+00:00",
                  "last_modified": "2025-01-12T02:10:10+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 8625380,
                  "likes": 858,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "LICENSE"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "generation_config.json"
                    },
                    {
                      "rfilename": "merges.txt"
                    },
                    {
                      "rfilename": "model-00001-of-00004.safetensors"
                    },
                    {
                      "rfilename": "model-00002-of-00004.safetensors"
                    },
                    {
                      "rfilename": "model-00003-of-00004.safetensors"
                    },
                    {
                      "rfilename": "model-00004-of-00004.safetensors"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [
                      "en"
                    ],
                    "library_name": "transformers",
                    "pipeline_tag": "text-generation",
                    "tags": [
                      "chat"
                    ],
                    "datasets": [],
                    "base_model": "Qwen/Qwen2.5-7B",
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "transformers",
                    "safetensors",
                    "qwen2",
                    "text-generation",
                    "chat",
                    "conversational",
                    "en",
                    "arxiv:2309.00071",
                    "arxiv:2407.10671",
                    "base_model:Qwen/Qwen2.5-7B",
                    "base_model:finetune:Qwen/Qwen2.5-7B",
                    "license:apache-2.0",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us"
                  ],
                  "pipeline_tag": "text-generation",
                  "library_name": "transformers",
                  "readme": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-7B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
                  "extracted_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
                }
              ],
              "datasets": []
            }
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation",
            "method_name": "proposed",
            "model_name": "Llama-2-7B-chat (7B)",
            "dataset_name": "CNN/DailyMail summarisation"
          },
          {
            "run_id": "comparative-1-iter1-Llama-2-7B-chat-7B-CNN-DailyMail-summarisation",
            "method_name": "comparative-1",
            "model_name": "Llama-2-7B-chat (7B)",
            "dataset_name": "CNN/DailyMail summarisation"
          }
        ]
      }
    ]
  }
}